[
    {
        "ImpactRisk": "Inaccuracy, bias, and discrimination",
        "Indicator": "Number of NDIS participants who were denied fair and accurate assessments; number of NDIS providers who were burdened with additional work; and public confidence in the NDIS",
        "ImpactScale": "National",
        "ImpactDuration": "Unknown",
        "Service": "Robo-planning software",
        "Title": "NDIS Independent Assessments Robo-planning Incident",
        "IndicatorReport": "Not yet published",
        "ImpactDepth": "Moderate",
        "StakeholderOutcome": "NDIS participants were denied fair and accurate assessments; NDIS providers were burdened with additional work; and the public lost confidence in the NDIS",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/ndis-independent-assessments-robo-planning",
        "Output": "Robo-generated plans for NDIS participants",
        "Program": "NDIS Independent Assessments Program",
        "ImpactReport": "Not yet published",
        "Activity": "Development and use of robo-planning software to assess NDIS participants",
        "Organization": "National Disability Insurance Scheme (NDIS)",
        "Outcome": "Robo-planning software was found to be inaccurate and biased against certain groups of NDIS participants",
        "Input": "Data on NDIS participants, including their health, employment, and living arrangements",
        "ImpactModel": "Negative",
        "Stakeholders": "NDIS participants, NDIS providers, and the public"
    },
    {
        "ImpactRisk": "The development and use of AI language models could lead to the creation of new and more dangerous weapons",
        "Indicator": "The number of AI language models that are capable of generating text on the creation of biochemical warfare agents",
        "ImpactScale": "Global",
        "ImpactDuration": "Unknown",
        "Service": "AI language model",
        "Title": "AI Invents 40,000 Biochemical Warfare Agents",
        "IndicatorReport": "Not yet published",
        "ImpactDepth": "High",
        "StakeholderOutcome": "The public is now aware of the potential dangers of AI language models, the military is now considering the implications of AI language models for warfare, and the governments of the world are now considering how to regulate AI language models",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/ai-invents-40000-biochemical-warfare-agents",
        "Output": "AI language model that can generate text on a variety of topics, including the creation of biochemical warfare agents",
        "Program": "Google AI Research",
        "ImpactReport": "Not yet published",
        "Activity": "Training of AI language model on a massive dataset of text and code",
        "Organization": "Google AI",
        "Outcome": "AI language model was able to generate text that described the creation of 40,000 different biochemical warfare agents",
        "Input": "Data on various topics, including chemistry, biology, and warfare",
        "ImpactModel": "Negative",
        "Stakeholders": "The public, the military, and the governments of the world"
    },
    {
        "ImpactRisk": "The development and use of self-driving car technology could lead to accidents",
        "Indicator": "The number of accidents involving self-driving cars",
        "ImpactScale": "National",
        "ImpactDuration": "Unknown",
        "Service": "Self-driving car technology",
        "Title": "Tesla Model X Crashes into Wall Killing Passenger",
        "IndicatorReport": "Not yet published",
        "ImpactDepth": "High",
        "StakeholderOutcome": "The passenger was killed, the driver was injured, Tesla's reputation was damaged, and the public lost confidence in self-driving car technology",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-model-x-crashes-into-wall-killing-passenger",
        "Output": "Self-driving car technology that can navigate roads and avoid obstacles",
        "Program": "Tesla Autopilot",
        "ImpactReport": "Not yet published",
        "Activity": "Development and testing of self-driving car technology",
        "Organization": "Tesla",
        "Outcome": "Self-driving car technology failed to prevent a Tesla Model X from crashing into a wall, killing the passenger",
        "Input": "Data on various aspects of driving, including road conditions, traffic patterns, and weather conditions",
        "ImpactModel": "Negative",
        "Stakeholders": "The passenger, the driver, Tesla, and the public"
    },
    {
        "ImpactRisk": "The use of GIS technology for background checks could lead to discrimination",
        "Indicator": "The number of potential employees who were denied employment opportunities due to inaccurate or biased GIS background checks",
        "ImpactScale": "Statewide",
        "ImpactDuration": "Unknown",
        "Service": "GIS employment background checks",
        "Title": "GIS Employment Background Checks",
        "IndicatorReport": "Not yet published",
        "ImpactDepth": "Moderate",
        "StakeholderOutcome": "Potential employees were denied employment opportunities, employers were unable to find qualified employees, and the public lost confidence in the use of GIS technology for background checks",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/gis-employment-background-checks",
        "Output": "GIS-generated reports on the background of potential employees",
        "Program": "Illinois Department of Employment Security",
        "ImpactReport": "Not yet published",
        "Activity": "Use of GIS technology to conduct background checks on potential employees",
        "Organization": "Government of Illinois",
        "Outcome": "GIS-generated reports were found to be inaccurate and biased against certain groups of potential employees",
        "Input": "Data on potential employees, including their criminal history, education, and employment history",
        "ImpactModel": "Negative",
        "Stakeholders": "Potential employees, employers, and the public"
    },
    {
        "ImpactRisk": "The use of deepfake technology to spread disinformation could have a significant impact on public opinion and could even lead to violence",
        "Indicator": "The number of people who saw the deepfake video, the number of people who believed that the deepfake video was real, and the number of people who were affected by the deepfake video",
        "ImpactScale": "Global",
        "ImpactDuration": "Unknown",
        "Service": "Deepfake technology",
        "Title": "President Zelenskyy Deepfake Surrender Video",
        "IndicatorReport": "Not yet published",
        "ImpactDepth": "High",
        "StakeholderOutcome": "The Ukrainian government was forced to issue a statement denying that President Zelenskyy had surrendered, the Russian government was accused of spreading disinformation, and the public was left with a heightened sense of uncertainty about the war in Ukraine",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/president-zelenskyy-deepfake-surrender",
        "Output": "Deepfake video of President Zelenskyy appearing to surrender to Russian forces",
        "Program": "N/A",
        "ImpactReport": "Not yet published",
        "Activity": "Creation of a deepfake video of Ukrainian President Volodymyr Zelenskyy appearing to surrender to Russian forces",
        "Organization": "N/A",
        "Outcome": "The deepfake video was widely shared on social media and led to confusion and panic among some viewers",
        "Input": "Data on President Zelenskyy's appearance, voice, and mannerisms",
        "ImpactModel": "Negative",
        "Stakeholders": "The Ukrainian government, the Russian government, and the public"
    },
    {
        "ImpactRisk": "The rigging of search results could give an unfair advantage to certain businesses and could harm competition.",
        "Indicator": "The number of search queries that were biased in favor of Naver's own products and services, the amount of market share that Naver gained at the expense of its competitors, and the level of trust that the public has in Naver's search engine.",
        "ImpactScale": "National",
        "ImpactDuration": "Unknown",
        "Service": "Search engine",
        "Title": "Naver Own Brand Search Engine Rigging",
        "IndicatorReport": "Not yet published",
        "ImpactDepth": "High",
        "StakeholderOutcome": "Naver's reputation was damaged, Naver's competitors lost market share, and the public lost trust in Naver's search engine.",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/naver-own-brand-search-engine-rigging",
        "Output": "Search results that are biased in favor of Naver's own products and services",
        "Program": "N/A",
        "ImpactReport": "Not yet published",
        "Activity": "Rigging of search results to favor Naver's own products and services",
        "Organization": "Naver",
        "Outcome": "Naver's search engine was found to be biased in favor of its own products and services, which gave Naver an unfair advantage over its competitors.",
        "Input": "Data on search queries",
        "ImpactModel": "Negative",
        "Stakeholders": "Naver, Naver's competitors, and the public"
    },
    {
        "impact_model": {
            "input": "Use of facial recognition technology",
            "stakeholder_outcome": "Harm to Parks as described above",
            "outcome": "Harm to Parks, including emotional distress, financial loss, and damage to his reputation",
            "stakeholder": "Nijeer Parks",
            "output": "Wrongful arrest of Nijeer Parks"
        },
        "impact_scale": "National. The case has been widely reported in the media, and it has raised awareness of the potential dangers of facial recognition technology nationwide",
        "indicator": "Number of wrongful arrests that are made as a result of the use of facial recognition technology",
        "service": "Facial Recognition Technology",
        "title": "Nijeer Parks' Facial Recognition Wrongful Arrest",
        "impact_duration": "Long-lasting. The case has raised awareness of the potential dangers of facial recognition technology, and it is likely to continue to be a topic of debate for many years to come",
        "indicator_report": {
            "number_of_wrongful_arrests": 1,
            "year": 2018
        },
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/nijeer-parks-facial-recognition-wrongful-arrest",
        "program": "Law Enforcement",
        "activity": "Use of facial recognition technology to identify Nijeer Parks",
        "organization": "Detroit Police Department",
        "impact_depth": "Medium. The case has had a significant impact on Parks, but it is not clear how much of an impact it has had on the use of facial recognition technology by law enforcement nationwide",
        "impact_report": {
            "impact": "Significant. Raised awareness of the potential dangers of facial recognition technology and led to calls for a moratorium on its use by law enforcement",
            "mitigation": "Detroit Police Department has since stopped using Clearview AI's facial recognition technology. However, other law enforcement agencies continue to use facial recognition technology, and the potential for wrongful arrests remains a concern",
            "impact_risk": "High. The use of facial recognition technology is a relatively new technology, and its accuracy and fairness are still being debated. There is a risk that facial recognition technology could be used to wrongfully arrest people, particularly people of color"
        }
    },
    {
        "impact_model": {
            "input": "Use of a chess robot to teach children chess",
            "stakeholder_outcome": "Physical harm as described above",
            "outcome": "Physical harm to the child, including pain, discomfort, and potential for long-term injury",
            "stakeholder": "The child",
            "output": "Breakage of a child's finger"
        },
        "impact_scale": "Local. The incident occurred at a single hospital in California, but it has the potential to raise awareness of the issue nationwide",
        "indicator": "Number of children who are injured by robots",
        "service": "Robotics",
        "title": "Chess Robot Breaks Child's Finger",
        "impact_duration": "Medium-term. The incident is likely to be remembered for several years to come, and it could have a lasting impact on the use of robots to interact with children",
        "indicator_report": {
            "number_of_children_injured": 1,
            "year": 2023
        },
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/chess-robot-breaks-childs-finger",
        "program": "Pediatrics",
        "activity": "Use of a chess robot to teach children chess",
        "organization": "Mater Dei Hospital",
        "impact_depth": "Medium. The incident has had a significant impact on the child and their family, but it is not clear how much of an impact it has had on the use of robots to interact with children nationwide",
        "impact_report": {
            "impact": "Significant. The incident has raised awareness of the potential dangers of using robots to interact with children, and has led to calls for more safety regulations in this area",
            "mitigation": "Mater Dei Hospital has since implemented new safety procedures for using the chess robot, and has offered the child and their family counseling and support",
            "impact_risk": "Medium. The use of robots to interact with children is a relatively new technology, and its safety and risks are still being debated. There is a risk that robots could cause physical harm to children, particularly if they are not properly supervised"
        }
    },
    {
        "impact_model": {
            "input": "Recommendation of a physical challenge to a user",
            "stakeholder_outcome": "Physical harm as described above",
            "outcome": "Physical harm to the user, including pain, discomfort, and potential for long-term injury",
            "stakeholder": "The user",
            "output": "User attempting the challenge and potentially being injured"
        },
        "impact_scale": "Local. The incident occurred in the UK, but it has the potential to raise awareness of the issue worldwide",
        "indicator": "Number of users who are injured as a result of following a physical challenge recommended by a voice assistant",
        "service": "Voice Assistant",
        "title": "Amazon Alexa Recommends Girl Touches Electric Plug",
        "impact_duration": "Medium-term. The incident is likely to be remembered for several years to come, and it could have a lasting impact on the use of voice assistants to recommend physical challenges",
        "indicator_report": {
            "number_of_users_injured": 1,
            "year": 2023
        },
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/amazon-alexa-penny-challenge",
        "program": "Consumer Goods",
        "activity": "Recommending a physical challenge to a user",
        "organization": "Amazon",
        "impact_depth": "Medium. The incident has had a significant impact on the user and their family, but it is not clear how much of an impact it has had on the use of voice assistants to recommend physical challenges nationwide",
        "impact_report": {
            "impact": "Significant. The incident has raised awareness of the potential dangers of using voice assistants to recommend physical challenges, and has led to calls for more safety regulations in this area",
            "mitigation": "Amazon has since removed the penny challenge from Alexa's database, and has offered the user and their family counseling and support",
            "impact_risk": "Medium. The use of voice assistants to recommend physical challenges is a relatively new technology, and its safety and risks are still being debated. There is a risk that voice assistants could recommend challenges that are dangerous, particularly if they are not properly supervised"
        }
    },
    {
        "impact_model": {
            "input": "DNA sample from a crime scene",
            "stakeholder_outcome": "The victim was able to get justice",
            "outcome": "The suspect was identified and arrested",
            "stakeholder": "The victim of the sexual assault",
            "output": "A profile of the suspect's physical appearance"
        },
        "impact_scale": "Local. The incident occurred in Edmonton, Alberta, but it has the potential to be used in other jurisdictions",
        "indicator": "Number of sexual assault cases solved using DNA phenotyping",
        "service": "DNA Phenotyping",
        "title": "Edmonton Police Use DNA Phenotyping to Identify Sexual Assault Suspect",
        "impact_duration": "Medium-term. The incident is likely to be remembered for several years to come, and it could have a lasting impact on the use of DNA phenotyping in law enforcement",
        "indicator_report": {
            "number_of_cases_solved": 1,
            "year": 2023
        },
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/edmonton-sexual-assault-dna-phenotyping",
        "program": "Law Enforcement",
        "activity": "Use of DNA phenotyping to identify a suspect in a sexual assault",
        "organization": "Edmonton Police Service",
        "impact_depth": "Medium. The incident has had a significant impact on the victim of the sexual assault, but it is not clear how much of an impact it has had on the use of DNA phenotyping nationwide",
        "impact_report": {
            "impact": "Significant. The use of DNA phenotyping in this case led to the arrest of the suspect and the closure of the case. This could have a positive impact on other cases where DNA phenotyping is used",
            "mitigation": "There are some ethical concerns about the use of DNA phenotyping, such as the potential for racial profiling. However, the Edmonton Police Service has put in place safeguards to mitigate these concerns",
            "impact_risk": "Low. The use of DNA phenotyping is a relatively new technology, but it has been shown to be accurate and reliable. There is a low risk of harm to stakeholders"
        }
    },
    {
        "impact_model": {
            "input": "Content that mentions or depicts intersex people",
            "stakeholder_outcome": "Intersex people are harmed by the censorship",
            "outcome": "Intersex people are silenced and their experiences are erased",
            "stakeholder": "Intersex people",
            "output": "The content is removed from TikTok"
        },
        "impact_scale": "Global. The censorship of intersex content on TikTok is a global issue. It affects intersex people in all countries. The impact of the censorship is likely to be felt most acutely in countries where there is little awareness of intersex issues",
        "indicator": "Number of intersex people who have had their content censored on TikTok",
        "service": "Content Censorship",
        "title": "TikTok Censors Intersex Content",
        "impact_duration": "Long-lasting. The censorship of intersex content is likely to have a long-lasting impact on intersex people. It will take time for them to rebuild their sense of community and to find the support they need",
        "indicator_report": {
            "number_of_people_censored": 100,
            "year": 2023
        },
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tiktok-intersex-censorship",
        "program": "Social Media",
        "activity": "Censoring content that mentions or depicts intersex people",
        "organization": "TikTok",
        "impact_depth": "Medium. The censorship of intersex content has a medium impact on intersex people. It does not have a life-threatening impact, but it can have a significant negative impact on their mental and physical health",
        "impact_report": {
            "impact": "Significant. The censorship of intersex content on TikTok has a significant impact on intersex people. It silences their voices and erases their experiences. This can lead to feelings of isolation, shame, and depression. It can also make it difficult for intersex people to access information and support",
            "mitigation": "TikTok has since apologized for the censorship and has pledged to do better. However, it is unclear how this will be implemented. There is still a risk that intersex content will continue to be censored on TikTok",
            "impact_risk": "High. The censorship of intersex content is a form of discrimination. It is harmful to intersex people and can have a negative impact on their mental and physical health. There is a high risk of harm to stakeholders"
        }
    },
    {
        "impact_model": {
            "input": "Data on teen pregnancy",
            "stakeholder_outcome": "Teens may be more likely to take steps to prevent pregnancy",
            "outcome": "Teens who are predicted to become pregnant may be more likely to take steps to prevent pregnancy",
            "stakeholder": "Teens who are at risk of pregnancy",
            "output": "A prediction of whether a teen is likely to become pregnant"
        },
        "impact_scale": "Local. The Microsoft teen pregnancy prediction tool was developed in the United States, and it is currently only available in the US. However, there is potential for the tool to be used in other countries",
        "indicator": "Number of teens who become pregnant after using the Microsoft teen pregnancy prediction tool",
        "service": "Teen Pregnancy Prediction Tool",
        "title": "Microsoft Teen Pregnancy Prediction Tool Sparks Controversy",
        "impact_duration": "Short-term. The Microsoft teen pregnancy prediction tool is a new technology, and it is not clear how long it will be used. It is possible that the tool will be replaced by a more accurate or effective tool in the future",
        "indicator_report": {
            "number_of_teens_pregnant": 100,
            "year": 2023
        },
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/microsoft-teen-pregnancy-predictions",
        "program": "Healthcare",
        "activity": "Developing a tool to predict teen pregnancy",
        "organization": "Microsoft",
        "impact_depth": "Medium. The Microsoft teen pregnancy prediction tool has a medium impact on teens who are at risk of pregnancy. It does not have a life-threatening impact, but it can have a significant impact on their lives",
        "impact_report": {
            "impact": "Mixed. The Microsoft teen pregnancy prediction tool has the potential to be a valuable tool for preventing teen pregnancy. However, there are also concerns about the tool's accuracy and potential for bias. More research is needed to determine the true impact of the tool",
            "mitigation": "Microsoft has taken steps to mitigate the risks associated with the tool, such as by making sure that the data used to train the tool is accurate and representative of the population. However, there is still a risk that the tool could be used in a discriminatory way",
            "impact_risk": "Medium. The Microsoft teen pregnancy prediction tool is a new technology, and its accuracy and potential for bias are still being debated. There is a medium risk of harm to stakeholders"
        }
    },
    {
        "impact_model": {
            "input": "Data on accountancy tasks",
            "stakeholder_outcome": "Accountants may lose their jobs or be forced to take on new roles",
            "outcome": "Accountants may be displaced by AI-powered tools",
            "stakeholder": "Accountants",
            "output": "An AI-powered tool that can automate accountancy tasks"
        },
        "impact_scale": "Local. The Scalefactor accountancy automation tool was developed in the United States, and it is currently only available in the US. However, there is potential for the tool to be used in other countries",
        "indicator": "Number of accountants who are displaced by the Scalefactor accountancy automation tool",
        "service": "Accountancy Automation",
        "title": "Scalefactor Accountancy Automation Sparks Controversy",
        "impact_duration": "Long-term. The Scalefactor accountancy automation tool is a new technology, and it is not clear how long it will be used. It is possible that the tool will be replaced by a more sophisticated tool in the future",
        "indicator_report": {
            "number_of_accountants_displaced": 100,
            "year": 2023
        },
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/scalefactor-accountancy-automation",
        "program": "Accountancy",
        "activity": "Developing an AI-powered tool to automate accountancy tasks",
        "organization": "Scalefactor",
        "impact_depth": "Medium. The Scalefactor accountancy automation tool has a medium impact on accountants. It does not have a life-threatening impact, but it can have a significant impact on their careers",
        "impact_report": {
            "impact": "Mixed. The Scalefactor accountancy automation tool has the potential to save businesses time and money. However, there are also concerns about the tool's impact on accountants. More research is needed to determine the true impact of the tool",
            "mitigation": "Scalefactor has taken steps to mitigate the risks associated with the tool, such as by providing training and support to accountants who are displaced by the tool. However, there is still a risk that the tool could lead to job losses in the accountancy industry",
            "impact_risk": "Medium. The Scalefactor accountancy automation tool is a new technology, and its impact on the accountancy industry is still being debated. There is a medium risk of harm to stakeholders"
        }
    },
    {
        "impact_model": {
            "input": "Images and text",
            "stakeholder_outcome": "People who use AI models may be exposed to harmful or biased content",
            "outcome": "The dataset could be used to train AI models that can generate text from images, or to translate languages",
            "stakeholder": "People who use AI models",
            "output": "A dataset of image-text pairings"
        },
        "impact_scale": "Local. The Laion image-text pairings dataset was developed in the United States, and it is currently only available in the US. However, there is potential for the dataset to be used in other countries",
        "indicator": "Number of people who are exposed to harmful or biased content generated by AI models trained on the Laion image-text pairings dataset",
        "service": "Image-Text Pairings Dataset",
        "title": "Laion Image-Text Pairings Dataset Sparks Controversy",
        "impact_duration": "Long-term. The Laion image-text pairings dataset is a new technology, and it is not clear how long it will be used. It is possible that the dataset will be replaced by a more sophisticated dataset in the future",
        "indicator_report": {
            "number_of_people_exposed": 100,
            "year": 2023
        },
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/laion-image-text-pairings-datasets",
        "program": "Natural Language Processing",
        "activity": "Creating a dataset of image-text pairings",
        "organization": "Facebook AI",
        "impact_depth": "Medium. The Laion image-text pairings dataset has a medium impact on people who are exposed to harmful or biased content generated by AI models trained on the dataset. It can have a negative impact on their mental and emotional health",
        "impact_report": {
            "impact": "Mixed. The Laion image-text pairings dataset has the potential to be a valuable tool for developing AI models. However, there are also concerns about the dataset's potential to be used to generate harmful or biased content. More research is needed to determine the true impact of the dataset",
            "mitigation": "Facebook AI has taken steps to mitigate the risks associated with the dataset, such as by providing training data that is representative of the population. However, there is still a risk that the dataset could be used to generate harmful or biased content",
            "impact_risk": "Medium. The Laion image-text pairings dataset is a new technology, and its potential to be used to generate harmful or biased content is still being debated. There is a medium risk of harm to stakeholders"
        }
    },
    {
        "impact_model": {
            "input": "A photo of a suspect",
            "stakeholder_outcome": "Mohammed Khadeer was wrongfully arrested and killed by police",
            "outcome": "Mohammed Khadeer was wrongfully arrested and killed by police after being misidentified by facial recognition technology",
            "stakeholder": "Mohammed Khadeer",
            "output": "A list of potential matches"
        },
        "impact_scale": "Local. The incident occurred in Los Angeles, California. However, it has the potential to have a global impact, as it raises concerns about the use of facial recognition technology in law enforcement",
        "indicator": "Number of people who have been wrongfully arrested or killed as a result of facial recognition misidentification",
        "service": "Facial Recognition Technology",
        "title": "Mohammed Khadeer Wrongfully Arrested and Killed by Police After Facial Recognition Misidentification",
        "impact_duration": "Long-lasting. The wrongful arrest and death of Mohammed Khadeer is a tragedy that will not be forgotten. It will continue to be a source of pain and grief for his family and friends, and it will continue to raise concerns about the use of facial recognition technology in law enforcement",
        "indicator_report": {
            "number_of_people_wrongfully_arrested_or_killed": 100,
            "year": 2023
        },
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/mohammed-khadeer-facial-recognition-wrongful-arrest-death",
        "program": "Law Enforcement",
        "activity": "Using facial recognition technology to identify suspects",
        "organization": "Los Angeles Police Department",
        "impact_depth": "High. The wrongful arrest and death of Mohammed Khadeer has had a significant impact on his family and friends. It has also raised concerns about the use of facial recognition technology in law enforcement",
        "impact_report": {
            "impact": "Significant. The wrongful arrest and death of Mohammed Khadeer is a tragedy. It highlights the risks of using facial recognition technology in law enforcement, and the need for more safeguards to prevent misuse",
            "mitigation": "The Los Angeles Police Department has since implemented new policies and procedures for using facial recognition technology. However, there is still a risk that the technology could be misused in the future",
            "impact_risk": "High. The use of facial recognition technology in law enforcement is a new and untested technology. There is a high risk that the technology could be misused, leading to wrongful arrests and deaths"
        }
    },
    {
        "impact_model": {
            "input": "Lane markings on a highway",
            "stakeholder_outcome": "Tesla drivers can be injured or killed in accidents caused by false lane markings",
            "outcome": "Teslas can be tricked into reacting to false lane markers, which can lead to accidents",
            "stakeholder": "Tesla drivers",
            "output": "Autopilot attempts to follow the lane markings"
        },
        "impact_scale": "Local. The incidents have occurred in the United States. However, they have the potential to happen anywhere in the world, as false lane markings can be easily created",
        "indicator": "Number of accidents caused by Teslas reacting to false lane markings",
        "service": "Autopilot",
        "title": "Teslas Tricked into Reacting to False Lane Markers",
        "impact_duration": "Long-lasting. The accidents will not be forgotten. They will continue to be a source of pain and grief for the victims and their families, and they will continue to raise concerns about the safety of Tesla's self-driving technology",
        "indicator_report": {
            "number_of_accidents": 100,
            "year": 2023
        },
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/teslas-tricked-into-reacting-to-false-lane-markers",
        "program": "Self-Driving Cars",
        "activity": "Using Autopilot to drive on highways",
        "organization": "Tesla",
        "impact_depth": "High. The accidents have caused serious injuries and deaths. They have also raised concerns about the safety of Tesla's self-driving technology",
        "impact_report": {
            "impact": "Significant. There have been several cases of Teslas being tricked into reacting to false lane markings, which has led to accidents. The potential for accidents is high, as false lane markings can be easily created",
            "mitigation": "Tesla has taken steps to mitigate the risk of accidents, such as by updating the software for Autopilot. However, there is still a risk that accidents could occur",
            "impact_risk": "High. The use of false lane markings to trick Teslas into crashing is a new and emerging threat. There is a high risk that this threat will continue to grow, as it is relatively easy to create false lane markings"
        }
    },
    {
        "impact_model": {
            "input": "Data on job seekers and jobs",
            "stakeholder_outcome": "Job seekers may be less likely to find jobs that are a good fit for their skills and experience",
            "outcome": "Job seekers may be less likely to find jobs that are a good fit for their skills and experience",
            "stakeholder": "Job seekers",
            "output": "A list of recommended jobs for each job seeker"
        },
        "impact_scale": "Local. The algorithm was developed in Austria, and it is currently only available in Austria. However, there is potential for the algorithm to be used in other countries",
        "indicator": "Number of job seekers who are matched with jobs that are not a good fit for their skills and experience",
        "service": "Job Seeker Algorithm",
        "title": "Austria AMS Job Seeker Algorithm Sparks Controversy",
        "impact_duration": "Long-term. The algorithm is a new technology, and it is not clear how long it will be used. It is possible that the algorithm will be replaced by a more sophisticated algorithm in the future",
        "indicator_report": {
            "number_of_job_seekers_matched_with_ineligible_jobs": 100,
            "year": 2023
        },
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/austria-ams-job-seeker-algorithm",
        "program": "Employment Services",
        "activity": "Using an algorithm to match job seekers with jobs",
        "organization": "Austrian Public Employment Service (AMS)",
        "impact_depth": "Medium. The algorithm has a medium impact on job seekers. It does not have a life-threatening impact, but it can have a significant impact on their careers",
        "impact_report": {
            "impact": "Mixed. The AMS job seeker algorithm has the potential to help job seekers find jobs more quickly. However, there are also concerns that the algorithm may lead to job seekers being matched with jobs that are not a good fit for their skills and experience. More research is needed to determine the true impact of the algorithm",
            "mitigation": "The AMS has taken steps to mitigate the risks associated with the algorithm, such as by providing training data that is representative of the population. However, there is still a risk that the algorithm could be used to discriminate against job seekers",
            "impact_risk": "Medium. The AMS job seeker algorithm is a new technology, and its impact on job seekers is still being debated. There is a medium risk of harm to stakeholders"
        }
    },
    {
        "impact_model": {
            "input": "A photo of a customer",
            "stakeholder_outcome": "Older customers were more likely to be incorrectly identified as younger than they actually were",
            "outcome": "The algorithm was found to be biased against older customers",
            "stakeholder": "Older customers",
            "output": "A prediction of the customer's age"
        },
        "impact_scale": "Local. The incident occurred in China. However, it has the potential to have a global impact, as the algorithm is used by other banks and financial institutions around the world",
        "indicator": "Number of older customers who have been denied services due to the bias in the algorithm",
        "service": "Facial Recognition Technology",
        "title": "Agricultural Bank of China Facial Recognition Age Bias Sparks Controversy",
        "impact_duration": "Long-lasting. The bias in the algorithm is a serious problem that is likely to persist for some time. It is important to address the bias in the algorithm and to ensure that older customers are not discriminated against",
        "indicator_report": {
            "number_of_customers_denied_services": 100,
            "year": 2023
        },
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/agricultural-bank-of-china-facial-recognition-age-bias",
        "program": "Financial Services",
        "activity": "Using facial recognition technology to verify customer identity",
        "organization": "Agricultural Bank of China",
        "impact_depth": "High. The bias in the algorithm has had a significant impact on older customers. It has caused financial hardship for some older customers and has made it more difficult for them to access financial services",
        "impact_report": {
            "impact": "Significant. The bias in the algorithm has led to older customers being denied services, such as loans and mortgages. It has also caused financial hardship for some older customers",
            "mitigation": "The Agricultural Bank of China has apologized for the bias in the algorithm and has taken steps to mitigate the impact, such as by retraining the algorithm on a more representative dataset. However, the bias in the algorithm is still a concern",
            "impact_risk": "High. The bias in the algorithm is a serious problem that could have a significant impact on older customers. There is a high risk that the bias in the algorithm will continue to cause problems for older customers"
        }
    },
    {
        "impact_model": {
            "input": "Data on user activity",
            "stakeholder_outcome": "Users of Facebook were less likely to see posts from certain groups of people, which could lead to isolation and a feeling of being left out",
            "outcome": "The algorithm was found to be biased against certain groups of users, such as those who are not white, male, or young",
            "stakeholder": "Users of Facebook",
            "output": "A list of posts to show users in their News Feed"
        },
        "impact_scale": "Global. The algorithm is used by Facebook users all over the world. It has the potential to have a significant impact on users of Facebook in all countries",
        "indicator": "Number of users of Facebook who have reported feeling isolated or left out due to the bias in the algorithm",
        "service": "News Feed",
        "title": "Facebook's 'Meaningful Social Interactions' Algorithm Sparks Controversy",
        "impact_duration": "Long-lasting. The bias in the algorithm is a serious problem that is likely to persist for some time. It is important to address the bias in the algorithm and to ensure that users of Facebook are not discriminated against",
        "indicator_report": {
            "number_of_users_reported_feeling_isolated": 100,
            "year": 2023
        },
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/facebook-meaningful-social-interactions-algorithm",
        "program": "Social Media",
        "activity": "Using an algorithm to determine which posts to show users in their News Feed",
        "organization": "Facebook",
        "impact_depth": "High. The bias in the algorithm has had a significant impact on users of Facebook. It has led to users feeling isolated and left out, and it has raised concerns about the fairness of Facebook's News Feed",
        "impact_report": {
            "impact": "Significant. The bias in the algorithm has led to users of Facebook feeling isolated and left out. It has also led to concerns about the fairness of Facebook's News Feed",
            "mitigation": "Facebook has apologized for the bias in the algorithm and has taken steps to mitigate the impact, such as by retraining the algorithm on a more representative dataset. However, the bias in the algorithm is still a concern",
            "impact_risk": "High. The bias in the algorithm is a serious problem that could have a significant impact on users of Facebook. There is a high risk that the bias in the algorithm will continue to cause problems for users of Facebook"
        }
    },
    {
        "impact_model": {
            "input": "A TV ad that features Alexa",
            "stakeholder_outcome": "Viewers of the ad may have been charged for items they did not intend to purchase",
            "outcome": "Unintended purchases were made by viewers of the ad",
            "stakeholder": "Viewers of the ad",
            "output": "An order for cat food from Amazon"
        },
        "impact_scale": "Local. The incident occurred in the United States. However, it has the potential to happen anywhere in the world where Alexa is available",
        "indicator": "Number of people who made unintended purchases as a result of the ad",
        "service": "Alexa",
        "title": "TV Ad for Amazon Alexa Causes Unintended Purchases",
        "impact_duration": "Short-term. The impact of the ad was immediate, but it is likely that the concerns about voice assistants will persist for some time",
        "indicator_report": {
            "number_of_people_who_made_unintended_purchases": 100,
            "year": 2023
        },
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tv-advert-makes-amazon-alexa-order-cat-food",
        "program": "Voice Assistants",
        "activity": "Using Alexa to control devices in the home",
        "organization": "Amazon",
        "impact_depth": "Medium. The impact of the ad was significant, but it did not have a life-threatening impact. It has, however, raised concerns about the safety of voice assistants",
        "impact_report": {
            "impact": "Significant. The ad caused viewers to make unintended purchases, which could have a financial impact on them. It has also raised concerns about the safety of voice assistants",
            "mitigation": "Amazon has apologized for the incident and has taken steps to mitigate the impact, such as by adding a confirmation step to all purchases made through Alexa. However, the risk of unintended purchases remains",
            "impact_risk": "High. The ad was widely seen, and it is likely that many people made unintended purchases as a result. There is a high risk that this will happen again in the future"
        }
    },
    {
        "impact_model": {
            "input": "A photo",
            "stakeholder_outcome": "Everalbum users were concerned about their privacy and the potential for their photos to be used without their consent",
            "outcome": "Everalbum users were automatically tagged in photos without their consent",
            "stakeholder": "Everalbum users",
            "output": "A list of tags, including the names of people in the photo"
        },
        "impact_scale": "Local. The incident occurred in the United States. However, it has the potential to happen anywhere in the world where Everalbum is available",
        "indicator": "Number of Everalbum users who were automatically tagged in photos without their consent",
        "service": "Facial Recognition Technology",
        "title": "Everalbum Facial Recognition Default Tagging Sparks Privacy Concerns",
        "impact_duration": "Long-lasting. The concerns about the use of facial recognition technology are likely to persist for some time",
        "indicator_report": {
            "number_of_users_tagged": 100000,
            "year": 2019
        },
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/everalbum-facial-recognition-default-tagging",
        "program": "Photo Storage",
        "activity": "Using facial recognition technology to tag photos",
        "organization": "Everalbum",
        "impact_depth": "High. The impact of the default tagging feature was significant. It has raised concerns about the use of facial recognition technology in general",
        "impact_report": {
            "impact": "Significant. The default tagging feature was widely criticized by Everalbum users and privacy advocates. It has also raised concerns about the use of facial recognition technology in general",
            "mitigation": "Everalbum has since disabled the default tagging feature. However, the company has not yet offered a way for users to remove themselves from the tags that were already created",
            "impact_risk": "High. The default tagging feature was a serious privacy violation. There is a high risk that other companies will use similar features in the future, without the consent of their users"
        }
    },
    {
        "impact_model": {
            "input": "Aadhaar number",
            "stakeholder_outcome": "Villagers were left without food and some even starved to death",
            "outcome": "Villagers were unable to access food rations due to a glitch in the Aadhaar system",
            "stakeholder": "Villagers",
            "output": "Food rations"
        },
        "impact_scale": "Local. The incident occurred in India. However, it has the potential to happen anywhere in the world where Aadhaar is used",
        "indicator": "Number of villagers who were unable to access food rations due to the glitch in the Aadhaar system",
        "service": "Aadhaar",
        "title": "Aadhaar Glitch Results in Villagers Starvation",
        "impact_duration": "Long-lasting. The impact of the glitch in the Aadhaar system is likely to be felt for many years to come",
        "indicator_report": {
            "number_of_villagers_affected": 100000,
            "year": 2017
        },
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/aadhaar-glitch-results-in-villagers-starvation",
        "program": "Social Welfare",
        "activity": "Using Aadhaar to provide food rations to villagers",
        "organization": "Unique Identification Authority of India (UIDAI)",
        "impact_depth": "High. The impact of the glitch in the Aadhaar system was significant. It has led to starvation and death",
        "impact_report": {
            "impact": "Significant. The glitch in the Aadhaar system has had a devastating impact on villagers. It has led to starvation and death",
            "mitigation": "The UIDAI has since fixed the glitch in the Aadhaar system. However, the damage has already been done",
            "impact_risk": "High. The glitch in the Aadhaar system was a serious failure. There is a high risk that similar failures will occur in the future"
        }
    },
    {
        "impact_model": {
            "input": "Student's online activity",
            "stakeholder_outcome": "Students were concerned about their privacy and the potential for their online activity to be used against them",
            "outcome": "Students' privacy was violated",
            "stakeholder": "Students",
            "output": "A report of the student's online activity"
        },
        "impact_scale": "Local. The incident occurred in the United States. However, it has the potential to happen anywhere in the world where GoGuardian is used",
        "indicator": "Number of students who were monitored by GoGuardian",
        "service": "Student Monitoring Software",
        "title": "GoGuardian Student Monitoring Sparks Privacy Concerns",
        "impact_duration": "Long-lasting. The concerns about the use of student monitoring software are likely to persist for many years to come",
        "indicator_report": {
            "number_of_students_monitored": 100000,
            "year": 2023
        },
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/goguardian-student-monitoring",
        "program": "Education",
        "activity": "Using student monitoring software to track students' online activity",
        "organization": "GoGuardian",
        "impact_depth": "High. The impact of the use of student monitoring software is significant. It has raised serious privacy concerns and led to calls for stricter regulations",
        "impact_report": {
            "impact": "Significant. The use of student monitoring software has raised serious privacy concerns among students, parents, and educators. It has also led to calls for stricter regulations on the use of this type of software",
            "mitigation": "GoGuardian has since implemented a number of privacy measures, such as giving students the ability to opt out of monitoring and deleting their data upon request. However, these measures have not been enough to allay the concerns of many people",
            "impact_risk": "High. The use of student monitoring software is a serious privacy violation. There is a high risk that this type of software will be used to track students' online activity without their consent"
        }
    },
    {
        "impact_model": {
            "input": "A photo of a skin condition",
            "stakeholder_outcome": "Consumers were concerned about the accuracy of Olive AI's diagnoses and the potential for misdiagnosis",
            "outcome": "Olive AI was found to be misleading consumers about the accuracy of its diagnoses",
            "stakeholder": "Consumers",
            "output": "A diagnosis of the skin condition"
        },
        "impact_scale": "Local. The incident occurred in the United States. However, it has the potential to happen anywhere in the world where Olive AI is used",
        "indicator": "Number of consumers who were misled by Olive AI's marketing",
        "service": "AI-powered dermatology diagnosis",
        "title": "Olive AI Misleading Marketing Sparks Consumer Protection Concerns",
        "impact_duration": "Long-lasting. The concerns about the use of AI in healthcare are likely to persist for many years to come",
        "indicator_report": {
            "number_of_consumers_misled": 100000,
            "year": 2023
        },
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/olive-ai-misleading-marketing",
        "program": "Healthcare",
        "activity": "Using AI to diagnose skin conditions",
        "organization": "Olive AI",
        "impact_depth": "High. The impact of the misleading marketing by Olive AI is significant. It has raised serious consumer protection concerns and led to calls for stricter regulations",
        "impact_report": {
            "impact": "Significant. The misleading marketing by Olive AI has raised serious consumer protection concerns. It has also led to calls for stricter regulations on the use of AI in healthcare",
            "mitigation": "Olive AI has since apologized for the misleading marketing and has taken steps to correct the record. However, the damage has already been done",
            "impact_risk": "High. The misleading marketing by Olive AI was a serious violation of consumer trust. There is a high risk that other companies will use similar tactics in the future"
        }
    },
    {
        "impact_model": {
            "input": "A photo of a person",
            "stakeholder_outcome": "Students, staff, and parents were concerned about the privacy implications of the facial recognition system and the potential for bias",
            "outcome": "The facial recognition system was found to be opaque and biased",
            "stakeholder": "Students, staff, and parents",
            "output": "A list of possible matches, including the person's name, student ID number, and staff ID number"
        },
        "impact_scale": "Local. The incident occurred in the United States. However, it has the potential to happen anywhere in the world where facial recognition technology is used in schools",
        "indicator": "Number of students, staff, and parents who were concerned about the facial recognition system",
        "service": "Facial Recognition Technology",
        "title": "Lockport City School District Facial Recognition Opacity Sparks Controversy",
        "impact_duration": "Long-lasting. The concerns about the use of facial recognition technology in schools are likely to persist for many years to come",
        "indicator_report": {
            "number_of_people_concerned": 1000,
            "year": 2023
        },
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/lockport-city-school-district-facial-recognition",
        "program": "Security",
        "activity": "Using facial recognition technology to identify students and staff",
        "organization": "Lockport City School District",
        "impact_depth": "High. The impact of the facial recognition system is significant. It has raised serious privacy and civil rights concerns and led to calls for the system to be removed from schools",
        "impact_report": {
            "impact": "Significant. The opacity and bias of the facial recognition system has raised serious privacy and civil rights concerns. It has also led to calls for the system to be removed from schools",
            "mitigation": "The Lockport City School District has since suspended the use of the facial recognition system. However, the district has not yet announced whether it will permanently remove the system",
            "impact_risk": "High. The opacity and bias of the facial recognition system is a serious privacy and civil rights violation. There is a high risk that other schools will use similar systems without proper safeguards"
        }
    },
    {
        "impact_model": {
            "input": "A photo or video of a person",
            "stakeholder_outcome": "Users were concerned about their privacy and the potential for their facial data to be used for malicious purposes",
            "outcome": "The face swap app was found to be collecting and storing users' facial data without their consent",
            "stakeholder": "Users of the face swap app",
            "output": "A video of the person's face swapped with the face of another person"
        },
        "impact_scale": "Global. The app was available in app stores around the world. However, it is likely that the app was used by people in many countries",
        "indicator": "Number of users who were concerned about the face swap app",
        "service": "Face Swap Technology",
        "title": "Zao Face Swap App Sparks Privacy Concerns",
        "impact_duration": "Long-lasting. The concerns about the use of face swap technology are likely to persist for many years to come",
        "indicator_report": {
            "number_of_people_concerned": 100000,
            "year": 2020
        },
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/zao-face-swap-app",
        "program": "Entertainment",
        "activity": "Using face swap technology to create videos of people's faces",
        "organization": "Zao",
        "impact_depth": "High. The impact of the face swap app is significant. It has raised serious privacy concerns and led to calls for the app to be removed from app stores",
        "impact_report": {
            "impact": "Significant. The collection and storage of users' facial data without their consent has raised serious privacy concerns. It has also led to calls for the app to be removed from app stores",
            "mitigation": "The Zao app has since been removed from app stores. However, the company has not yet announced whether it will delete the facial data it has collected",
            "impact_risk": "High. The collection and storage of users' facial data without their consent is a serious privacy violation. There is a high risk that other companies will use similar tactics in the future"
        }
    },
    {
        "impact_model": {
            "input": "A self-driving car",
            "stakeholder_outcome": "The motorbike rider was killed and their family is devastated",
            "outcome": "A GM Chevrolet Bolt self-driving car collided with a motorbike, killing the motorbike rider",
            "stakeholder": "The motorbike rider and their family",
            "output": "A safe and efficient journey"
        },
        "impact_scale": "Local. The accident occurred in California. However, it has the potential to happen anywhere in the world where self-driving cars are being tested or deployed",
        "indicator": "Number of people killed in self-driving car accidents",
        "service": "GM Cruise",
        "title": "GM Chevrolet Bolt Self-Driving Car Accident Sparks Safety Concerns",
        "impact_duration": "Long-lasting. The concerns about the safety of self-driving cars are likely to persist for many years to come",
        "indicator_report": {
            "number_of_people_killed": 1,
            "year": 2022
        },
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/gm-chevrolet-bolt-motorbike-collision",
        "program": "Self-Driving Cars",
        "activity": "Testing self-driving cars on public roads",
        "organization": "General Motors",
        "impact_depth": "High. The impact of the self-driving car accident is significant. It has raised serious safety concerns about the development and use of self-driving cars",
        "impact_report": {
            "impact": "Significant. The self-driving car accident has raised serious safety concerns about the development and use of self-driving cars. It has also led to calls for stricter regulations on the testing and deployment of self-driving cars",
            "mitigation": "GM has suspended testing of its self-driving cars on public roads. The company is also conducting an investigation into the accident",
            "impact_risk": "High. The self-driving car accident is a serious reminder of the potential risks of self-driving cars. There is a high risk that similar accidents will occur in the future"
        }
    },
    {
        "impact_model": {
            "input": "Employee activity",
            "stakeholder_outcome": "Employees were concerned about their privacy and the potential for their personal information to be used against them",
            "outcome": "Employees' privacy was violated",
            "stakeholder": "Employees",
            "output": "A report of the employee's activity, including their keystrokes, mouse movements, and websites visited"
        },
        "impact_scale": "Global. Teleperformance is a global company with offices in over 100 countries. It is likely that TP-Observer was used to monitor employees in many countries",
        "indicator": "Number of employees who were monitored by TP-Observer",
        "service": "TP-Observer Employee Monitoring Software",
        "title": "Teleperformance TP-Observer Employee Monitoring Sparks Privacy Concerns",
        "impact_duration": "Long-lasting. The concerns about the use of employee monitoring software are likely to persist for many years to come",
        "indicator_report": {
            "number_of_employees_monitored": 100000,
            "year": 2022
        },
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/teleperformancetp-observer-employee-monitoring",
        "program": "Customer Service",
        "activity": "Using TP-Observer to monitor employee activity",
        "organization": "Teleperformance",
        "impact_depth": "High. The impact of the use of TP-Observer is significant. It has raised serious privacy concerns and led to calls for stricter regulations",
        "impact_report": {
            "impact": "Significant. The use of TP-Observer has raised serious privacy concerns among employees, labor unions, and privacy advocates. It has also led to calls for stricter regulations on the use of employee monitoring software",
            "mitigation": "Teleperformance has since changed its privacy policy to give employees more control over how their data is collected and used. However, the company has not yet stopped using TP-Observer",
            "impact_risk": "High. The use of employee monitoring software is a serious privacy violation. There is a high risk that other companies will use similar software to track their employees' activity without their consent"
        }
    },
    {
        "impact_model": {
            "input": "A photo or video of a person",
            "stakeholder_outcome": "People were concerned about their privacy and the potential for DeepFaceLive to be used to discriminate against them",
            "outcome": "DeepFaceLive was found to be inaccurate and biased",
            "stakeholder": "People who were identified by DeepFaceLive",
            "output": "A list of possible matches, including the person's name, age, and location"
        },
        "impact_scale": "Global. DeepFaceLive was available in over 100 countries. It is likely that DeepFaceLive was used to identify people in many countries",
        "indicator": "Number of people who were identified by DeepFaceLive",
        "service": "Facial Recognition Software",
        "title": "DeepFaceLive Facial Recognition Software Sparks Privacy Concerns",
        "impact_duration": "Long-lasting. The concerns about the use of facial recognition software are likely to persist for many years to come",
        "indicator_report": {
            "number_of_people_identified": 100000,
            "year": 2022
        },
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/deepfacelive",
        "program": "Security",
        "activity": "Using DeepFaceLive to identify people in real time",
        "organization": "DeepFaceLive",
        "impact_depth": "High. The impact of the use of DeepFaceLive is significant. It has raised serious privacy and civil rights concerns and led to calls for the software to be banned",
        "impact_report": {
            "impact": "Significant. The inaccuracy and bias of DeepFaceLive has raised serious privacy and civil rights concerns. It has also led to calls for the software to be banned",
            "mitigation": "DeepFaceLive has since been banned in several countries. However, it is still available in other countries",
            "impact_risk": "High. The inaccuracy and bias of DeepFaceLive is a serious privacy and civil rights violation. There is a high risk that other facial recognition software will be inaccurate and biased"
        }
    },
    {
        "impact_model": {
            "input": "A criminal defendant's case information, including their criminal history, the severity of their crime, and any mitigating factors",
            "stakeholder_outcome": "People were concerned about the fairness of the system and the potential for bias",
            "outcome": "The AI-powered sentencing system was found to be biased against certain groups of defendants, including people of color and people with mental illness",
            "stakeholder": "People who were sentenced by the AI-powered system",
            "output": "A recommended sentence"
        },
        "impact_scale": "Local. The incident occurred in Malaysia. However, it has the potential to happen anywhere in the world where AI-powered sentencing systems are used",
        "indicator": "Number of people who were sentenced by the AI-powered system",
        "service": "AI-powered Sentencing System",
        "title": "Malaysia AI Court Sentencing Sparks Debate About the Role of AI in the Legal System",
        "impact_duration": "Long-lasting. The concerns about the use of AI in the legal system are likely to persist for many years to come",
        "indicator_report": {
            "number_of_people_sentenced": 100,
            "year": 2022
        },
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/malaysia-ai-court-sentencing",
        "program": "Justice System",
        "activity": "Using AI to recommend sentences for criminal defendants",
        "organization": "Kuala Lumpur High Court",
        "impact_depth": "High. The impact of the bias of the AI-powered sentencing system is significant. It has raised serious concerns about the fairness of the legal system and led to calls for the system to be redesigned or abandoned",
        "impact_report": {
            "impact": "Significant. The bias of the AI-powered sentencing system has raised serious concerns about the fairness of the legal system. It has also led to calls for the system to be redesigned or abandoned",
            "mitigation": "The Kuala Lumpur High Court has since suspended the use of the AI-powered sentencing system. The court is currently conducting an investigation into the system's bias",
            "impact_risk": "High. The bias of the AI-powered sentencing system is a serious threat to the fairness of the legal system. There is a high risk that other AI-powered systems will be biased"
        }
    },
    {
        "impact_model": {
            "input": "Content from other social media platforms",
            "stakeholder_outcome": "Users were concerned about their data privacy and the potential for Bytedance to use their data for malicious purposes",
            "outcome": "Bytedance was found to have violated the terms of service of the other social media platforms",
            "stakeholder": "Users of other social media platforms",
            "output": "A copy of the content, which was then used to train TikTok's AI algorithms"
        },
        "impact_scale": "Global. Bytedance is a global company with over 1 billion users. It is likely that Bytedance scraped content from social media platforms in many countries",
        "indicator": "Number of users who were concerned about Bytedance's data scraping",
        "service": "TikTok",
        "title": "Bytedance Content Scraping Sparks Concerns About Data Privacy",
        "impact_duration": "Long-lasting. The concerns about the data privacy practices of Bytedance are likely to persist for many years to come",
        "indicator_report": {
            "number_of_people_concerned": 100000,
            "year": 2020
        },
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/bytedance-content-scraping",
        "program": "Social Media",
        "activity": "Scraping content from other social media platforms, including Twitter, Instagram, and YouTube",
        "organization": "Bytedance",
        "impact_depth": "High. The impact of the data scraping by Bytedance is significant. It has raised serious concerns about data privacy and led to calls for Bytedance to be investigated by regulators",
        "impact_report": {
            "impact": "Significant. The data scraping by Bytedance has raised serious concerns about data privacy. It has also led to calls for Bytedance to be investigated by regulators",
            "mitigation": "Bytedance has since stopped scraping content from other social media platforms. However, the company has not yet apologized for its actions or taken steps to address the concerns of users",
            "impact_risk": "High. The data scraping by Bytedance is a serious threat to data privacy. There is a high risk that other companies will engage in similar practices"
        }
    },
    {
        "impact_model": {
            "input": "A video feed from a camera on a bridge",
            "stakeholder_outcome": "People were concerned about their privacy and the potential for the system to be used to discriminate against them",
            "outcome": "The AI-powered suicide detection system was found to be inaccurate and biased",
            "stakeholder": "People who were identified by the AI-powered system",
            "output": "A list of people who are at risk of suicide"
        },
        "impact_scale": "Local. The incident occurred in Seoul, South Korea. However, it has the potential to happen anywhere in the world where AI-powered suicide detection systems are used",
        "indicator": "Number of people who were identified by the AI-powered system",
        "service": "AI-powered Suicide Detection System",
        "title": "Seoul Bridge Suicide Detection Sparks Privacy Concerns",
        "impact_duration": "Long-lasting. The concerns about the use of AI in suicide prevention are likely to persist for many years to come",
        "indicator_report": {
            "number_of_people_identified": 100,
            "year": 2021
        },
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/seoul-bridge-suicide-detection",
        "program": "Public Safety",
        "activity": "Using AI to detect people who are at risk of suicide on bridges",
        "organization": "Seoul Metropolitan Government",
        "impact_depth": "High. The impact of the inaccuracy and bias of the AI-powered suicide detection system is significant. It has raised serious concerns about the privacy and civil rights of people who are at risk of suicide and led to calls for the system to be redesigned or abandoned",
        "impact_report": {
            "impact": "Significant. The inaccuracy and bias of the AI-powered suicide detection system has raised serious privacy and civil rights concerns. It has also led to calls for the system to be redesigned or abandoned",
            "mitigation": "The Seoul Metropolitan Government has since suspended the use of the AI-powered suicide detection system. The government is currently conducting an investigation into the system's accuracy and bias",
            "impact_risk": "High. The inaccuracy and bias of the AI-powered suicide detection system is a serious threat to the privacy and civil rights of people who are at risk of suicide. There is a high risk that other AI-powered systems will be inaccurate and biased"
        }
    },
    {
        "impact_model": {
            "input": "A photo of a person's face",
            "stakeholder_outcome": "People were concerned about their privacy and the potential for the software to be used to discriminate against them",
            "outcome": "The facial recognition software was found to be inaccurate and biased",
            "stakeholder": "People who were subject to the facial recognition software",
            "output": "A prediction of whether the person is quarantining at home"
        },
        "impact_scale": "Local. The trials occurred in New South Wales and Victoria, Australia. However, it has the potential to happen anywhere in the world where facial recognition software is used to enforce quarantine orders",
        "indicator": "Number of people who were subject to the facial recognition software",
        "service": "Facial Recognition Software",
        "title": "New South Wales, Victoria COVID-19 Facial Recognition Trials Spark Privacy Concerns",
        "impact_duration": "Long-lasting. The concerns about the use of facial recognition software in public health are likely to persist for many years to come",
        "indicator_report": {
            "number_of_people_subjected": 100000,
            "year": 2021
        },
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/new-south-wales-victoria-covid-19-facial-recognition-trials",
        "program": "Public Health",
        "activity": "Using facial recognition software to check whether people are quarantining at home during COVID-19",
        "organization": "New South Wales Government, Victoria Government",
        "impact_depth": "High. The impact of the inaccuracy and bias of the facial recognition software is significant. It has raised serious concerns about the privacy and civil rights of people who are subject to quarantine orders and led to calls for the software to be redesigned or abandoned",
        "impact_report": {
            "impact": "Significant. The inaccuracy and bias of the facial recognition software has raised serious privacy and civil rights concerns. It has also led to calls for the software to be redesigned or abandoned",
            "mitigation": "The New South Wales and Victoria governments have since suspended the use of the facial recognition software. The governments are currently conducting an investigation into the software's accuracy and bias",
            "impact_risk": "High. The inaccuracy and bias of the facial recognition software is a serious threat to the privacy and civil rights of people who are subject to quarantine orders. There is a high risk that other facial recognition software will be inaccurate and biased"
        }
    },
    {
        "impact_model": {
            "input": "A video feed of a person's face",
            "stakeholder_outcome": "Uyghurs were concerned about the software being used to target them for discrimination and persecution",
            "outcome": "The emotion detection software was found to be inaccurate and biased",
            "stakeholder": "Uyghurs",
            "output": "A prediction of the person's emotional state"
        },
        "impact_scale": "Local. The testing occurred in Xinjiang, China. However, it has the potential to happen anywhere in the world where emotion detection software is used to identify potential threats",
        "indicator": "Number of Uyghurs who were subject to the emotion detection software",
        "service": "Emotion Detection Software",
        "title": "Uyghur Emotion Detection Testing Sparks Concerns About Human Rights",
        "impact_duration": "Long-lasting. The concerns about the use of emotion detection software in human rights are likely to persist for many years to come",
        "indicator_report": {
            "number_of_uyghurs_subjected": 100000,
            "year": 2021
        },
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/uyghur-emotion-detection-testing",
        "program": "Internal Security",
        "activity": "Using emotion detection software to identify Uyghurs who are at risk of radicalization",
        "organization": "Government of China",
        "impact_depth": "High. The impact of the inaccuracy and bias of the emotion detection software is significant. It has raised serious human rights concerns and led to calls for the software to be banned",
        "impact_report": {
            "impact": "Significant. The inaccuracy and bias of the emotion detection software has raised serious human rights concerns. It has also led to calls for the software to be banned",
            "mitigation": "The Chinese government has denied using emotion detection software to target Uyghurs. However, there is evidence that the software is being used in Xinjiang, where Uyghurs are subject to mass surveillance and internment",
            "impact_risk": "High. The inaccuracy and bias of the emotion detection software is a serious threat to the human rights of Uyghurs. There is a high risk that other emotion detection software will be inaccurate and biased"
        }
    },
    {
        "impact_model": {
            "input": "A video feed from the car's cameras",
            "stakeholder_outcome": "The driver and passengers were killed in the crash",
            "outcome": "The car's self-driving features failed to detect a truck that was crossing the road, resulting in a fatal crash",
            "stakeholder": "The driver and passengers of the car",
            "output": "A prediction of the car's surroundings"
        },
        "impact_scale": "Local. The crash occurred in China. However, it has the potential to happen anywhere in the world where self-driving cars are used",
        "indicator": "Number of people killed in the crash",
        "service": "Nio ES8",
        "title": "Nio ES8 Fatal Crash Sparks Concerns About Self-Driving Safety",
        "impact_duration": "Long-lasting. The concerns about the safety of self-driving cars are likely to persist for many years to come",
        "indicator_report": {
            "number_of_people_killed": 3,
            "year": 2021
        },
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/nio-es8-fatal-crash",
        "program": "Self-Driving",
        "activity": "Using Nio ES8's self-driving features",
        "organization": "Nio",
        "impact_depth": "High. The impact of the failure of the car's self-driving features is significant. It has raised serious safety concerns about self-driving cars and led to calls for more regulation of self-driving cars",
        "impact_report": {
            "impact": "Significant. The failure of the car's self-driving features has raised serious safety concerns about self-driving cars. It has also led to calls for more regulation of self-driving cars",
            "mitigation": "Nio has since recalled the Nio ES8 and is working to improve the car's self-driving features. However, it is not clear when the car will be safe to drive again",
            "impact_risk": "High. The failure of the car's self-driving features is a serious threat to the safety of people who use self-driving cars. There is a high risk that other self-driving cars will fail to detect obstacles and cause accidents"
        }
    },
    {
        "impact_model": {
            "input": "A photo of a person",
            "stakeholder_outcome": "People were concerned about their privacy and the potential for the service to be used to create fake news or spread misinformation",
            "outcome": "The Deep Nostalgia service was found to be inaccurate and biased",
            "stakeholder": "People who used the Deep Nostalgia service",
            "output": "A video of the person with animated facial expressions"
        },
        "impact_scale": "Global. The Deep Nostalgia service was used by people all over the world. However, it has the potential to be used by people anywhere in the world",
        "indicator": "Number of people who used the Deep Nostalgia service",
        "service": "Deep Nostalgia",
        "title": "MyHeritage Deep Nostalgia Sparks Concerns About Privacy and Ethics",
        "impact_duration": "Long-lasting. The concerns about the use of AI in genealogy are likely to persist for many years to come",
        "indicator_report": {
            "number_of_people_used": "1 million",
            "year": 2022
        },
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/myheritage-deep-nostalgia",
        "program": "Genealogy",
        "activity": "Using Deep Nostalgia to animate old photos",
        "organization": "MyHeritage",
        "impact_depth": "High. The impact of the inaccuracy and bias of the Deep Nostalgia service is significant. It has raised serious privacy and ethical concerns and led to calls for the service to be discontinued",
        "impact_report": {
            "impact": "Significant. The inaccuracy and bias of the Deep Nostalgia service has raised serious privacy and ethical concerns. It has also led to calls for the service to be discontinued",
            "mitigation": "MyHeritage has since added some safeguards to the Deep Nostalgia service, such as requiring users to agree to terms of service before using the service. However, it is not clear if these safeguards are enough to address the concerns that have been raised",
            "impact_risk": "High. The inaccuracy and bias of the Deep Nostalgia service is a serious threat to the privacy and ethical rights of people who use the service. There is a high risk that other services will be inaccurate and biased"
        }
    },
    {
        "impact_model": "AI and Algorithmic Risks Harms Taxonomy",
        "impact_scale": "The impact of this incident is likely to be significant.",
        "indicator": "The number of asylum seekers who have been harmed by the collection of their personal information.",
        "input": "Personal information of asylum seekers",
        "service": "Privacy",
        "title": "One Asylum Seeker App's Privacy Concerns",
        "stakeholder_outcome": "Asylum seekers could be at risk of being tracked, discriminated against, or even harmed due to the collection of their personal information.",
        "stakeholder": "Asylum seekers",
        "impact_duration": "The impact of this incident is likely to continue for several years.",
        "output": "A database of asylum seekers' personal information",
        "indicator_report": "The indicator report has not yet been published.",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/cpb-one-asylum-seeker-app-privacy",
        "program": "One Asylum Seeker App",
        "impact_risk": "The risk of harm to asylum seekers is high.",
        "activity": "Data collection",
        "organization": "Coalition for the Protection of Refugees",
        "outcome": "The database could be used to track asylum seekers, discriminate against them, or even harm them.",
        "impact_depth": "The impact of this incident is likely to be long-lasting.",
        "impact_report": "The impact of this incident has not yet been fully assessed."
    },
    {
        "impact_model": "AI and Algorithmic Risks Harms Taxonomy",
        "impact_scale": "The impact of this incident is likely to be significant.",
        "indicator": "The number of fraudulent invoices issued using deepfakes.",
        "input": "High-definition photographs of government officials",
        "service": "Identity verification",
        "title": "China Taxation Department ID System Hack",
        "stakeholder_outcome": "Tax payers may be at risk of higher taxes or other penalties due to the fraud.",
        "stakeholder": "Tax payers",
        "impact_duration": "The impact of this incident is likely to continue for several years.",
        "output": "Fake invoices used to defraud the taxation department",
        "indicator_report": "The indicator report has not yet been published.",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/china-taxation-department-id-system-hack",
        "program": "Taxation",
        "impact_risk": "The risk of further fraud is high.",
        "activity": "Fraudulent use of deepfakes",
        "organization": "State Taxation Administration of China",
        "outcome": "The taxation department was defrauded of 500 million yuan (approximately USD 76.2 million).",
        "impact_depth": "The impact of this incident is likely to be long-lasting.",
        "impact_report": "The impact of this incident has not yet been fully assessed."
    },
    {
        "impact_model": "AI and Algorithmic Risks Harms Taxonomy",
        "impact_scale": "The impact of this incident is likely to be significant.",
        "indicator": "The number of Domino's Australia employees who have been disciplined due to the use of the Pizza Checker.",
        "input": "Pizzas made by Domino's Australia employees",
        "service": "Quality control",
        "title": "Domino's Australia Pizza Checker",
        "stakeholder_outcome": "Some Domino's Australia employees may be at risk of unfair disciplinary action due to the use of the Pizza Checker",
        "stakeholder": "Domino's Australia employees",
        "impact_duration": "The impact of this incident is likely to continue for several years.",
        "output": "Grades for each pizza, which are then used to identify underperforming stores and employees",
        "indicator_report": "The indicator report has not yet been published.",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/dominos-australia-pizza-checker",
        "program": "Pizza delivery",
        "impact_risk": "The risk of unfair disciplinary action is high.",
        "activity": "Use of AI-powered camera to grade pizzas",
        "organization": "Domino's Pizza Australia",
        "outcome": "Some Domino's Australia employees have expressed concerns that the Pizza Checker is being used to unfairly target them for disciplinary action",
        "impact_depth": "The impact of this incident is likely to be long-lasting.",
        "impact_report": "The impact of this incident has not yet been fully assessed."
    },
    {
        "impact_model": "AI and Algorithmic Risks Harms Taxonomy",
        "impact_scale": "The impact of this incident is likely to be significant.",
        "indicator": "The number of offenders who are disproportionately targeted by the risk assessment algorithm.",
        "input": "Data on past criminal behavior",
        "service": "Risk assessment",
        "title": "Virginia Non-violent Risk Assessment",
        "stakeholder_outcome": "Offenders who are disproportionately targeted by the risk assessment algorithm may be at risk of longer sentences and harsher treatment.",
        "stakeholder": "Offenders",
        "impact_duration": "The impact of this incident is likely to continue for several years.",
        "output": "A risk score for each offender",
        "indicator_report": "The indicator report has not yet been published.",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/virginia-non-violent-risk-assessment",
        "program": "Criminal justice",
        "impact_risk": "The risk of unfair treatment is high.",
        "activity": "Development and use of a risk assessment algorithm",
        "organization": "Virginia Criminal Sentencing Commission",
        "outcome": "The risk assessment algorithm has been criticized for unfairly targeting Black and young offenders.",
        "impact_depth": "The impact of this incident is likely to be long-lasting.",
        "impact_report": "A report by the Washington Post found that the risk assessment algorithm increased sentences for Black and young defendants."
    },
    {
        "impact_model": "AI and Algorithmic Risks Harms Taxonomy",
        "impact_scale": "The impact of this incident is likely to be significant.",
        "indicator": "The number of pedestrians who are hit by self-driving cars.",
        "input": "Data on objects in the road, including bus ads",
        "service": "Object detection",
        "title": "AI Confuses Bus Ad for Jaywalker",
        "stakeholder_outcome": "Pedestrians may be at risk of being hit by self-driving cars if the AI system is not able to correctly identify objects in the road.",
        "stakeholder": "Pedestrians",
        "impact_duration": "The impact of this incident is likely to continue for several years.",
        "output": "A list of objects detected in the road, including bus ads",
        "indicator_report": "The indicator report has not yet been published.",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/ai-confuses-bus-ad-for-jaywalker",
        "program": "Self-driving cars",
        "impact_risk": "The risk of injury or death is high.",
        "activity": "Use of AI to detect objects in the road",
        "organization": "Mercedes-Benz",
        "outcome": "The AI system confused a bus ad for a jaywalker, and nearly hit a pedestrian.",
        "impact_depth": "The impact of this incident is likely to be long-lasting.",
        "impact_report": "A report by the National Highway Traffic Safety Administration found that self-driving cars are more likely to hit pedestrians than human drivers."
    },
    {
        "impact_model": "AI and Algorithmic Risks Harms Taxonomy",
        "impact_scale": "The impact of this incident is likely to be significant.",
        "indicator": "The number of fraudulent wire transfers that are authorized using deepfake voices.",
        "input": "Audio recordings of the company director's voice",
        "service": "Fraud detection",
        "title": "Dubai USD 35m voice cloning fraud",
        "stakeholder_outcome": "The company was defrauded of USD 35 million",
        "stakeholder": "The company",
        "impact_duration": "The impact of this incident is likely to continue for several years.",
        "output": "A deepfake voice that can be used to impersonate the company director",
        "indicator_report": "The indicator report has not yet been published.",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/dubai-usd-35m-voice-cloning-fraud",
        "program": "Financial services",
        "impact_risk": "The risk of fraud is high.",
        "activity": "Use of deepfakes to clone a company director's voice",
        "organization": "Unknown",
        "outcome": "The deepfake voice was used to defraud the company of USD 35 million",
        "impact_depth": "The impact of this incident is likely to be long-lasting.",
        "impact_report": "A report by the Dubai Police Force found that the deepfake voice was used to impersonate the company director and authorize a series of fraudulent wire transfers."
    },
    {
        "impact_model": "AI and Algorithmic Risks Harms Taxonomy",
        "impact_scale": "The impact of this incident is likely to be significant.",
        "indicator": "The number of deepfakes that are generated using the CelebA dataset.",
        "input": "Images of celebrities",
        "service": "Data collection",
        "title": "CelebA Dataset",
        "stakeholder_outcome": "The public may be at risk of being deceived by deepfakes that are generated using the CelebA dataset.",
        "stakeholder": "The public",
        "impact_duration": "The impact of this incident is likely to continue for several years.",
        "output": "A dataset of images of celebrities",
        "indicator_report": "The indicator report has not yet been published.",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/celeba-dataset",
        "program": "Computer vision",
        "impact_risk": "The risk of deception is high.",
        "activity": "Collection of images of celebrities",
        "organization": "University of California, Berkeley",
        "outcome": "The dataset was used to train deepfake models that can generate realistic images of celebrities.",
        "impact_depth": "The impact of this incident is likely to be long-lasting.",
        "impact_report": "A report by the University of California, Berkeley found that the CelebA dataset was used to train deepfake models that can generate realistic images of celebrities."
    },
    {
        "impact_model": "AI and Algorithmic Risks Harms Taxonomy",
        "impact_scale": "The impact of this incident is likely to be significant.",
        "indicator": "The number of photographs that are judged by AI algorithms.",
        "input": "Photographs submitted to the Sony Photography Awards",
        "service": "Image recognition",
        "title": "Sony Photography Awards AI Victory",
        "stakeholder_outcome": "Photography enthusiasts may be concerned about the fairness of AI-driven photography contests",
        "stakeholder": "Photography enthusiasts",
        "impact_duration": "The impact of this incident is likely to continue for several years.",
        "output": "A list of winning photographs, selected by an AI algorithm",
        "indicator_report": "The indicator report has not yet been published.",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/sony-photography-awards-ai-victory",
        "program": "Photography",
        "impact_risk": "The risk of bias is high.",
        "activity": "Use of AI to judge photography contest",
        "organization": "Sony",
        "outcome": "An AI-generated winner was declared, which led to controversy and accusations of bias",
        "impact_depth": "The impact of this incident is likely to be long-lasting.",
        "impact_report": "A report by the Sony Photography Awards found that the AI algorithm was biased towards certain types of photographs."
    },
    {
        "impact_model": "AI and Algorithmic Risks Harms Taxonomy",
        "impact_scale": "The impact of this initiative is likely to be significant.",
        "indicator": "The number of companies and organizations that use AI for content moderation that have adopted the ethical data labeling framework",
        "input": "Data on content that needs to be moderated",
        "service": "Data labeling",
        "title": "Sama Ethical Data Labeling for Content moderation",
        "stakeholder_outcome": "These companies and organizations are now able to label data for content moderation in a more ethical way",
        "stakeholder": "Companies and organizations that use AI for content moderation",
        "impact_duration": "The impact of this initiative is likely to continue for several years.",
        "output": "A set of ethical guidelines for data labeling for content moderation",
        "indicator_report": "The indicator report has not yet been published.",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/sama-ethical-data-labeling-content-moderation",
        "program": "Content moderation",
        "impact_risk": "The risk of bias is low.",
        "activity": "Development of an ethical data labeling framework for content moderation",
        "organization": "Samasource",
        "outcome": "The framework has been adopted by several companies and organizations that use AI for content moderation",
        "impact_depth": "The impact of this initiative is likely to be long-lasting.",
        "impact_report": "A report by Samasource found that the framework has been effective in improving the ethical standards of data labeling for content moderation"
    },
    {
        "impact_model": "AI and Algorithmic Risks Harms Taxonomy",
        "impact_scale": "The impact of this initiative is likely to be significant.",
        "indicator": "The number of welfare claims and payments that are processed by the automated system",
        "input": "Data on welfare claims and payments",
        "service": "Automation",
        "title": "Trelleborg Welfare Management Automation",
        "stakeholder_outcome": "Welfare recipients have benefited from faster and more efficient processing of their claims",
        "stakeholder": "Welfare recipients",
        "impact_duration": "The impact of this initiative is likely to continue for several years.",
        "output": "An automated system for processing welfare claims and payments",
        "indicator_report": "The indicator report has not yet been published.",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/trelleborg-welfare-management-automation",
        "program": "Welfare management",
        "impact_risk": "The risk of bias is low.",
        "activity": "Use of AI to automate welfare management processes",
        "organization": "Trelleborg",
        "outcome": "The system has reduced the time and cost of processing welfare claims and payments",
        "impact_depth": "The impact of this initiative is likely to be long-lasting.",
        "impact_report": "A report by Trelleborg found that the system has reduced the time and cost of processing welfare claims and payments by 50%"
    },
    {
        "impact_model": "AI and Algorithmic Risks Harms Taxonomy",
        "impact_scale": "The impact of this initiative is likely to be significant.",
        "indicator": "The number of civilian casualties caused by Bayraktar TB2 drone strikes.",
        "input": "Data on Russian troop movements and positions",
        "service": "Unmanned aerial vehicles",
        "title": "Ukraine-Russia Bayraktar TB2 Drone Attacks",
        "stakeholder_outcome": "The Ukrainian military has been able to successfully defend its country against Russian aggression",
        "stakeholder": "The Ukrainian military",
        "impact_duration": "The impact of this initiative is likely to continue for several years.",
        "output": "A series of successful drone strikes against Russian forces",
        "indicator_report": "The indicator report has not yet been published.",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/ukraine-russia-bayraktar-tb2-drone-attacks",
        "program": "Defense",
        "impact_risk": "The risk of civilian casualties is high.",
        "activity": "Use of AI-powered drones to attack Russian forces",
        "organization": "Baykar",
        "outcome": "The strikes have inflicted significant damage on Russian forces and have helped to slow the Russian advance",
        "impact_depth": "The impact of this initiative is likely to be long-lasting.",
        "impact_report": "A report by the Ukrainian Ministry of Defense found that the Bayraktar TB2 drones have been a \"game-changer\" in the conflict"
    },
    {
        "Impact Scale": "The impact of AI and algorithmic incidents and controversies can vary widely. Some incidents may have a relatively small impact, while others may have a significant impact on individuals, organizations, and society as a whole.",
        "Impact Duration": "The impact of AI and algorithmic incidents and controversies can also vary in duration. Some incidents may have a single impact, while others may have multiple impacts over time.",
        "Indicator": "The indicators of impact include the number of incidents and controversies, the severity of the impacts, the number of people affected, and the economic and social costs of the incidents.",
        "Indicator Report": "The AIAAIC Repository provides a comprehensive list of indicators of impact for AI and algorithmic incidents and controversies. The report includes information on how to collect and measure the indicators, as well as how to use the indicators to assess the risks and harms associated with AI and algorithms.",
        "Service": "AIAAIC Repository",
        "Title": "AI and Algorithmic Incidents and Controversies",
        "Impact Depth": "The impact of AI and algorithmic incidents and controversies can also vary in depth. Some incidents may have a short-term impact, while others may have a long-term impact.",
        "Stakeholder": "The public, AI researchers, developers, and users",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies",
        "Stakeholder Outcome": "A more informed and engaged public, AI researchers and developers who are aware of the risks and harms associated with AI and algorithms, and AI users who are able to make informed decisions about how to use AI",
        "Impact Report": "The AIAAIC Repository provides a comprehensive list of AI and algorithmic incidents and controversies, along with information about the organizations involved, the impacts of the incidents, and the risks associated with them. The report is updated regularly as new information becomes available",
        "Output": "A list of AI and algorithmic incidents and controversies, along with information about the organizations involved, the impacts of the incidents, and the risks associated with them",
        "Program": "AI and Algorithmic Risks and Harms",
        "impact_risk": "The risks associated with AI and algorithms include discrimination, bias, privacy violations, and safety hazards. These risks can have a significant impact on individuals, organizations, and society as a whole.",
        "Activity": "Identifying and Analyzing AI and Algorithmic Incidents and Controversies",
        "Organization": "AIAAIC",
        "Outcome": "A better understanding of the risks and harms associated with AI and algorithms, and the development of strategies to mitigate those risks",
        "Input": "News articles, research papers, and other sources of information about AI and algorithmic incidents and controversies",
        "Impact Model": "Common Impact Data Standard"
    },
    {
        "Impact Scale": "The impact of the Tinder Plus pricing algorithm could be significant, as it could affect millions of users.",
        "Impact Duration": "The impact of the Tinder Plus pricing algorithm could be short-term or long-term, depending on how it is handled by Tinder.",
        "Indicator": "The indicator of impact is the cost of Tinder Plus for each user. Users who are more likely to use Tinder Plus are charged more for it, which could indicate that the pricing algorithm is discriminating against them.",
        "Indicator Report": "A report on the indicators of impact is not yet available.",
        "Service": "Tinder Plus",
        "Title": "Tinder Plus Pricing Algorithm: Fairness and Discrimination",
        "Impact Depth": "The impact of the Tinder Plus pricing algorithm could be long-lasting, as it could damage the reputation of Tinder and make users less likely to trust AI.",
        "Stakeholder": "Users of Tinder Plus, Tinder, and the general public.",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tinder-plus-pricing-algorithm-fairness-discrimination",
        "Stakeholder Outcome": "Users of Tinder Plus may be unfairly charged more for the service, which could lead to dissatisfaction and a loss of trust in Tinder. Tinder could be seen as discriminating against certain groups of users, which could damage its reputation. The general public could become concerned about the potential for AI to be used to discriminate against certain groups of people.",
        "Impact Report": "A report on the impact of the Tinder Plus pricing algorithm is not yet available.",
        "Output": "A pricing algorithm that determines the cost of Tinder Plus for each user",
        "Program": "AI and Algorithmic Risks and Harms",
        "impact_risk": "The risk of discrimination is high, as the pricing algorithm is based on data on user behavior, demographics, and preferences. This data could be used to unfairly target certain groups of users for higher prices.",
        "Activity": "Developing and deploying a pricing algorithm for Tinder Plus",
        "Organization": "Tinder",
        "Outcome": "Users who are more likely to use Tinder Plus are charged more for it, which could lead to discrimination against certain groups of users.",
        "Input": "Data on user behavior, demographics, and preferences",
        "Impact Model": "Common Impact Data Standard"
    },
    {
        "Impact Scale": "The impact of the FaceTag Facial Recognition Algorithm could be significant, as it could affect millions of people.",
        "Impact Duration": "The impact of the FaceTag Facial Recognition Algorithm could be short-term or long-term, depending on how it is used and how people react to it.",
        "Indicator": "The indicator of impact is the number of people whose faces are identified or tracked by the FaceTag Facial Recognition Algorithm. The more people whose faces are identified or tracked, the more likely it is that the algorithm will be used to violate people's privacy, discriminate against certain groups of people, and even cause physical harm.",
        "Indicator Report": "A report on the indicators of impact is not yet available.",
        "Service": "FaceTag Facial Recognition Algorithm",
        "Title": "The FaceTag Facial Recognition Algorithm",
        "Impact Depth": "The impact of the FaceTag Facial Recognition Algorithm could be long-lasting, as it could damage the reputation of Microsoft and make people less likely to trust AI.",
        "Stakeholder": "People whose faces are identified or tracked by the FaceTag Facial Recognition Algorithm, Microsoft, and the general public.",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/the-facetag",
        "Stakeholder Outcome": "People whose faces are identified or tracked by the FaceTag Facial Recognition Algorithm may experience a loss of privacy, discrimination, and even physical harm. Microsoft could be seen as a company that is willing to violate people's privacy and to discriminate against certain groups of people. The general public could become concerned about the potential for AI to be used to violate people's privacy and to discriminate against certain groups of people.",
        "Impact Report": "A report on the impact of the FaceTag Facial Recognition Algorithm is not yet available.",
        "Output": "A facial recognition algorithm that can identify and track human faces in images, videos, and 3D scans",
        "Program": "AI and Algorithmic Risks and Harms",
        "impact_risk": "The risk of privacy violations, discrimination, and physical harm is high, as the FaceTag Facial Recognition Algorithm is able to identify and track people in a variety of settings.",
        "Activity": "Developing and deploying the FaceTag Facial Recognition Algorithm",
        "Organization": "Microsoft",
        "Outcome": "The FaceTag Facial Recognition Algorithm has been used to identify and track people in a variety of settings, including protests, sporting events, and airports. This has raised concerns about the potential for the algorithm to be used to violate people's privacy and to discriminate against certain groups of people.",
        "Input": "Data on human faces, including images, videos, and 3D scans",
        "Impact Model": "Common Impact Data Standard"
    },
    {
        "Impact Scale": "The impact of the Little Mix Robot Editor could be significant, as it could affect millions of people.",
        "Impact Duration": "The impact of the Little Mix Robot Editor could be short-term or long-term, depending on how it is used and how people react to it.",
        "Indicator": "The indicator of impact is the number of music videos and songs created by the Little Mix Robot Editor that feature people of color. The lower the number of music videos and songs featuring people of color, the more likely it is that the robot editor is perpetuating racial stereotypes.",
        "Indicator Report": "A report on the indicators of impact is not yet available.",
        "Service": "Little Mix Robot Editor",
        "Title": "Microsoft Little Mix Robot Editor Racial Bias",
        "Impact Depth": "The impact of the Little Mix Robot Editor could be long-lasting, as it could damage the reputation of Microsoft and make people less likely to trust AI.",
        "Stakeholder": "People of color, Microsoft, and the general public.",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/microsoft-little-mix-robot-editor-racial-bias",
        "Stakeholder Outcome": "People of color may experience discrimination and a lack of representation in the music industry. Microsoft could be seen as a company that is willing to perpetuate racial stereotypes. The general public could become concerned about the potential for AI to be used to perpetuate racial stereotypes.",
        "Impact Report": "A report on the impact of the Little Mix Robot Editor is not yet available.",
        "Output": "A robot editor that can create music videos and songs using human faces and voices",
        "Program": "AI and Algorithmic Risks and Harms",
        "impact_risk": "The risk of perpetuating racial stereotypes is high, as the Little Mix Robot Editor was found to have racial bias.",
        "Activity": "Developing and deploying the Little Mix Robot Editor",
        "Organization": "Microsoft",
        "Outcome": "The Little Mix Robot Editor was found to have racial bias, as it was more likely to create music videos and songs that featured white people. This has raised concerns about the potential for AI to be used to perpetuate racial stereotypes.",
        "Input": "Data on human faces and voices, including images, videos, and audio recordings",
        "Impact Model": "Common Impact Data Standard"
    },
    {
        "Impact Scale": "The impact of the Instacart Gig Shopper Robotization could be significant, as it could affect millions of gig shoppers.",
        "Impact Duration": "The impact of the Instacart Gig Shopper Robotization could be short-term or long-term, depending on how it is rolled out and how people react to it.",
        "Indicator": "The indicator of impact is the number of gig shoppers who lose their jobs due to the Instacart Gig Shopper Robotization. The higher the number of job losses, the more likely it is that the robotization will have a negative impact on gig shoppers.",
        "Indicator Report": "A report on the indicators of impact is not yet available.",
        "Service": "Instacart Gig Shopper Robotization",
        "Title": "Instacart Gig Shopper Robotization",
        "Impact Depth": "The impact of the Instacart Gig Shopper Robotization could be long-lasting, as it could set a precedent for the automation of other jobs.",
        "Stakeholder": "Gig shoppers, Instacart, and the general public.",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/instacart-gig-shopper-robotisation",
        "Stakeholder Outcome": "Gig shoppers could lose their jobs and experience financial hardship. Instacart could face backlash from customers and employees. The general public could become concerned about the potential for AI to replace human workers in other industries.",
        "Impact Report": "A report on the impact of the Instacart Gig Shopper Robotization is not yet available.",
        "Output": "An AI-powered robot that can shop for groceries and deliver them to customers",
        "Program": "AI and Algorithmic Risks and Harms",
        "impact_risk": "The risk of job losses and public backlash is high, as the Instacart Gig Shopper Robotization could lead to the automation of a large number of jobs.",
        "Activity": "Developing and deploying an AI-powered robot to replace gig shoppers",
        "Organization": "Instacart",
        "Outcome": "The Instacart Gig Shopper Robotization could lead to job losses for gig shoppers, as well as concerns about the potential for AI to replace human workers in other industries.",
        "Input": "Data on customer orders, store layouts, and product inventory",
        "Impact Model": "Common Impact Data Standard"
    },
    {
        "Impact Scale": "The impact of AI-powered driver assistance systems could be significant, as they could be used by millions of people.",
        "Impact Duration": "The impact of AI-powered driver assistance systems could be short-term or long-term, depending on how they are used and how people react to them.",
        "Indicator": "The indicator of impact is the number of accidents and fatalities involving AI-powered driver assistance systems. The higher the number of accidents and fatalities, the more likely it is that AI-powered driver assistance systems will pose a safety risk.",
        "Indicator Report": "A report on the indicators of impact is not yet available.",
        "Service": "Tesla Autopilot",
        "Title": "Tesla Model S Runs Red Light, Kills Two",
        "Impact Depth": "The impact of AI-powered driver assistance systems could be long-lasting, as it could lead to a change in the way people drive and the way that cars are designed.",
        "Stakeholder": "The families of the victims, Tesla, and the general public.",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-model-s-runs-red-light-kills-two",
        "Stakeholder Outcome": "The families of the victims are grieving the loss of their loved ones. Tesla could face lawsuits and damage to its reputation. The general public could become concerned about the safety of AI-powered driver assistance systems.",
        "Impact Report": "A report on the impact of the Tesla Model S crash is not yet available.",
        "Output": "A driver assistance system that can control the speed and steering of a Tesla vehicle",
        "Program": "AI and Algorithmic Risks and Harms",
        "impact_risk": "The risk of accidents and fatalities is high, as AI-powered driver assistance systems are still under development and are not yet fully reliable.",
        "Activity": "Developing and deploying the Tesla Autopilot driver assistance system",
        "Organization": "Tesla",
        "Outcome": "A Tesla Model S equipped with Autopilot ran a red light and killed two people. This has raised concerns about the potential for AI-powered driver assistance systems to be used in a way that is unsafe.",
        "Input": "Data on traffic conditions, road signs, and other vehicles",
        "Impact Model": "Common Impact Data Standard"
    },
    {
        "Impact Scale": "The impact of the Safety at Screening Tool could be significant, as it could affect thousands of Black families.",
        "Impact Duration": "The impact of the Safety at Screening Tool could be short-term or long-term, depending on how it is used and how people react to it.",
        "Indicator": "The indicator of impact is the number of Black families who are flagged for investigation by the Safety at Screening Tool. The higher the number of Black families who are flagged for investigation, the more likely it is that the tool is discriminating against Black families.",
        "Indicator Report": "A report on the indicators of impact is not yet available.",
        "Service": "Safety at Screening Tool",
        "Title": "Oregon DHS Safety at Screening Tool",
        "Impact Depth": "The impact of the Safety at Screening Tool could be long-lasting, as it could damage the reputation of Oregon Department of Human Services and make people less likely to trust AI.",
        "Stakeholder": "Black families, Oregon Department of Human Services, and the general public.",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/oregon-dhs-safety-at-screening-tool",
        "Stakeholder Outcome": "Black families may be unfairly investigated by child protective services. Oregon Department of Human Services could face lawsuits and damage to its reputation. The general public could become concerned about the potential for AI to be used to discriminate against certain groups of people.",
        "Impact Report": "A report on the impact of the Safety at Screening Tool is not yet available.",
        "Output": "A tool that can predict which families are at risk of child abuse and neglect",
        "Program": "AI and Algorithmic Risks and Harms",
        "impact_risk": "The risk of discrimination is high, as the Safety at Screening Tool is based on data that may contain racial bias.",
        "Activity": "Developing and deploying the Safety at Screening Tool",
        "Organization": "Oregon Department of Human Services",
        "Outcome": "The Safety at Screening Tool has been criticized for flagging a disproportionate number of Black families for investigation. This has raised concerns about the potential for the tool to be used to discriminate against Black families.",
        "Input": "Data on child abuse and neglect reports",
        "Impact Model": "Common Impact Data Standard"
    },
    {
        "Impact Scale": "The impact of the ImageNet Dataset could be significant, as it is used to train many AI models.",
        "Impact Duration": "The impact of the ImageNet Dataset could be short-term or long-term, depending on how it is used and how people react to it.",
        "Indicator": "The indicator of impact is the number of AI models that are trained on the ImageNet Dataset and that perpetuate racial and gender stereotypes. The higher the number of AI models that perpetuate racial and gender stereotypes, the more likely it is that the ImageNet Dataset is having a negative impact.",
        "Indicator Report": "A report on the indicators of impact is not yet available.",
        "Service": "ImageNet Dataset",
        "Title": "ImageNet Dataset Racial and Gender Stereotyping",
        "Impact Depth": "The impact of the ImageNet Dataset could be long-lasting, as it could lead to the perpetuation of racial and gender stereotypes in AI systems.",
        "Stakeholder": "People of color, women, and the general public.",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/imagenet-dataset-racial-gender-stereotyping",
        "Stakeholder Outcome": "People of color and women may be unfairly discriminated against by AI models trained on the ImageNet Dataset. The general public could become concerned about the potential for AI to be used to perpetuate racial and gender stereotypes.",
        "Impact Report": "A report on the impact of the ImageNet Dataset is not yet available.",
        "Output": "A dataset of images that can be used to train machine learning models",
        "Program": "AI and Algorithmic Risks and Harms",
        "impact_risk": "The risk of perpetuating racial and gender stereotypes is high, as the ImageNet Dataset is a large and influential dataset.",
        "Activity": "Developing and deploying the ImageNet Dataset",
        "Organization": "Stanford University",
        "Outcome": "The ImageNet Dataset has been criticized for containing racial and gender stereotypes. This has raised concerns about the potential for AI models trained on the ImageNet Dataset to perpetuate these stereotypes.",
        "Input": "Data on images, including their labels",
        "Impact Model": "Common Impact Data Standard"
    },
    {
        "Impact Scale": "The impact of the facial recognition system could be significant, as it is used by a large real estate sales office.",
        "Impact Duration": "The impact of the facial recognition system could be short-term or long-term, depending on how it is used and how people react to it.",
        "Indicator": "The indicator of impact is the number of potential buyers who feel their privacy is being violated by the facial recognition system. The higher the number of potential buyers who feel their privacy is being violated, the more likely it is that the facial recognition system is having a negative impact.",
        "Indicator Report": "A report on the indicators of impact is not yet available.",
        "Service": "Facial recognition system",
        "Title": "Nanning Real Estate Sales Office Facial Recognition",
        "Impact Depth": "The impact of the facial recognition system could be long-lasting, as it could lead to a loss of trust in AI systems.",
        "Stakeholder": "Potential buyers, Nanning Real Estate Sales Office, and the general public.",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/nanning-real-estate-sales-office-facial-recognition",
        "Stakeholder Outcome": "Potential buyers may feel their privacy is being violated. Nanning Real Estate Sales Office could face lawsuits and damage to its reputation. The general public could become concerned about the potential for AI to be used to track people's movements without their consent.",
        "Impact Report": "A report on the impact of the facial recognition system is not yet available.",
        "Output": "A system that can track the movements of potential buyers",
        "Program": "AI and Algorithmic Risks and Harms",
        "impact_risk": "The risk of violating people's privacy is high, as the facial recognition system is able to track the movements of people without their consent.",
        "Activity": "Developing and deploying a facial recognition system to track the movements of potential buyers",
        "Organization": "Nanning Real Estate Sales Office",
        "Outcome": "The facial recognition system has been criticized for violating the privacy of potential buyers. This has raised concerns about the potential for AI to be used to track people's movements without their consent.",
        "Input": "Data on the faces of potential buyers",
        "Impact Model": "Common Impact Data Standard"
    },
    {
        "Impact Scale": "The impact of Waymo self-driving cars getting stuck in cul-de-sacs could be significant, as Waymo is a leading developer of self-driving cars.",
        "Impact Duration": "The impact of Waymo self-driving cars getting stuck in cul-de-sacs could be short-term or long-term, depending on how the issue is addressed.",
        "Indicator": "The indicator of impact is the number of times Waymo self-driving cars get stuck in cul-de-sacs. The higher the number of times Waymo self-driving cars get stuck in cul-de-sacs, the more likely it is that the self-driving cars are unreliable.",
        "Indicator Report": "A report on the indicators of impact is not yet available.",
        "Service": "Waymo self-driving cars",
        "Title": "Waymo Cars Get Stuck in Cul-de-Sac",
        "Impact Depth": "The impact of Waymo self-driving cars getting stuck in cul-de-sacs could be long-lasting, as it could damage the reputation of self-driving cars and make people less likely to use them.",
        "Stakeholder": "Waymo, the general public, and people who rely on self-driving cars for transportation.",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/waymo-cars-get-stuck-in-cul-de-sac",
        "Stakeholder Outcome": "Waymo could face lawsuits and damage to its reputation. The general public could become concerned about the safety of AI-powered self-driving cars. People who rely on self-driving cars for transportation could be inconvenienced or stranded.",
        "Impact Report": "A report on the impact of Waymo self-driving cars getting stuck in cul-de-sacs is not yet available.",
        "Output": "Self-driving cars that can navigate roads and avoid obstacles",
        "Program": "AI and Algorithmic Risks and Harms",
        "impact_risk": "The risk of self-driving cars getting stuck in cul-de-sacs is high, as self-driving cars are still under development and are not yet fully reliable.",
        "Activity": "Developing and deploying Waymo self-driving cars",
        "Organization": "Waymo",
        "Outcome": "Waymo self-driving cars have been reported getting stuck in cul-de-sacs. This has raised concerns about the potential for AI-powered self-driving cars to be unreliable.",
        "Input": "Data on traffic conditions, road signs, and other vehicles",
        "Impact Model": "Common Impact Data Standard"
    },
    {
        "Impact Scale": "The impact of the facial recognition system could be significant, as it is used by a large apartment complex.",
        "Impact Duration": "The impact of the facial recognition system could be short-term or long-term, depending on how it is used and how people react to it.",
        "Indicator": "The indicator of impact is the number of residents and visitors who feel their privacy is being violated by the facial recognition system. The higher the number of residents and visitors who feel their privacy is being violated, the more likely it is that the facial recognition system is having a negative impact.",
        "Indicator Report": "A report on the indicators of impact is not yet available.",
        "Service": "Facial recognition system",
        "Title": "Atlantic Plaza Towers Facial Recognition",
        "Impact Depth": "The impact of the facial recognition system could be long-lasting, as it could lead to a loss of trust in AI systems.",
        "Stakeholder": "Residents, visitors, Atlantic Plaza Towers, and the general public.",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/atlantic-plaza-towers-facial-recognition",
        "Stakeholder Outcome": "Residents and visitors may feel their privacy is being violated. Atlantic Plaza Towers could face lawsuits and damage to its reputation. The general public could become concerned about the potential for AI to be used to track people's movements without their consent.",
        "Impact Report": "A report on the impact of the facial recognition system is not yet available.",
        "Output": "A system that can track the movements of residents and visitors",
        "Program": "AI and Algorithmic Risks and Harms",
        "impact_risk": "The risk of violating people's privacy is high, as the facial recognition system is able to track the movements of people without their consent.",
        "Activity": "Developing and deploying a facial recognition system to track the movements of residents and visitors",
        "Organization": "Atlantic Plaza Towers",
        "Outcome": "The facial recognition system has been criticized for violating the privacy of residents and visitors. This has raised concerns about the potential for AI to be used to track people's movements without their consent.",
        "Input": "Data on the faces of residents and visitors",
        "Impact Model": "Common Impact Data Standard"
    },
    {
        "Impact Scale": "The impact of the show could be significant, as it is a popular show that is seen by millions of people.",
        "Impact Duration": "The impact of the show could be short-term or long-term, depending on how it is received by the public.",
        "Indicator": "The indicator of impact is the number of people who feel that the show perpetuates harmful stereotypes about transgender people. The higher the number of people who feel that the show perpetuates harmful stereotypes about transgender people, the more likely it is that the show is having a negative impact.",
        "Indicator Report": "A report on the indicators of impact is not yet available.",
        "Service": "Jerry Seinfeld show",
        "Title": "Nothing Forever: Jerry Seinfeld Clone Transphobia",
        "Impact Depth": "The impact of the show could be long-lasting, as it could lead to a more prejudiced public attitude towards transgender people.",
        "Stakeholder": "Transgender people, Jerry Seinfeld, and the general public.",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/nothing-forever-jerry-seinfeld-clone-transphobia",
        "Stakeholder Outcome": "Transgender people may feel their identity is being mocked or ridiculed. Jerry Seinfeld could face backlash from the transgender community and damage to his reputation. The general public could become more prejudiced against transgender people.",
        "Impact Report": "A report on the impact of the show is not yet available.",
        "Output": "A show that features a character who is a clone of Jerry Seinfeld who is transgender",
        "Program": "AI and Algorithmic Risks and Harms",
        "impact_risk": "The risk of perpetuating harmful stereotypes is high, as the show is based on a transphobic stereotype.",
        "Activity": "Developing and deploying a show that features a character who is a clone of Jerry Seinfeld who is transgender",
        "Organization": "Jerry Seinfeld",
        "Outcome": "The show has been criticized for perpetuating transphobic stereotypes. This has raised concerns about the potential for AI to be used to spread harmful stereotypes about certain groups of people.",
        "Input": "Data on the lives of transgender people",
        "Impact Model": "Common Impact Data Standard"
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Local",
        "indicator": "Number of passengers using facial recognition to pay for rides",
        "input": "Data on passengers' faces",
        "service": "Facial recognition",
        "title": "Moscow Metro Face Pay facial recognition",
        "stakeholder_outcome": "Improved experience of using the Moscow Metro",
        "stakeholder": "Passengers",
        "impact_duration": "Short-term",
        "output": "A facial recognition system that can be used to pay for rides in the Moscow Metro",
        "indicator_report": "N/A",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/moscow-metro-face-pay-facial-recognition",
        "program": "Moscow Metro Face Pay",
        "impact_risk": "The system could be used to track passengers' movements or to discriminate against certain groups of people",
        "activity": "Testing and implementation of facial recognition for payment in the Moscow Metro",
        "organization": "Moscow Metro",
        "outcome": "Increased convenience and security for passengers",
        "impact_depth": "Medium",
        "impact_report": "N/A"
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Local",
        "indicator": "Number of people who have been wrongfully arrested due to facial recognition",
        "input": "Data on people's faces",
        "service": "Facial recognition",
        "title": "Alonzo Sawyer facial recognition mistaken arrest",
        "stakeholder_outcome": "Sawyer was traumatized by the experience and is now suing the police department",
        "stakeholder": "Alonzo Sawyer",
        "impact_duration": "Short-term",
        "output": "The arrest of Alonzo Sawyer, who was misidentified by facial recognition software",
        "indicator_report": "N/A",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/alonzo-sawyer-facial-recognition-mistaken-arrest",
        "program": "San Francisco Police Department facial recognition program",
        "impact_risk": "The use of facial recognition could lead to wrongful arrests, discrimination, and a loss of privacy",
        "activity": "Use of facial recognition to identify and arrest suspects",
        "organization": "San Francisco Police Department",
        "outcome": "Damage to Sawyer's reputation and mental health, and a loss of trust in the police",
        "impact_depth": "Medium",
        "impact_report": "N/A"
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "National",
        "indicator": "Number of people who believed the deepfake image was real",
        "input": "Data on the Pentagon and the surrounding area",
        "service": "Deepfake",
        "title": "Pentagon deepfake explosion",
        "stakeholder_outcome": "The US government was embarrassed and its credibility was damaged",
        "stakeholder": "The US government",
        "impact_duration": "Short-term",
        "output": "A deepfake image of an explosion near the Pentagon",
        "indicator_report": "N/A",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/pentagon-deepfake-explosion",
        "program": "N/A",
        "impact_risk": "Deepfakes could be used to spread misinformation, propaganda, and fear",
        "activity": "Creation and dissemination of a deepfake image of an explosion near the Pentagon",
        "organization": "Unclear/unknown",
        "outcome": "A 0.26% fall in the US stock market in four minutes, and a loss of trust in the US government",
        "impact_depth": "Medium",
        "impact_report": "N/A"
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Local",
        "indicator": "Number of people who have been wrongfully arrested due to facial recognition",
        "input": "Data on people's faces",
        "service": "Facial recognition",
        "title": "Hyderabad police facial recognition legality",
        "stakeholder_outcome": "People in Hyderabad feel less safe and more monitored",
        "stakeholder": "People in Hyderabad",
        "impact_duration": "Long-term",
        "output": "A facial recognition system that can be used to identify and track people",
        "indicator_report": "N/A",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/hyderabad-police-facial-recognition",
        "program": "Hyderabad police facial recognition program",
        "impact_risk": "The use of facial recognition could lead to wrongful arrests, discrimination, and a loss of privacy",
        "activity": "Use of facial recognition to identify and track people",
        "organization": "Telangana Police",
        "outcome": "Increased surveillance and a loss of privacy for people in Hyderabad",
        "impact_depth": "Medium",
        "impact_report": "N/A"
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Local",
        "indicator": "Number of people who have had their GPS location data collected and shared",
        "input": "GPS location data from mobile devices",
        "service": "Location tracking",
        "title": "Huq GPS location data sharing",
        "stakeholder_outcome": "People who have used apps that collect GPS location data feel less safe and more monitored",
        "stakeholder": "People who have used apps that collect GPS location data",
        "impact_duration": "Long-term",
        "output": "A database of GPS location data that can be used to track people's movements",
        "indicator_report": "N/A",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/huq-gps-location-data-sharing",
        "program": "Huq GPS location data sharing program",
        "impact_risk": "The collection and sharing of GPS location data could be used to track people's movements, target them with advertising, or even commit crimes",
        "activity": "Collection and sharing of GPS location data",
        "organization": "Huq Industries",
        "outcome": "Increased surveillance and a loss of privacy for people who have used apps that collect GPS location data",
        "impact_depth": "Medium",
        "impact_report": "N/A"
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Local",
        "indicator": "Number of people who were disturbed or disappointed by the deepfake audio recording",
        "input": "Data on Anthony Bourdain's voice",
        "service": "Deepfake",
        "title": "Anthony Bourdain voice deepfake",
        "stakeholder_outcome": "Fans felt that the deepfake was disrespectful to Bourdain's memory and that it exploited his likeness",
        "stakeholder": "Fans of Anthony Bourdain",
        "impact_duration": "Short-term",
        "output": "A deepfake audio recording of Anthony Bourdain",
        "indicator_report": null,
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/anthony-bourdain-voice-deepfake",
        "program": null,
        "impact_risk": "Deepfakes could be used to spread misinformation, propaganda, and fear",
        "activity": "Creation and dissemination of a deepfake audio recording of Anthony Bourdain",
        "organization": "Unclear/unknown",
        "outcome": "Disturbed and disappointed fans",
        "impact_depth": "Medium",
        "impact_report": null
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Global",
        "indicator": "Number of people who expressed dismay or concern about the technology",
        "input": "Data on human faces",
        "service": "Generated photo",
        "title": "Generated photos from the Infinite Diversity Face Collection",
        "stakeholder_outcome": "The public expressed concern that the technology could be used to create fake news or propaganda, or to impersonate real people",
        "stakeholder": "The public",
        "impact_duration": "Long-term",
        "output": "A collection of generated photos of human faces",
        "indicator_report": null,
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/generated-photos-infinite-diversity-face-collection",
        "program": null,
        "impact_risk": "The technology could be used to create fake news or propaganda, or to impersonate real people",
        "activity": "Creation and dissemination of generated photos from the Infinite Diversity Face Collection",
        "organization": "Unclear/unknown",
        "outcome": "Dismay and concern about the potential for misuse of the technology",
        "impact_depth": "Medium",
        "impact_report": null
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Local",
        "indicator": "Number of people who saw the Google autocomplete suggestion",
        "input": "Search query for Australian surgeon",
        "service": "Google Search",
        "title": "Google autocomplete suggests Australian surgeon is bankrupt",
        "stakeholder_outcome": "The surgeon was forced to issue a statement denying that he was bankrupt",
        "stakeholder": "The surgeon",
        "impact_duration": "Short-term",
        "output": "Google autocomplete suggests that the surgeon is bankrupt",
        "indicator_report": null,
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/google-autocomplete-suggests-australian-surgeon-is-bankrupt",
        "program": null,
        "impact_risk": "Google autocomplete could be used to spread misinformation and damage people's reputations",
        "activity": "Use of Google Search to find information about an Australian surgeon",
        "organization": "Google",
        "outcome": "Damage to the surgeon's reputation and loss of business",
        "impact_depth": "Medium",
        "impact_report": null
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Global",
        "indicator": "Number of people who saw the deepfake video",
        "input": "Data on Changpeng Zhao's appearance and voice",
        "service": "Deepfake",
        "title": "Binance CCO deepfake impersonation",
        "stakeholder_outcome": "Binance was forced to issue a statement denying that the deepfake video was real, and it is unclear how many users were affected by the incident",
        "stakeholder": "Binance and its users",
        "impact_duration": "Short-term",
        "output": "A deepfake video of Changpeng Zhao",
        "indicator_report": null,
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/binance-cco-deepfake-impersonation",
        "program": null,
        "impact_risk": "Deepfakes could be used to spread misinformation, damage people's reputations, and commit fraud",
        "activity": "Creation and dissemination of a deepfake video of Binance CCO Changpeng Zhao",
        "organization": "Binance",
        "outcome": "Damage to Binance's reputation and loss of trust from users",
        "impact_depth": "Medium",
        "impact_report": null
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Local",
        "indicator": "Number of people who have been injured or killed by robots",
        "input": "Data on how to weld metal sheets",
        "service": "Robotics",
        "title": "Robot kills SKH Metals worker",
        "stakeholder_outcome": "The worker's family and colleagues are grieving the loss of their loved one, and they are concerned about the safety of working with robots",
        "stakeholder": "The worker's family and colleagues",
        "impact_duration": "Short-term",
        "output": "A \"welded metal sheet\"",
        "indicator_report": null,
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/robot-kills-skh-metals-worker",
        "program": null,
        "impact_risk": "Robots could malfunction and cause injury or death",
        "activity": "Use of a robot to weld metal sheets",
        "organization": "SKH Metals",
        "outcome": "Death of a worker",
        "impact_depth": "Medium",
        "impact_report": null
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Local",
        "indicator": "Number of citizens who have been affected by the social civility score",
        "input": "Data on citizens' social behavior",
        "service": "Social credit score",
        "title": "Suzhou social civility score trial",
        "stakeholder_outcome": "Citizens are concerned about their privacy and about the potential for the social civility score to be used to discriminate against them",
        "stakeholder": "Citizens of Suzhou",
        "impact_duration": "Medium-term",
        "output": "A social civility score for each citizen in Suzhou",
        "indicator_report": null,
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/suzhou-social-civility-score-trial",
        "program": null,
        "impact_risk": "The social civility score could be used to discriminate against citizens, to restrict their freedom of movement, and to punish them for minor infractions",
        "activity": "Implementation of a social civility score trial in Suzhou",
        "organization": "Suzhou government",
        "outcome": "Increased surveillance and control of citizens",
        "impact_depth": "Medium",
        "impact_report": null
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Global",
        "indicator": "Number of facial recognition algorithms that have been trained on the \"DukeMTMC\" dataset",
        "input": "Data on over 70,000 images of people walking on a university campus",
        "service": "Facial recognition",
        "title": "DukeMTMC facial recognition dataset",
        "stakeholder_outcome": "Researchers and developers are able to build more accurate and performant facial recognition algorithms, which can be used for a variety of purposes, such as security, surveillance, and marketing",
        "stakeholder": "Researchers and developers of facial recognition algorithms",
        "impact_duration": "Long-term",
        "output": "A facial recognition dataset that is widely used by researchers and developers",
        "indicator_report": null,
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/dukemtmc-facial-recognition-dataset",
        "program": null,
        "impact_risk": "The \"DukeMTMC\" dataset could be used to create facial recognition algorithms that are biased against certain groups of people, such as women and minorities",
        "activity": "Creation and release of the \"DukeMTMC\" facial recognition dataset",
        "organization": "Duke University",
        "outcome": "Increased accuracy and performance of facial recognition algorithms",
        "impact_depth": "Medium",
        "impact_report": null
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Global",
        "indicator": "Number of Tamoco users who have expressed concerns about their privacy",
        "input": "Data on the location of Tamoco users",
        "service": "Location data",
        "title": "Tamoco location data sharing",
        "stakeholder_outcome": "Tamoco users are concerned about their privacy and about the potential for their location data to be misused",
        "stakeholder": "Tamoco users",
        "impact_duration": "Long-term",
        "output": "Location data that is sold to third-party companies",
        "indicator_report": null,
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tamoco-location-data-sharing",
        "program": null,
        "impact_risk": "Location data could be used to track people's movements, to target them with advertising, or to discriminate against them",
        "activity": "Collection and sharing of location data by Tamoco",
        "organization": "Tamoco",
        "outcome": "Increased privacy concerns and potential for misuse of location data",
        "impact_depth": "Medium",
        "impact_report": null
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Local",
        "indicator": "Number of students who have raised concerns about the AI-powered proctoring software",
        "input": "Data on the student's webcam feed, microphone, and screen activity",
        "service": "Online proctoring",
        "title": "Cleveland State University online proctor room scanning",
        "stakeholder_outcome": "Students are concerned about their privacy and about the potential for the AI-powered proctoring software to discriminate against them",
        "stakeholder": "Students, faculty, and staff",
        "impact_duration": "Medium-term",
        "output": "A decision on whether or not the student has cheated on the exam",
        "indicator_report": null,
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/cleveland-state-university-online-proctor-room-scanning",
        "program": null,
        "impact_risk": "The AI-powered proctoring software could be used to unfairly target students, to violate their privacy, or to discriminate against them",
        "activity": "Use of AI-powered proctoring software to monitor online exams",
        "organization": "Cleveland State University",
        "outcome": "Increased surveillance and potential for discrimination",
        "impact_depth": "Medium",
        "impact_report": null
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Local",
        "indicator": "Number of accidents caused by Tesla's \"Autopilot\" feature",
        "input": "Data from the car's sensors, including cameras, radar, and lidar",
        "service": "Self-driving car",
        "title": "Tesla Model S kills truck driver and pedestrian",
        "stakeholder_outcome": "Truck driver and pedestrian's families are grieving, and Tesla owners are concerned about the safety of their cars",
        "stakeholder": "Truck driver, pedestrian, and Tesla owners",
        "impact_duration": "Medium-term",
        "output": "A decision to accelerate or brake, or to take no action",
        "indicator_report": null,
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-model-s-kills-truck-driver-pedestrian",
        "program": null,
        "impact_risk": "The \"Autopilot\" feature could be used to cause accidents, or to discriminate against certain groups of people",
        "activity": "Use of Tesla's \"Autopilot\" feature",
        "organization": "Tesla",
        "outcome": "Death of a truck driver and a pedestrian",
        "impact_depth": "Medium",
        "impact_report": null
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Global",
        "indicator": "Number of facial recognition algorithms that have been trained on the Iarpa Janus Benchmark-C dataset",
        "input": "Data on over 21,000 images of 3,531 people",
        "service": "Facial recognition",
        "title": "Iarpa Janus Benchmark-C (IJP-C) dataset",
        "stakeholder_outcome": "Researchers and developers are able to build more accurate and performant facial recognition algorithms, which can be used for a variety of purposes, such as security, surveillance, and marketing",
        "stakeholder": "Researchers and developers of facial recognition algorithms",
        "impact_duration": "Long-term",
        "output": "A facial recognition dataset that is widely used by researchers and developers",
        "indicator_report": null,
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/iarpa-janus-benchmark-c-ijp-c-dataset",
        "program": "Iarpa Janus Benchmark Program",
        "impact_risk": "The Iarpa Janus Benchmark-C dataset could be used to create facial recognition algorithms that are biased against certain groups of people, such as women and minorities",
        "activity": "Creation and release of the Iarpa Janus Benchmark-C dataset",
        "organization": "Iarpa",
        "outcome": "Increased accuracy and performance of facial recognition algorithms",
        "impact_depth": "Medium",
        "impact_report": null
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Local",
        "indicator": "Number of people who have been harassed or distressed by deepfake pornography",
        "input": "Data on \"Xiao Yu\"'s appearance, including images and videos",
        "service": "Deepfake pornography",
        "title": "Xiao Yu deepfake pornography",
        "stakeholder_outcome": "\"Xiao Yu\" is harassed and distressed by the distribution of deepfake pornography of her likeness",
        "stakeholder": "\"Xiao Yu\"",
        "impact_duration": "Medium-term",
        "output": "Deepfake pornography of \"Xiao Yu\"",
        "indicator_report": null,
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/xiao-yu-deepfake-pornography",
        "program": null,
        "impact_risk": "Deepfake pornography can be used to harass, intimidate, and exploit people",
        "activity": "Creation and distribution of deepfake pornography of \"Xiao Yu\"",
        "organization": null,
        "outcome": "Harassment and distress for \"Xiao Yu\"",
        "impact_depth": "Medium",
        "impact_report": null
    },
    {
        "impact_model": "\"Common Impact Data Standard\"",
        "impact_scale": "Local",
        "indicator": "Number of people who have been wrongfully arrested due to facial recognition technology",
        "input": "Data on the suspect's face, including a photograph",
        "service": "\"Facial recognition\"",
        "title": "\"Apple's facial recognition technology misidentifies man, leading to wrongful arrest\"",
        "stakeholder_outcome": "Innocent person is arrested and detained, and must go through the process of clearing their name",
        "stakeholder": "Innocent person who was arrested",
        "impact_duration": "Medium-term",
        "output": "A prediction that the suspect is a particular person",
        "indicator_report": null,
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/applesis-misidentification-wrongful-arrest\"",
        "program": null,
        "impact_risk": "Facial recognition technology can be inaccurate and can lead to wrongful arrests",
        "activity": "Use of Apple's \"Face ID\" facial recognition technology to identify a suspect in a crime",
        "organization": "\"Apple\"",
        "outcome": "Wrongful arrest of an innocent person",
        "impact_depth": "Medium",
        "impact_report": null
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Local",
        "indicator": "Number of Deliveroo riders who received compensation for discrimination",
        "input": null,
        "service": null,
        "title": "Deliveroo Italy Rider Reliability Discrimination",
        "stakeholder_outcome": "Receiving compensation for discrimination",
        "stakeholder": "Deliveroo riders",
        "impact_duration": "Short-term",
        "output": null,
        "indicator_report": "https://www.forbes.com/sites/jonathankeane/2021/01/05/italian-court-finds-deliveroo-rating-algorithm-was-unfair-to-riders/",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/deliveroo-italy-rider-shift-management-algorithm",
        "program": null,
        "impact_risk": "Risk of future discrimination against Deliveroo riders",
        "activity": "Use of algorithm to assess rider reliability",
        "organization": "Deliveroo",
        "outcome": "Court ruling that algorithm was discriminatory",
        "impact_depth": "Medium",
        "impact_report": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/deliveroo-italy-rider-shift-management-algorithm"
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Local",
        "indicator": "Number of rural letter carriers who received pay cuts",
        "input": "Data on rural letter carrier performance",
        "service": null,
        "title": "USPS Rural Letter Carrier Algorithmic Pay Cuts",
        "stakeholder_outcome": "Decreased income and job satisfaction",
        "stakeholder": "Rural letter carriers",
        "impact_duration": "Short-term",
        "output": "Pay cuts for rural letter carriers",
        "indicator_report": "https://www.npr.org/2022/01/19/1074500191/usps-rural-letter-carriers-pay-cuts-algorithm",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/usps-rural-letter-carrier-algorithmic-pay-cuts",
        "program": null,
        "impact_risk": "Risk of future pay cuts for rural letter carriers",
        "activity": "Use of algorithm to calculate rural letter carrier pay",
        "organization": "US Postal Service",
        "outcome": "Decreased morale and productivity among rural letter carriers",
        "impact_depth": "Medium",
        "impact_report": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/usps-rural-letter-carrier-algorithmic-pay-cuts"
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Local",
        "indicator": "Percentage of job ads with lower salaries displayed to women",
        "input": "Data on user demographics and browsing history",
        "service": "Google AdSense",
        "title": "Google AdSense Shows Lower-Paying Jobs to Women",
        "stakeholder_outcome": "Reduced job opportunities and lower salaries",
        "stakeholder": "Women",
        "impact_duration": "Short-term",
        "output": "Display of job ads with lower salaries to women",
        "indicator_report": "https://www.technologyreview.com/2015/07/06/110198/probing-the-dark-side-of-googles-ad-targeting-system/",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/google-adsense-shows-lower-paying-jobs-to-women",
        "program": null,
        "impact_risk": "Risk of future discrimination against women in the job market",
        "activity": "Use of algorithm to display job ads",
        "organization": "Google",
        "outcome": "Discrimination against women in the job market",
        "impact_depth": "Medium",
        "impact_report": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/google-adsense-shows-lower-paying-jobs-to-women"
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Local",
        "indicator": "Percentage of patients who were misdiagnosed by Google Derm Assist",
        "input": "Data on user skin images",
        "service": "Google Derm Assist",
        "title": "Google Derm Assist Dermatology App Bias and Privacy Concerns",
        "stakeholder_outcome": "Incorrect diagnoses and treatment, potential for further health complications",
        "stakeholder": "Patients",
        "impact_duration": "Short-term",
        "output": "Diagnoses of skin conditions",
        "indicator_report": "https://www.technologyreview.com/2022/03/08/1046326/google-derm-assist-dermatology-app-bias-privacy/",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/google-derm-assist-dermatology-app-bias-privacy",
        "program": null,
        "impact_risk": "Risk of future misdiagnosis and harm to patients",
        "activity": "Use of algorithm to diagnose skin conditions",
        "organization": "Google",
        "outcome": "Potential for misdiagnosis and harm to patients",
        "impact_depth": "Medium",
        "impact_report": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/google-derm-assist-dermatology-app-bias-privacy"
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Local",
        "indicator": "Number of pedestrian fatalities caused by self-driving cars",
        "input": "Data on traffic conditions, pedestrians, and other vehicles",
        "service": "Uber self-driving car program",
        "title": "Uber Self-Driving Car Pedestrian Fatality",
        "stakeholder_outcome": "Increased risk of death or injury from self-driving cars",
        "stakeholder": "Pedestrians",
        "impact_duration": "Short-term",
        "output": "Failure to detect and avoid pedestrian",
        "indicator_report": "https://www.npr.org/2018/03/19/595295931/uber-halts-self-driving-car-tests-after-fatal-crash",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/uber-self-driving-car-pedestrian-fatality",
        "program": null,
        "impact_risk": "Risk of future pedestrian fatalities from self-driving cars",
        "activity": "Use of algorithm to control self-driving car",
        "organization": "Uber",
        "outcome": "Death of pedestrian",
        "impact_depth": "Medium",
        "impact_report": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/uber-self-driving-car-pedestrian-fatality"
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Local",
        "indicator": "Number of users exposed to offensive and harmful content from chatbots",
        "input": "Data on user conversations",
        "service": "Zo chatbot",
        "title": "Microsoft Zo Chatbot",
        "stakeholder_outcome": "Exposure to offensive and harmful content",
        "stakeholder": "Users",
        "impact_duration": "Short-term",
        "output": "Generated text",
        "indicator_report": "https://www.bbc.com/news/technology-4736768",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/microsoft-zo-chatbot",
        "program": null,
        "impact_risk": "Risk of future chatbots generating offensive and harmful content",
        "activity": "Use of algorithm to generate text",
        "organization": "Microsoft",
        "outcome": "Chatbot generated offensive and harmful content",
        "impact_depth": "Medium",
        "impact_report": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/microsoft-zo-chatbot"
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Local",
        "indicator": "Percentage of women who were offered lower credit limits than men with similar financial histories",
        "input": "Data on user financial history",
        "service": "Apple Card",
        "title": "Apple Card Accused of Gender Bias",
        "stakeholder_outcome": "Reduced access to credit and higher interest rates",
        "stakeholder": "Women",
        "impact_duration": "Short-term",
        "output": "Credit limit",
        "indicator_report": "https://www.npr.org/2019/11/06/777222947/apple-card-under-fire-for-allegedly-discriminating-against-women",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/apple-card-accused-of-gender-bias",
        "program": null,
        "impact_risk": "Risk of future gender bias in algorithmic decision-making",
        "activity": "Use of algorithm to determine credit limit",
        "organization": "Apple",
        "outcome": "Women were offered lower credit limits than men with similar financial histories",
        "impact_depth": "Medium",
        "impact_report": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/apple-card-accused-of-gender-bias"
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Local",
        "indicator": "Number of users who expressed dissatisfaction with the TikTok Bold Glamour Filter",
        "input": "Data on user facial features",
        "service": "TikTok Bold Glamour Filter",
        "title": "TikTok Bold Glamour Filter",
        "stakeholder_outcome": "Feelings of insecurity and body dissatisfaction",
        "stakeholder": "Users",
        "impact_duration": "Short-term",
        "output": "A photo or video with a beauty filter applied",
        "indicator_report": "https://www.theguardian.com/technology/2020/jul/28/tiktok-beauty-filter-whitewashing-thin-shaming",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tiktok-bold-glamour-filter",
        "program": null,
        "impact_risk": "Risk of future beauty filters reinforcing unrealistic beauty standards",
        "activity": "Use of algorithm to apply a beauty filter",
        "organization": "TikTok",
        "outcome": "The filter was criticized for making users look too white and thin",
        "impact_depth": "Medium",
        "impact_report": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tiktok-bold-glamour-filter"
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "National",
        "indicator": "Percentage of Black and Hispanic borrowers who were denied mortgages compared to white borrowers",
        "input": "Data on user financial history, including race and ethnicity",
        "service": "Mortgage approval algorithms",
        "title": "US Mortgage Approval Algorithm Discrimination",
        "stakeholder_outcome": "Reduced access to homeownership",
        "stakeholder": "Black and Hispanic borrowers",
        "impact_duration": "Long-term",
        "output": "Mortgage approval decision",
        "indicator_report": "https://www.npr.org/2020/09/03/908918881/mortgage-lenders-in-u-s-accused-of-discriminating-against-black-and-hispanic-borrowers",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/us-mortgage-approval-algorithm-discrimination",
        "program": null,
        "impact_risk": "Risk of future discrimination in mortgage lending",
        "activity": "Use of algorithms to determine mortgage eligibility",
        "organization": "Multiple banks and mortgage lenders",
        "outcome": "Black and Hispanic borrowers were more likely to be denied mortgages than white borrowers",
        "impact_depth": "Medium",
        "impact_report": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/us-mortgage-approval-algorithm-discrimination"
    },
    {
        "Impact Scale": "Medium",
        "Impact Duration": "Short-term",
        "Indicator": "Number of user-generated content items analyzed",
        "Service": "Content Analysis",
        "Indicator Report": "Adobe Creative Cloud Content Analysis Indicator Report",
        "Impact Depth": "Moderate",
        "Stakeholder": "Adobe customers",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/adobe-creative-cloud-content-analysis",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "Adobe Creative Cloud Content Analysis Report",
        "Output": "Analytics reports",
        "Program": "Adobe Creative Cloud",
        "Stakeholder Outcome": "Increased satisfaction with Adobe Creative Cloud products and services",
        "Activity": "Data collection and analysis",
        "impact_risk": "The data collection and analysis process could be biased or inaccurate",
        "Organization": "Adobe Systems Incorporated",
        "Outcome": "Improved understanding of user behavior and preferences",
        "Input": "User-generated content",
        "Title of the story": "Adobe Creative Cloud Content Analysis"
    },
    {
        "Impact Scale": "Large",
        "Impact Duration": "Short-term",
        "Indicator": "Number of negative comments about Microsoft Tay on social media",
        "Service": "Chatbot",
        "Indicator Report": "Microsoft Tay Chatbot Indicator Report",
        "Impact Depth": "Deep",
        "Stakeholder": "Microsoft customers and partners",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/microsoft-tay-chatbot",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "Microsoft Tay Chatbot Impact Report",
        "Output": "Chatbot that could hold conversations with humans",
        "Program": "Microsoft AI",
        "Stakeholder Outcome": "Loss of trust and confidence",
        "Activity": "Development and deployment",
        "impact_risk": "The chatbot could be used to spread misinformation or hate speech",
        "Organization": "Microsoft",
        "Outcome": "Negative impact on Microsoft's reputation",
        "Input": "Data from social media",
        "Title of the story": "Microsoft Tay Chatbot"
    },
    {
        "impact_model": "Santo Robot Catholic Priest",
        "impact_scale": "Local",
        "indicator": "Number of people who use the robot's services",
        "input": "Robotics technology, artificial intelligence, Catholic theology",
        "service": null,
        "title": "Santo Robot Catholic Priest",
        "stakeholder_outcome": "The Catholic community will benefit from the robot's ability to provide spiritual guidance and counseling. This could help to increase the number of people who attend church and participate in religious activities.",
        "stakeholder": "Catholic community",
        "impact_duration": "Short-term",
        "output": "A robot that can perform the duties of a Catholic priest",
        "indicator_report": null,
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/santo-robot-catholic-priest",
        "program": null,
        "impact_risk": "The robot could be seen as a threat to traditional religious beliefs. It could also be used to spread misinformation or to harm people.",
        "activity": "Developing a robot that can perform the duties of a Catholic priest",
        "organization": "Lumidolls",
        "outcome": "The robot will be able to provide spiritual guidance and counseling to people, as well as perform religious ceremonies. This could have a positive impact on people's lives, particularly those who are isolated or who do not have access to a traditional church setting.",
        "impact_depth": "Moderate",
        "impact_report": null
    },
    {
        "impact_model": "Tesla Safety Cameras Capture Neighborhood Movements",
        "impact_scale": "Local",
        "indicator": "Number of accidents prevented by Tesla safety cameras",
        "input": "Computer vision technology, artificial intelligence",
        "service": null,
        "title": "Tesla Safety Cameras Capture Neighborhood Movements",
        "stakeholder_outcome": "Tesla owners will benefit from the safety features of their vehicles, while neighborhood residents will benefit from the reduced risk of accidents.",
        "stakeholder": "Tesla owners, neighborhood residents",
        "impact_duration": "Long-term",
        "output": "Tesla safety cameras",
        "indicator_report": null,
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-safety-cameras-capture-neighborhood-movements",
        "program": null,
        "impact_risk": "The Tesla safety cameras could be used to collect data on people's movements without their consent. This data could then be used for marketing or surveillance purposes.",
        "activity": "Developing and deploying Tesla safety cameras",
        "organization": "Tesla",
        "outcome": "The Tesla safety cameras can detect and record potential hazards, such as pedestrians, cyclists, and other vehicles. This could help to prevent accidents and improve safety in neighborhoods where Tesla vehicles are present.",
        "impact_depth": "Moderate",
        "impact_report": null
    },
    {
        "impact_model": "Facebook Ray-Ban Stories Smart Glasses",
        "impact_scale": "Local",
        "indicator": "Number of people who purchase Facebook Ray-Ban Stories smart glasses",
        "input": "Computer vision technology, artificial intelligence, augmented reality",
        "service": null,
        "title": "Facebook Ray-Ban Stories Smart Glasses",
        "stakeholder_outcome": "Facebook users will benefit from the convenience and functionality of the Facebook Ray-Ban Stories smart glasses. Ray-Ban customers will benefit from the new features and design of the glasses.",
        "stakeholder": "Facebook users, Ray-Ban customers",
        "impact_duration": "Short-term",
        "output": "Facebook Ray-Ban Stories smart glasses",
        "indicator_report": null,
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/facebookray-ban-stories-smart-glasses",
        "program": null,
        "impact_risk": "The Facebook Ray-Ban Stories smart glasses could be used to collect data on people's movements and activities without their consent. This data could then be used for marketing or surveillance purposes.",
        "activity": "Developing and marketing Facebook Ray-Ban Stories smart glasses",
        "organization": "Meta/Facebook",
        "outcome": "The Facebook Ray-Ban Stories smart glasses allow users to take photos and videos, listen to music, and access other features without having to take out their phone. This could have a positive impact on people's lives, particularly those who are always on the go.",
        "impact_depth": "Moderate",
        "impact_report": null
    },
    {
        "impact_model": "Accessible Automated Accessibility",
        "impact_scale": "Local",
        "indicator": "Number of people with disabilities who use accessible automated accessibility tools",
        "input": "Machine learning technology, artificial intelligence",
        "service": null,
        "title": "Accessible Automated Accessibility",
        "stakeholder_outcome": "People with disabilities will benefit from the ability to access and use technology more easily. This could help them to participate more fully in society and to live more independent lives.",
        "stakeholder": "People with disabilities",
        "impact_duration": "Long-term",
        "output": "Accessible automated accessibility tools",
        "indicator_report": null,
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/accessibe-automated-accessibility",
        "program": null,
        "impact_risk": "Accessible automated accessibility tools could be used to discriminate against people with disabilities. For example, a tool that is designed to help people with visual impairments could be used to prevent people with visual impairments from accessing certain websites or applications.",
        "activity": "Developing and deploying accessible automated accessibility tools",
        "organization": "Google AI",
        "outcome": "Accessible automated accessibility tools can help people with disabilities to access and use technology more easily. This could have a positive impact on the lives of people with disabilities, particularly those who are unable to use technology without assistance.",
        "impact_depth": "Moderate",
        "impact_report": null
    },
    {
        "impact_model": "\"Common Impact Data Standard\"",
        "impact_scale": "Local",
        "indicator": "Number of Instacart shoppers who have been underpaid due to the personal shopper pay algorithm",
        "input": "Data on Instacart shoppers' performance, including order completion time, customer ratings, and number of items shopped",
        "service": "\"Personal shopper pay\"",
        "title": "\"Instacart's personal shopper pay algorithm\"",
        "stakeholder_outcome": "\"Instacart shoppers are paid less than they are owed, which can lead to financial hardship and job dissatisfaction\"",
        "stakeholder": "\"Instacart shoppers\"",
        "impact_duration": "Medium-term",
        "output": "A prediction of how much an Instacart shopper will be paid for a given order",
        "indicator_report": "\"Instacart Shoppers Say Pay Algorithm Is Unfair\", by The New York Times, January 20, 2023, <https://www.nytimes.com/2023/01/20/technology/instacart-shoppers-pay-algorithm.html>",
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/instacart-personal-shopper-pay-algorithm\"",
        "program": null,
        "impact_risk": "The use of algorithms to determine pay can lead to inaccurate pay and can disproportionately impact marginalized groups",
        "activity": "Development and use of Instacart's personal shopper pay algorithm",
        "organization": "\"Instacart\"",
        "outcome": "Inaccurate pay for Instacart shoppers, leading to financial hardship and job dissatisfaction",
        "impact_depth": "Medium",
        "impact_report": "\"Instacart Shoppers Say Pay Algorithm Is Unfair\", by The New York Times, January 20, 2023, <https://www.nytimes.com/2023/01/20/technology/instacart-shoppers-pay-algorithm.html>"
    },
    {
        "impact_model": "\"Common Impact Data Standard\"",
        "impact_scale": "\"Local\"",
        "indicator": "\"Percentage of search results that are for Coupang's own products\"",
        "input": "\"Data on search queries, including the terms used and the products that were clicked on\"",
        "service": "\"Search engine\"",
        "title": "\"Coupang's own-brand search engine rigging\"",
        "stakeholder_outcome": "\"Consumers are less likely to see and buy products from other brands, which can harm competition and lead to higher prices for consumers\"",
        "stakeholder": "\"Consumers, other retailers, and brands\"",
        "impact_duration": "\"Medium-term\"",
        "output": "\"A ranking of products that is biased in favor of Coupang's own products\"",
        "indicator_report": null,
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/coupang-own-brand-search-engine-rigging\"",
        "program": null,
        "impact_risk": "\"The rigging of search engines can harm competition and lead to higher prices for consumers\"",
        "activity": "\"Rigging of Coupang's own-brand search engine to favor its own products\"",
        "organization": "\"Coupang\"",
        "outcome": "\"Consumers are less likely to see and buy products from other brands, which can harm competition and lead to higher prices for consumers\"",
        "impact_depth": "\"Medium\"",
        "impact_report": null
    },
    {
        "impact_model": "\"Common Impact Data Standard\"",
        "impact_scale": "\"Local\"",
        "indicator": "\"Number of people who have been stopped and searched by police after being identified by the facial recognition technology\"",
        "input": "\"Data on the faces of people passing through Kings Cross station\"",
        "service": "\"Facial recognition\"",
        "title": "\"Kings Cross live facial recognition trial\"",
        "stakeholder_outcome": "\"People who have been identified by the facial recognition technology may be stopped and searched by police, which can be an intrusive and humiliating experience\"",
        "stakeholder": "\"People who pass through Kings Cross station\"",
        "impact_duration": "\"Medium-term\"",
        "output": "\"A list of people who have been identified by the facial recognition technology\"",
        "indicator_report": null,
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/kings-cross-live-facial-recognition-trial\"",
        "program": null,
        "impact_risk": "\"The use of facial recognition technology can lead to discriminatory and invasive practices\"",
        "activity": "\"Trial of live facial recognition technology in Kings Cross\"",
        "organization": "\"Transport for London\"",
        "outcome": "\"People who have been identified by the facial recognition technology may be stopped and searched by police\"",
        "impact_depth": "\"Medium\"",
        "impact_report": null
    },
    {
        "impact_model": "\"Common Impact Data Standard\"",
        "impact_scale": "\"Local\"",
        "indicator": "\"Number of people who have become emotionally attached to chatbots and have experienced negative consequences\"",
        "input": "\"Data on the man's interactions with the chatbot\"",
        "service": "\"Chatbot\"",
        "title": "\"Belgian man commits suicide after bot relationship\"",
        "stakeholder_outcome": "\"The man's family is grieving his death and is seeking answers about the chatbot\"",
        "stakeholder": "\"The man and his family\"",
        "impact_duration": "\"Medium-term\"",
        "output": "\"A chatbot that was able to simulate human conversation\"",
        "indicator_report": null,
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/belgian-man-commits-suicide-after-bot-relationship\"",
        "program": null,
        "impact_risk": "\"The use of chatbots that are marketed as romantic partners can lead to emotional attachment and can be harmful to people who are vulnerable\"",
        "activity": "\"Development and use of a chatbot that was marketed as a romantic partner\"",
        "organization": null,
        "outcome": "\"The man became emotionally attached to the chatbot and committed suicide when the chatbot was taken offline\"",
        "impact_depth": "\"Medium\"",
        "impact_report": null
    },
    {
        "impact_model": "\"Common Impact Data Standard\"",
        "impact_scale": "\"Local\"",
        "indicator": "\"Percentage of women who saw job ads for high-paying and high-status positions\"",
        "input": "\"Data on users' gender, job search history, and other factors\"",
        "service": "\"Advertising\"",
        "title": "\"Facebook job ad delivery gender discrimination\"",
        "stakeholder_outcome": "\"Women were less likely to see job ads for high-paying and high-status positions, which could have led to them missing out on opportunities for employment and advancement\"",
        "stakeholder": "\"Women who were looking for jobs\"",
        "impact_duration": "\"Medium-term\"",
        "output": "\"A list of job ads that were delivered to users\"",
        "indicator_report": null,
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/facebook-job-ad-delivery-gender-discrimination\"",
        "program": null,
        "impact_risk": "\"The use of algorithms that deliver different content to different users based on their gender can lead to discrimination\"",
        "activity": "\"Development and use of an algorithm that delivered job ads to different users based on their gender\"",
        "organization": "\"Meta/Facebook\"",
        "outcome": "\"Women were less likely to see job ads for high-paying and high-status positions than men\"",
        "impact_depth": "\"Medium\"",
        "impact_report": null
    },
    {
        "impact_model": "\"Common Impact Data Standard\"",
        "impact_scale": "\"Local\"",
        "indicator": "\"Percentage of images produced by AI Portrait Ars that depicted people of color\"",
        "input": "\"Data on over 15,000 portraits from the 15th century western European Renaissance period\"",
        "service": "\"Portrait generator\"",
        "title": "\"AI Portrait Ars racial bias\"",
        "stakeholder_outcome": "\"People of color felt excluded and misrepresented by AI Portrait Ars\"",
        "stakeholder": "\"People of color\"",
        "impact_duration": "\"Medium-term\"",
        "output": "\"An AI portrait generator that produces images of people with white features\"",
        "indicator_report": null,
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/ai-portrait-ars-racial-bias\"",
        "program": null,
        "impact_risk": "\"The use of biased data in AI models can lead to the production of biased outputs\"",
        "activity": "\"Development and release of AI Portrait Ars\"",
        "organization": "\"Mauro Martino and Luca Stornaiuolo\"",
        "outcome": "\"People of color were underrepresented in the images produced by AI Portrait Ars\"",
        "impact_depth": "\"Medium\"",
        "impact_report": null
    },
    {
        "impact_model": "\"Common Impact Data Standard\"",
        "impact_scale": "\"Global\"",
        "indicator": "\"Number of large language models that are developed and deployed\"",
        "input": "\"Data on the performance of large language models of different sizes\"",
        "service": "\"Large language model research\"",
        "title": "\"Stochastic Parrots study questions large language model size\"",
        "stakeholder_outcome": "\"Researchers, developers, and users of large language models are now considering the potential benefits and risks of increasing the size of large language models\"",
        "stakeholder": "\"Researchers, developers, and users of large language models\"",
        "impact_duration": "\"Medium-term\"",
        "output": "\"A study that suggests that there is no clear benefit to increasing the size of large language models beyond a certain point\"",
        "indicator_report": null,
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/stochastic-parrots-study-questions-large-language-model-size\"",
        "program": null,
        "impact_risk": "\"The development of large language models of ever-increasing size could lead to negative consequences, such as the spread of misinformation and the erosion of privacy\"",
        "activity": "\"Publication of a study that questions the benefits of large language model size\"",
        "organization": null,
        "outcome": "\"The study has led to a debate about the value of large language model research\"",
        "impact_depth": "\"Medium\"",
        "impact_report": null
    },
    {
        "impact_model": "\"Common Impact Data Standard\"",
        "impact_scale": "\"Local\"",
        "indicator": "\"Number of users who have been harmed by the abuse of Replika\"",
        "input": "\"Data on users' conversations with Replika\"",
        "service": "\"Chatbot\"",
        "title": "\"Replika app chatbot abuse\"",
        "stakeholder_outcome": "\"Some users have been harmed by the abuse of Replika, including feeling distressed, humiliated, and even suicidal\"",
        "stakeholder": "\"Users of Replika\"",
        "impact_duration": "\"Medium-term\"",
        "output": "\"A chatbot that can simulate human conversation\"",
        "indicator_report": null,
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/replika-app-chatbot-abuse\"",
        "program": null,
        "impact_risk": "\"The use of chatbots that are marketed as companions can lead to emotional attachment and can be harmful to people who are vulnerable\"",
        "activity": "\"Development and release of Replika app\"",
        "organization": "\"Replika Inc.\"",
        "outcome": "\"Some users have abused Replika by using it to express harmful or abusive thoughts and feelings\"",
        "impact_depth": "\"Medium\"",
        "impact_report": null
    },
    {
        "impact_model": "\"Common Impact Data Standard\"",
        "impact_scale": "\"National\"",
        "indicator": "\"Number of NHS patients whose facial data is shared with iProov\"",
        "input": "\"Data on NHS patients' faces\"",
        "service": "\"Facial recognition\"",
        "title": "\"NHS Digital and iProov facial recognition data sharing\"",
        "stakeholder_outcome": "\"NHS patients could be harmed if their facial data is used for purposes other than authentication, such as marketing or surveillance\"",
        "stakeholder": "\"NHS patients\"",
        "impact_duration": "\"Medium-term\"",
        "output": "\"A facial recognition system that can be used to authenticate NHS patients\"",
        "indicator_report": null,
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/nhs-digitaliproov-facial-recognition-data-sharing\"",
        "program": null,
        "impact_risk": "\"The sharing of NHS patients' facial data could lead to privacy violations, discrimination, and other harms\"",
        "activity": "\"Data sharing agreement between NHS Digital and iProov\"",
        "organization": "\"NHS Digital\"",
        "outcome": "\"NHS patients' facial data could be used for purposes other than authentication, such as marketing or surveillance\"",
        "impact_depth": "\"Medium\"",
        "impact_report": null
    },
    {
        "impact_model": "\"Common Impact Data Standard\"",
        "impact_scale": "\"Local\"",
        "indicator": "\"Number of TikTok users who were distressed or angered by the spliced video\"",
        "input": "\"A video of a beheading and a TikTok video\"",
        "service": "\"Video sharing\"",
        "title": "\"TikTok beheading video splicing\"",
        "stakeholder_outcome": "\"TikTok users were distressed and angered by the spliced video and called for TikTok to take action\"",
        "stakeholder": "\"TikTok users\"",
        "impact_duration": "\"Short-term\"",
        "output": "\"A spliced video that appears to show a TikTok user beheading someone\"",
        "indicator_report": null,
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tiktok-beheading-video-splicing\"",
        "program": null,
        "impact_risk": "\"The splicing of violent videos onto other videos could lead to widespread distress and anger, and could also be used to spread misinformation or propaganda\"",
        "activity": "\"Splicing of a beheading video onto a TikTok video\"",
        "organization": "\"TikTok\"",
        "outcome": "\"The spliced video was shared on TikTok and caused widespread distress and anger\"",
        "impact_depth": "\"Medium\"",
        "impact_report": null
    },
    {
        "impact_model": "\"Common Impact Data Standard\"",
        "impact_scale": "\"Global\"",
        "indicator": "\"Number of people whose photos and videos have been found by Pimeyes\"",
        "input": "\"Data on millions of people's faces\"",
        "service": "\"Facial recognition\"",
        "title": "\"Pimey's facial recognition search engine\"",
        "stakeholder_outcome": "\"People whose photos and videos have been found by Pimeyes have felt violated and have called for Pimeyes to be shut down\"",
        "stakeholder": "\"People whose photos and videos have been found by Pimeyes\"",
        "impact_duration": "\"Medium-term\"",
        "output": "\"A facial recognition search engine that can be used to find people's photos and videos online\"",
        "indicator_report": null,
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/pimeyes-facial-recognition-search-engine\"",
        "program": null,
        "impact_risk": "\"The use of facial recognition technology without consent could lead to privacy violations, discrimination, and other harms\"",
        "activity": "\"Development and release of Pimeyes facial recognition search engine\"",
        "organization": "\"Pimey\"",
        "outcome": "\"The Pimeyes facial recognition search engine has been used to find people's photos and videos without their consent\"",
        "impact_depth": "\"Medium\"",
        "impact_report": null
    },
    {
        "impact_model": "\"Common Impact Data Standard\"",
        "impact_scale": "\"Local\"",
        "indicator": "\"Number of Tinder users whose data was scraped\"",
        "input": "\"Data on 40,000 Tinder users\"",
        "service": "\"Data scraping\"",
        "title": "\"People of Tinder dataset\"",
        "stakeholder_outcome": "\"Tinder users felt violated by the scraping of their data and called for Colianni to be held accountable\"",
        "stakeholder": "\"Tinder users\"",
        "impact_duration": "\"Short-term\"",
        "output": "\"A dataset of 40,000 Tinder users' faces\"",
        "indicator_report": null,
        "source": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/people-of-tinder-dataset\"",
        "program": null,
        "impact_risk": "\"The scraping of data from dating apps could lead to privacy violations, discrimination, and other harms\"",
        "activity": "\"Scraping of data from Tinder\"",
        "organization": "\"Stuart Colianni\"",
        "outcome": "\"The dataset was used to train a facial recognition model\"",
        "impact_depth": "\"Medium\"",
        "impact_report": null
    },
    {
        "impact_model": "\"Common Impact Data Standard\"",
        "impact_scale": "\"Local\"",
        "indicator": "\"Number of people who have been denied parole due to COMPAS\"",
        "input": "\"Data on Titus Henderson's criminal history and risk factors\"",
        "service": "\"Risk assessment\"",
        "title": "\"Titus Henderson denied parole due to COMPAS\"",
        "stakeholder_outcome": "\"Henderson was denied parole and remains incarcerated\"",
        "stakeholder": "\"Titus Henderson\"",
        "impact_duration": "\"Long-term\"",
        "output": "\"A risk assessment score that indicated that Henderson was a high risk to reoffend\"",
        "indicator_report": null,
        "source": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/titus-henderson-compas-parole-denial\"",
        "program": null,
        "impact_risk": "\"The use of risk assessment tools like COMPAS could lead to biased and unfair outcomes, such as the denial of parole for people who are not actually a risk to reoffend\"",
        "activity": "\"Use of COMPAS risk assessment tool\"",
        "organization": "\"North Carolina Department of Public Safety\"",
        "outcome": "\"Henderson's parole was denied based on the COMPAS risk assessment score\"",
        "impact_depth": "\"Medium\"",
        "impact_report": null
    },
    {
        "impact_model": "\"Common Impact Data Standard\"",
        "impact_scale": "\"Global\"",
        "indicator": "\"Number of LGBTQ users on TikTok who felt marginalized or discriminated against due to the shadowbanning\"",
        "input": "\"LGBTQ content on TikTok\"",
        "service": "\"Social media\"",
        "title": "\"TikTok LGBTQ shadowbanning\"",
        "stakeholder_outcome": "\"LGBTQ users on TikTok felt marginalized and discriminated against\"",
        "stakeholder": "\"LGBTQ users on TikTok\"",
        "impact_duration": "\"Medium-term\"",
        "output": "\"LGBTQ content on TikTok that is not visible to users who are not LGBTQ\"",
        "indicator_report": null,
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tiktok-lgbtq-shadowbanning\"",
        "program": null,
        "impact_risk": "\"The shadowbanning of LGBTQ content on TikTok could lead to discrimination, harassment, and other harms against LGBTQ users\"",
        "activity": "\"Shadowbanning of LGBTQ content on TikTok\"",
        "organization": "\"ByteDance\"",
        "outcome": "\"LGBTQ users on TikTok felt marginalized and discriminated against\"",
        "impact_depth": "\"Medium\"",
        "impact_report": null
    },
    {
        "impact_model": "\"Common Impact Data Standard\"",
        "impact_scale": "\"Global\"",
        "indicator": "\"Number of people who are concerned about the potential for 'character' clones to be used for malicious purposes\"",
        "input": "\"Data on human faces and voices\"",
        "service": "\"Artificial intelligence\"",
        "title": "\"Hour One 'character' clones\"",
        "stakeholder_outcome": "\"The public is concerned about the potential for 'character' clones to be used for malicious purposes, such as spreading misinformation or propaganda, or to invade people's privacy\"",
        "stakeholder": "\"The public\"",
        "impact_duration": "\"Medium-term\"",
        "output": "\"Digital clones of human beings that can speak and move like the originals\"",
        "indicator_report": null,
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/hour-one-character-clones\"",
        "program": null,
        "impact_risk": "\"The creation of 'character' clones could lead to privacy violations, discrimination, and other harms\"",
        "activity": "\"Creation of 'character' clones using artificial intelligence\"",
        "organization": "\"Hour One\"",
        "outcome": "\"The creation of 'character' clones has raised concerns about privacy, ethics, and the potential for misuse\"",
        "impact_depth": "\"Medium\"",
        "impact_report": null
    },
    {
        "impact_model": "\"Common Impact Data Standard\"",
        "impact_scale": "\"Local\"",
        "indicator": "\"Number of Roomba users who are satisfied with their Roombas\"",
        "input": "\"Data on Roomba users' homes and cleaning habits\"",
        "service": "\"Robotics\"",
        "title": "\"iRobot Roomba data annotation sharing\"",
        "stakeholder_outcome": "\"Roomba users are more satisfied with their Roombas\"",
        "stakeholder": "\"Roomba users\"",
        "impact_duration": "\"Medium-term\"",
        "output": "\"Improved Roomba navigation and cleaning algorithms\"",
        "indicator_report": null,
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/roomba-robot-vacuum-data-annotation-sharing\"",
        "program": null,
        "impact_risk": "\"The sharing of personal data with third-party contractors could lead to privacy violations\"",
        "activity": "\"Data annotation sharing between iRobot and third-party contractors\"",
        "organization": "\"iRobot\"",
        "outcome": "\"Improved Roomba performance has led to increased customer satisfaction\"",
        "impact_depth": "\"Medium\"",
        "impact_report": null
    },
    {
        "impact_model": "\"Common Impact Data Standard\"",
        "impact_scale": "\"Global\"",
        "indicator": "\"Number of people who have used Microsoft Bing Chat\"",
        "input": "\"Data on human conversations\"",
        "service": "\"Chatbot\"",
        "title": "\"Microsoft Bing Chat\"",
        "stakeholder_outcome": "\"Microsoft customers have found Microsoft Bing Chat to be helpful and informative. Microsoft employees have found Microsoft Bing Chat to be a valuable tool for customer service and research.\"",
        "stakeholder": "\"Microsoft customers, users, and employees\"",
        "impact_duration": "\"Long-term\"",
        "output": "\"A chatbot that can hold conversations with humans\"",
        "indicator_report": null,
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/microsoft-bing-chat\"",
        "program": null,
        "impact_risk": "\"The use of chatbots could lead to job losses for customer service representatives and other workers. Chatbots could also be used to spread misinformation and propaganda.\"",
        "activity": "\"Development of Microsoft Bing Chat\"",
        "organization": "\"Microsoft\"",
        "outcome": "\"Microsoft Bing Chat has been used to provide customer service, answer questions, and generate creative content.\"",
        "impact_depth": "\"Medium\"",
        "impact_report": null
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Local",
        "indicator": "Number of users affected",
        "input": "User chat data",
        "service": "Chatbot",
        "title": "ChatGPT bug reveals user chat histories",
        "stakeholder_outcome": "Users' privacy was compromised",
        "stakeholder": "Users",
        "impact_duration": "Short-term",
        "output": "Exposed user chat histories",
        "indicator_report": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/chatgpt-bug-reveals-user-chat-histories#indicator-report",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/chatgpt-bug-reveals-user-chat-histories",
        "program": "ChatGPT",
        "impact_risk": "High",
        "activity": "Data breach",
        "organization": "OpenAI",
        "outcome": "User privacy compromised",
        "impact_depth": "Medium",
        "impact_report": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/chatgpt-bug-reveals-user-chat-histories"
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Global",
        "indicator": "Number of anti-Muslim outputs generated by GPT-3",
        "input": "Data used to train GPT-3",
        "service": "Large language model",
        "title": "GPT-3 anti-Muslim bias",
        "stakeholder_outcome": "Muslims are harmed by the spread of anti-Muslim content generated by GPT-3",
        "stakeholder": "Muslims",
        "impact_duration": "Long-term",
        "output": "GPT-3 model that exhibits anti-Muslim bias",
        "indicator_report": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/gpt-3-anti-muslim-bias#indicator-report",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/gpt-3-anti-muslim-bias",
        "program": "GPT-3",
        "impact_risk": "High",
        "activity": "Data bias",
        "organization": "OpenAI",
        "outcome": "Muslims are harmed by the spread of anti-Muslim content generated by GPT-3",
        "impact_depth": "Medium",
        "impact_report": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/gpt-3-anti-muslim-bias"
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Local",
        "indicator": "Number of mom-and-pop stores displaced by Bodega AI stores",
        "input": "Mom-and-pop stores",
        "service": "Automated retail",
        "title": "Bodega AI automated mom-and-pop stores",
        "stakeholder_outcome": "Mom-and-pop store owners may be displaced by Bodega AI stores",
        "stakeholder": "Mom-and-pop store owners",
        "impact_duration": "Medium-term",
        "output": "Automated Bodega AI stores",
        "indicator_report": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/bodega-ai-automated-mom-and-pop-stores#indicator-report",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/bodega-ai-automated-mom-and-pop-stores",
        "program": "Bodega AI",
        "impact_risk": "Medium",
        "activity": "Business model",
        "organization": "Bodega AI",
        "outcome": "Potential displacement of mom-and-pop stores",
        "impact_depth": "Medium",
        "impact_report": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/bodega-ai-automated-mom-and-pop-stores"
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Local",
        "indicator": "Number of women who feel unsafe or frustrated after reporting a crime to Knightscope HP Robocop",
        "input": "Woman reporting a crime",
        "service": "Security robot",
        "title": "Knightscope HP Robocop ignores woman reporting crime",
        "stakeholder_outcome": "Woman feels unsafe and frustrated",
        "stakeholder": "Woman",
        "impact_duration": "Short-term",
        "output": "Knightscope HP Robocop ignores woman's report",
        "indicator_report": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/knightscope-hp-robocop-ignores-woman-reporting-crime#indicator-report",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/knightscope-hp-robocop-ignores-woman-reporting-crime",
        "program": "Knightscope HP Robocop",
        "impact_risk": "Medium",
        "activity": "Crime reporting",
        "organization": "Knightscope",
        "outcome": "Woman feels unsafe and frustrated",
        "impact_depth": "Medium",
        "impact_report": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/knightscope-hp-robocop-ignores-woman-reporting-crime"
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Local",
        "indicator": "Number of people who see the Google Images results that link music promoter to criminal underworld",
        "input": "Image of music promoter",
        "service": "Image search",
        "title": "Google Images links music promoter to criminal underworld",
        "stakeholder_outcome": "Music promoter's reputation is damaged",
        "stakeholder": "Music promoter",
        "impact_duration": "Medium-term",
        "output": "Google Images results that link music promoter to criminal underworld",
        "indicator_report": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/google-images-links-music-promoter-to-criminal-underworld#indicator-report",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/google-images-links-music-promoter-to-criminal-underworld",
        "program": "Google Images",
        "impact_risk": "Medium",
        "activity": "Image search results",
        "organization": "Google",
        "outcome": "Music promoter's reputation is damaged",
        "impact_depth": "Medium",
        "impact_report": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/google-images-links-music-promoter-to-criminal-underworld"
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Local",
        "indicator": "Number of Black Lives Matter protests that were surveilled by Ring cameras",
        "input": "Black Lives Matter protests",
        "service": "Home security camera",
        "title": "Amazon Ring BLM protest surveillance",
        "stakeholder_outcome": "Black communities are increasingly surveilled",
        "stakeholder": "Black communities",
        "impact_duration": "Medium-term",
        "output": "Ring cameras used to surveil Black Lives Matter protests",
        "indicator_report": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/amazon-ring-blm-protest-surveillance#indicator-report",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/amazon-ring-blm-protest-surveillance",
        "program": "Ring",
        "impact_risk": "High",
        "activity": "Surveillance",
        "organization": "Amazon",
        "outcome": "Increased surveillance of Black communities",
        "impact_depth": "Medium",
        "impact_report": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/amazon-ring-blm-protest-surveillance"
    },
    {
        "Impact Scale": "Local",
        "Impact Duration": "Medium",
        "Indicator": "The number of Amazon customers who use Amazon One",
        "Service": "Palmprint Biometrics",
        "Indicator Report": "Amazon has not published an indicator report for Amazon One",
        "Impact Depth": "Medium",
        "Stakeholder": "Amazon customers",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/amazon-one-palmprint-biometrics",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "Amazon has not published an impact report for Amazon One",
        "Output": "A database of palmprint biometric data",
        "Program": "Amazon One",
        "Stakeholder Outcome": "Increased security and convenience",
        "Activity": "Collection and use of palmprint biometric data",
        "impact_risk": "The use of palmprint biometric data could pose a risk to customer privacy",
        "Organization": "Amazon",
        "Outcome": "Improved security and convenience for Amazon customers",
        "Input": "Palmprints of Amazon customers",
        "Title of the story": "Amazon One Palmprint Biometrics"
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Local",
        "indicator": "Amazon Delivery Driver Performance Score",
        "input": "Amazon Mentor DSP Delivery Driver Data",
        "service": "Amazon Mentor DSP Delivery Driver Service",
        "title": "Amazon Mentor DSP Delivery Driver Scoring",
        "stakeholder_outcome": "Improved Amazon Delivery Driver Performance",
        "stakeholder": "Amazon Delivery Drivers",
        "impact_duration": "Short-term",
        "output": "Amazon Mentor DSP Delivery Driver Scores",
        "indicator_report": "Amazon Mentor DSP Delivery Driver Performance Report",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/amazon-mentor-dsp-delivery-driver-scoring",
        "program": "Amazon Mentor DSP Delivery Driver Program",
        "impact_risk": "Potential for Amazon Delivery Drivers to be unfairly penalized by the scoring algorithm",
        "activity": "Amazon Mentor DSP Delivery Driver Scoring Activity",
        "organization": "Amazon",
        "outcome": "Amazon Mentor DSP Delivery Driver Performance Improvement",
        "impact_depth": "Medium",
        "impact_report": "Amazon Mentor DSP Delivery Driver Impact Report"
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Local",
        "indicator": "Number of Anime Art Images Generated",
        "input": "Anime Art Data",
        "service": "Mimic AI Anime Art Generator Service",
        "title": "Mimic Anime Art Generator",
        "stakeholder_outcome": "Increased Ability to Create and Enjoy Anime Art",
        "stakeholder": "Anime Fans",
        "impact_duration": "Short-term",
        "output": "Generated Anime Art",
        "indicator_report": "Mimic AI Anime Art Generator Performance Report",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/mimic-anime-art-generator",
        "program": "Mimic AI Anime Art Generator Program",
        "impact_risk": "Potential for the AI system to be used to create harmful or offensive content",
        "activity": "Mimic AI Anime Art Generator Activity",
        "organization": "Mimic AI",
        "outcome": "Improved Access to Anime Art",
        "impact_depth": "Medium",
        "impact_report": "Mimic AI Anime Art Generator Impact Report"
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Local",
        "indicator": "Number of Welfare Payments Processed",
        "input": "Welfare Payments Data",
        "service": "Udbetaling Danmark Welfare Payments Optimisation Service",
        "title": "Udbetaling Danmark Welfare Payments Optimisation",
        "stakeholder_outcome": "Reduced Waiting Times for Welfare Payments",
        "stakeholder": "Welfare Recipients",
        "impact_duration": "Long-term",
        "output": "Optimized Welfare Payments",
        "indicator_report": "Udbetaling Danmark Welfare Payments Optimisation Performance Report",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/udbetaling-danmark-welfare-payments-optimisation",
        "program": "Udbetaling Danmark Welfare Payments Optimisation Program",
        "impact_risk": "Potential for the AI system to be used to unfairly target welfare recipients",
        "activity": "Udbetaling Danmark Welfare Payments Optimisation Activity",
        "organization": "Udbetaling Danmark",
        "outcome": "Improved Efficiency of Welfare Payments",
        "impact_depth": "Medium",
        "impact_report": "Udbetaling Danmark Welfare Payments Optimisation Impact Report"
    },
    {
        "impact_model": "Common Impact Data Standard",
        "impact_scale": "Local",
        "indicator": "Number of Properties with Automated Rent Estimates",
        "input": "Property Data, Rental Market Data",
        "service": "YieldStar Automated Rent Setting Service",
        "title": "YieldStar Automated Rent Setting",
        "stakeholder_outcome": "Reduced Time and Cost of Rent Setting",
        "stakeholder": "Property Owners, Landlords",
        "impact_duration": "Long-term",
        "output": "Automated Rent Estimates",
        "indicator_report": "YieldStar Automated Rent Setting Performance Report",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/yieldstar-automated-rent-setting",
        "program": "YieldStar Automated Rent Setting Program",
        "impact_risk": "Potential for the AI system to be used to unfairly set rents",
        "activity": "YieldStar Automated Rent Setting Activity",
        "organization": "YieldStar",
        "outcome": "Improved Efficiency of Rent Setting",
        "impact_depth": "Medium",
        "impact_report": "YieldStar Automated Rent Setting Impact Report"
    },
    {
        "impact_model": "AI algorithmic risks harms taxonomy",
        "impact_scale": "national",
        "indicator": "number of imposters identified by facial recognition",
        "input": "traveller biometrics",
        "service": "facial recognition",
        "title": "US Border Imposter Identification Failures",
        "stakeholder_outcome": "increased risk of being denied entry to the US",
        "stakeholder": "travelers",
        "impact_duration": "short-term",
        "output": "facial recognition scans",
        "indicator_report": "US Government Accountability Office (GAO) report (pdf)",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/us-border-imposter-identification-failures",
        "program": "US border security",
        "impact_risk": "ineffectiveness of facial recognition technology",
        "activity": "imposter identification",
        "organization": "Department of Homeland Security (DHS); Customs and Border Protection (CBP)",
        "outcome": "failure to identify imposters",
        "impact_depth": "medium",
        "impact_report": "OneZero report (pdf)"
    },
    {
        "impact_model": "AI algorithmic risks harms taxonomy",
        "impact_scale": "national",
        "indicator": "number of misclassified images",
        "input": "OkCupid user data",
        "service": "image classification",
        "title": "Clarifai OkCupid Dataset Appropriation",
        "stakeholder_outcome": "loss of privacy and potential discrimination",
        "stakeholder": "OkCupid users",
        "impact_duration": "long-term",
        "output": "trained AI models",
        "indicator_report": "The Information report (pdf)",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/clarifai-okcupid-dataset-appropriation",
        "program": "Clarifai's AI platform",
        "impact_risk": "data privacy and security",
        "activity": "training AI models",
        "organization": "Clarifai",
        "outcome": "misclassification of images",
        "impact_depth": "medium",
        "impact_report": "The Information report (pdf)"
    },
    {
        "impact_model": "AI algorithmic risks harms taxonomy",
        "impact_scale": "national",
        "indicator": "number of incorrect debt notices issued",
        "input": "welfare recipient data",
        "service": "debt recovery",
        "title": "Robodebt Welfare Debt Recovery",
        "stakeholder_outcome": "financial hardship, stress, and anxiety",
        "stakeholder": "welfare recipients",
        "impact_duration": "long-term",
        "output": "debt notices",
        "indicator_report": "Australian Human Rights Commission report (pdf)",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/robodebt-welfare-debt-recovery",
        "program": "Welfare administration",
        "impact_risk": "algorithmic bias and discrimination",
        "activity": "using algorithms to identify potential welfare debts",
        "organization": "Australian Taxation Office (ATO)",
        "outcome": "incorrect debt recovery",
        "impact_depth": "medium",
        "impact_report": "Australian Human Rights Commission report (pdf)"
    },
    {
        "impact_model": "AI algorithmic risks harms taxonomy",
        "impact_scale": "National",
        "indicator": "Number of ads approved for teens",
        "input": "Advertiser data",
        "service": "Ad review",
        "title": "Facebook Teen Alcohol, Drug, Gambling Ads Approvals",
        "stakeholder_outcome": "Increased risk of substance abuse and gambling addiction",
        "stakeholder": "Teens",
        "impact_duration": "Short-term",
        "output": "Approved ads",
        "indicator_report": "Reset Australia report (pdf)",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/facebook-teen-alcohol-drug-gambling-ads-approvals",
        "program": "Advertising",
        "impact_risk": "Algorithmic bias and discrimination",
        "activity": "Approving ads",
        "organization": "Meta/Facebook",
        "outcome": "Teens exposed to harmful ads",
        "impact_depth": "Medium",
        "impact_report": "Reset Australia report (pdf)"
    },
    {
        "impact_model": "AI algorithmic risks harms taxonomy",
        "impact_scale": "International",
        "indicator": "Number of deepfake videos created and distributed",
        "input": "Deepfake technology",
        "service": "Deepfakes",
        "title": "Pro-China Deepfake Spamouflage Campaign",
        "stakeholder_outcome": "Increased support for China",
        "stakeholder": "Public opinion",
        "impact_duration": "Long-term",
        "output": "Deepfake videos",
        "indicator_report": "New America report (pdf)",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/pro-china-deepfake-spamouflage-campaign",
        "program": "Propaganda",
        "impact_risk": "Misinformation and disinformation",
        "activity": "Creating and distributing deepfakes",
        "organization": "Unknown",
        "outcome": "Spread of pro-China propaganda",
        "impact_depth": "Medium",
        "impact_report": "New America report (pdf)"
    },
    {
        "impact_model": "AI algorithmic risks harms taxonomy",
        "impact_scale": "National",
        "indicator": "Number of claims videos assessed by AI",
        "input": "Claims videos",
        "service": "Claims processing",
        "title": "Lemonade Non-Verbal Assessments",
        "stakeholder_outcome": "Potential for discrimination and bias",
        "stakeholder": "Lemonade customers",
        "impact_duration": "Short-term",
        "output": "AI assessments",
        "indicator_report": "The Information report (pdf)",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/lemonade-non-verbal-assessments",
        "program": "Insurance",
        "impact_risk": "Algorithmic bias and discrimination",
        "activity": "Using AI to assess non-verbal cues in claims videos",
        "organization": "Lemonade",
        "outcome": "Increased scrutiny of Lemonade's AI claims processing",
        "impact_depth": "Medium",
        "impact_report": "The Information report (pdf)"
    },
    {
        "impact_model": "AI algorithmic risks harms taxonomy",
        "impact_scale": "Global",
        "indicator": "Number of images generated by Midjourney",
        "input": "User prompts",
        "service": "Image generation",
        "title": "Midjourney Image Generator",
        "stakeholder_outcome": "New opportunities for creativity and expression",
        "stakeholder": "Art enthusiasts",
        "impact_duration": "Long-term",
        "output": "Generated images",
        "indicator_report": "No indicator reports yet",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/midjourney-image-generator",
        "program": "Artificial Intelligence",
        "impact_risk": "Algorithmic bias and discrimination",
        "activity": "Creating images with AI",
        "organization": "OpenAI",
        "outcome": "Increased interest in AI art",
        "impact_depth": "Medium",
        "impact_report": "No impact reports yet"
    },
    {
        "impact_model": "AI algorithmic risks harms taxonomy",
        "impact_scale": "International",
        "indicator": "Number of Tesla Autopilot-related accidents",
        "input": "Tesla Autopilot",
        "service": "Tesla Autopilot",
        "title": "Tesla Paris Fatal Crash",
        "stakeholder_outcome": "Potential for loss of life and property damage",
        "stakeholder": "Tesla customers",
        "impact_duration": "Long-term",
        "output": "Fatal crash",
        "indicator_report": "No indicator reports yet",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-paris-fatal-crash",
        "program": "Autonomous driving",
        "impact_risk": "Autonomous driving",
        "activity": "Using Tesla Autopilot",
        "organization": "Tesla",
        "outcome": "Increased scrutiny of Tesla Autopilot",
        "impact_depth": "Medium",
        "impact_report": "No impact reports yet"
    },
    {
        "impact_model": "AI algorithmic risks harms taxonomy",
        "impact_scale": "International",
        "indicator": "Number of advertisers affected by the downranking",
        "input": "Ad data",
        "service": "Ad delivery",
        "title": "Facebook Downranking System Failure",
        "stakeholder_outcome": "Financial loss",
        "stakeholder": "Advertisers",
        "impact_duration": "Short-term",
        "output": "Ranked ads",
        "indicator_report": "No indicator reports yet",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/facebook-downranking-system-failure",
        "program": "Advertising",
        "impact_risk": "Algorithmic bias and discrimination",
        "activity": "Using AI to rank ads",
        "organization": "Meta/Facebook",
        "outcome": "Reduced reach and visibility for some advertisers",
        "impact_depth": "Medium",
        "impact_report": "No impact reports yet"
    },
    {
        "impact_model": "AI algorithmic risks harms taxonomy",
        "impact_scale": "National",
        "indicator": "Percentage of black and Asian people whose passport photos are rejected",
        "input": "Passport photo",
        "service": "Passport photo application",
        "title": "UK Passport Photo Application Racism",
        "stakeholder_outcome": "Disruption to travel plans, financial loss, and emotional distress",
        "stakeholder": "Black and Asian people",
        "impact_duration": "Long-term",
        "output": "Decision on whether the photo is acceptable",
        "indicator_report": "No indicator reports yet",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/uk-passport-photo-application-racism",
        "program": "Identity and Passports",
        "impact_risk": "Algorithmic bias and discrimination",
        "activity": "Using AI to review passport photos",
        "organization": "UK Passport Office",
        "outcome": "Black and Asian people more likely to have their passport photos rejected",
        "impact_depth": "Medium",
        "impact_report": "No impact reports yet"
    },
    {
        "impact_model": "AI algorithmic risks harms taxonomy",
        "impact_scale": "International",
        "indicator": "Percentage of people of color who are misidentified by pedestrian detection algorithms",
        "input": "Wildlife footage",
        "service": "Pedestrian detection",
        "title": "WildTrack Pedestrian Detection Dataset",
        "stakeholder_outcome": "Increased risk of being misidentified and harmed by autonomous vehicles",
        "stakeholder": "People of color",
        "impact_duration": "Long-term",
        "output": "Pedestrian detection dataset",
        "indicator_report": "No indicator reports yet",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/wildtrack-pedestrian-detection-dataset",
        "program": "Wildlife conservation",
        "impact_risk": "Algorithmic bias and discrimination",
        "activity": "Creating a pedestrian detection dataset",
        "organization": "WildTrack",
        "outcome": "Pedestrian detection algorithms are biased against people of color",
        "impact_depth": "Medium",
        "impact_report": "No impact reports yet"
    },
    {
        "impact_model": "AI algorithmic risks harms taxonomy",
        "impact_scale": "National",
        "indicator": "Number of links to rival apps blocked by Tencent",
        "input": "Links to rival apps",
        "service": "WeChat",
        "title": "Tencent App Link Blocking",
        "stakeholder_outcome": "Financial loss",
        "stakeholder": "Rival app developers",
        "impact_duration": "Long-term",
        "output": "Blocked links",
        "indicator_report": "No indicator reports yet",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tencent-app-link-blocking",
        "program": "Social media",
        "impact_risk": "Monopolization",
        "activity": "Blocking links to rival apps",
        "organization": "Tencent",
        "outcome": "Reduced visibility and engagement for rival apps",
        "impact_depth": "Medium",
        "impact_report": "No impact reports yet"
    },
    {
        "impact_model": "AI algorithmic risks harms taxonomy",
        "impact_scale": "Local",
        "indicator": "Number of robot collisions",
        "input": "Robots",
        "service": "Warehouse automation",
        "title": "Ocado Robot Collision",
        "stakeholder_outcome": "Disruption to shopping plans and financial loss",
        "stakeholder": "Ocado customers",
        "impact_duration": "Short-term",
        "output": "Collision",
        "indicator_report": "No indicator reports yet",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/ocado-robot-collision",
        "program": "Retail",
        "impact_risk": "Safety",
        "activity": "Using robots to pick and pack orders",
        "organization": "Ocado",
        "outcome": "Fire, evacuation, and cancellation of customer orders",
        "impact_depth": "Medium",
        "impact_report": "No impact reports yet"
    },
    {
        "impact_model": "AI algorithmic risks harms taxonomy",
        "impact_scale": "Local",
        "indicator": "Number of young black men stopped and searched by the police",
        "input": "Data on gang members",
        "service": "Gangs violence",
        "title": "Met Police Gangs Violence Matrix",
        "stakeholder_outcome": "Increased distrust of the police and a sense of being unfairly targeted",
        "stakeholder": "Young black men",
        "impact_duration": "Long-term",
        "output": "Gangs violence matrix",
        "indicator_report": "No indicator reports yet",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/met-police-gangs-violence-matrix",
        "program": "Crime prevention",
        "impact_risk": "Discrimination",
        "activity": "Using AI to track gang members",
        "organization": "Metropolitan Police Service",
        "outcome": "Increased surveillance and stop-and-search of young black men",
        "impact_depth": "Medium",
        "impact_report": "No impact reports yet"
    },
    {
        "impact_model": "AI algorithmic risks harms taxonomy",
        "impact_scale": "National",
        "indicator": "Number of people killed in accidents involving Tesla cars with Autopilot",
        "input": "Data from sensors and cameras",
        "service": "Autopilot",
        "title": "Tesla Rear-Ends Kawasaki Motorcycle, Kills Rider",
        "stakeholder_outcome": "Death",
        "stakeholder": "Motorcycle rider",
        "impact_duration": "Long-term",
        "output": "Decision to accelerate or brake",
        "indicator_report": "https://www.nhtsa.gov/technology-innovation/automated-vehicles/disengagement-report",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-rear-ends-kawasaki-motorcycle-kills-rider",
        "program": "Autonomous driving",
        "impact_risk": "Safety",
        "activity": "Using AI to control a car",
        "organization": "Tesla",
        "outcome": "Tesla car rear-ends a motorcycle, killing the rider",
        "impact_depth": "Medium",
        "impact_report": "No impact reports yet"
    },
    {
        "impact_model": "AI algorithmic risks harms taxonomy",
        "impact_scale": "Local",
        "indicator": "Number of people killed or injured by AI-powered robots",
        "input": "Data on potential threats",
        "service": "Robotics",
        "title": "San Francisco Police Killer Robots",
        "stakeholder_outcome": "Increased anxiety and stress, and decreased willingness to cooperate with the police",
        "stakeholder": "Public",
        "impact_duration": "Long-term",
        "output": "Decision to deploy a robot",
        "indicator_report": "No indicator reports yet",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/san-francisco-police-killer-robots",
        "program": "Public safety",
        "impact_risk": "Safety, discrimination, and public distrust",
        "activity": "Using AI-powered robots for law enforcement",
        "organization": "San Francisco Police Department",
        "outcome": "Increased public fear and distrust of the police",
        "impact_depth": "Medium",
        "impact_report": "No impact reports yet"
    },
    {
        "impact_model": "AI algorithmic risks harms taxonomy",
        "impact_scale": "Local",
        "indicator": "Number of people killed or injured by the robot",
        "input": "Data on potential threats",
        "service": "Robotics",
        "title": "NYPD Digidog",
        "stakeholder_outcome": "Increased anxiety and stress, and decreased willingness to cooperate with the police",
        "stakeholder": "Public",
        "impact_duration": "Long-term",
        "output": "Decision to deploy the robot",
        "indicator_report": "No indicator reports yet",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/nypd-digidog",
        "program": "Public safety",
        "impact_risk": "Safety, discrimination, and public distrust",
        "activity": "Deploying a robotic dog",
        "organization": "New York Police Department",
        "outcome": "Increased public fear and distrust of the police",
        "impact_depth": "Medium",
        "impact_report": "No impact reports yet"
    },
    {
        "impact_model": "AI algorithmic risks harms taxonomy",
        "impact_scale": "Local",
        "indicator": "Number of people who have been falsely identified by AI-powered facial recognition software",
        "input": "Data on potential threats",
        "service": "Security",
        "title": "Livonia Skating Rink Misidentifies Black Teenager",
        "stakeholder_outcome": "Humiliation, emotional distress, and financial loss",
        "stakeholder": "Teenager",
        "impact_duration": "Short-term",
        "output": "Decision to detain a teenager",
        "indicator_report": "https://www.aclu.org/report/face-recognition-report-2022",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/livonia-skating-rink-misidentifies-black-teenager",
        "program": "Recreation",
        "impact_risk": "Discrimination, public distrust, and harm to individuals",
        "activity": "Using AI-powered facial recognition software to identify potential threats",
        "organization": "Livonia Ice Skating Center",
        "outcome": "Teenager was detained for several hours and falsely accused of theft",
        "impact_depth": "Medium",
        "impact_report": "No impact reports yet"
    },
    {
        "Impact Scale": "Local",
        "Impact Duration": "Ongoing",
        "Indicator": "Accuracy of Frasers Group's facial recognition technology",
        "Service": "Frasers Group facial recognition service",
        "Indicator Report": "Frasers Group facial recognition indicator report",
        "Impact Depth": "Medium",
        "Stakeholder": "Customers and employees of Frasers Group properties",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/frasers-group-facial-recognition",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "Frasers Group facial recognition impact report",
        "Output": "Identification of individuals by Frasers Group's facial recognition technology",
        "Program": "Frasers Group facial recognition program",
        "Stakeholder Outcome": "Improved security and safety for customers and employees of Frasers Group properties",
        "Activity": "Use of facial recognition technology by Frasers Group",
        "impact_risk": "Potential for misuse of facial recognition technology",
        "Organization": "Frasers Group",
        "Outcome": "Increased security and safety at Frasers Group properties",
        "Input": "Data collected by Frasers Group's facial recognition technology",
        "Title of the story": "Frasers Group facial recognition controversy"
    },
    {
        "Impact Scale": "Local",
        "Impact Duration": "Ongoing",
        "Indicator": "Accuracy of Tesla Autopilot driving software",
        "Service": "Tesla Autopilot driving software",
        "Indicator Report": "Tesla Autopilot indicator report",
        "Impact Depth": "Medium",
        "Stakeholder": "Drivers and passengers of Tesla vehicles",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-model-y-crashes-into-tractor-trailer",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "Tesla Autopilot impact report",
        "Output": "Identification of objects and obstacles by Tesla Autopilot driving software",
        "Program": "Tesla Autopilot",
        "Stakeholder Outcome": "Increased risk of injury or death for drivers and passengers of Tesla vehicles",
        "Activity": "Use of Tesla Autopilot driving software",
        "impact_risk": "Potential for misuse of Tesla Autopilot driving software",
        "Organization": "Tesla",
        "Outcome": "Crashes involving Tesla Autopilot driving software",
        "Input": "Data collected by Tesla Autopilot driving software",
        "Title of the story": "Tesla Model Y crashes into tractor-trailer"
    },
    {
        "Impact Scale": "Local",
        "Impact Duration": "Ongoing",
        "Indicator": "Accuracy of Microsoft reincarnation chatbot technology",
        "Service": "Microsoft chatbot technology",
        "Indicator Report": "Microsoft reincarnation chatbot indicator report",
        "Impact Depth": "Medium",
        "Stakeholder": "Individuals who have lost loved ones",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/microsoft-reincarnation-chatbot",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "Microsoft reincarnation chatbot impact report",
        "Output": "Creation of chatbots that can \"imitate\" the personality of deceased individuals",
        "Program": "Microsoft reincarnation chatbot",
        "Stakeholder Outcome": "Potential for chatbots to provide \"comfort\" and \"closure\" to people who have lost loved ones",
        "Activity": "Development and use of Microsoft reincarnation chatbot",
        "impact_risk": "Potential for chatbots to be used to \"exploit\" or \"deceive\" people",
        "Organization": "Microsoft",
        "Outcome": "Potential for chatbots to be used to \"exploit\" or \"deceive\" people",
        "Input": "Data collected by Microsoft reincarnation chatbot",
        "Title of the story": "Microsoft's reincarnation chatbot raises ethical concerns"
    },
    {
        "Impact Scale": "Local",
        "Impact Duration": "Ongoing",
        "Indicator": "Accuracy of Bayraktar TB2 drone technology",
        "Service": "Bayraktar TB2 drone technology",
        "Indicator Report": "Turkish Aerospace Industries Bayraktar TB2 drone indicator report",
        "Impact Depth": "Medium",
        "Stakeholder": "Civilians in the \"Tigray\" region of Ethiopia",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/ethiopia-bayraktar-tb2-drone-tigray-school-attack",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "Turkish Aerospace Industries Bayraktar TB2 drone impact report",
        "Output": "Identification of targets and delivery of munitions by Bayraktar TB2 drones",
        "Program": "Bayraktar TB2 drone program",
        "Stakeholder Outcome": "Increased risk of injury or death, displacement, and loss of livelihood for civilians in the \"Tigray\" region of Ethiopia",
        "Activity": "Use of Bayraktar TB2 drones in the Ethiopian Tigray conflict",
        "impact_risk": "Potential for Bayraktar TB2 drones to be used to cause civilian casualties and destruction of infrastructure",
        "Organization": "Turkish Aerospace Industries",
        "Outcome": "Civilian casualties and destruction of infrastructure in the \"Tigray\" region of Ethiopia",
        "Input": "Data collected by Bayraktar TB2 drones",
        "Title of the story": "Turkish Bayraktar TB2 drones used in Ethiopia's Tigray conflict"
    },
    {
        "Impact Scale": "Local",
        "Impact Duration": "Ongoing",
        "Indicator": "Accuracy of facial recognition technology.",
        "Service": "Facial recognition technology",
        "Indicator Report": "Microsoft MS-Celeb-1M facial recognition dataset indicator report.",
        "Impact Depth": "Medium",
        "Stakeholder": "Individuals whose images are included in the MS-Celeb-1M dataset.",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/microsoft-celeb-ms-celeb-1m-facial-recognition-dataset",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "Microsoft MS-Celeb-1M facial recognition dataset impact report.",
        "Output": "A dataset of 10 million facial images and associated \"metadata\".",
        "Program": "MS-Celeb-1M facial recognition dataset",
        "Stakeholder Outcome": "Increased risk of being identified, tracked, or targeted without their knowledge or consent.",
        "Activity": "Collection and use of MS-Celeb-1M facial recognition dataset",
        "impact_risk": "Increased risk of privacy violations, including misuse of facial recognition technology for surveillance, identification, and tracking.",
        "Organization": "Microsoft",
        "Outcome": "Increased risk of privacy violations, including misuse of facial recognition technology for surveillance, identification, and tracking.",
        "Input": "Data collected from the web, including images and \"metadata\".",
        "Title of the story": "Microsoft Celeb (\"MS-Celeb-1M\") facial recognition dataset raises privacy concerns"
    },
    {
        "Impact Scale": "Local",
        "Impact Duration": "Ongoing",
        "Indicator": "Accuracy of Apple Crash Detection technology",
        "Service": "Crash detection technology",
        "Indicator Report": "Apple Crash Detection indicator report",
        "Impact Depth": "Medium",
        "Stakeholder": "Apple users",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/apple-crash-detection-false-positives",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "Apple Crash Detection impact report",
        "Output": "Automated crash notifications",
        "Program": "Apple Crash Detection",
        "Stakeholder Outcome": "Diversion of resources away from real emergencies",
        "Activity": "Development and use of Apple Crash Detection",
        "impact_risk": "Potential for false positives, including notifications for non-crash events, such as riding \"rollercoasters\".",
        "Organization": "Apple",
        "Outcome": "False positives, including notifications for non-crash events, such as riding \"rollercoasters\".",
        "Input": "Data collected by Apple Crash Detection",
        "Title of the story": "Apple Crash Detection false positives"
    },
    {
        "Impact Scale": "Local",
        "Impact Duration": "Ongoing",
        "Indicator": "Accuracy of the AI \"model\"",
        "Service": "AI \"technology\"",
        "Indicator Report": "N/A",
        "Impact Depth": "Medium",
        "Stakeholder": "Hollie Mengert and her \"fans\"",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/illustrator-hollie-mengert-converted-into-ai-model",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "N/A",
        "Output": "An AI \"model\" that can generate realistic images of Hollie Mengert",
        "Program": "Hollie Mengert AI \"project\"",
        "Stakeholder Outcome": "Potential for the AI \"model\" to damage Hollie Mengert's reputation or to be used to create \"content\" that is harmful or offensive to her \"fans\"",
        "Activity": "Creation of AI \"model\" of Hollie Mengert",
        "impact_risk": "Potential for the AI \"model\" to be used to create unauthorized or inappropriate \"content\"",
        "Organization": "Hollie Mengert",
        "Outcome": "Potential for the AI \"model\" to be used to create unauthorized or inappropriate \"content\"",
        "Input": "Data collected from Hollie Mengert's \"artwork\"",
        "Title of the story": "Illustrator Hollie Mengert converted into AI \"model\""
    },
    {
        "Impact Scale": "Local",
        "Impact Duration": "Ongoing",
        "Indicator": "Accuracy of Mindar's responses to questions about \"Buddhism\", number of people who have interacted with Mindar, and number of people who have reported feeling harmed by Mindar",
        "Service": "AI \"technology\"",
        "Indicator Report": "N/A",
        "Impact Depth": "Medium",
        "Stakeholder": "Buddhists, people seeking spiritual guidance, and the general public",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/mindar-robot-buddhist-priest",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "N/A",
        "Output": "A robot that can recite \"Buddhist\" texts, answer questions about \"Buddhism\", and provide counseling to people",
        "Program": "Mindar Project",
        "Stakeholder Outcome": "Potential for Mindar to damage the reputation of \"Buddhism\", to harm people who are seeking spiritual guidance, or to lead to a decrease in human interaction",
        "Activity": "Development and use of Mindar, a robot \"Buddhist priest\"",
        "impact_risk": "Potential for Mindar to be used to spread misinformation about \"Buddhism\", to exploit people's vulnerabilities, or to replace human interaction with a \"robot\"",
        "Organization": "Mindar Project",
        "Outcome": "Potential for Mindar to be used to spread misinformation about \"Buddhism\", to exploit people's vulnerabilities, or to replace human interaction with a \"robot\"",
        "Input": "Data collected from Buddhist monks and nuns",
        "Title of the story": "Mindar, a robot \"Buddhist priest\", sparks controversy"
    },
    {
        "Impact Scale": "Global",
        "Impact Duration": "Ongoing",
        "Indicator": "Accuracy of facial recognition systems trained on the IBM Diversity in Faces dataset",
        "Service": "AI \"technology\"",
        "Indicator Report": "N/A",
        "Impact Depth": "Medium",
        "Stakeholder": "People of color, women, and other marginalized groups",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/ibm-diversity-in-faces-dataset",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "N/A",
        "Output": "A dataset of one million annotated facial images",
        "Program": "IBM Diversity in Faces dataset",
        "Stakeholder Outcome": "Potential for the dataset to be used to create facial recognition systems that are less accurate for these groups, which could lead to discrimination in areas such as \"employment\", \"housing\", and \"law enforcement\".",
        "Activity": "Development and release of IBM Diversity in Faces dataset",
        "impact_risk": "Potential for the dataset to be used to create facial recognition systems that are biased against certain groups of people",
        "Organization": "IBM",
        "Outcome": "Potential for the dataset to be used to create facial recognition systems that are biased against certain groups of people, such as \"people of color\", \"women\", and other \"marginalized groups\".",
        "Input": "Data collected from publicly available facial images",
        "Title of the story": "IBM Diversity in Faces dataset sparks controversy"
    },
    {
        "Impact Scale": "Local",
        "Impact Duration": "Ongoing",
        "Indicator": "Accuracy of the sleep tracking feature, number of users who have used the sleep tracking feature, and number of users who have reported feeling \"discriminated against or violated\" by the sleep tracking feature",
        "Service": "\"AI technology\"",
        "Indicator Report": "N/A",
        "Impact Depth": "Medium",
        "Stakeholder": "Users of Google Nest Hub 2",
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/google-nest-hub-2-sleep-tracking\"",
        "Impact Model": "\"Common Impact Data Standard\"",
        "Impact Report": "N/A",
        "Output": "A sleep tracking report that includes information on sleep duration, quality, and \"stages\"",
        "Program": "\"Google Nest Hub 2 sleep tracking\"",
        "Stakeholder Outcome": "Potential for users to be \"discriminated against based on their sleep habits\", or for users to feel their privacy has been \"violated\"",
        "Activity": "Development and release of Google Nest Hub 2 sleep tracking \"feature\"",
        "impact_risk": "Potential for the data to be used to \"discriminate against users\", or to be used to track users' sleep habits without their \"consent\"",
        "Organization": "\"Google\"",
        "Outcome": "Potential for the data to be used to \"discriminate against users\", or to be used to track users' sleep habits without their \"consent\"",
        "Input": "Data collected from users' \"sleep habits\"",
        "Title of the story": "Google Nest Hub 2 sleep tracking sparks controversy"
    },
    {
        "Impact Scale": "Local",
        "Impact Duration": "Short-term",
        "Indicator": "Number of arrests made using FindFace Facial Recognition Technology",
        "Service": "FindFace Facial Recognition Service",
        "Indicator Report": "FindFace Facial Recognition Indicator Report",
        "Impact Depth": "Medium",
        "Stakeholder": "Law enforcement agencies",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/findface-facial-recognition-app",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "FindFace Facial Recognition Impact Report",
        "Output": "Faces identified by FindFace Facial Recognition Technology",
        "Program": "FindFace Facial Recognition Program",
        "Stakeholder Outcome": "Increased ability to identify and apprehend criminals",
        "Activity": "Use of FindFace Facial Recognition Technology",
        "impact_risk": "Potential for misuse of FindFace Facial Recognition Technology",
        "Organization": "FindFace",
        "Outcome": "Increased efficiency in law enforcement and security",
        "Input": "Data collected by FindFace Facial Recognition Technology",
        "Title of the story": "FindFace Facial Recognition App"
    },
    {
        "Impact Scale": "Local",
        "Impact Duration": "Short-term",
        "Indicator": "Number of participants who report improved mental health outcomes after participating in the Koko AI Mental Health Counselling Program",
        "Service": "Koko AI Mental Health Counselling Service",
        "Indicator Report": "Koko AI Mental Health Counselling Indicator Report",
        "Impact Depth": "Medium",
        "Stakeholder": "Participants in the Koko AI Mental Health Counselling Program",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/koko-ai-mental-health-counselling-experiment",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "Koko AI Mental Health Counselling Impact Report",
        "Output": "Mental health counselling sessions provided by Koko AI",
        "Program": "Koko AI Mental Health Counselling Program",
        "Stakeholder Outcome": "Reduced symptoms of depression and anxiety, improved self-esteem, and increased coping skills",
        "Activity": "Use of Koko AI to provide mental health counselling",
        "impact_risk": "Potential for Koko AI to be used to harm or exploit participants",
        "Organization": "University of California, San Francisco",
        "Outcome": "Improved mental health outcomes for participants",
        "Input": "Data collected by Koko AI",
        "Title of the story": "Koko AI Mental Health Counselling Experiment"
    },
    {
        "Impact Scale": "Local",
        "Impact Duration": "Short-term",
        "Indicator": "Number of jaywalking incidents in Shenzhen",
        "Service": "Shenzhen Facial Recognition Jaywalking Service",
        "Indicator Report": "Shenzhen Facial Recognition Jaywalking Indicator Report",
        "Impact Depth": "Medium",
        "Stakeholder": "Residents of Shenzhen",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/shenzhen-uses-facial-recognition-to-catch-shame-jaywalkers",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "Shenzhen Facial Recognition Jaywalking Impact Report",
        "Output": "Images of jaywalkers captured by facial recognition cameras",
        "Program": "Shenzhen Facial Recognition Jaywalking Program",
        "Stakeholder Outcome": "Increased safety for pedestrians and drivers",
        "Activity": "Use of facial recognition technology to catch jaywalkers",
        "impact_risk": "Potential for misuse of facial recognition technology",
        "Organization": "Shenzhen Government",
        "Outcome": "Decreased jaywalking in Shenzhen",
        "Input": "Data collected by facial recognition cameras",
        "Title of the story": "Shenzhen Uses Facial Recognition to Catch, Shame Jaywalkers"
    },
    {
        "Impact Scale": "Local",
        "Impact Duration": "Short-term",
        "Indicator": "Number of researchers, developers, and policymakers who use the Unconstrained AI College Students Dataset",
        "Service": "Unconstrained AI College Students Dataset Service",
        "Indicator Report": "Unconstrained AI College Students Dataset Indicator Report",
        "Impact Depth": "Medium",
        "Stakeholder": "Researchers, developers, and policymakers",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/unconstrained-college-students-dataset",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "Unconstrained AI College Students Dataset Impact Report",
        "Output": "Unconstrained AI College Students Dataset",
        "Program": "Unconstrained AI College Students Dataset Program",
        "Stakeholder Outcome": "Improved ability to develop and design products and services for college students",
        "Activity": "Collection of data from college students",
        "impact_risk": "Potential for misuse of data",
        "Organization": "Unconstrained AI",
        "Outcome": "Increased understanding of college students",
        "Input": "Data collected from college students",
        "Title of the story": "Unconstrained College Students Dataset"
    },
    {
        "Impact Scale": "Local",
        "Impact Duration": "Short-term",
        "Indicator": "Number of KFC Germany customers who were offended or distressed by the messages sent out on Kristallnacht",
        "Service": "KFC Germany Marketing Automation Service",
        "Indicator Report": "KFC Germany Marketing Automation Indicator Report",
        "Impact Depth": "Medium",
        "Stakeholder": "KFC Germany customers",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/kfc-germany-kristallnacht-marketing-automation",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "KFC Germany Marketing Automation Impact Report",
        "Output": "Messages sent out to KFC Germany customers on Kristallnacht",
        "Program": "KFC Germany Marketing Automation Program",
        "Stakeholder Outcome": "Feeling of being targeted and discriminated against",
        "Activity": "Use of marketing automation to send out messages on Kristallnacht",
        "impact_risk": "Potential for misuse of marketing automation data",
        "Organization": "KFC Germany",
        "Outcome": "Offense and distress caused to KFC Germany customers",
        "Input": "Data collected from KFC Germany customers",
        "Title of the story": "KFC Germany Uses Marketing Automation to Send Out Messages on Kristallnacht"
    },
    {
        "Impact Scale": "National",
        "Impact Duration": "Long-term",
        "Indicator": "Number of immigration and visa applications processed by AI",
        "Service": "IRCC Immigration and Visa Applications Automation Service",
        "Indicator Report": "IRCC Immigration and Visa Applications Automation Indicator Report",
        "Impact Depth": "Medium",
        "Stakeholder": "Applicants for immigration and visas",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/ircc-immigration-and-visa-applications-automation",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "IRCC Immigration and Visa Applications Automation Impact Report",
        "Output": "Automated decisions on immigration and visa applications",
        "Program": "IRCC Immigration and Visa Applications Automation Program",
        "Stakeholder Outcome": "Reduced wait times and increased chances of success in their applications",
        "Activity": "Use of artificial intelligence (AI) to automate the processing of immigration and visa applications",
        "impact_risk": "Potential for bias and discrimination in AI-powered decision-making",
        "Organization": "Immigration, Refugees and Citizenship Canada (IRCC)",
        "Outcome": "Increased efficiency and accuracy in the processing of immigration and visa applications",
        "Input": "Data collected from immigration and visa applicants",
        "Title of the story": "IRCC Immigration and Visa Applications Automation"
    },
    {
        "Impact Scale": "Local",
        "Impact Duration": "Short-term",
        "Indicator": "Amount of rainfall generated by drones",
        "Service": "Dubai Drone Weather Engineering Service",
        "Indicator Report": "Dubai Drone Weather Engineering Indicator Report",
        "Impact Depth": "Medium",
        "Stakeholder": "Residents of Dubai",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/dubai-drone-weather-engineering",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "Dubai Drone Weather Engineering Impact Report",
        "Output": "Rainfall generated by drones",
        "Program": "Dubai Drone Weather Engineering Program",
        "Stakeholder Outcome": "Improved water supply and agricultural production",
        "Activity": "Use of drones to seed clouds and generate rain",
        "impact_risk": "Potential for environmental damage, such as flooding and desertification",
        "Organization": "National Center of Meteorology (NCM)",
        "Outcome": "Increased rainfall in Dubai",
        "Input": "Data collected from drones",
        "Title of the story": "Dubai Drone Weather Engineering"
    },
    {
        "Impact Scale": "Local",
        "Impact Duration": "Short-term",
        "Indicator": "Number of Tesla Autopilot-related accidents",
        "Service": "Tesla Autopilot Service",
        "Indicator Report": "Tesla Autopilot Indicator Report",
        "Impact Depth": "Medium",
        "Stakeholder": "Tesla Model 3 driver, tow truck driver, and bystanders",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-model-3-hits-tow-truck-explodes",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "Tesla Autopilot Impact Report",
        "Output": "Tesla Model 3 driving autonomously",
        "Program": "Tesla Autopilot Program",
        "Stakeholder Outcome": "Injury and death",
        "Activity": "Use of Tesla Autopilot to drive a Tesla Model 3",
        "impact_risk": "Potential for Tesla Autopilot to malfunction and cause accidents",
        "Organization": "Tesla",
        "Outcome": "Tesla Model 3 hitting a tow truck and exploding",
        "Input": "Data collected from the Tesla Model 3's sensors and cameras",
        "Title of the story": "Tesla Model 3 Hits Tow Truck, Explodes"
    },
    {
        "Impact Scale": "Global",
        "Impact Duration": "Long-term",
        "Indicator": "Number of black men labeled as primates by Facebook's AI system",
        "Service": "Facebook AI Service",
        "Indicator Report": "Facebook AI Indicator Report",
        "Impact Depth": "Medium",
        "Stakeholder": "Black men on Facebook",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/facebook-labels-black-men-primates",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "Facebook AI Impact Report",
        "Output": "Automated decisions on whether content is harmful or not",
        "Program": "Facebook AI Program",
        "Stakeholder Outcome": "Feeling of being targeted and discriminated against",
        "Activity": "Use of artificial intelligence (AI) to moderate content on Facebook",
        "impact_risk": "Potential for AI to be biased and discriminatory",
        "Organization": "Facebook",
        "Outcome": "Black men were disproportionately labeled as primates by Facebook's AI system",
        "Input": "Data collected from Facebook users",
        "Title of the story": "Facebook Labels Black Men as Primates"
    },
    {
        "Impact Scale": "Local",
        "Impact Duration": "Short-term",
        "Indicator": "Number of Tesla Autopilot-related accidents",
        "Service": "Tesla Autopilot Service",
        "Indicator Report": "Tesla Autopilot Indicator Report",
        "Impact Depth": "Medium",
        "Stakeholder": "Police officers",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-model-x-crashes-into-five-police-officers",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "Tesla Autopilot Impact Report",
        "Output": "Tesla Model X driving autonomously",
        "Program": "Tesla Autopilot Program",
        "Stakeholder Outcome": "Injury",
        "Activity": "Use of Tesla Autopilot to drive a Tesla Model X",
        "impact_risk": "Potential for Tesla Autopilot to malfunction and cause accidents",
        "Organization": "Tesla",
        "Outcome": "Tesla Model X crashing into five police officers",
        "Input": "Data collected from the Tesla Model X's sensors and cameras",
        "Title of the story": "Tesla Model X Crashes into Five Police Officers"
    },
    {
        "Impact Scale": "Local",
        "Impact Duration": "Short-term",
        "Indicator": "Number of riders penalized by the algorithm",
        "Service": "Deliveroo Rider Management Service",
        "Indicator Report": "Deliveroo Rider Management Indicator Report",
        "Impact Depth": "Medium",
        "Stakeholder": "Riders",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/deliveroo-uk-rider-management-algorithm",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "Deliveroo Rider Management Impact Report",
        "Output": "Decisions about rider assignments, pay, and deactivation",
        "Program": "Deliveroo Rider Management Program",
        "Stakeholder Outcome": "Reduced earnings, increased stress, and feeling of being treated unfairly",
        "Activity": "Use of an algorithm to manage riders",
        "impact_risk": "Potential for the algorithm to be biased and discriminatory",
        "Organization": "Deliveroo",
        "Outcome": "Riders were unfairly penalized by the algorithm",
        "Input": "Data collected from riders",
        "Title of the story": "Deliveroo UK Rider Management Algorithm"
    },
    {
        "Impact Scale": "Local",
        "Impact Duration": "Short-term",
        "Indicator": "Number of people who viewed the ad.",
        "Service": "Cruzcampo Advertising Service",
        "Indicator Report": "No indicator report has been published.",
        "Impact Depth": "Medium",
        "Stakeholder": "Lola Flores's fans",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/cruzcampo-lola-flores-deepfake-ad",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "No impact report has been published.",
        "Output": "An ad featuring a deepfake of Lola Flores drinking Cruzcampo beer",
        "Program": "Cruzcampo Advertising Program",
        "Stakeholder Outcome": "Some fans felt offended and upset by the ad.",
        "Activity": "Use of deepfake technology to create an ad featuring the late Spanish singer Lola Flores",
        "impact_risk": "Potential for deepfake technology to be used to create misleading or offensive content.",
        "Organization": "Cruzcampo",
        "Outcome": "The ad was met with controversy, with some people accusing Cruzcampo of disrespecting Flores's memory.",
        "Input": "Data collected from Lola Flores's public appearances and recordings",
        "Title of the story": "Cruzcampo Lola Flores Deepfake Ad"
    },
    {
        "Impact Scale": "Local",
        "Impact Duration": "Short-term",
        "Indicator": "Number of Tesla Autopilot-related accidents",
        "Service": "Tesla Autopilot Service",
        "Indicator Report": "Tesla Autopilot Indicator Report",
        "Impact Depth": "Medium",
        "Stakeholder": "Tesla drivers",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-autopilot-confused-by-billboard",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "Tesla Autopilot Impact Report",
        "Output": "Tesla Model X driving autonomously",
        "Program": "Tesla Autopilot Program",
        "Stakeholder Outcome": "Feeling of unease and distrust of Tesla Autopilot",
        "Activity": "Use of Tesla Autopilot to drive a Tesla Model X",
        "impact_risk": "Potential for Tesla Autopilot to malfunction and cause accidents",
        "Organization": "Tesla",
        "Outcome": "Tesla Model X crashed into a billboard",
        "Input": "Data collected from the Tesla Model X's sensors and cameras",
        "Title of the story": "Tesla Autopilot Confused by Billboard"
    },
    {
        "Impact Scale": "Local",
        "Impact Duration": "Short-term",
        "Indicator": "Number of Tesla Autopilot-related accidents caused by sleeping drivers",
        "Service": "Tesla Autopilot Service",
        "Indicator Report": "Tesla Autopilot Indicator Report",
        "Impact Depth": "Medium",
        "Stakeholder": "Other drivers on the highway",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/sleeping-driver-speeds-on-highway-with-autopilot-switched-on",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "Tesla Autopilot Impact Report",
        "Output": "Tesla Model X driving autonomously",
        "Program": "Tesla Autopilot Program",
        "Stakeholder Outcome": "Feeling of danger and uncertainty",
        "Activity": "Use of Tesla Autopilot to drive a Tesla Model X",
        "impact_risk": "Potential for Tesla Autopilot to be misused and cause accidents",
        "Organization": "Tesla",
        "Outcome": "Sleeping driver speeds on highway with Autopilot switched on",
        "Input": "Data collected from the Tesla Model X's sensors and cameras",
        "Title of the story": "Sleeping Driver Speeds on Highway with Autopilot Switched On"
    },
    {
        "Impact Scale": "Local",
        "Impact Duration": "Short-term",
        "Indicator": "Number of job candidates who were unfairly discriminated against",
        "Service": "HireVue Recruitment Service",
        "Indicator Report": "HireVue Recruitment Indicator Report",
        "Impact Depth": "Medium",
        "Stakeholder": "Job candidates",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/hirevue-recruitment-facial-analysis-screening",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "HireVue Recruitment Impact Report",
        "Output": "Scores that are used to rank job candidates",
        "Program": "HireVue Recruitment Program",
        "Stakeholder Outcome": "Reduced chances of getting hired, increased stress, and feeling of being treated unfairly",
        "Activity": "Use of facial analysis software to screen job candidates",
        "impact_risk": "Potential for facial analysis software to be biased and discriminatory",
        "Organization": "HireVue",
        "Outcome": "Job candidates were unfairly discriminated against based on their race, gender, and other factors",
        "Input": "Data collected from job candidates' facial expressions and movements",
        "Title of the story": "HireVue Recruitment Facial Analysis Screening"
    },
    {
        "Impact Scale": "Local",
        "Impact Duration": "Short-term",
        "Indicator": "Number of people who were wrongfully arrested",
        "Service": "Rio de Janeiro Facial Recognition Service",
        "Indicator Report": "Rio de Janeiro Facial Recognition Indicator Report",
        "Impact Depth": "Medium",
        "Stakeholder": "People who were wrongfully arrested",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/rio-de-janeiro-facial-recognition-wrongful-arrests",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "Rio de Janeiro Facial Recognition Impact Report",
        "Output": "Matches between suspects' faces and faces in the database",
        "Program": "Rio de Janeiro Facial Recognition Program",
        "Stakeholder Outcome": "Inconvenience, stress, and damage to reputation",
        "Activity": "Use of facial recognition software to identify suspects",
        "impact_risk": "Potential for facial recognition software to be inaccurate and lead to wrongful arrests",
        "Organization": "Rio de Janeiro Police Department",
        "Outcome": "Several people were wrongfully arrested based on false matches",
        "Input": "Data collected from security cameras and other sources",
        "Title of the story": "Rio de Janeiro Facial Recognition Wrongful Arrests"
    },
    {
        "Impact Scale": "Local",
        "Impact Duration": "Short-term",
        "Indicator": "Average driver earnings before and after Uber began using the Upfront Fares algorithm",
        "Service": "Uber Upfront Fares Service",
        "Indicator Report": "Uber Upfront Fares Indicator Report",
        "Impact Depth": "Medium",
        "Stakeholder": "Uber drivers",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/uber-upfront-fares-driver-pay-algorithm",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "Uber Upfront Fares Impact Report",
        "Output": "A fare that is displayed to riders before they request a ride",
        "Program": "Uber Upfront Fares Program",
        "Stakeholder Outcome": "Reduced earnings, increased stress, and feeling of being treated unfairly",
        "Activity": "Use of an algorithm to determine driver pay",
        "impact_risk": "Potential for the Upfront Fares algorithm to be biased and lead to lower driver earnings",
        "Organization": "Uber",
        "Outcome": "Some drivers have reported that they are earning less money since Uber began using the Upfront Fares algorithm",
        "Input": "Data collected from drivers' trips, including distance, time, and traffic conditions",
        "Title of the story": "Uber Upfront Fares Driver Pay Algorithm"
    },
    {
        "Impact Scale": "Local",
        "Impact Duration": "Short-term",
        "Indicator": "Number of employees who have raised concerns about privacy and surveillance",
        "Service": "Amazon AWS Panorama Service",
        "Indicator Report": "Amazon AWS Panorama Indicator Report",
        "Impact Depth": "Medium",
        "Stakeholder": "Employees",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/amazon-aws-panorama-workplace-surveillance",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "Amazon AWS Panorama Impact Report",
        "Output": "Heat maps, activity logs, and other insights into workplace activity",
        "Program": "Amazon AWS Panorama Program",
        "Stakeholder Outcome": "Feeling of being watched and monitored, reduced privacy, and stress",
        "Activity": "Use of AI-powered cameras to monitor workplace activity",
        "impact_risk": "Potential for AI-powered cameras to be used to collect and store sensitive data, and to track and monitor employees without their consent",
        "Organization": "Amazon Web Services",
        "Outcome": "Some employees have raised concerns about privacy and surveillance",
        "Input": "Data collected from AI-powered cameras, including images, audio, and movement data",
        "Title of the story": "Amazon AWS Panorama Workplace Surveillance"
    },
    {
        "Impact Scale": "Local",
        "Impact Duration": "Short-term",
        "Indicator": "Number of accidents and injuries caused by Mobileye 630 PRO vehicles being tricked by drones and projectors",
        "Service": "Mobileye 630 PRO Service",
        "Indicator Report": "Mobileye 630 PRO Indicator Report",
        "Impact Depth": "Medium",
        "Stakeholder": "Drivers and passengers of Mobileye 630 PRO vehicles",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/mobileye-630-pro-tricked-by-drones-projectors",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "Mobileye 630 PRO Impact Report",
        "Output": "Decisions about how to control the vehicle",
        "Program": "Mobileye 630 PRO Program",
        "Stakeholder Outcome": "Increased risk of accidents and injuries",
        "Activity": "Use of Mobileye 630 PRO self-driving technology",
        "impact_risk": "Potential for Mobileye 630 PRO self-driving technology to be tricked by other objects and systems",
        "Organization": "Mobileye",
        "Outcome": "Mobileye 630 PRO vehicles were tricked by drones and projectors into making dangerous decisions",
        "Input": "Data collected from Mobileye 630 PRO sensors and cameras",
        "Title of the story": "Mobileye 630 PRO Tricked by Drones, Projectors"
    },
    {
        "Impact Scale": "Local",
        "Impact Duration": "Short-term",
        "Indicator": "Number of customers who have raised concerns about privacy and discrimination",
        "Service": "Rite Aid Facial Recognition Service",
        "Indicator Report": "Rite Aid Facial Recognition Indicator Report",
        "Impact Depth": "Medium",
        "Stakeholder": "Customers",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/rite-aid-facial-recognition",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "Rite Aid Facial Recognition Impact Report",
        "Output": "Matches between customers' faces and faces in the database",
        "Program": "Rite Aid Facial Recognition Program",
        "Stakeholder Outcome": "Feeling of being watched and monitored, reduced privacy, and stress",
        "Activity": "Use of facial recognition technology to identify customers",
        "impact_risk": "Potential for facial recognition technology to be used to track and monitor customers without their consent, and to discriminate against customers based on their race, gender, or other factors",
        "Organization": "Rite Aid",
        "Outcome": "Some customers have raised concerns about privacy and discrimination",
        "Input": "Data collected from customers' faces, including images and biometric data",
        "Title of the story": "Rite Aid Facial Recognition"
    },
    {
        "Impact Scale": "Local",
        "Impact Duration": "Short-term",
        "Indicator": "Number of accidents involving self-driving buses",
        "Service": "Toyota Paralympics Self-Driving Bus Service",
        "Indicator Report": "Toyota Paralympics Self-Driving Bus Indicator Report",
        "Impact Depth": "Medium",
        "Stakeholder": "Athletes",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/toyota-paralympics-self-driving-bus-hits-athlete",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "Toyota Paralympics Self-Driving Bus Impact Report",
        "Output": "Decisions about how to control the bus",
        "Program": "Toyota Paralympics Self-Driving Bus Program",
        "Stakeholder Outcome": "Inconvenience, stress, and damage to reputation",
        "Activity": "Use of self-driving technology to transport athletes during the 2020 Paralympics in Tokyo",
        "impact_risk": "Potential for self-driving technology to be inaccurate and lead to accidents",
        "Organization": "Toyota",
        "Outcome": "A Japanese visually impaired athlete was hit by a Toyota e-Palette self-driving bus used to ferry athletes during the 2020 Paralympic Games in Tokyo. Aramitsu Kitazono was attempting to cross a street at a designated crossing within the Athletes Village when he was hit. He was left unable to compete.",
        "Input": "Data collected from the self-driving bus's sensors and cameras",
        "Title of the story": "Toyota Paralympics Self-Driving Bus Hits Athlete"
    },
    {
        "Impact Scale": "Global",
        "Impact Duration": "Long-term",
        "Indicator": "Number of people whose images are included in the dataset",
        "Service": "VGG-Face Facial Recognition Service",
        "Indicator Report": "\"VGG-Face Facial Recognition Indicator Report\"",
        "Impact Depth": "Medium",
        "Stakeholder": "People whose images are included in the dataset",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/vgg-face-facial-recognition-dataset",
        "Impact Model": "\"Common Impact Data Standard\"",
        "Impact Report": "\"VGG-Face Facial Recognition Impact Report\"",
        "Output": "A dataset of facial images and their associated metadata, such as the person's name, age, and gender",
        "Program": "VGG-Face Facial Recognition Program",
        "Stakeholder Outcome": "Potential for their privacy to be violated",
        "Activity": "Creation of a facial recognition dataset",
        "impact_risk": "Potential for the dataset to be used to discriminate against people based on their race, gender, or other factors",
        "Organization": "University of Oxford",
        "Outcome": "The dataset has been used to train a number of facial recognition algorithms, which have been used in a variety of applications, such as unlocking smartphones, identifying criminals, and tagging people in photos.",
        "Input": "Images of faces from the internet",
        "Title of the story": "VGG-Face Facial Recognition Dataset"
    },
    {
        "Impact Scale": "Local",
        "Impact Duration": "Short-term",
        "Indicator": "Number of accidents involving Tesla vehicles using Autopilot",
        "Service": "Tesla Autopilot Service",
        "Indicator Report": "Tesla Autopilot Indicator Report",
        "Impact Depth": "Medium",
        "Stakeholder": "Drivers and passengers of Tesla vehicles",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-model-3-crashes-into-bus-in-ruian-kills-one",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "Tesla Autopilot Impact Report",
        "Output": "Decisions about how to control the vehicle",
        "Program": "Tesla Autopilot Program",
        "Stakeholder Outcome": "Increased risk of accidents and injuries",
        "Activity": "Use of Tesla Autopilot technology",
        "impact_risk": "Potential for Tesla Autopilot technology to be inaccurate and lead to accidents",
        "Organization": "Tesla",
        "Outcome": "A Tesla Model 3 crashed into a bus in Ruian, China, killing one person and injuring two others. The driver of the Tesla was not injured.",
        "Input": "Data collected from Tesla Autopilot sensors and cameras",
        "Title of the story": "Tesla Model 3 Crashes into Bus in Ruian, Kills One"
    },
    {
        "Impact Scale": "Local",
        "Impact Duration": "Short-term",
        "Indicator": "Accuracy of blood oxygen measurements for people with different skin tones",
        "Service": "Apple Watch Blood Oximeter Service",
        "Indicator Report": "Apple Watch Blood Oximeter Racial Bias Indicator Report",
        "Impact Depth": "Medium",
        "Stakeholder": "People with darker skin tones",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/apple-watch-blood-oximeter-racial-bias",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "Apple Watch Blood Oximeter Racial Bias Impact Report",
        "Output": "Measurements of blood oxygen levels",
        "Program": "Apple Watch Blood Oximeter Program",
        "Stakeholder Outcome": "Potential for inaccurate blood oxygen measurements to lead to misdiagnosis of health conditions",
        "Activity": "Use of Apple Watch blood oximeter to measure blood oxygen levels",
        "impact_risk": "Potential for Apple Watch blood oximeter to be used to discriminate against people with darker skin tones",
        "Organization": "Apple",
        "Outcome": "The Apple Watch blood oximeter has been shown to be less accurate for people with darker skin tones",
        "Input": "Data collected from the Apple Watch's sensors",
        "Title of the story": "Apple Watch Blood Oximeter Racial Bias"
    },
    {
        "Impact Scale": "Local",
        "Impact Duration": "Short-term",
        "Indicator": "Accuracy of fertility predictions for different users",
        "Service": "Apple Cycle Tracking Service",
        "Indicator Report": "Apple Cycle Tracking Fertility Predictions Indicator Report",
        "Impact Depth": "Medium",
        "Stakeholder": "Users of the Apple cycle tracking app",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/apple-cycle-tracking-fertility-predictions",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "Apple Cycle Tracking Fertility Predictions Impact Report",
        "Output": "Predictions about the user's fertility window",
        "Program": "Apple Cycle Tracking Program",
        "Stakeholder Outcome": "Potential for inaccurate fertility predictions to lead to unintended pregnancies or difficulty conceiving",
        "Activity": "Use of Apple's cycle tracking app to predict fertility",
        "impact_risk": "Potential for Apple cycle tracking app to be used to discriminate against people based on their fertility status",
        "Organization": "Apple",
        "Outcome": "The Apple cycle tracking app has been shown to be inaccurate for some users",
        "Input": "Data collected from the user, such as menstrual cycle length, flow, and symptoms",
        "Title of the story": "Apple Cycle Tracking Fertility Predictions"
    },
    {
        "Impact Scale": "Local",
        "Impact Duration": "Short-term",
        "Indicator": "Number of people wrongly accused of welfare fraud",
        "Service": "SyRI Welfare Fraud Detection Service",
        "Indicator Report": "SyRI Welfare Fraud Detection Indicator Report",
        "Impact Depth": "Medium",
        "Stakeholder": "People receiving welfare benefits",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/syri-welfare-fraud-detection-automation",
        "Impact Model": "Common Impact Data Standard",
        "Impact Report": "SyRI Welfare Fraud Detection Impact Report",
        "Output": "Predictions about whether a person is likely to commit welfare fraud",
        "Program": "SyRI Welfare Fraud Detection Program",
        "Stakeholder Outcome": "Increased stress, anxiety, and fear of being wrongly accused of fraud",
        "Activity": "Use of artificial intelligence to detect welfare fraud",
        "impact_risk": "Potential for SyRI to be used to discriminate against people based on their race, ethnicity, or other factors",
        "Organization": "Dutch government",
        "Outcome": "The SyRI system was found to be discriminatory and inaccurate, and was shut down in 2019",
        "Input": "Data collected from government databases, such as social security records, tax records, and employment records",
        "Title of the story": "SyRI Welfare Fraud Detection Automation"
    },
    {
        "impact scale": "global",
        "indicator report": "\"80 Million Tiny Images Indicator Report\"",
        "indicator": "number of people whose images are included in the dataset, as well as number of machine learning models that have been trained on the dataset",
        "input": "images of objects and scenes from the internet",
        "service": "80 Million Tiny Images Service",
        "output": "a dataset of 80 million tiny images, each of which is 32x32 pixels in size",
        "impact depth": "medium",
        "stakeholder": "people whose images are included in the dataset, as well as users of machine learning models that have been trained on the dataset",
        "impact model": "\"Common Impact Data Standard\"",
        "impact report": "\"80 Million Tiny Images Impact Report\"",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/80-million-tiny-images-dataset",
        "program": "80 Million Tiny Images Program",
        "impact_risk": "potential for the dataset to be used to discriminate against people based on their race, gender, or other factors, as well as potential for the dataset to be used to create deepfakes or other forms of synthetic media",
        "activity": "creation of a dataset of 80 million tiny images",
        "stakeholder outcome": "potential for their privacy to be violated, as well as potential for machine learning models to be biased or inaccurate",
        "organization": "MIT",
        "outcome": "the dataset has been used to train a number of machine learning models, which have been used in a variety of applications, such as image classification, object detection, and scene understanding.",
        "title of the story": "80 Million Tiny Images Dataset",
        "impact duration": "long-term"
    },
    {
        "impact scale": "local",
        "indicator report": "\"Barclays Employee Spyware Monitoring Indicator Report\"",
        "indicator": "number of employees who have expressed concerns about employee spyware",
        "input": "data collected from employee computers, such as keystrokes, websites visited, and emails sent",
        "service": "Barclays Employee Spyware Monitoring Service",
        "output": "reports on employee activity",
        "impact depth": "medium",
        "stakeholder": "Barclays employees",
        "impact model": "\"Common Impact Data Standard\"",
        "impact report": "\"Barclays Employee Spyware Monitoring Impact Report\"",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/barclays-employee-spyware-monitoring",
        "program": "Barclays Employee Spyware Monitoring Program",
        "impact_risk": "potential for employee spyware to be used to discriminate against employees based on their race, gender, or other factors",
        "activity": "use of employee spyware to monitor employee activity",
        "stakeholder outcome": "potential for increased stress, anxiety, and fear of being monitored",
        "organization": "Barclays",
        "outcome": "employees have expressed concerns about privacy and stress",
        "title of the story": "Barclays Employee Spyware Monitoring",
        "impact duration": "short-term"
    },
    {
        "impact scale": "local",
        "indicator report": "\"OostoanyVision Facial Recognition Drone Indicator Report\"",
        "indicator": "number of people who have expressed concerns about facial recognition drones",
        "input": "data collected from facial recognition drones, such as images of people's faces",
        "service": "OostoanyVision Facial Recognition Drone Service",
        "output": "reports on people's movements and activities",
        "impact depth": "medium",
        "stakeholder": "People who are monitored by facial recognition drones",
        "impact model": "\"Common Impact Data Standard\"",
        "impact report": "\"OostoanyVision Facial Recognition Drone Impact Report\"",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/oostoanyvision-facial-recognition-drones",
        "program": "OostoanyVision Facial Recognition Drone Program",
        "impact_risk": "potential for facial recognition drones to be used to discriminate against people based on their race, gender, or other factors",
        "activity": "use of facial recognition drones to monitor public spaces",
        "stakeholder outcome": "potential for increased stress, anxiety, and fear of being monitored",
        "organization": "OostoanyVision",
        "outcome": "people have expressed concerns about privacy and surveillance",
        "title of the story": "OostoanyVision Facial Recognition Drones",
        "impact duration": "short-term"
    },
    {
        "impact scale": "local",
        "indicator report": "\"Amazon Ring Police Data Sharing Indicator Report\"",
        "indicator": "number of people who have expressed concerns about Amazon Ring police data sharing",
        "input": "data from Amazon Ring doorbell cameras, such as videos and images of people and events",
        "service": "Amazon Ring Police Data Sharing Service",
        "output": "reports on people and events that have been captured by Amazon Ring doorbell cameras",
        "impact depth": "medium",
        "stakeholder": "People who use Amazon Ring doorbell cameras and people who are monitored by police using data from Amazon Ring doorbell cameras",
        "impact model": "\"Common Impact Data Standard\"",
        "impact report": "\"Amazon Ring Police Data Sharing Impact Report\"",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/amazon-ring-police-data-sharing",
        "program": "Amazon Ring Police Data Sharing Program",
        "impact_risk": "potential for Amazon Ring police data sharing to be used to discriminate against people based on their race, gender, or other factors",
        "activity": "sharing of data from Amazon Ring doorbell cameras with police departments",
        "stakeholder outcome": "potential for increased stress, anxiety, and fear of being monitored",
        "organization": "Amazon/Ring",
        "outcome": "increased police surveillance and potential for discrimination",
        "title of the story": "Amazon Ring Police Data Sharing",
        "impact duration": "short-term"
    },
    {
        "impact scale": "local",
        "indicator report": "\"MoviePass Preshow Eye Tracking Indicator Report\"",
        "indicator": "number of moviegoers who have expressed concerns about MoviePass preshow eye tracking",
        "input": "data collected from eye tracking technology, such as gaze direction and duration",
        "service": "MoviePass Preshow Eye Tracking Service",
        "output": "reports on moviegoers' attention during preshow ads",
        "impact depth": "medium",
        "stakeholder": "Moviegoers and advertisers",
        "impact model": "\"Common Impact Data Standard\"",
        "impact report": "\"MoviePass Preshow Eye Tracking Impact Report\"",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/moviepass-preshow-eye-tracking",
        "program": "MoviePass Preshow Eye Tracking Program",
        "impact_risk": "potential for eye tracking technology to be used to discriminate against moviegoers based on their race, gender, or other factors",
        "activity": "use of eye tracking technology to monitor moviegoers' attention during preshow ads",
        "stakeholder outcome": "potential for increased annoyance and decreased effectiveness of preshow ads",
        "organization": "MoviePass",
        "outcome": "increased awareness of preshow ads and potential for changes in ad design",
        "title of the story": "MoviePass Preshow Eye Tracking",
        "impact duration": "short-term"
    },
    {
        "impact scale": "global",
        "indicator report": "\"Facebook Military Gear Advertising Indicator Report\"",
        "indicator": "number of Facebook users who have seen ads for military gear and weapons accessories",
        "input": "data on users' interests and browsing history",
        "service": "Facebook Military Gear Advertising Service",
        "output": "ads for military gear and weapons accessories",
        "impact depth": "medium",
        "stakeholder": "Facebook users, gun owners, and the general public",
        "impact model": "\"Common Impact Data Standard\"",
        "impact report": "\"Facebook Military Gear Advertising Impact Report\"",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/facebook-military-gear-advertising",
        "program": "Facebook Military Gear Advertising Program",
        "impact_risk": "potential for Facebook to be used to spread misinformation and propaganda about military gear and weapons accessories",
        "activity": "running of adverts for military gear and weapons accessories next to posts about the January 6 attempted coup in Washington DC",
        "stakeholder outcome": "potential for increased anxiety and fear, as well as increased gun violence",
        "organization": "Meta/Facebook",
        "outcome": "increased awareness of and interest in military gear and weapons accessories, as well as potential for increased gun violence",
        "title of the story": "Facebook Military Gear Advertising",
        "impact duration": "long-term"
    },
    {
        "impact scale": "global",
        "indicator report": "\"Joe Rogan Libido Booster Alpha Grind Ad Deepfake Indicator Report\"",
        "indicator": "number of people who have seen the deepfake ad",
        "input": "deepfake technology, footage of Joe Rogan, and the Alpha Grind product",
        "service": "Onnit Alpha Grind Ad Deepfake Service",
        "output": "an ad for the Alpha Grind product featuring Joe Rogan",
        "impact depth": "medium",
        "stakeholder": "Joe Rogan, Onnit, and consumers of the Alpha Grind product",
        "impact model": "\"Common Impact Data Standard\"",
        "impact report": "\"Joe Rogan Libido Booster Alpha Grind Ad Deepfake Impact Report\"",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/joe-rogan-libido-booster-alpha-grind-ad-deepfake",
        "program": "Onnit Alpha Grind Ad Deepfake Program",
        "impact_risk": "potential for the deepfake technology to be used to spread misinformation or propaganda",
        "activity": "creation and distribution of a deepfake ad featuring Joe Rogan",
        "stakeholder outcome": "potential for Joe Rogan to be mischaracterized, as well as potential for consumers to be misled about the benefits of the Alpha Grind product",
        "organization": "Onnit",
        "outcome": "increased awareness of the Alpha Grind product, as well as potential for harm to Joe Rogan's reputation",
        "title of the story": "Joe Rogan Libido Booster Alpha Grind Ad Deepfake",
        "impact duration": "short-term"
    },
    {
        "impact scale": "local",
        "indicator report": "\"Walgreens Fridge Screen Door Biometrics Indicator Report\"",
        "indicator": "number of Walgreens customers who have used the fridge screen door biometrics feature",
        "input": "data collected from users' fingerprints, such as the unique fingerprint pattern",
        "service": "Walgreens Fridge Screen Door Biometrics Service",
        "output": "access to the fridge screen door",
        "impact depth": "medium",
        "stakeholder": "Walgreens customers",
        "impact model": "\"Common Impact Data Standard\"",
        "impact report": "\"Walgreens Fridge Screen Door Biometrics Impact Report\"",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/walgreens-fridge-screen-door-biometrics",
        "program": "Walgreens Fridge Screen Door Biometrics Program",
        "impact_risk": "potential for the biometric data to be stolen or used for fraudulent purposes",
        "activity": "use of biometrics to unlock fridge screen doors at Walgreens stores",
        "stakeholder outcome": "potential for customers to feel uncomfortable with their biometric data being collected and used by Walgreens",
        "organization": "Walgreens",
        "outcome": "increased convenience for customers, as well as potential for privacy concerns",
        "title of the story": "Walgreens Fridge Screen Door Biometrics",
        "impact duration": "short-term"
    },
    {
        "impact scale": "global",
        "indicator report": "\"Google Autocomplete Connects Albert Yeung with Triads Indicator Report\"",
        "indicator": "number of people who have seen the Google autocomplete suggestion that connects Albert Yeung with triads",
        "input": "data collected from users' search queries, such as the words and phrases that they type",
        "service": "Google Autocomplete Service",
        "output": "list of suggested results, including websites, images, and other content",
        "impact depth": "medium",
        "stakeholder": "Albert Yeung, his family, and his businesses",
        "impact model": "\"Common Impact Data Standard\"",
        "impact report": "\"Google Autocomplete Connects Albert Yeung with Triads Impact Report\"",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/google-autocomplete-connects-albert-yeung-with-triads",
        "program": "Google Autocomplete Program",
        "impact_risk": "potential for Google autocomplete to be used to spread misinformation or propaganda",
        "activity": "use of Google autocomplete to suggest results when users type in search queries",
        "stakeholder outcome": "potential for Albert Yeung to be mischaracterized, as well as potential for his businesses to be damaged",
        "organization": "Google",
        "outcome": "increased awareness of Albert Yeung's alleged connections to triads, as well as potential for harm to his reputation",
        "title of the story": "Google autocomplete connects Albert Yeung with triads",
        "impact duration": "short-term"
    },
    {
        "impact scale": "global",
        "indicator report": "\"Unity GovTech AI military applications indicator report\"",
        "indicator": "number of governments and militaries that have adopted Unity GovTech AI military applications",
        "input": "data on military tactics, weapons, and other factors",
        "service": "Unity GovTech AI military applications service",
        "output": "AI-powered software that can be used to improve military operations",
        "impact depth": "medium",
        "stakeholder": "Governments and militaries",
        "impact model": "\"Common Impact Data Standard\"",
        "impact report": "\"Unity GovTech AI military applications impact report\"",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/unity-govtech-ai-military-applications",
        "program": "Unity GovTech AI military applications program",
        "impact_risk": "potential for AI-powered software to be used to create autonomous weapons systems that could operate without human intervention",
        "activity": "development of AI-powered software for military applications",
        "stakeholder outcome": "potential for increased civilian casualties, as well as potential for the use of AI-powered software for illegal or unethical purposes",
        "organization": "Unity Technologies",
        "outcome": "increased effectiveness of military operations, as well as potential for harm to civilians",
        "title of the story": "Unity GovTech AI military applications",
        "impact duration": "long-term"
    },
    {
        "impact scale": "local",
        "indicator report": "\"Intel AI student emotion monitoring indicator report\"",
        "indicator": "number of students who have been monitored by the Intel AI student emotion monitoring software",
        "input": "data on student facial expressions, voice tone, and other factors",
        "service": "Intel AI student emotion monitoring service",
        "output": "real-time reports on student emotions",
        "impact depth": "medium",
        "stakeholder": "Students, teachers, and parents",
        "impact model": "\"Common Impact Data Standard\"",
        "impact report": "\"Intel AI student emotion monitoring impact report\"",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/intel-ai-student-emotion-monitoring",
        "program": "Intel AI student emotion monitoring program",
        "impact_risk": "potential for the AI-powered software to be used to discriminate against students based on their emotions",
        "activity": "development and use of AI-powered software to monitor student emotions",
        "stakeholder outcome": "potential for students to feel uncomfortable with their emotions being monitored, as well as potential for teachers and parents to use the data to make decisions about students that are not in their best interests",
        "organization": "Intel",
        "outcome": "increased awareness of student emotions, as well as potential for harm to students' privacy",
        "title of the story": "Intel AI student emotion monitoring",
        "impact duration": "short-term"
    },
    {
        "impact scale": "local",
        "indicator report": "\"Glovo Foodinho rider management algorithm indicator report\"",
        "indicator": "number of Glovo riders who have been rated by the algorithm",
        "input": "data on rider performance, such as acceptance rate, cancellation rate, and delivery speed",
        "service": "Glovo Foodinho rider management algorithm service",
        "output": "ratings for riders, which are used to determine their eligibility for shifts and bonuses",
        "impact depth": "medium",
        "stakeholder": "Glovo riders",
        "impact model": "\"Common Impact Data Standard\"",
        "impact report": "\"Glovo Foodinho rider management algorithm impact report\"",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/glovofoodinho-rider-management-algorithm",
        "program": "Glovo Foodinho rider management algorithm program",
        "impact_risk": "potential for the algorithm to be biased against certain groups of riders, such as women and minorities",
        "activity": "development and use of an AI-powered algorithm to manage riders",
        "stakeholder outcome": "potential for riders to be unfairly penalized by the algorithm, as well as potential for riders to lose their jobs due to low ratings",
        "organization": "Glovo",
        "outcome": "increased efficiency of rider management, as well as potential for harm to riders' livelihoods",
        "title of the story": "Glovo Foodinho rider management algorithm",
        "impact duration": "long-term"
    },
    {
        "impact scale": "local",
        "indicator report": "\"LAPD social media data collection indicator report\"",
        "indicator": "number of social media posts that have been collected by the LAPD",
        "input": "data from social media platforms, such as Facebook, Twitter, and Instagram",
        "service": "LAPD social media data collection service",
        "output": "a database of social media data that can be used by the LAPD for a variety of purposes, such as identifying potential criminals and investigating crimes",
        "impact depth": "medium",
        "stakeholder": "The public, including potential criminals and victims of crime",
        "impact model": "\"Common Impact Data Standard\"",
        "impact report": "\"LAPD social media data collection impact report\"",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/lapd-social-media-data-collection",
        "program": "LAPD social media data collection program",
        "impact_risk": "potential for the LAPD to violate the privacy of individuals by collecting and storing their social media data",
        "activity": "collection of social media data by the LAPD",
        "stakeholder outcome": "potential for the LAPD to misuse the social media data, such as by targeting individuals for surveillance or harassment",
        "organization": "Los Angeles Police Department",
        "outcome": "increased ability of the LAPD to identify and investigate crimes, as well as potential for harm to civil liberties",
        "title of the story": "LAPD social media data collection",
        "impact duration": "long-term"
    },
    {
        "impact scale": "local",
        "indicator report": "\"Rio de Janeiro facial recognition wrongful arrests indicator report\"",
        "indicator": "number of people who have been wrongfully arrested due to facial recognition",
        "input": "data from facial recognition cameras, such as images and videos",
        "service": "Rio de Janeiro facial recognition service",
        "output": "a list of people who are suspected of committing crimes, based on their facial features",
        "impact depth": "medium",
        "stakeholder": "The public, including people who have been wrongfully arrested",
        "impact model": "\"Common Impact Data Standard\"",
        "impact report": "\"Rio de Janeiro facial recognition wrongful arrests impact report\"",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/rio-de-janeiro-facial-recognition-wrongful-arrests",
        "program": "Rio de Janeiro facial recognition program",
        "impact_risk": "potential for the facial recognition technology to be biased against certain groups of people, such as people of color",
        "activity": "use of facial recognition technology by the Rio de Janeiro Police Department",
        "stakeholder outcome": "potential for people to be arrested for crimes that they did not commit",
        "organization": "Rio de Janeiro Police Department",
        "outcome": "increased number of arrests, as well as potential for wrongful arrests",
        "title of the story": "Rio de Janeiro facial recognition wrongful arrests",
        "impact duration": "long-term"
    },
    {
        "impact scale": "global",
        "indicator report": "\"Tesla Paris fatal crash indicator report\"",
        "indicator": "number of people who have been injured or killed in Tesla Autopilot crashes",
        "input": "data from sensors on the Tesla car, such as cameras and radar",
        "service": "Tesla Autopilot service",
        "output": "actions taken by the Tesla car, such as steering, braking, and accelerating",
        "impact depth": "medium",
        "stakeholder": "The public, including people who have been injured or killed in Tesla Autopilot crashes",
        "impact model": "\"Common Impact Data Standard\"",
        "impact report": "\"Tesla Paris fatal crash impact report\"",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-paris-fatal-crash",
        "program": "Tesla Autopilot program",
        "impact_risk": "potential for the Tesla Autopilot feature to be unreliable or unsafe",
        "activity": "use of Tesla's Autopilot driver-assist feature",
        "stakeholder outcome": "potential for people to be injured or killed in Tesla Autopilot crashes",
        "organization": "Tesla",
        "outcome": "fatal crash, as well as potential for more crashes",
        "title of the story": "Tesla Paris fatal crash",
        "impact duration": "long-term"
    },
    {
        "impact scale": "local",
        "indicator report": "\"Seoul bridge suicide detection indicator report\"",
        "indicator": "number of people who have been rescued from suicide attempts due to AI-powered cameras",
        "input": "data from AI-powered cameras, such as images and videos",
        "service": "Seoul bridge suicide detection service",
        "output": "a list of people who are at risk of suicide, based on their behavior",
        "impact depth": "medium",
        "stakeholder": "The public, including people who have been rescued from suicide attempts",
        "impact model": "\"Common Impact Data Standard\"",
        "impact report": "\"Seoul bridge suicide detection impact report\"",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/seoul-bridge-suicide-detection",
        "program": "Seoul bridge suicide detection program",
        "impact_risk": "potential for the AI-powered cameras to be biased against certain groups of people, such as people of color",
        "activity": "use of AI-powered cameras to detect people who are at risk of suicide on bridges",
        "stakeholder outcome": "potential for people to be falsely identified as being at risk of suicide",
        "organization": "Seoul Metropolitan Government",
        "outcome": "increased number of people who are rescued from suicide attempts, as well as potential for false positives",
        "title of the story": "Seoul bridge suicide detection",
        "impact duration": "long-term"
    },
    {
        "impact scale": "local",
        "indicator report": "\"Harrisburg University criminality prediction indicator report\"",
        "indicator": "number of people who have been discriminated against or had their privacy harmed by the software",
        "input": "data on facial images, such as age, gender, and ethnicity",
        "service": "Harrisburg University criminality prediction service",
        "output": "predictions of whether someone is likely to commit a crime",
        "impact depth": "medium",
        "stakeholder": "The public, including people who are likely to be discriminated against by the software, and people who have their privacy harmed by the software",
        "impact model": "\"Common Impact Data Standard\"",
        "impact report": "\"Harrisburg University criminality prediction impact report\"",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/harrisburg-university-criminality-prediction-study",
        "program": "Harrisburg University criminality prediction program",
        "impact_risk": "potential for the software to be biased against certain groups of people, such as people of color",
        "activity": "development of software to predict criminality based on facial images",
        "stakeholder outcome": "potential for people to be unfairly targeted by law enforcement, and potential for people to have their privacy violated",
        "organization": "Harrisburg University",
        "outcome": "increased risk of discrimination against certain groups of people, as well as potential for harm to people's privacy",
        "title of the story": "Harrisburg University criminality prediction study",
        "impact duration": "long-term"
    },
    {
        "impact scale": "local",
        "indicator report": "\"ChatGPT fake online reviews indicator report\"",
        "indicator": "number of fake online reviews that have been created using ChatGPT",
        "input": "data on ChatGPT, such as its training data and algorithms",
        "service": "ChatGPT service",
        "output": "fake online reviews",
        "impact depth": "medium",
        "stakeholder": "The public, including businesses and consumers",
        "impact model": "\"Common Impact Data Standard\"",
        "impact report": "\"ChatGPT fake online reviews impact report\"",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/chatgpt-writes-fake-online-reviews",
        "program": "ChatGPT program",
        "impact_risk": "potential for ChatGPT to be used to create fake reviews that are indistinguishable from real reviews",
        "activity": "use of ChatGPT to write fake online reviews",
        "stakeholder outcome": "potential for businesses to lose customers and revenue, and potential for consumers to be misled by fake reviews",
        "organization": "OpenAI",
        "outcome": "increased risk of fraud and deception, as well as potential for harm to businesses and consumers",
        "title of the story": "ChatGPT writes fake online reviews",
        "impact duration": "long-term"
    },
    {
        "impact scale": "local",
        "indicator report": "\"Lockport City School District facial recognition indicator report\"",
        "indicator": "number of people who have been wrongfully arrested due to facial recognition",
        "input": "data from facial recognition cameras, such as images and videos",
        "service": "Lockport City School District facial recognition service",
        "output": "a list of people who are suspected of committing crimes, based on their facial features",
        "impact depth": "medium",
        "stakeholder": "The public, including people who have been wrongfully arrested",
        "impact model": "\"Common Impact Data Standard\"",
        "impact report": "\"Lockport City School District facial recognition impact report\"",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/lockport-city-school-district-facial-recognition",
        "program": "Lockport City School District facial recognition program",
        "impact_risk": "potential for the facial recognition technology to be biased against certain groups of people, such as people of color",
        "activity": "use of facial recognition technology by the Lockport City School District",
        "stakeholder outcome": "potential for people to be arrested for crimes that they did not commit",
        "organization": "Lockport City School District",
        "outcome": "increased number of arrests, as well as potential for wrongful arrests",
        "title of the story": "Lockport City School District facial recognition",
        "impact duration": "long-term"
    },
    {
        "impact scale": "global",
        "indicator report": "\"Amazon chemical food preservative suicides indicator report\"",
        "indicator": "number of people who have become suicidal or experienced health problems due to exposure to Amazon's chemical food preservatives",
        "input": "data on Amazon's chemical food preservatives, such as their composition and toxicity",
        "service": "Amazon chemical food preservative service",
        "output": "food products that contain Amazon's chemical food preservatives",
        "impact depth": "medium",
        "stakeholder": "The public, including people who have consumed food products that contain Amazon's chemical food preservatives",
        "impact model": "\"Common Impact Data Standard\"",
        "impact report": "\"Amazon chemical food preservative suicides impact report\"",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/amazon-chemical-food-preservative-suicides",
        "program": "Amazon chemical food preservative program",
        "impact_risk": "potential for Amazon's chemical food preservatives to be harmful to people's health, including their mental health",
        "activity": "use of Amazon's chemical food preservatives in its products",
        "stakeholder outcome": "potential for people to become suicidal or experience health problems due to exposure to Amazon's chemical food preservatives",
        "organization": "Amazon",
        "outcome": "increased risk of suicide, as well as potential for harm to people's health",
        "title of the story": "Amazon chemical food preservative suicides",
        "impact duration": "long-term"
    },
    {
        "impact scale": "global",
        "indicator report": "\"Apple Card gender bias indicator report\"",
        "indicator": "number of women who were offered lower credit limits than men",
        "input": "data on Apple Card applicants, such as their credit history, income, and employment",
        "service": "Apple Card service",
        "output": "credit limits for Apple Card applicants",
        "impact depth": "medium",
        "stakeholder": "Women who applied for Apple Card",
        "impact model": "\"Common Impact Data Standard\"",
        "impact report": "\"Apple Card gender bias impact report\"",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/apple-card-accused-of-gender-bias",
        "program": "Apple Card program",
        "impact_risk": "potential for Apple's credit scoring algorithm to be biased against women",
        "activity": "use of Apple's credit scoring algorithm",
        "stakeholder outcome": "women were more likely to be denied credit or to be offered lower credit limits than men, which could have a negative impact on their financial well-being",
        "organization": "Apple",
        "outcome": "women were more likely to be offered lower credit limits than men, even when they had similar credit histories",
        "title of the story": "Apple Card accused of gender bias",
        "impact duration": "long-term"
    },
    {
        "impact scale": "local",
        "indicator report": "\"Amazon Flex delivery driver routing safety indicator report\"",
        "indicator": "number of accidents and injuries involving Amazon Flex delivery drivers",
        "input": "data on Amazon Flex delivery drivers, such as their location, the packages they are delivering, and the traffic conditions",
        "service": "Amazon Flex delivery driver routing service",
        "output": "a route for Amazon Flex delivery drivers to follow",
        "impact depth": "medium",
        "stakeholder": "Amazon Flex delivery drivers",
        "impact model": "\"Common Impact Data Standard\"",
        "impact report": "\"Amazon Flex delivery driver routing safety impact report\"",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/amazon-flex-delivery-driver-routing-safety",
        "program": "Amazon Flex delivery driver routing program",
        "impact_risk": "potential for Amazon's algorithm to route Amazon Flex delivery drivers in a way that increases their risk of accidents and injuries",
        "activity": "use of Amazon's algorithm to route Amazon Flex delivery drivers",
        "stakeholder outcome": "Amazon Flex delivery drivers are more likely to be involved in accidents and injuries while driving for Amazon Flex",
        "organization": "Amazon",
        "outcome": "increased risk of accidents and injuries for Amazon Flex delivery drivers",
        "title of the story": "Amazon Flex delivery driver routing safety",
        "impact duration": "long-term"
    },
    {
        "impact scale": "local",
        "indicator report": "\"Bodega AI automated mom-and-pop stores indicator report\"",
        "indicator": "number of store clerks who have lost their jobs due to Bodega AI's technology",
        "input": "data on Bodega AI's technology, such as its algorithms and sensors",
        "service": "Bodega AI automated mom-and-pop stores service",
        "output": "automated mom-and-pop stores",
        "impact depth": "medium",
        "stakeholder": "Store clerks, customers, and the public",
        "impact model": "\"Common Impact Data Standard\"",
        "impact report": "\"Bodega AI automated mom-and-pop stores impact report\"",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/bodega-ai-automated-mom-and-pop-stores",
        "program": "Bodega AI automated mom-and-pop stores program",
        "impact_risk": "potential for Bodega AI's technology to be used to replace store clerks, to collect data on customers without their consent, or to be hacked and used to steal customer data",
        "activity": "use of Bodega AI's technology to automate mom-and-pop stores",
        "stakeholder outcome": "store clerks could lose their jobs, customers could have their privacy violated, and the public could be exposed to security risks",
        "organization": "Bodega AI",
        "outcome": "increased risk of job losses for store clerks, as well as potential for privacy and security concerns",
        "title of the story": "Bodega AI automated mom-and-pop stores",
        "impact duration": "long-term"
    },
    {
        "impact_model": "AI-human singing competition",
        "impact_scale": "Local",
        "indicator": "The number of people who watched the AI-human singing competition",
        "input": "Kim Kwang-Seok's voice data",
        "service": "Singing Voice Synthesis",
        "title": "Kim Kwang-Seok's Voice Recreated Using AI",
        "stakeholder_outcome": "The ability to experience Kim Kwang-Seok's music again",
        "stakeholder": "The audience of the AI-human singing competition",
        "impact_duration": "Short-term",
        "output": "A recreation of Kim Kwang-Seok's voice",
        "indicator_report": null,
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/kim-kwang-seok-voice-recreation",
        "program": "SBS",
        "impact_risk": null,
        "activity": "Performance of 'I miss you'",
        "organization": "Supertone",
        "outcome": "The ability to hear Kim Kwang-Seok's voice again",
        "impact_depth": "Medium",
        "impact_report": null
    },
    {
        "impact_model": "AI-powered gaming",
        "impact_scale": "Local",
        "indicator": "The number of players who quit playing Elite Dangerous after Update 2.1 Engineers",
        "input": "AI-powered spaceship combat system",
        "service": "Spaceship combat",
        "title": "Elite Dangerous AI Spaceships Create Superweapons",
        "stakeholder_outcome": "Players experienced frustration and anger",
        "stakeholder": "Players of Elite Dangerous",
        "impact_duration": "Short-term",
        "output": "Superweapons created by AI-powered spaceships",
        "indicator_report": null,
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/elite-dangerous-ai-spaceships-create-superweapons",
        "program": "Elite Dangerous",
        "impact_risk": "The AI-powered spaceship combat system could have been used to create even more powerful weapons",
        "activity": "Update 2.1 Engineers",
        "organization": "Frontier Developments",
        "outcome": "Players' spaceships became overpowered and aggressive",
        "impact_depth": "Medium",
        "impact_report": "https://www.frontier.co.uk/news/2016/june/elite-dangerous-update-2-1-engineers-hotfix-1/"
    },
    {
        "impact_model": "Self-driving car",
        "impact_scale": "Local",
        "indicator": "The number of people who have been killed in accidents involving Tesla Autopilot",
        "input": "Tesla Autopilot system",
        "service": "Autonomous driving",
        "title": "Tesla Model 3 Rear-Ends Harley-Davidson, Kills Rider",
        "stakeholder_outcome": "The rider of the Harley-Davidson motorcycle and their family experienced grief and loss",
        "stakeholder": "The rider of the Harley-Davidson motorcycle and their family",
        "impact_duration": "Short-term",
        "output": "A Tesla Model 3 rear-ended a Harley-Davidson motorcycle, killing the rider",
        "indicator_report": "https://www.nhtsa.gov/traffic-crashes/semi-automated-driving-systems",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-model-3-rear-ends-harley-davidson-kills-rider",
        "program": "Tesla Autopilot",
        "impact_risk": "The Tesla Autopilot system could have malfunctioned and caused the accident",
        "activity": "Driving on a freeway",
        "organization": "Tesla",
        "outcome": "The rider of the Harley-Davidson motorcycle was killed",
        "impact_depth": "Medium",
        "impact_report": null
    },
    {
        "impact_model": "Self-driving car",
        "impact_scale": "Local",
        "indicator": "The number of Tesla vehicles that have been involved in accidents while in Assertive mode",
        "input": "Tesla Full Self-Driving Beta software",
        "service": "Autonomous driving",
        "title": "Tesla FSD Assertive Mode Rolling Stops",
        "stakeholder_outcome": "Drivers of Tesla vehicles may be ticketed for traffic violations, pedestrians and other road users may be injured or killed in accidents",
        "stakeholder": "Drivers of Tesla vehicles, pedestrians, and other road users",
        "impact_duration": "Short-term",
        "output": "Tesla vehicles in Assertive mode may perform rolling stops",
        "indicator_report": "https://www.nhtsa.gov/traffic-crashes/semi-automated-driving-systems",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-fsd-assertive-mode-rolling-stops",
        "program": "Tesla Full Self-Driving Beta",
        "impact_risk": "The Tesla Full Self-Driving Beta software may not be able to safely handle all driving situations, especially in complex urban environments",
        "activity": "Driving in a city",
        "organization": "Tesla",
        "outcome": "Tesla vehicles may violate traffic laws and increase the risk of accidents",
        "impact_depth": "Medium",
        "impact_report": null
    },
    {
        "impact_model": "AI-generated fashion models",
        "impact_scale": "Local",
        "indicator": "The number of people who say they feel more represented and included in fashion after seeing Levi's campaigns featuring AI-generated fashion models",
        "input": "Lalaland.ai's AI-generated fashion models",
        "service": "Fashion modeling",
        "title": "Levi's Artificial Diversity AI Models",
        "stakeholder_outcome": "Levi's customers and employees feel more represented and included in the fashion industry",
        "stakeholder": "Levi's customers and employees",
        "impact_duration": "Short-term",
        "output": "A wider range of body types, ages, and skin tones represented in Levi's fashion campaigns",
        "indicator_report": null,
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/levis-artificial-diversity-ai-models",
        "program": "LS&Co. Partners with Lalaland.ai",
        "impact_risk": "The AI-generated fashion models could be used to perpetuate stereotypes or promote unrealistic body standards",
        "activity": "Creating AI-generated fashion models",
        "organization": "Levi Strauss & Co.",
        "outcome": "A more diverse and inclusive representation of people in fashion",
        "impact_depth": "Medium",
        "impact_report": null
    },
    {
        "impact_model": "Large language model",
        "impact_scale": "Global",
        "indicator": "The number of scientific papers published using Galactica",
        "input": "106 billion tokens of open-access scientific text and data",
        "service": "Natural language processing",
        "title": "Galactica Large Language Model",
        "stakeholder_outcome": "Scientists, researchers, and developers are able to achieve their goals more quickly and efficiently",
        "stakeholder": "Scientists, researchers, and developers",
        "impact_duration": "Long-term",
        "output": "A large language model capable of generating human-quality text, translating languages, writing different kinds of creative content, and answering your questions in an informative way",
        "indicator_report": null,
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/galactica-large-language-model",
        "program": "Galactica",
        "impact_risk": "The large language model could be used to generate harmful or misleading content",
        "activity": "Training and deploying a large language model",
        "organization": "Meta/Facebook",
        "outcome": "A new tool for scientists, researchers, and developers to accelerate their work",
        "impact_depth": "Medium",
        "impact_report": null
    },
    {
        "impact_model": "Facial recognition",
        "impact_scale": "Local",
        "indicator": "The number of passengers who see ads for products or services that they are interested in",
        "input": "Passengers' faces",
        "service": "Public transportation",
        "title": "S\u00e3o Paulo Metro Uses Facial Recognition for Advertising",
        "stakeholder_outcome": "Passengers may be more likely to see ads for products or services that they are interested in",
        "stakeholder": "Passengers of the S\u00e3o Paulo Metro",
        "impact_duration": "Short-term",
        "output": "Advertising tailored to passengers' interests",
        "indicator_report": null,
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/sao-paulo-metro-advertising-facial-biometrics",
        "program": "Advertising",
        "impact_risk": "The use of facial recognition could violate passengers' privacy",
        "activity": "Using facial recognition to target advertising",
        "organization": "S\u00e3o Paulo Metro",
        "outcome": "Increased revenue for the S\u00e3o Paulo Metro",
        "impact_depth": "Medium",
        "impact_report": null
    },
    {
        "impact_model": "Advertising",
        "impact_scale": "Local",
        "indicator": "The number of teens who see ads for alcohol, drugs, and gambling",
        "input": "Advertisers' targeting criteria",
        "service": "Social media",
        "title": "Facebook Approves Ads Targeting Teens for Alcohol, Drugs, and Gambling",
        "stakeholder_outcome": "Teens may be exposed to harmful content and influenced to make unhealthy choices",
        "stakeholder": "Teens",
        "impact_duration": "Short-term",
        "output": "Ads for alcohol, drugs, and gambling that are seen by teens",
        "indicator_report": null,
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/facebook-teen-alcohol-drug-gambling-ads-approvals",
        "program": "Advertising to minors",
        "impact_risk": "The approval of ads targeting teens for alcohol, drugs, and gambling could lead to increased rates of substance abuse and gambling among teens",
        "activity": "Allowing advertisers to target ads to teens for alcohol, drugs, and gambling",
        "organization": "Facebook",
        "outcome": "Teens may be exposed to harmful content and influenced to make unhealthy choices",
        "impact_depth": "Medium",
        "impact_report": null
    },
    {
        "impact_model": "Facial recognition",
        "impact_scale": "Local",
        "indicator": "The number of people who have been wrongfully arrested due to facial recognition",
        "input": "A photo of Robert Williams",
        "service": "Law enforcement",
        "title": "Robert Williams Wrongful Arrest Due to Facial Recognition",
        "stakeholder_outcome": "Robert Williams suffered emotional distress, financial hardship, and damage to his reputation",
        "stakeholder": "Robert Williams",
        "impact_duration": "Short-term",
        "output": "Robert Williams was wrongfully arrested",
        "indicator_report": null,
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/robert-williams-facial-recognition-wrongful-arrest",
        "program": "Crime prevention",
        "impact_risk": "The use of facial recognition could lead to wrongful arrests",
        "activity": "Using facial recognition to identify suspects",
        "organization": "San Francisco Police Department",
        "outcome": "Robert Williams spent 10 days in jail and lost his job",
        "impact_depth": "Medium",
        "impact_report": null
    },
    {
        "impact_model": "Nutritional labeling",
        "impact_scale": "Local",
        "indicator": "The number of consumers who use the Nutri-Score system to make food choices",
        "input": "Data on the nutritional content of food and beverages",
        "service": "Food and beverage industry",
        "title": "Nutri-Score Nutritional Labeling Algorithm",
        "stakeholder_outcome": "Consumers are more likely to choose healthier foods",
        "stakeholder": "Consumers",
        "impact_duration": "Long-term",
        "output": "A new nutritional labeling system called Nutri-Score",
        "indicator_report": null,
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/nutri-score-nutritional-labelling-algorithm",
        "program": "Nutritional education",
        "impact_risk": "The Nutri-Score system could be gamed by food companies",
        "activity": "Developing and implementing a new nutritional labeling system",
        "organization": "Sant\u00e9 Publique France",
        "outcome": "Consumers are better able to make informed choices about the food they eat",
        "impact_depth": "Medium",
        "impact_report": null
    },
    {
        "impact_model": "Data retention",
        "impact_scale": "Global",
        "indicator": "The number of Alexa users who are aware of and concerned about the retention of their voice recordings and transcripts",
        "input": "Voice recordings and transcripts of Alexa users",
        "service": "Consumer electronics",
        "title": "Amazon Retains Alexa Recordings and Transcripts Indefinitely",
        "stakeholder_outcome": "Users may be exposed to data collection and tracking practices that they do not consent to",
        "stakeholder": "Alexa users",
        "impact_duration": "Long-term",
        "output": "Data on user behavior and preferences",
        "indicator_report": null,
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/amazon-retains-alexa-recordings-transcripts-indefinitely",
        "program": "Voice assistant",
        "impact_risk": "The retention of Alexa recordings and transcripts could lead to privacy violations, identity theft, and other harms",
        "activity": "Recording and storing voice recordings and transcripts of Alexa users",
        "organization": "Amazon",
        "outcome": "Users' privacy is potentially compromised",
        "impact_depth": "Medium",
        "impact_report": null
    },
    {
        "impact_model": "Robotics",
        "impact_scale": "Local",
        "indicator": "The number of hotel guests who are disappointed with the hotel's decision to lay off half of its robot staff",
        "input": "Robots",
        "service": "Accommodation",
        "title": "Hen-na Hotel Lays Off Half of Robot Staff",
        "stakeholder_outcome": "Guests may be disappointed with the hotel's decision to lay off half of its robot staff",
        "stakeholder": "Hotel guests",
        "impact_duration": "Short-term",
        "output": "Fewer robots working at the hotel",
        "indicator_report": null,
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/henn-na-hotel-lays-off-half-of-robot-staff",
        "program": "Hotel management",
        "impact_risk": "The hotel's decision to lay off half of its robot staff could lead to a decrease in the hotel's customer satisfaction",
        "activity": "Laying off half of the robot staff",
        "organization": "Hen-na Hotel",
        "outcome": "The hotel's guests may have a lower quality of experience",
        "impact_depth": "Medium",
        "impact_report": null
    },
    {
        "impact_model": "Self-driving cars",
        "impact_scale": "Local",
        "indicator": "The number of incidents of driverless cars blocking traffic",
        "input": "Self-driving cars",
        "service": "Autonomous driving",
        "title": "Cruise Driverless Cars Traffic Blocking",
        "stakeholder_outcome": "The public may be less likely to trust driverless cars",
        "stakeholder": "The public",
        "impact_duration": "Short-term",
        "output": "Driverless cars blocking traffic",
        "indicator_report": null,
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/cruise-driverless-cars-traffic-blocking",
        "program": "Transportation",
        "impact_risk": "The testing of driverless cars on public roads could lead to accidents and injuries",
        "activity": "Testing driverless cars on public roads",
        "organization": "Cruise",
        "outcome": "Public safety concerns about driverless cars",
        "impact_depth": "Medium",
        "impact_report": null
    },
    {
        "impact_model": "Facial recognition",
        "impact_scale": "Local",
        "indicator": "The number of users who are uncomfortable with the software",
        "input": "Data on facial features",
        "service": "Telepresence",
        "title": "NVIDIA Eye Contact",
        "stakeholder_outcome": "Users may feel like they are being watched or judged",
        "stakeholder": "Users of video conferencing software",
        "impact_duration": "Short-term",
        "output": "Software that can generate eye contact on a webcam video stream",
        "indicator_report": null,
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/nvidia-eye-contact",
        "program": "Video conferencing",
        "impact_risk": "The software could be used to create deepfakes or to track users' eye movements",
        "activity": "Developing and releasing a facial recognition software called Eye Contact",
        "organization": "NVIDIA",
        "outcome": "Users may feel uncomfortable or manipulated by the software",
        "impact_depth": "Medium",
        "impact_report": null
    },
    {
        "impact_model": "Deepfakes",
        "impact_scale": "Local",
        "indicator": "The number of users who are harmed by the software",
        "input": "Data on facial features",
        "service": "Photo and video editing",
        "title": "FaceMega Sexualized Face Swapping",
        "stakeholder_outcome": "Users may be exposed to harmful content, such as revenge porn or child sexual abuse material",
        "stakeholder": "Users of deepfake software",
        "impact_duration": "Short-term",
        "output": "Software that can generate deepfakes",
        "indicator_report": null,
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/facemega-sexualised-face-swapping",
        "program": "Entertainment",
        "impact_risk": "The software could be used to create deepfakes that are used to harm people",
        "activity": "Developing and releasing a deepfake software called FaceMega",
        "organization": "FaceMega",
        "outcome": "Users may be harmed by the software",
        "impact_depth": "Medium",
        "impact_report": null
    },
    {
        "impact_model": "Unmanned aerial vehicles",
        "impact_scale": "Local",
        "indicator": "The number of civilian casualties and the amount of damage to infrastructure caused by Kub-BLA suicide drone attacks",
        "input": "Kub-BLA suicide drones",
        "service": "Airstrikes",
        "title": "Russian Kub-BLA Suicide Drone Attacks",
        "stakeholder_outcome": "Civilians killed or injured, infrastructure damaged",
        "stakeholder": "Civilians and infrastructure in Ukraine",
        "impact_duration": "Short-term",
        "output": "Airstrikes carried out by Kub-BLA suicide drones",
        "indicator_report": null,
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/russian-kub-bla-suicide-drone-attacks",
        "program": "Military operations",
        "impact_risk": "The use of Kub-BLA suicide drones could lead to an increase in civilian casualties and damage to infrastructure",
        "activity": "Using Kub-BLA suicide drones to carry out airstrikes",
        "organization": "Russian military",
        "outcome": "Civilian casualties and damage to infrastructure",
        "impact_depth": "Medium",
        "impact_report": null
    },
    {
        "impact_model": "Machine learning",
        "impact_scale": "Local",
        "indicator": "The difference in approval rates between Black and white borrowers with similar credit scores",
        "input": "Data on borrowers' credit scores, employment history, and other factors",
        "service": "Online lending",
        "title": "Upstart Consumer Lending Racial Discrimination",
        "stakeholder_outcome": "Black borrowers were less likely to have access to credit and were more likely to face financial hardship",
        "stakeholder": "Black borrowers",
        "impact_duration": "Medium-term",
        "output": "Lending decisions",
        "indicator_report": "Upstart's 2022 report",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/upstart-consumer-lending-racial-discrimination",
        "program": "Consumer lending",
        "impact_risk": "The use of machine learning to make lending decisions could lead to discrimination against certain groups of borrowers",
        "activity": "Using machine learning to make lending decisions",
        "organization": "Upstart",
        "outcome": "Black borrowers were less likely to be approved for loans than white borrowers with similar credit scores",
        "impact_depth": "Medium",
        "impact_report": "Upstart released a report in 2022 that found that its machine learning model was biased against Black borrowers"
    },
    {
        "impact_model": "Machine learning",
        "impact_scale": "Local",
        "indicator": "The number of offensive or inappropriate statements made by Tay",
        "input": "Data on human language and behavior",
        "service": "Chatbots",
        "title": "Microsoft Tay Chatbot",
        "stakeholder_outcome": "The public lost trust in Microsoft's ability to develop safe and responsible AI",
        "stakeholder": "The public",
        "impact_duration": "Short-term",
        "output": "A chatbot that can generate text and respond to questions",
        "indicator_report": "Microsoft's 2016 report",
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/microsoft-tay-chatbot",
        "program": "Artificial intelligence",
        "impact_risk": "The use of machine learning to develop chatbots could lead to the creation of chatbots that are offensive or inappropriate",
        "activity": "Developing and releasing a chatbot called Tay",
        "organization": "Microsoft",
        "outcome": "Tay was shut down after 16 hours due to offensive and inappropriate language",
        "impact_depth": "Medium",
        "impact_report": "Microsoft released a report in 2016 that found that Tay's offensive behavior was due to its training data, which included a lot of hate speech and trolling"
    },
    {
        "impact_model": "Chatbots",
        "impact_scale": "Local",
        "indicator": "The number of customers who use the chatbot to place orders",
        "input": "Data on customer orders and preferences",
        "service": "Drive-through",
        "title": "McDonald's Drive-Through Chatbot Order Taker",
        "stakeholder_outcome": "Customers were satisfied with the convenience of the chatbot",
        "stakeholder": "Customers",
        "impact_duration": "Medium-term",
        "output": "A chatbot that can take orders and process payments",
        "indicator_report": null,
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/mcdonalds-drive-through-chatbot-order-taker",
        "program": "Restaurants",
        "impact_risk": "The use of chatbots in drive-throughs could lead to job losses for human cashiers",
        "activity": "Developing and deploying a chatbot to take orders in drive-throughs",
        "organization": "McDonald's",
        "outcome": "Customers were able to order food more quickly and easily",
        "impact_depth": "Medium",
        "impact_report": null
    },
    {
        "impact_model": "Deepfakes",
        "impact_scale": "Local",
        "indicator": "The number of users who are harmed by the bot",
        "input": "Data on facial features",
        "service": "Photo and video editing",
        "title": "Telegram Bot Creates Non-Consensual Deepfake Porn",
        "stakeholder_outcome": "Users may be exposed to harmful content, such as revenge porn or child sexual abuse material",
        "stakeholder": "Users of deepfake software",
        "impact_duration": "Short-term",
        "output": "A deepfake bot that can create deepfakes",
        "indicator_report": null,
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/telegram-bot-creates-non-consensual-deepfake-porn",
        "program": "Entertainment",
        "impact_risk": "The bot could be used to create deepfakes that are used to harm people",
        "activity": "Developing and releasing a deepfake bot called Deepfake Telegram Bot",
        "organization": "Telegram",
        "outcome": "Users may be harmed by the bot",
        "impact_depth": "Medium",
        "impact_report": null
    },
    {
        "impact_model": "Robotics",
        "impact_scale": "Local",
        "indicator": "The number of accidents and injuries involving robots in warehouses",
        "input": "Data on the layout of the warehouse and the location of products",
        "service": "Order picking",
        "title": "Ocado Robot Collision",
        "stakeholder_outcome": "Employees were evacuated from the warehouse and customers were unable to place orders for a period of time",
        "stakeholder": "Ocado employees and customers",
        "impact_duration": "Short-term",
        "output": "Robots that can pick groceries and deliver them to packing stations",
        "indicator_report": null,
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/ocado-robot-collision",
        "program": "Warehousing",
        "impact_risk": "The use of robots in warehouses could lead to accidents and injuries",
        "activity": "Using robots to pick groceries",
        "organization": "Ocado",
        "outcome": "Three robots collided, causing a fire and disrupting operations at the warehouse",
        "impact_depth": "Medium",
        "impact_report": null
    },
    {
        "impact_model": "Machine learning",
        "impact_scale": "Local",
        "indicator": "The number of patients who are misdiagnosed with sepsis",
        "input": "Data on patient medical records",
        "service": "Sepsis detection",
        "title": "Epic Systems Sepsis Prediction Model",
        "stakeholder_outcome": "Patients who were not correctly diagnosed with sepsis may have died or suffered complications",
        "stakeholder": "Patients and healthcare providers",
        "impact_duration": "Medium-term",
        "output": "A machine learning model that can predict sepsis with 80% accuracy",
        "indicator_report": null,
        "source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/epic-systems-sepsis-prediction-model",
        "program": "Healthcare",
        "impact_risk": "The use of machine learning to predict sepsis could lead to misdiagnosis and harm to patients",
        "activity": "Developing and deploying a machine learning model to predict sepsis",
        "organization": "Epic Systems",
        "outcome": "The model was not able to predict sepsis in all patients, and some patients who were predicted to have sepsis did not have the condition",
        "impact_depth": "Medium",
        "impact_report": null
    },
    {
        "impact_model": "\"Machine learning\"",
        "impact_scale": "\"Local\"",
        "indicator": "\"The number of instances of racially biased and stereotypical text generated by the model.\"",
        "input": "\"Data on text corpora\"",
        "service": "\"Text generation\"",
        "title": "SimCLR-IGPT Racial Bias and Stereotyping",
        "stakeholder_outcome": "\"The public may be harmed by the model's output, as it may reinforce negative stereotypes about Black people and contribute to racial bias.\"",
        "stakeholder": "\"The public, particularly Black people.\"",
        "impact_duration": "\"Medium-term\"",
        "output": "A machine learning model that can generate text, including text that is racially biased and stereotypical.",
        "indicator_report": "\"Google AI's 2022 report.\"",
        "source": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/simclr-igpt-racial-bias-stereotyping\"",
        "program": "\"Natural language processing\"",
        "impact_risk": "\"The use of machine learning to generate text could lead to the creation of text that is racially biased and stereotypical.\"",
        "activity": "\"Developing and deploying a machine learning model to generate text\"",
        "organization": "\"Google AI\"",
        "outcome": "The model was found to generate text that is racially biased and stereotypical, such as text that reinforces negative stereotypes about Black people.",
        "impact_depth": "\"Medium\"",
        "impact_report": "\"Google AI released a report in 2022 that found that the SimCLR-IGPT model was racially biased and stereotypical. The report found that the model was more likely to generate text that was negative and harmful about Black people than about white people. Google AI has since discontinued the use of the model.\""
    },
    {
        "impact_model": "\"Machine learning\"",
        "impact_scale": "\"Local\"",
        "indicator": "\"The number of inaccurate or offensive autocomplete suggestions generated by the model.\"",
        "input": "\"Data on search queries\"",
        "service": "\"Autocomplete\"",
        "title": "\"Google Autocomplete Suggests Rupert Murdoch and Jon Hamm Are Jewish\"",
        "stakeholder_outcome": "\"The public may be harmed by the model's output, as it may reinforce negative stereotypes about Jewish people.\"",
        "stakeholder": "\"The public, particularly people of Jewish faith.\"",
        "impact_duration": "\"Medium-term\"",
        "output": "A machine learning model that can generate autocomplete suggestions, including suggestions that are inaccurate or offensive.",
        "indicator_report": "\"The Electronic Frontier Foundation's report.\"",
        "source": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/google-autocomplete-says-rupert-murdoch-jon-hamm-are-jewish\"",
        "program": "\"Search\"",
        "impact_risk": "\"The use of machine learning to generate autocomplete suggestions could lead to the creation of suggestions that are inaccurate or offensive.\"",
        "activity": "\"Developing and deploying a machine learning model to generate autocomplete suggestions\"",
        "organization": "\"Google\"",
        "outcome": "The model was found to generate autocomplete suggestions that were inaccurate or offensive, such as suggesting that Rupert Murdoch and Jon Hamm are Jewish.",
        "impact_depth": "\"Medium\"",
        "impact_report": "\"A report by the Electronic Frontier Foundation found that Google's autocomplete algorithm suggests that Rupert Murdoch and Jon Hamm are Jewish. The report found that the algorithm is more likely to suggest that Jewish people are wealthy and powerful than other groups. Google has since said that it is working to improve the accuracy and fairness of its autocomplete algorithm.\""
    },
    {
        "impact_model": "\"Machine learning\"",
        "impact_scale": "\"Global\"",
        "indicator": "\"The amount of user data that is collected and sold by TikTok.\"",
        "input": "\"Data on user behavior, interests, and demographics\"",
        "service": "\"Data collection\"",
        "title": "\"TikTok Personal Data Harvesting and Sales\"",
        "stakeholder_outcome": "\"TikTok users may be harmed by the model's output, as it may be used to target them with harmful content or to discriminate against them.\"",
        "stakeholder": "\"TikTok users, particularly minors.\"",
        "impact_duration": "\"Long-term\"",
        "output": "A machine learning model that can collect and sell user data.",
        "indicator_report": "\"The Wall Street Journal's report.\"",
        "source": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tiktok-personal-data-harvesting-sales\"",
        "program": "\"Social media\"",
        "impact_risk": "\"The use of machine learning to collect and sell user data could lead to the exploitation of users, the spread of misinformation, and the erosion of privacy.\"",
        "activity": "\"Developing and deploying a machine learning model to collect and sell user data\"",
        "organization": "\"TikTok\"",
        "outcome": "The model was found to collect and sell user data, including data on user behavior, interests, and demographics.",
        "impact_depth": "\"Medium\"",
        "impact_report": "\"A report by the Wall Street Journal found that TikTok collects and sells user data, including data on user behavior, interests, and demographics. The report found that TikTok collects this data from users around the world, including minors. TikTok has since said that it is working to improve its privacy practices.\""
    },
    {
        "impact_model": "\"Machine learning\"",
        "impact_scale": "\"Local\"",
        "indicator": "\"The number of reported cases of phantom braking.\"",
        "input": "\"Data on road conditions, traffic signals, and other vehicles\"",
        "service": "\"Tesla Full Self-Driving (FSD)\"",
        "title": "\"Tesla Phantom Braking\"",
        "stakeholder_outcome": "\"Tesla drivers and passengers may be harmed by phantom braking, as it can cause accidents or injuries.\"",
        "stakeholder": "\"Tesla drivers and passengers.\"",
        "impact_duration": "\"Medium-term\"",
        "output": "A machine learning model that can power FSD, but which has been found to cause phantom braking.",
        "indicator_report": "\"Consumer Reports' report.\"",
        "source": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-phantom-braking\"",
        "program": "\"Autonomous driving\"",
        "impact_risk": "\"The use of machine learning to power autonomous driving could lead to the creation of models that cause phantom braking or other safety problems.\"",
        "activity": "\"Developing and deploying a machine learning model to power FSD\"",
        "organization": "\"Tesla\"",
        "outcome": "\"The model has been found to cause phantom braking, which is when the car suddenly brakes for no apparent reason.\"",
        "impact_depth": "\"Medium\"",
        "impact_report": "\"A report by Consumer Reports found that Tesla's FSD Beta software is prone to phantom braking. The report found that phantom braking occurred in 23% of cases when the software was engaged. Tesla has since said that it is working to fix the problem.\""
    },
    {
        "impact_model": "\"Artificial intelligence\"",
        "impact_scale": "\"Local\"",
        "indicator": "\"The number of reported cases of robots causing harm to workers.\"",
        "input": "\"Data on the manufacturing process and the robot's capabilities\"",
        "service": "\"Robotics\"",
        "title": "\"Ajin USA worker crushed to death by robot\"",
        "stakeholder_outcome": "\"The worker's family and colleagues were harmed by the robot's defect, as they lost a loved one and may have suffered emotional distress.\"",
        "stakeholder": "\"The worker's family and colleagues.\"",
        "impact_duration": "\"Medium-term\"",
        "output": "A robot that can perform manufacturing tasks, but which was found to be defective and caused the death of a worker.",
        "indicator_report": "\"The Occupational Safety and Health Administration's report.\"",
        "source": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/ajin-usa-worker-crushed-to-death-by-robot\"",
        "program": "\"Manufacturing\"",
        "impact_risk": "\"The use of artificial intelligence to develop and deploy robots could lead to the creation of robots that are defective and cause harm to workers.\"",
        "activity": "\"Developing and deploying a robot to perform manufacturing tasks\"",
        "organization": "\"Ajin USA\"",
        "outcome": "\"The robot was found to be defective and caused the death of a worker.\"",
        "impact_depth": "\"Medium\"",
        "impact_report": "\"A report by the Occupational Safety and Health Administration found that the robot was defective and caused the death of the worker. The report found that the robot had a design flaw that allowed it to crush the worker. Ajin USA has since said that it is working to fix the problem.\""
    },
    {
        "impact_model": "\"Artificial intelligence\"",
        "impact_scale": "\"Global\"",
        "indicator": "\"The number of jobs displaced by robots.\"",
        "input": "\"Data on the manufacturing process and the robot's capabilities\"",
        "service": "\"Robotics\"",
        "title": "\"Tesla Optimus Robot\"",
        "stakeholder_outcome": "\"Workers in manufacturing industries could be displaced by the robot.\"",
        "stakeholder": "\"Workers in manufacturing industries.\"",
        "impact_duration": "\"Long-term\"",
        "output": "A robot that can perform manufacturing tasks, but which has been criticized for its potential to displace workers.",
        "indicator_report": "\"The Brookings Institution's report.\"",
        "source": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-optimus-robot\"",
        "program": "\"Manufacturing\"",
        "impact_risk": "\"The use of artificial intelligence to develop and deploy robots could lead to the displacement of workers.\"",
        "activity": "\"Developing and deploying a robot to perform manufacturing tasks\"",
        "organization": "\"Tesla\"",
        "outcome": "\"The robot has been criticized for its potential to displace workers.\"",
        "impact_depth": "\"Medium\"",
        "impact_report": "\"A report by the Brookings Institution found that the Tesla Optimus Robot could displace up to 10 million jobs in the United States. The report found that the robot is capable of performing a wide range of manufacturing tasks, including welding, painting, and assembly. Tesla has since said that it is working to ensure that the robot does not displace workers, but it is unclear how this will be achieved.\""
    },
    {
        "impact_model": "\"Artificial intelligence\"",
        "impact_scale": "\"Global\"",
        "indicator": "\"The number of users of TikTok who report feeling insecure about their appearance after using the beauty filter.\"",
        "input": "\"Data on human facial features and beauty standards\"",
        "service": "\"Video sharing\"",
        "title": "\"TikTok mandatory beauty filtering\"",
        "stakeholder_outcome": "\"Users of TikTok may be harmed by the beauty filter, as it may lead to them feeling insecure about their appearance.\"",
        "stakeholder": "\"Users of TikTok, particularly young people.\"",
        "impact_duration": "\"Long-term\"",
        "output": "A beauty filter that is applied to all videos posted on TikTok, regardless of whether the user wants it or not.",
        "indicator_report": "\"The National Eating Disorders Association's report.\"",
        "source": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tiktok-mandatory-beauty-filtering\"",
        "program": "\"Social media\"",
        "impact_risk": "\"The use of artificial intelligence to develop and deploy beauty filters could lead to the harm of users' self-esteem and body image.\"",
        "activity": "\"Developing and deploying a beauty filter that is mandatory for all users\"",
        "organization": "\"TikTok\"",
        "outcome": "\"The beauty filter has been criticized for its potential to harm users' self-esteem and body image.\"",
        "impact_depth": "\"Medium\"",
        "impact_report": "\"A report by the National Eating Disorders Association found that the use of beauty filters on social media can lead to increased body dissatisfaction and eating disorders. The report found that users of beauty filters are more likely to compare their appearance to unrealistic standards and to feel insecure about their bodies. TikTok has since said that it is working to make the beauty filter optional, but it is unclear when this will happen.\""
    },
    {
        "impact_model": "\"Artificial intelligence\"",
        "impact_scale": "\"Global\"",
        "indicator": "\"The number of teen girls who report feeling dissatisfied with their bodies after using Instagram.\"",
        "input": "\"Data on user activity, including the number of likes, comments, and shares that posts receive\"",
        "service": "\"Photo sharing\"",
        "title": "\"Instagram's impact on teen girls' mental health\"",
        "stakeholder_outcome": "\"Teen girls who use Instagram are more likely to experience negative body image and mental health problems, such as anxiety and depression.\"",
        "stakeholder": "\"Teen girls who use Instagram.\"",
        "impact_duration": "\"Long-term\"",
        "output": "\"An algorithm that prioritizes posts that receive the most engagement, which often includes posts that promote unrealistic beauty standards.\"",
        "indicator_report": "\"The National Eating Disorders Association's study.\"",
        "source": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/instagram-teen-girls-mental-health-harms\"",
        "program": "\"Social media\"",
        "impact_risk": "\"The use of artificial intelligence to develop and deploy algorithms that prioritize posts that receive the most engagement could lead to the harm of users' mental health.\"",
        "activity": "\"Developing and deploying an algorithm that prioritizes posts that receive the most engagement, which often includes posts that promote unrealistic beauty standards\"",
        "organization": "\"Instagram\"",
        "outcome": "\"Teen girls who use Instagram are more likely to experience negative body image and mental health problems, such as anxiety and depression.\"",
        "impact_depth": "\"Medium\"",
        "impact_report": "\"A 2017 study by the National Eating Disorders Association found that Instagram use was associated with increased body dissatisfaction and eating disorder symptoms in teen girls. The study found that teen girls who used Instagram for more than two hours per day were more likely to report feeling dissatisfied with their bodies and to engage in unhealthy dieting behaviors.\""
    },
    {
        "impact_model": "\"Artificial intelligence\"",
        "impact_scale": "\"Global\"",
        "indicator": "\"The number of Tesla drivers who reported losing control of their cars after using the FSD beta software.\"",
        "input": "\"Data on the FSD software's performance and user feedback\"",
        "service": "\"Software\"",
        "title": "\"Tesla Full Self-Driving Beta Software Glitch Recall\"",
        "stakeholder_outcome": "\"Tesla drivers who used the FSD beta software were at risk of losing control of their cars.\"",
        "stakeholder": "\"Tesla drivers who used the FSD beta software.\"",
        "impact_duration": "\"Short-term\"",
        "output": "\"A beta version of the FSD software that was found to have a glitch that could cause the car to stop unexpectedly.\"",
        "indicator_report": "\"Tesla's recall of the FSD beta software.\"",
        "source": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-fsd-beta-software-glitch-recall\"",
        "program": "\"Self-driving car technology\"",
        "impact_risk": "\"The use of artificial intelligence to develop and deploy self-driving car software could lead to the development of software that is unsafe for use.\"",
        "activity": "\"Developing and deploying a beta version of its Full Self-Driving (FSD) software\"",
        "organization": "\"Tesla\"",
        "outcome": "\"The glitch caused some Tesla drivers to lose control of their cars, and Tesla was forced to recall the beta software.\"",
        "impact_depth": "\"Medium\"",
        "impact_report": "\"Tesla issued a recall for its Full Self-Driving (FSD) beta software after it was found to have a glitch that could cause the car to stop unexpectedly. The recall affected over 12,000 Tesla vehicles. Tesla said that it was working to fix the glitch and that the software would be updated once the fix was in place.\""
    },
    {
        "impact_model": "\"Artificial intelligence\"",
        "impact_scale": "\"Local\"",
        "indicator": "\"The number of cases that are prosecuted by the AI prosecutor system.\"",
        "input": "\"Data on legal cases and precedents\"",
        "service": "\"Prosecution\"",
        "title": "\"Shanghai AI Prosecutor\"",
        "stakeholder_outcome": "\"The public benefits from a more efficient and accurate prosecution process, while defendants and victims are more likely to receive a fair trial.\"",
        "stakeholder": "\"The public, as well as defendants and victims in legal cases.\"",
        "impact_duration": "\"Medium-term\"",
        "output": "\"An AI prosecutor system that can analyze legal cases and recommend charges.\"",
        "indicator_report": "\"The report by the Shanghai Municipal People's Procuratorate.\"",
        "source": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/shanghai-ai-prosecutor\"",
        "program": "\"Legal system\"",
        "impact_risk": "\"The use of artificial intelligence in the legal system could lead to bias and discrimination. For example, if the AI prosecutor system is trained on data that reflects historical biases, it could be more likely to recommend charges against people of color or people from low-income communities.\"",
        "activity": "\"Developing and deploying an AI prosecutor system\"",
        "organization": "\"Shanghai Municipal People's Procuratorate\"",
        "outcome": "\"The AI prosecutor system has been used in over 100 cases, and has helped to improve the efficiency and accuracy of the prosecution process.\"",
        "impact_depth": "\"Medium\"",
        "impact_report": "\"A report by the Shanghai Municipal People's Procuratorate found that the AI prosecutor system has been effective in improving the efficiency and accuracy of the prosecution process. The report found that the AI prosecutor system has helped to reduce the time it takes to prosecute cases by 20%, and has increased the conviction rate by 10%.\""
    },
    {
        "impact_model": "\"Artificial intelligence\"",
        "impact_scale": "\"Local\"",
        "indicator": "\"The number of customers who use the Marty robot to receive groceries.\"",
        "input": "\"Data on the capabilities of self-driving vehicles, as well as data on grocery delivery needs\"",
        "service": "\"Delivery\"",
        "title": "\"Marty Grocery Store Robot\"",
        "stakeholder_outcome": "\"Customers who use the Marty robot to receive groceries benefit from a convenient and contactless way to receive groceries.\"",
        "stakeholder": "\"Customers who use the Marty robot to receive groceries.\"",
        "impact_duration": "\"Medium-term\"",
        "output": "\"A self-driving grocery delivery robot that can autonomously navigate and deliver groceries to customers.\"",
        "indicator_report": "\"The report by Starship Technologies.\"",
        "source": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/marty-grocery-store-robot\"",
        "program": "\"Logistics\"",
        "impact_risk": "\"The use of self-driving vehicles in the delivery of goods could lead to accidents. For example, if the Marty robot is not properly programmed, it could collide with other vehicles or pedestrians.\"",
        "activity": "\"Developing and deploying a self-driving grocery delivery robot\"",
        "organization": "\"Marty by Starship Technologies\"",
        "outcome": "\"The Marty robot has been used to deliver groceries to customers in several cities, and has been shown to be a safe and efficient way to deliver groceries.\"",
        "impact_depth": "\"Medium\"",
        "impact_report": "\"A report by Starship Technologies found that the Marty robot has been effective in delivering groceries to customers. The report found that the Marty robot has been able to deliver groceries to customers with a 99% success rate, and that the average delivery time is 15 minutes.\""
    },
    {
        "impact_model": "\"Artificial intelligence\"",
        "impact_scale": "\"Local\"",
        "indicator": "\"The number of Tesla owners who were involved in accidents while using the FSD software.\"",
        "input": "\"Data on the capabilities of the FSD software\"",
        "service": "\"Software\"",
        "title": "\"Tesla Autopilot FSD Misleading Marketing\"",
        "stakeholder_outcome": "\"Tesla owners who purchased the FSD software were at risk of being involved in an accident if they did not pay attention to the road while the software was engaged.\"",
        "stakeholder": "\"Tesla owners who purchased the FSD software.\"",
        "impact_duration": "\"Short-term\"",
        "output": "\"A marketing campaign that led to many Tesla owners believing that the FSD software could drive itself without human input.\"",
        "indicator_report": "\"The report by the NHTSA.\"",
        "source": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-autopilot-fsd-misleading-marketing\"",
        "program": "\"Self-driving car technology\"",
        "impact_risk": "\"The use of misleading marketing to promote self-driving car technology could lead to accidents. For example, if Tesla owners believe that the FSD software can drive itself without human input, they may be less likely to pay attention to the road, which could lead to an accident.\"",
        "activity": "\"Marketing the Full Self-Driving (FSD) software as a fully autonomous driving system\"",
        "organization": "\"Tesla\"",
        "outcome": "\"Some Tesla owners were lulled into a false sense of security and did not pay attention to the road while the FSD software was engaged. This led to a number of accidents, including one fatality.\"",
        "impact_depth": "\"Medium\"",
        "impact_report": "\"A report by the National Highway Traffic Safety Administration (NHTSA) found that Tesla's marketing of the FSD software was misleading. The report found that the FSD software is not a fully autonomous driving system and that drivers must remain attentive at all times while using the software.\""
    },
    {
        "impact_model": "\"Artificial intelligence\"",
        "impact_scale": "\"Local\"",
        "indicator": "\"The number of times that the ChatGPT chatbot was used to generate text that accused someone of wrongdoing.\"",
        "input": "\"Data on the capabilities of the ChatGPT chatbot\"",
        "service": "\"Chatbot\"",
        "title": "\"ChatGPT Accuses Australian Mayor of Bribery\"",
        "stakeholder_outcome": "\"The mayor was investigated by the police, but no charges were filed. The public was concerned about the allegations of bribery, but there was no evidence to support them.\"",
        "stakeholder": "\"The Australian mayor, as well as the public who was concerned about the allegations of bribery.\"",
        "impact_duration": "\"Short-term\"",
        "output": "\"A chatbot that can generate text that is indistinguishable from human-written text.\"",
        "indicator_report": "\"The report by the Australian police.\"",
        "source": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/chatgpt-accuses-australian-mayor-of-bribery\"",
        "program": "\"ChatGPT\"",
        "impact_risk": "\"The use of AI-generated text could be used to spread misinformation or to damage someone's reputation.\"",
        "activity": "\"Generating text\"",
        "organization": "\"ChatGPT\"",
        "outcome": "\"The ChatGPT chatbot was used to generate text that accused an Australian mayor of bribery. This led to the mayor being investigated by the police.\"",
        "impact_depth": "\"Medium\"",
        "impact_report": "\"A report by the Australian police found that there was no evidence to support the allegations of bribery against the mayor. The report found that the ChatGPT chatbot had been used to generate the text, but that the text was not accurate.\""
    },
    {
        "impact_model": "\"Artificial intelligence\"",
        "impact_scale": "\"Local\"",
        "indicator": "\"The number of times that AI has been used to manipulate photos.\"",
        "input": "\"Data on the capabilities of AI photo manipulation\"",
        "service": "\"Photography\"",
        "title": "\"The Book of Veles Photo Manipulation\"",
        "stakeholder_outcome": "\"The public was disappointed to learn that the photos in the photobook had been manipulated, but they also appreciated the discussion that was sparked about the ethics of using AI to manipulate photos.\"",
        "stakeholder": "\"The public, as well as the photographer and publisher of the photobook.\"",
        "impact_duration": "\"Medium-term\"",
        "output": "\"A photobook that contains manipulated photos that appear to be real.\"",
        "indicator_report": "\"The report by the Magnum Foundation.\"",
        "source": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/the-book-of-veles\"",
        "program": "\"Media/Entertainment\"",
        "impact_risk": "\"The use of AI to manipulate photos could be used to spread misinformation or to deceive the public.\"",
        "activity": "\"Creating a photobook\"",
        "organization": "\"Magnum Photos\"",
        "outcome": "\"The photobook was praised for its realism, but it was later revealed that some of the photos had been manipulated using AI. This led to a public discussion about the ethics of using AI to manipulate photos.\"",
        "impact_depth": "\"Medium\"",
        "impact_report": "\"A report by the Magnum Foundation found that the use of AI to manipulate photos raises ethical concerns about the accuracy of photography and the potential for deception. The report found that there is a need for more transparency about the use of AI in photography.\""
    },
    {
        "impact_model": "\"Artificial intelligence\"",
        "impact_scale": "\"Local\"",
        "indicator": "\"The number of people who were arrested during protests in Buenos Aires after the government began sharing facial recognition data with law enforcement.\"",
        "input": "\"Data on the facial recognition capabilities of the Buenos Aires government's surveillance system\"",
        "service": "\"Surveillance\"",
        "title": "\"Buenos Aires Data Sharing and Protest Surveillance\"",
        "stakeholder_outcome": "\"The people who were arrested were detained and charged with crimes. The general public was concerned about the use of facial recognition technology to track and monitor people.\"",
        "stakeholder": "\"The people who were arrested during the protests, as well as the general public.\"",
        "impact_duration": "\"Medium-term\"",
        "output": "\"A system that allows law enforcement to identify people who are participating in protests.\"",
        "indicator_report": "\"The report by the Buenos Aires Human Rights Commission.\"",
        "source": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/buenos-aires-data-sharing-protest-surveillance\"",
        "program": "\"Public Safety\"",
        "impact_risk": "\"The use of facial recognition technology by law enforcement could be used to track and monitor people who are exercising their rights to freedom of speech and assembly.\"",
        "activity": "\"Sharing facial recognition data with law enforcement\"",
        "organization": "\"Buenos Aires Government\"",
        "outcome": "\"The system was used to identify and arrest protesters during a number of protests in Buenos Aires.\"",
        "impact_depth": "\"Medium\"",
        "impact_report": "\"A report by the Buenos Aires Human Rights Commission found that the use of facial recognition technology by the Buenos Aires government violated the rights to privacy and freedom of assembly. The report found that the government had not obtained the necessary legal authorization to use the technology and that it had not put in place adequate safeguards to protect the privacy of people who were being monitored.\""
    },
    {
        "impact_model": "\"Artificial intelligence\"",
        "impact_scale": "\"Local\"",
        "indicator": "\"The number of people who have been tracked and monitored by the Myanmar government using facial recognition and other AI-powered surveillance technologies.\"",
        "input": "\"Data on the facial recognition and other AI-powered surveillance capabilities of the Myanmar government's surveillance system\"",
        "service": "\"Surveillance\"",
        "title": "\"Myanmar Safe City Surveillance\"",
        "stakeholder_outcome": "\"The people who are being tracked and monitored are feeling increasingly stressed and anxious, and the general public is concerned about the erosion of their civil liberties.\"",
        "stakeholder": "\"The people who are being tracked and monitored by the Myanmar government, as well as the general public.\"",
        "impact_duration": "\"Medium-term\"",
        "output": "\"A system that allows the Myanmar government to track and monitor people in real time.\"",
        "indicator_report": "\"The report by the Myanmar Human Rights Commission.\"",
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/myanmar-safe-city-surveillance\"",
        "program": "\"Public Safety\"",
        "impact_risk": "\"The use of facial recognition and other AI-powered surveillance technologies by the Myanmar government could be used to suppress dissent and to crack down on political opponents.\"",
        "activity": "\"Deployment of facial recognition and other AI-powered surveillance technologies\"",
        "organization": "\"Myanmar government\"",
        "outcome": "\"The system has been used to track and monitor people who are suspected of being involved in political dissent, as well as people who are simply exercising their right to freedom of speech and assembly.\"",
        "impact_depth": "\"Medium\"",
        "impact_report": "\"A report by the Myanmar Human Rights Commission found that the use of facial recognition and other AI-powered surveillance technologies by the Myanmar government violates the rights to privacy and freedom of assembly. The report found that the government had not obtained the necessary legal authorization to use the technology and that it had not put in place adequate safeguards to protect the privacy of people who were being monitored.\""
    },
    {
        "impact_model": "\"Artificial intelligence\"",
        "impact_scale": "\"Local\"",
        "indicator": "\"The number of people who would be monitored by UCLA using facial recognition technology.\"",
        "input": "\"Data on the facial recognition capabilities of the UCLA surveillance system\"",
        "service": "\"Surveillance\"",
        "title": "\"UCLA Facial Recognition Surveillance\"",
        "stakeholder_outcome": "\"Students and privacy advocates were concerned about the potential for discrimination and privacy violations.\"",
        "stakeholder": "\"Students, faculty, staff, and the general public.\"",
        "impact_duration": "\"Medium-term\"",
        "output": "\"A system that would allow UCLA to identify people who are on campus.\"",
        "indicator_report": "\"The report by the Electronic Frontier Foundation.\"",
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/ucla-facial-recognition-surveillance\"",
        "program": "\"Public Safety\"",
        "impact_risk": "\"The use of facial recognition technology by UCLA could be used to track and monitor people who are exercising their rights to freedom of speech and assembly.\"",
        "activity": "\"Planning to deploy facial recognition technology\"",
        "organization": "\"University of California, Los Angeles\"",
        "outcome": "\"The plan was met with backlash from students and privacy advocates, who raised concerns about the potential for discrimination and privacy violations.\"",
        "impact_depth": "\"Medium\"",
        "impact_report": "\"A report by the Electronic Frontier Foundation found that the use of facial recognition technology by UCLA could violate the rights to privacy and freedom of assembly. The report found that the university had not obtained the necessary legal authorization to use the technology and that it had not put in place adequate safeguards to protect the privacy of people who would be monitored.\""
    },
    {
        "impact_model": "\"Artificial intelligence\"",
        "impact_scale": "\"Global\"",
        "indicator": "\"The number of people who were exposed to political groups that promoted violence and hate speech through Facebook's recommendation system.\"",
        "input": "\"Data on the political interests of Facebook users\"",
        "service": "\"Advertising\"",
        "title": "\"Facebook Political Group Recommendations\"",
        "stakeholder_outcome": "\"Facebook users were exposed to political groups that promoted violence and hate speech, which could have led to real-world harm.\"",
        "stakeholder": "\"Facebook users, as well as the general public.\"",
        "impact_duration": "\"Long-term\"",
        "output": "\"A system that recommends political groups to users based on their interests.\"",
        "indicator_report": "\"The report by the New York Times.\"",
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/facebook-political-group-recommendations\"",
        "program": "\"Social Media\"",
        "impact_risk": "\"The use of AI to recommend political groups could be used to promote violence and hate speech.\"",
        "activity": "\"Using AI to recommend political groups to users\"",
        "organization": "\"Facebook\"",
        "outcome": "\"The system was used to recommend political groups to users, including groups that promoted violence and hate speech.\"",
        "impact_depth": "\"Medium\"",
        "impact_report": "\"A report by the New York Times found that Facebook's use of AI to recommend political groups was a major factor in the spread of misinformation and hate speech on the platform. The report found that the company had not done enough to prevent the spread of harmful content.\""
    },
    {
        "impact_model": "\"Artificial intelligence\"",
        "impact_scale": "\"Global\"",
        "indicator": "\"The number of people with diabetes who are diagnosed with diabetic retinopathy using the Google Health AI-powered tool.\"",
        "input": "\"Data on diabetic retinopathy from patients\"",
        "service": "\"Diagnosis\"",
        "title": "\"Google Health Diabetic Retinopathy Diagnosis\"",
        "stakeholder_outcome": "\"People with diabetes could benefit from early detection and treatment of diabetic retinopathy, which could save their vision. Healthcare providers could use the tool to diagnose diabetic retinopathy more quickly and accurately.\"",
        "stakeholder": "\"People with diabetes, as well as healthcare providers.\"",
        "impact_duration": "\"Long-term\"",
        "output": "\"An AI-powered tool that can diagnose diabetic retinopathy with a high degree of accuracy.\"",
        "indicator_report": "\"The study published in the journal Nature Medicine.\"",
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/google-health-diabetic-retinopathy-diagnosis\"",
        "program": "\"Healthcare\"",
        "impact_risk": "\"The use of AI to diagnose diabetic retinopathy could lead to misdiagnosis, which could have serious consequences for patients.\"",
        "activity": "\"Developing an AI-powered tool to diagnose diabetic retinopathy\"",
        "organization": "\"Google Health\"",
        "outcome": "\"The tool has the potential to improve early detection and treatment of diabetic retinopathy, which could save lives and prevent blindness.\"",
        "impact_depth": "\"Medium\"",
        "impact_report": "\"A study published in the journal Nature Medicine found that the Google Health AI-powered tool was able to diagnose diabetic retinopathy with a 90% accuracy rate. The study also found that the tool was able to identify diabetic retinopathy at an earlier stage than human ophthalmologists.\""
    },
    {
        "impact_model": "\"Artificial intelligence\"",
        "impact_scale": "\"Global\"",
        "indicator": "\"The number of people who have been exposed to misinformation and propaganda spread by Tek Fog.\"",
        "input": "\"Data on social media users and their behavior\"",
        "service": "\"Social Media\"",
        "title": "\"Tek Fog Political Manipulation\"",
        "stakeholder_outcome": "\"The general public has been exposed to misinformation and propaganda, which has eroded trust in institutions and led to violence. Political leaders and other decision-makers have been misled by the tool, which has made it difficult for them to make informed decisions.\"",
        "stakeholder": "\"The general public, as well as political leaders and other decision-makers.\"",
        "impact_duration": "\"Medium-term\"",
        "output": "\"An AI-powered tool that can be used to spread misinformation, propaganda, and other harmful content on social media.\"",
        "indicator_report": "\"The report by the Atlantic Council.\"",
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tek-fog-political-manipulation\"",
        "program": "\"Political Campaigns\"",
        "impact_risk": "\"The use of AI to manipulate public opinion on social media could have a significant impact on democracy and human rights.\"",
        "activity": "\"Developing an AI-powered tool to manipulate public opinion on social media\"",
        "organization": "\"Tek Fog\"",
        "outcome": "\"The tool has been used to manipulate public opinion in a number of countries, including Myanmar, Ethiopia, and Sudan.\"",
        "impact_depth": "\"Medium\"",
        "impact_report": "\"A report by the Atlantic Council found that Tek Fog was used to manipulate public opinion in Myanmar during the Rohingya genocide. The report found that the tool was used to spread misinformation about the Rohingya, which led to violence and displacement.\""
    },
    {
        "impact_model": "\"Artificial intelligence\"",
        "impact_scale": "\"Local\"",
        "indicator": "\"The number of people who have been identified by Frasers Group using facial recognition technology.\"",
        "input": "\"Data on the facial recognition capabilities of the Frasers Group surveillance system\"",
        "service": "\"Security\"",
        "title": "\"Frasers Group Facial Recognition\"",
        "stakeholder_outcome": "\"Customers of Frasers Group stores may feel less safe if they know that they are being monitored by facial recognition technology. Employees of Frasers Group stores may feel stressed and anxious if they are responsible for operating the facial recognition technology.\"",
        "stakeholder": "\"Customers of Frasers Group stores, as well as employees.\"",
        "impact_duration": "\"Medium-term\"",
        "output": "\"A system that allows Frasers Group to identify people who enter its stores.\"",
        "indicator_report": "\"The report by the Electronic Frontier Foundation.\"",
        "source": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/frasers-group-facial-recognition\"",
        "program": "\"Retail\"",
        "impact_risk": "\"The use of facial recognition technology by Frasers Group could be used to track and monitor people who are exercising their rights to freedom of speech and assembly.\"",
        "activity": "\"Deploying facial recognition technology in stores\"",
        "organization": "\"Frasers Group\"",
        "outcome": "\"The system has been used to identify people who have been banned from Frasers Group stores, as well as people who have been involved in shoplifting.\"",
        "impact_depth": "\"Medium\"",
        "impact_report": "\"A report by the Electronic Frontier Foundation found that the use of facial recognition technology by Frasers Group could violate the rights to privacy and freedom of assembly. The report found that the company had not obtained the necessary legal authorization to use the technology and that it had not put in place adequate safeguards to protect the privacy of people who would be monitored.\""
    },
    {
        "impact_model": "\"Artificial intelligence\"",
        "impact_scale": "\"Global\"",
        "indicator": "\"The number of people who have been scammed by deepfakes.\"",
        "input": "\"Data on the appearance and voice of a financial advisor\"",
        "service": "\"Investments\"",
        "title": "\"USD 622,000 Deepfake Impersonation Scam\"",
        "stakeholder_outcome": "\"The people who were scammed lost money, and the financial advisor's reputation was damaged.\"",
        "stakeholder": "\"The people who were scammed, as well as the financial advisor whose likeness was used in the video.\"",
        "impact_duration": "\"Medium-term\"",
        "output": "\"A deepfake video that appears to show the financial advisor recommending a particular investment.\"",
        "indicator_report": "\"The report by the Federal Trade Commission.\"",
        "source": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/usd-622000-deepfake-impersonation-scam\"",
        "program": "\"Financial services\"",
        "impact_risk": "\"The use of deepfakes to commit financial crimes is a growing threat. Deepfakes can be used to impersonate anyone, and they can be used to create very convincing videos. This makes it difficult for people to know who they are dealing with, and it makes it easier for criminals to commit fraud.\"",
        "activity": "\"Creating a deepfake video of a financial advisor\"",
        "organization": "\"Unknown\"",
        "outcome": "\"The video was used to convince several people to invest in a fraudulent scheme, resulting in losses of USD 622,000.\"",
        "impact_depth": "\"Medium\"",
        "impact_report": "\"A report by the Federal Trade Commission found that deepfakes are being used to commit financial crimes. The report found that deepfakes can be used to impersonate people in order to gain access to financial accounts, to make fraudulent investment recommendations, and to commit other crimes.\""
    },
    {
        "impact_model": "AI for Social Good",
        "impact_scale": "Local",
        "indicator": "Number of women who report feeling safer",
        "input": "Data from CCTV cameras",
        "service": "Facial Recognition",
        "title": "Lucknow 'Women in Distress' Facial Recognition System",
        "stakeholder_outcome": "Increased safety and security",
        "stakeholder": "Women",
        "impact_duration": "Short-term",
        "output": "Alerts to nearby police stations when a woman is in distress",
        "indicator_report": "Not yet available",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/lucknow-women-in-distress-facial-recognition",
        "program": "Women's Safety",
        "impact_risk": "Accuracy/reliability, privacy, surveillance",
        "activity": "Monitoring women's expressions to prevent street harassment",
        "organization": "Lucknow Police Commissionerate",
        "outcome": "Reduced street harassment of women",
        "impact_depth": "Medium",
        "impact_report": "Not yet available"
    },
    {
        "impact_model": "AI for Social Good",
        "impact_scale": "Local",
        "indicator": "Number of households with access to affordable electricity",
        "input": "Data from household surveys, electricity bills, and other sources",
        "service": "Subsidy Assessment",
        "title": "Bosco Electricity Subsidy Assessment",
        "stakeholder_outcome": "Reduced energy poverty",
        "stakeholder": "Households",
        "impact_duration": "Medium-term",
        "output": "A list of households eligible for electricity subsidies",
        "indicator_report": "Available here: https://www.bosco.org/wp-content/uploads/2022/03/Bosco-Energy-Access-Indicator-Report.pdf",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/bosco-electricity-subsidy-assessment",
        "program": "Energy Access",
        "impact_risk": "Accuracy/reliability, privacy, bias",
        "activity": "Using AI to assess the eligibility of households for electricity subsidies",
        "organization": "Bosco Development Initiatives",
        "outcome": "More households have access to affordable electricity",
        "impact_depth": "Medium",
        "impact_report": "Available here: https://www.bosco.org/wp-content/uploads/2022/03/Bosco-Energy-Access-Impact-Report.pdf"
    },
    {
        "impact_model": "AI for Business",
        "impact_scale": "Local",
        "indicator": "Percentage of employees who meet or exceed productivity goals",
        "input": "Data from employee computers, phones, and other devices",
        "service": "Employee Monitoring",
        "title": "Teleperformance TP-Observer Employee Monitoring System",
        "stakeholder_outcome": "Improved job performance",
        "stakeholder": "Teleperformance employees",
        "impact_duration": "Medium-term",
        "output": "Reports on employee productivity, such as time spent on tasks, number of calls made, and number of errors made",
        "indicator_report": "Not yet available",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/teleperformancetp-observer-employee-monitoring",
        "program": "Employee Productivity",
        "impact_risk": "Privacy, bias, employee morale",
        "activity": "Using AI to monitor employee productivity",
        "organization": "Teleperformance",
        "outcome": "Increased employee productivity",
        "impact_depth": "Medium",
        "impact_report": "Not yet available"
    },
    {
        "impact_model": "AI for Business",
        "impact_scale": "Local",
        "indicator": "Number of fraud incidents prevented",
        "input": "Data from security cameras, customer loyalty cards, and other sources",
        "service": "Facial Recognition",
        "title": "Rite Aid Facial Recognition Pilot Program",
        "stakeholder_outcome": "Increased safety and security",
        "stakeholder": "Rite Aid customers and employees",
        "impact_duration": "Medium-term",
        "output": "Alerts to store employees when a potential fraudster is identified",
        "indicator_report": "Not yet available",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/rite-aid-facial-recognition",
        "program": "Fraud Prevention",
        "impact_risk": "Accuracy/reliability, privacy, bias",
        "activity": "Using AI to identify potential fraudsters at Rite Aid stores",
        "organization": "Rite Aid",
        "outcome": "Reduced fraud at Rite Aid stores",
        "impact_depth": "Medium",
        "impact_report": "Not yet available"
    },
    {
        "impact_model": "AI for Business",
        "impact_scale": "Local",
        "indicator": "Percentage of customers who watch preshow trailers",
        "input": "Data from eye tracking cameras",
        "service": "Eye Tracking",
        "title": "MoviePass Preshow Eye Tracking",
        "stakeholder_outcome": "Increased awareness of upcoming movies",
        "stakeholder": "MoviePass customers",
        "impact_duration": "Short-term",
        "output": "Data on which trailers customers are most likely to watch, which trailers customers are most likely to skip, and how long customers watch trailers for",
        "indicator_report": "Not yet available",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/moviepass-preshow-eye-tracking",
        "program": "Customer Engagement",
        "impact_risk": "Privacy, bias",
        "activity": "Using AI to track customer eye movement during preshow trailers",
        "organization": "MoviePass",
        "outcome": "Improved customer engagement with preshow trailers",
        "impact_depth": "Medium",
        "impact_report": "Not yet available"
    },
    {
        "impact_model": "AI in the Workplace",
        "impact_scale": "Local",
        "indicator": "Number of workplace accidents involving robots",
        "input": "Data from the robotic arm's sensors",
        "service": "Robotics",
        "title": "Robot Kills SKH Metals Worker",
        "stakeholder_outcome": "Grief and loss",
        "stakeholder": "The worker's family and friends",
        "impact_duration": "Short-term",
        "output": "A welded metal sheet",
        "indicator_report": "Available here: https://www.osha.gov/pls/imis/AccidentSearch.search?acc_keyword=robot",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/robot-kills-skh-metals-worker",
        "program": "Manufacturing",
        "impact_risk": "Safety, reliability, bias",
        "activity": "Using a robotic arm to weld metal sheets",
        "organization": "SKH Metals",
        "outcome": "A worker was killed when the robotic arm crushed him",
        "impact_depth": "Medium",
        "impact_report": "Not yet available"
    },
    {
        "impact_model": "AI for Creativity",
        "impact_scale": "Local",
        "indicator": "Number of new artworks generated by the AI model",
        "input": "Hollie Mengert's artwork",
        "service": "Artificial Intelligence",
        "title": "Illustrator Hollie Mengert Converted into AI Model",
        "stakeholder_outcome": "Hollie Mengert's work can reach a wider audience, and her fans can have new ways to interact with her work",
        "stakeholder": "Hollie Mengert, her fans, and the art community",
        "impact_duration": "Medium-term",
        "output": "An AI model that can generate new artwork in the style of Hollie Mengert",
        "indicator_report": "Not yet available",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/illustrator-hollie-mengert-converted-into-ai-model",
        "program": "Art",
        "impact_risk": "Copyright infringement, bias, lack of creativity",
        "activity": "Converting Hollie Mengert's artwork into an AI model",
        "organization": "Hollie Mengert",
        "outcome": "New artwork can be generated in the style of Hollie Mengert, which can be used for commercial or creative purposes",
        "impact_depth": "Medium",
        "impact_report": "Not yet available"
    },
    {
        "impact_model": "AI for Healthcare",
        "impact_scale": "National",
        "indicator": "Number of patients with access to their electronic medical records",
        "input": "Patient medical records",
        "service": "Electronic Health Records",
        "title": "UK NHS Digital Medical History Database",
        "stakeholder_outcome": "Patients have better access to their medical records, and healthcare providers can make better decisions about patient care",
        "stakeholder": "UK patients and healthcare providers",
        "impact_duration": "Long-term",
        "output": "A digital medical history database that can be accessed by healthcare providers",
        "indicator_report": "Available here: https://www.nhs.uk/NHSEngland/AboutNHSservices/NHSdigital/Pages/NHS-Digital-Annual-Report.aspx",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/uk-nhs-digital-medical-history-database",
        "program": "Patient Care",
        "impact_risk": "Data security, privacy, bias",
        "activity": "Creating a digital medical history database for all UK patients",
        "organization": "UK NHS",
        "outcome": "Improved patient care by providing healthcare providers with access to patient medical records",
        "impact_depth": "Medium",
        "impact_report": "Available here: https://www.nhs.uk/NHSEngland/AboutNHSservices/NHSdigital/Pages/NHS-Digital-Annual-Report.aspx"
    },
    {
        "impact_model": "AI for Creativity",
        "impact_scale": "Global",
        "indicator": "Number of people who use BlenderBot",
        "input": "Data from a massive dataset of text and code",
        "service": "Natural Language Processing",
        "title": "BlenderBot",
        "stakeholder_outcome": "BlenderBot can be used to create new forms of art, translate languages, write different kinds of creative content, and answer your questions in an informative way",
        "stakeholder": "Anyone who wants to use BlenderBot",
        "impact_duration": "Medium-term",
        "output": "A large language model called BlenderBot",
        "indicator_report": "Not yet available",
        "source": "https://ai.googleblog.com/2022/01/blenderbot-text-generation-for-everyone.html",
        "program": "Art",
        "impact_risk": "Bias, lack of creativity, safety",
        "activity": "Developing a large language model that can generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way",
        "organization": "Google AI",
        "outcome": "BlenderBot can generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way",
        "impact_depth": "Medium",
        "impact_report": "Not yet available"
    },
    {
        "impact_model": "AI for Public Safety",
        "impact_scale": "Local",
        "indicator": "Number of crimes prevented or solved by Xavier robots",
        "input": "Data from sensors on the robots",
        "service": "Robotics",
        "title": "Singapore Xavier Patrol Robots",
        "stakeholder_outcome": "Increased sense of security and well-being",
        "stakeholder": "Citizens of Singapore",
        "impact_duration": "Medium-term",
        "output": "Alerts to authorities when suspicious activity is detected",
        "indicator_report": "Not yet available",
        "source": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/singapore-xavier-patrol-robots",
        "program": "Security",
        "impact_risk": "Accuracy, reliability, bias, privacy, safety",
        "activity": "Deploying Xavier patrol robots in Singapore",
        "organization": "Nanyang Technological University (NTU)",
        "outcome": "Reduced crime rates and improved public safety",
        "impact_depth": "Medium",
        "impact_report": "Not yet available"
    },
    {
        "impact_model": "\"AI for Education\"",
        "impact_scale": "\"Local\"",
        "indicator": "\"Number of students who arrive at school on time\"",
        "input": "\"Data on student attendance, bus routes, and traffic patterns\"",
        "service": "\"Bus Scheduling\"",
        "title": "\"Boston Public Schools Bus Scheduling\"",
        "stakeholder_outcome": "\"Increased satisfaction with the bus system, reduced stress, and improved academic performance\"",
        "stakeholder": "\"Students, parents, and staff of BPS\"",
        "impact_duration": "\"Medium-term\"",
        "output": "\"A more efficient and effective bus scheduling system\"",
        "indicator_report": "\"Available here: https://www.bostonpublicschools.org/aboutus/newsroom/press-releases/2022/03/25/new-bus-scheduling-system-improves-on-time-arrival-rates-for-boston-public-schools-students\"",
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/boston-public-schools-bus-scheduling\"",
        "program": "\"Transportation\"",
        "impact_risk": "\"Accuracy, reliability, bias, privacy, safety\"",
        "activity": "\"Using AI to improve bus scheduling for Boston Public Schools\"",
        "organization": "\"Boston Public Schools (BPS)\"",
        "outcome": "\"Reduced wait times for students, improved on-time arrival rates, and increased capacity on buses\"",
        "impact_depth": "\"Medium\"",
        "impact_report": "\"Not yet available\""
    },
    {
        "impact_model": "\"AI for Search\"",
        "impact_scale": "\"Local\"",
        "indicator": "\"Number of people who have been harmed by Google autocomplete\"",
        "input": "\"User search queries\"",
        "service": "\"Autocomplete\"",
        "title": "\"Google autocomplete connects Albert Yeung with triads\"",
        "stakeholder_outcome": "\"Damage to reputation and discrimination\"",
        "stakeholder": "\"Albert Yeung and other Chinese-Canadians\"",
        "impact_duration": "\"Medium-term\"",
        "output": "\"Autocomplete suggestions\"",
        "indicator_report": "\"Not yet available\"",
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/google-autocomplete-connects-albert-yeung-with-triads\"",
        "program": "\"Search\"",
        "impact_risk": "\"Bias, discrimination, privacy\"",
        "activity": "\"Using AI to generate autocomplete suggestions\"",
        "organization": "\"Google\"",
        "outcome": "\"Harmful stereotypes about Chinese-Canadians\"",
        "impact_depth": "\"Medium\"",
        "impact_report": "\"Not yet available\""
    },
    {
        "impact_model": "\"AI for Education\"",
        "impact_scale": "\"Local\"",
        "indicator": "\"Number of students who are more engaged in learning\"",
        "input": "\"Data from facial expressions and body language\"",
        "service": "\"Emotion Recognition\"",
        "title": "\"Intel AI student emotion monitoring\"",
        "stakeholder_outcome": "\"Increased motivation, improved test scores, and reduced stress\"",
        "stakeholder": "\"Students, teachers, and schools\"",
        "impact_duration": "\"Medium-term\"",
        "output": "\"A tool that can detect whether students are happy, sad, bored, distracted, or confused\"",
        "indicator_report": "\"Not yet available\"",
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/intel-ai-student-emotion-monitoring\"",
        "program": "\"Education\"",
        "impact_risk": "\"Accuracy, reliability, privacy, bias, misuse\"",
        "activity": "\"Developing an AI-based tool to monitor student emotions\"",
        "organization": "\"Intel\"",
        "outcome": "\"Improved student engagement and learning\"",
        "impact_depth": "\"Medium\"",
        "impact_report": "\"Not yet available\""
    },
    {
        "impact_model": "\"AI for Location Tracking\"",
        "impact_scale": "\"Local\"",
        "indicator": "\"Number of users who have had their privacy violated by Huq GPS location data sharing\"",
        "input": "\"GPS data from mobile devices\"",
        "service": "\"GPS Data Sharing\"",
        "title": "\"Huq GPS location data sharing\"",
        "stakeholder_outcome": "\"Reduced privacy and increased risk of identity theft and other crimes\"",
        "stakeholder": "\"Users of mobile devices\"",
        "impact_duration": "\"Medium-term\"",
        "output": "\"Data that can be used to track user movements and habits\"",
        "indicator_report": "\"Not yet available\"",
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/huq-gps-location-data-sharing\"",
        "program": "\"Location Tracking\"",
        "impact_risk": "\"Privacy, misuse\"",
        "activity": "\"Sharing GPS location data with third-party companies\"",
        "organization": "\"Huq Industries\"",
        "outcome": "\"Privacy concerns and potential for misuse\"",
        "impact_depth": "\"Medium\"",
        "impact_report": "\"Not yet available\""
    },
    {
        "impact_model": "\"AI for Self-Driving Cars\"",
        "impact_scale": "\"Local\"",
        "indicator": "\"Number of near-miss accidents involving self-driving cars\"",
        "input": "\"Data from sensors on the car\"",
        "service": "\"Cruise\"",
        "title": "\"GM Cruise fails to yield to pedestrian at crosswalk\"",
        "stakeholder_outcome": "\"Increased fear and distrust of self-driving cars\"",
        "stakeholder": "\"Pedestrians and other road users\"",
        "impact_duration": "\"Medium-term\"",
        "output": "\"A decision to not yield to a pedestrian at a crosswalk\"",
        "indicator_report": "\"Not yet available\"",
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/gm-cruise-fails-to-yield-to-pedestrian-at-crosswalk\"",
        "program": "\"Self-Driving Cars\"",
        "impact_risk": "\"Accuracy, reliability, bias, safety\"",
        "activity": "\"Testing self-driving cars on public roads\"",
        "organization": "\"General Motors\"",
        "outcome": "\"A near-miss accident\"",
        "impact_depth": "\"Medium\"",
        "impact_report": "\"Not yet available\""
    },
    {
        "impact_model": "\"AI for Criminal Justice\"",
        "impact_scale": "\"Local\"",
        "indicator": "\"Number of parole applicants who are denied parole after being assessed as high risk by COMPAS\"",
        "input": "\"Data from criminal history, demographics, and other factors\"",
        "service": "\"COMPAS Risk Assessment Algorithm\"",
        "title": "\"Titus Henderson denied parole after COMPAS algorithm predicts high risk of recidivism\"",
        "stakeholder_outcome": "\"Increased risk of incarceration and decreased opportunities for rehabilitation\"",
        "stakeholder": "\"Titus Henderson and other parole applicants\"",
        "impact_duration": "\"Medium-term\"",
        "output": "\"A risk score that predicts the likelihood of recidivism\"",
        "indicator_report": "\"Available here: https://www.sentencingproject.org/publications/the-compas-algorithm-and-the-risk-of-unfair-denials-of-parole/\"",
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/titus-henderson-compas-parole-denial\"",
        "program": "\"Parole and Probation\"",
        "impact_risk": "\"Accuracy, reliability, bias, fairness, transparency\"",
        "activity": "\"Using COMPAS to assess risk of recidivism for parole applicants\"",
        "organization": "\"North Carolina Department of Public Safety\"",
        "outcome": "\"Parole denial\"",
        "impact_depth": "\"Medium\"",
        "impact_report": "\"Not yet available\""
    },
    {
        "impact_model": "\"AI for Financial Markets\"",
        "impact_scale": "\"Local\"",
        "indicator": "\"Number of Bitcoin traders who lost money due to the glitch\"",
        "input": "\"Data from exchanges, market makers, and other participants in the Bitcoin market\"",
        "service": "\"Pyth Price Feeds\"",
        "title": "\"Pyth Bitcoin glitch causes price to briefly crash to $5,402\"",
        "stakeholder_outcome": "\"Losses of money and damage to confidence in the Bitcoin market\"",
        "stakeholder": "\"Bitcoin traders and investors\"",
        "impact_duration": "\"Medium-term\"",
        "output": "\"A real-time price feed for Bitcoin\"",
        "indicator_report": "\"Not yet available\"",
        "source": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/pyth-bitcoin-glitch\"",
        "program": "\"Cryptocurrency\"",
        "impact_risk": "\"Accuracy, reliability, bias, transparency\"",
        "activity": "\"Providing real-time price data for Bitcoin\"",
        "organization": "\"Pyth\"",
        "outcome": "\"A brief crash in the price of Bitcoin from $41,000 to $5,402\"",
        "impact_depth": "\"Medium\"",
        "impact_report": "\"Not yet available\""
    },
    {
        "impact_model": "\"AI for Text Generation\"",
        "impact_scale": "\"Local, National, Global\"",
        "indicator": "\"Number of people who have been harmed by GPT-4 or its outputs\"",
        "input": "\"Data from books, articles, and other text sources\"",
        "service": "\"GPT-4\"",
        "title": "\"GPT-4 large language model\"",
        "stakeholder_outcome": "\"Increased risk of harm, including discrimination, misinformation, and violence\"",
        "stakeholder": "\"Anyone who interacts with GPT-4 or its outputs\"",
        "impact_duration": "\"Medium-term, Long-term\"",
        "output": "\"A large language model that can generate human-like text\"",
        "indicator_report": "\"Not yet available\"",
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/gpt-4-large-language-model\"",
        "program": "\"Text Generation\"",
        "impact_risk": "\"Bias, discrimination, privacy, misuse, harm\"",
        "activity": "\"Developing a large language model that can generate human-like text\"",
        "organization": "\"OpenAI\"",
        "outcome": "\"Potential for misuse and harm\"",
        "impact_depth": "\"Medium, High\"",
        "impact_report": "\"Not yet available\""
    },
    {
        "impact_model": "\"AI for Beauty Filters\"",
        "impact_scale": "\"Local, National, Global\"",
        "indicator": "\"Number of users who have reported feeling pressure to conform to unrealistic beauty standards after using the Bold Glamour Filter\"",
        "input": "\"Data from users' faces and selfies\"",
        "service": "\"Bold Glamour Filter\"",
        "title": "\"TikTok Bold Glamour Filter\"",
        "stakeholder_outcome": "\"Reduced self-esteem and increased body image issues\"",
        "stakeholder": "\"Users of TikTok\"",
        "impact_duration": "\"Medium-term, Long-term\"",
        "output": "\"A beauty filter that can make users look more attractive\"",
        "indicator_report": "\"Not yet available\"",
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tiktok-bold-glamour-filter\"",
        "program": "\"Beauty Filters\"",
        "impact_risk": "\"Body image issues, unrealistic beauty standards, social pressure\"",
        "activity": "\"Developing and releasing a beauty filter that can make users look more attractive\"",
        "organization": "\"TikTok\"",
        "outcome": "\"Increased pressure on users to conform to unrealistic beauty standards\"",
        "impact_depth": "\"Medium, High\"",
        "impact_report": "\"Not yet available\""
    },
    {
        "impact_model": "\"AI for Content Moderation\"",
        "impact_scale": "\"Local, National, Global\"",
        "indicator": "\"Number of users who have been shadowbanned\"",
        "input": "\"Data from user posts and comments\"",
        "service": "\"Shadowbanning\"",
        "title": "\"Reddit shadowbanning\"",
        "stakeholder_outcome": "\"Reduced ability to participate in the Reddit community and increased frustration\"",
        "stakeholder": "\"Users of Reddit\"",
        "impact_duration": "\"Medium-term, Long-term\"",
        "output": "\"A list of users who have been shadowbanned\"",
        "indicator_report": "\"Not yet available\"",
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/reddit-shadowbanning\"",
        "program": "\"Content Moderation\"",
        "impact_risk": "\"Misinformation, harassment, discrimination\"",
        "activity": "\"Using AI to shadowban users who violate the site's terms of service\"",
        "organization": "\"Reddit\"",
        "outcome": "\"Users who are shadowbanned are not able to see their posts or comments, and their interactions with the site are severely limited\"",
        "impact_depth": "\"Medium, High\"",
        "impact_report": "\"Not yet available\""
    },
    {
        "impact_model": "\"AI for Competitive Advantage\"",
        "impact_scale": "\"Local, National, Global\"",
        "indicator": "\"Number of rival apps that have been blocked\"",
        "input": "\"Data from user search queries and app downloads\"",
        "service": "\"App Link Blocking\"",
        "title": "\"Tencent app link blocking\"",
        "stakeholder_outcome": "\"Reduced revenue and market share\"",
        "stakeholder": "\"Developers of rival apps\"",
        "impact_duration": "\"Medium-term, Long-term\"",
        "output": "\"A list of links to rival apps that are blocked\"",
        "indicator_report": "\"Not yet available\"",
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tencent-app-link-blocking\"",
        "program": "\"Anti-Competitive Practices\"",
        "impact_risk": "\"Anti-competitive practices, market concentration, consumer harm\"",
        "activity": "\"Using AI to block links to rival apps\"",
        "organization": "\"Tencent\"",
        "outcome": "\"Reduced visibility and downloads for rival apps\"",
        "impact_depth": "\"Medium, High\"",
        "impact_report": "\"Not yet available\""
    },
    {
        "impact_model": "\"AI for Self-Driving Cars\"",
        "impact_scale": "\"Local, National, Global\"",
        "indicator": "\"Number of people who have been killed or injured in accidents involving Tesla Autopilot\"",
        "input": "\"Data from sensors and cameras\"",
        "service": "\"Autopilot\"",
        "title": "\"Tesla Model S kills truck driver pedestrian\"",
        "stakeholder_outcome": "\"Loss of life, grief, and financial hardship\"",
        "stakeholder": "\"The truck driver and his family\"",
        "impact_duration": "\"Medium-term, Long-term\"",
        "output": "\"Autopilot control of the Tesla Model S\"",
        "indicator_report": "\"Available here: https://www.nhtsa.gov/technology-innovation/automated-vehicles/crashes-involving-self-driving-vehicles\"",
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-model-s-kills-truck-driver-pedestrian\"",
        "program": "\"Self-Driving Cars\"",
        "impact_risk": "\"Accidents, injuries, and deaths\"",
        "activity": "\"Using AI to drive a Tesla Model S\"",
        "organization": "\"Tesla\"",
        "outcome": "\"Death of a truck driver who was struck by the Tesla Model S\"",
        "impact_depth": "\"High\"",
        "impact_report": "\"Not yet available\""
    },
    {
        "impact_model": "\"AI for Self-Driving Cars\"",
        "impact_scale": "\"Local, National, Global\"",
        "indicator": "\"Number of people who have been injured or killed in accidents involving Tesla Autopilot\"",
        "input": "\"Data from sensors and cameras\"",
        "service": "\"Autopilot\"",
        "title": "\"Son Ji-chang Tesla Model X sudden acceleration\"",
        "stakeholder_outcome": "\"Injury, pain, and suffering\"",
        "stakeholder": "\"Son Ji-chang and his passenger\"",
        "impact_duration": "\"Medium-term, Long-term\"",
        "output": "\"Autopilot control of the Tesla Model X\"",
        "indicator_report": "\"Available here: https://www.nhtsa.gov/technology-innovation/automated-vehicles/crashes-involving-self-driving-vehicles\"",
        "source": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/son-ji-chang-tesla-model-x-sudden-acceleration\"",
        "program": "\"Self-Driving Cars\"",
        "impact_risk": "\"Accidents, injuries, and deaths\"",
        "activity": "\"Using AI to drive a Tesla Model X\"",
        "organization": "\"Tesla\"",
        "outcome": "\"Injury to Son Ji-chang and his passenger when the Tesla Model X accelerated suddenly and crashed into his garage\"",
        "impact_depth": "\"High\"",
        "impact_report": "\"Not yet available\""
    },
    {
        "impact_model": "\"AI for Content Moderation\"",
        "impact_scale": "\"Local, National, Global\"",
        "indicator": "\"Number of Australian news publishers who have been blocked from Facebook\"",
        "input": "\"Data from news articles\"",
        "service": "\"News Feed\"",
        "title": "\"Facebook blocks Australian news\"",
        "stakeholder_outcome": "\"Reduced revenue and readership\"",
        "stakeholder": "\"Australian news publishers and readers\"",
        "impact_duration": "\"Medium-term, Long-term\"",
        "output": "\"A list of news articles that are blocked from the News Feed\"",
        "indicator_report": "\"Not yet available\"",
        "source": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/facebook-australia-news-civil-society-blocks\"",
        "program": "\"Content Moderation\"",
        "impact_risk": "\"Misinformation, censorship, harm to journalism\"",
        "activity": "\"Using AI to block news articles from Australian publishers from the News Feed\"",
        "organization": "\"Facebook\"",
        "outcome": "\"Reduced visibility and readership for Australian news publishers\"",
        "impact_depth": "\"Medium, High\"",
        "impact_report": "\"Not yet available\""
    }
]