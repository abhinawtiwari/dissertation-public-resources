- Occurred: October 2021
- Can you improve this page?Share your insights with us
- Leaked documents reveal Facebook deliberately pushed users into seeing more emotional, provocative, and negative content in order to deepen engagement and drive more revenue.
- Between 2017 and 2020, an update to Facebook's 'Meaningful Social Interactions' (MSI) algorithm saw the weighting of angry emoji substantially increased.
- The update was made despite Facebook researchers having warned senior leadership at the company, including Mark Zuckerberg, that it would likely lead to higher levels of spam, misinformation and disinformation, and abuse.
- The update also had the effect of undermining efforts by the company's content moderators and integrity teams to manage toxic and harmful content.
- The leaked documents were shared with the SEC and the US Congress by whistleblower Frances Haugen. Speaking in the UK parliament, Haugen argued 'anger and hate is the easiest way to grow on Facebook.'
- Operator: Meta/Facebook Developer: Meta/FacebookCountry: USA; Global Sector: Technology Purpose: Increase engagement, revenue Technology: Content ranking system Issue: Ethics; Hate speech/violence; Mis/disinformationTransparency: Governance; Black box; Marketing
URL: https://en.wikipedia.org/wiki/2021_Facebook_leak
- 
- In 2021, an internal document leak from the company then known as Facebook (now Meta Platforms, or Meta) showed it was aware of harmful societal effects from its platforms, yet persisted in prioritizing profit over addressing these harms. The leak, released by whistleblower Frances Haugen, resulted in reporting from The Wall Street Journal in September, as The Facebook Files series, as well as the Facebook Papers, by a consortium of news outlets the next month.
- Primarily, the reports revealed that, based on internally-commissioned studies, the company was fully aware of negative impacts on teenage users of Instagram, and the contribution of Facebook activity to violence in developing countries. Other takeaways of the leak include the impact of the company's platforms on spreading false information, and Facebook's policy of promoting inflammatory posts.
Furthermore, Facebook was fully aware that harmful content was being pushed through Facebook algorithms reaching young users. The types of content included posts promoting anorexia nervosa and self-harm photos.
- In October 2021, Whistleblower Aid filed eight anonymous whistleblower complaints with the U.S. Securities and Exchange Commission (SEC) on behalf of Haugen alleging securities fraud by the company, after Haugen leaked the company documents the previous month.[1][2][3] After publicly revealing her identity on 60 Minutes,[4][5] Haugen testified before the U.S. Senate Commerce Subcommittee on Consumer Protection, Product Safety, and Data Security about the content of the leaked documents and the complaints.[6] After the company renamed itself as Meta Platforms,[7] Whistleblower Aid filed two additional securities fraud complaints with the SEC against the company on behalf of Haugen in February 2022.[8]
- There were conflicts of interest between what was good for the public and what was good for Facebook. And Facebook, over and over again, chose to optimize for its own interests, like making more money.
- Whistleblower Frances Haugen on 60 Minutes, October 3, 2021
- In mid September 2021, The Wall Street Journal began publishing articles on Facebook based on internal documents from unknown provenance. Revelations included reporting of special allowances on posts from high-profile users ("XCheck"), subdued responses to flagged information on human traffickers and drug cartels, a shareholder lawsuit concerning the cost of Facebook (now Meta) CEO Mark Zuckerberg's personal liability protection in resolving the Cambridge Analytica data scandal, an initiative to increase pro-Facebook news within user news feeds, and internal knowledge of how Instagram exacerbated negative self-image in surveyed teenage girls.[9]
- Siva Vaidhyanathan wrote for The Guardian that the documents were from a team at Facebook "devoted to social science and data analytics that is supposed to help the company's leaders understand the consequences of their policies and technological designs."[10] Casey Newton of The Verge wrote that it is the company's biggest challenge since its Cambridge Analytica data scandal.[11]
- The leaked documents include internal research from Facebook that studied the impact of Instagram on teenage mental health.[12] Although Facebook earlier claimed that its rules applies equally to everyone on the platform, internal documents shared with The Wall Street Journal point to special policy exceptions reserved for VIP users, including celebrities and politicians.[13] After this reporting, Facebook's oversight board said it would review the system.[14][15]
- On October 3, 2021, the former Facebook employee behind the leak, Frances Haugen, revealed her identity on 60 Minutes.[16]
- Beginning October 22, a group of news outlets began publishing articles based on documents provided by Haugen's lawyers, collectively referred to as The Facebook Papers.[17][18]
- The Files show that Facebook (now Meta) has been conducting internal research of how Instagram affects young users for the past three years. While the findings point to Instagram being harmful to a large portion of young users, teenage girls were among the most harmed. Researchers within the company reported that "we make body issues worse for one in three teenage girls". Furthermore, internal research revealed that teen boys were also affected by negative social comparison, citing 14% of boys in the US in 2019.[19] Instagram was concluded to contribute to problems more specific to its app use, such as social comparison among teens.[20] Facebook published some of its internal research on September 29, 2021, saying these reports mischaracterized the purpose and results of its research.[21]
- The Files show that Facebook formed a team to study preteens, set a three year goal to create more products for this demographic, and commissioned strategy papers about the long-term business prospects of attracting the preteen demographic. A 2020 document from Facebook states: "Why do we care about tweens?" and answers that question by saying that "They are a valuable but untapped audience."[22]
- An internal memo seen by the Washington Post revealed that Facebook has been aware of hate speech and calls for violence against groups like Muslims and Kashmiris, including posts of photos of piles of dead Kashmiri bodies with glorifying captions on its platform in India. Still, none of their publishers were blocked.[23] Documents reveal Facebook has responded to these incidents by removing posts which violate their policy, but has not made any substantial efforts to prevent repeat offenses.[23] As 90% of monthly Facebook users are now located outside of the US and Canada, Facebook claims language barriers are one obstacle that is preventing widespread reform.
- The New York Times pointed to internal discussions where employees raised concerns that Facebook was spreading content about the QAnon conspiracy theory more than a year before the 2020 United States elections. After the election, a data scientist mentioned in an internal note that 10 percent of all U.S. views of political content were of posts alleging that the election was fraudulent.[24]
- In 2015, in addition to the Like button on posts, Facebook introduced a set of other emotional reaction options: love, haha, yay, wow, sad and angry.[25] The Washington Post reported that for three years, Facebook's algorithms promoted posts that received the 'angry' reaction from its users, based on internal analysis showing that such posts lead to five times more engagement than posts with regular likes. Years later, Facebook's researchers pointed out that posts with 'angry' reactions were much more likely to be toxic, polarizing, fake or low quality.[26]
- In 2018, Facebook overhauled its News Feed algorithm, implementing a new algorithm which favored "Meaningful Social Interations" or "MSI". The new algorithm increased the weight of reshared material - a move which aimed to "reverse the decline in comments and encourage more original posting". While the algorithm was successful in its efforts, consequences such as user reports of feed quality decreasing along with increased anger on the site were observed. Leaked documents reveal that employees presented several potential changes to fix some of the highlighted issues with their algorithm. However, documents claim Mark Zuckerberg denied the proposed changes due to his worry that they might cause less users to engage with Facebook. Documents have also pointed to another 2019 study conducted by Facebook where a fake account based in India was created and studied to see what type of content it was presented and interacted with. Results of the study showed that within three weeks, the fake account's newsfeed was being presented pornography and "filled with polarizing and graphic content, hate speech and misinformation", according to an internal company report.[27]
- Politico quotes several Facebook staff expressing concerns about the company's willingness and ability to respond to damage caused by the platform. A 2020 post reads: "It's not normal for a large number of people in the 'make the site safe' team to leave saying, 'hey, we're actively making the world worse FYI.' Every time this gets raised it gets shrugged off with 'hey people change jobs all the time' but this is NOT normal."[28]
- In 2019, following concerns about Facebook and Instagram being used to trade maids in the Middle East, Apple threatened to remove their iOS apps from the App Store.[29]
- The documents have shown a private program known as "XCheck" or "cross-check" that Facebook has employed in order to whitelist posts from users deemed as "high-profile". The system began as a quality control measure but has since grown to protect "millions of VIP users from the company's normal enforcement process". XCheck has led to celebrities and other public figures being exempt from punishment that the average Facebook user would receive from violating policies. In 2019, football player Neymar had posted nude photos of a woman who had accused him of rape which were left up for more than a day. According to The Wall Street Journal, "XCheck grew to include at least 5.8 million users in 2020" according to Facebook's internal documents.[30] The goal of XCheck was "to never publicly tangle with anyone who is influential enough to do you harm".[31]
- In 2020, Vietnam's communist government has threatened to shut down Facebook if the social media company does not co-operate on censoring political content in the country, Meta's (then known as Facebook) big market in Southeast Asia.[32] The decision to comply was personally approved by Mark Zuckerberg.[33][34]
- In 2021, Facebook developed a new strategy for addressing harmful content on their site, implementing measures which were designed to reduce and suppress the spread of movements that were deemed hateful. According to a senior security official at Facebook, the company "would seek to disrupt on-platform movements only if there was compelling evidence that they were the product of tightly knit circles of users connected to real-world violence or other harm and committed to violating Facebook's rules". As part of their recently coordinated initiative, this included less promotion of the movement's posts within users' News Feed as well as not notifying users of new posts from these pages. Specific groups that have been highlighted as being affected by Facebook's social harm policy include the Patriot Party, previously linked to the Capitol attack, as well as a newer German conspiracy group known as Querdenken, who had been placed under surveillance by German intelligence after protests it organized repeatedly "resulted in violence and injuries to the police".[35]
- According to The Wall Street Journal, documents show that in 2019, Facebook reduced the time spent by human reviewers on hate-speech complaints, shifting towards a stronger dependence on their artificial intelligence systems to regulate the matter. However, internal documents from employees claim that their AI has been largely unsuccessful, seeing trouble detecting videos of cars crashing, cockfighting, as well as understanding hate speech in foreign languages.[36] Internal engineers and researchers within Facebook have estimated that their AI has only been able to detect and remove 0.6% of "all content that violated Facebook's policies against violence and incitement".
- For The Facebook Files series of reports, The Wall Street Journal produced a podcast on its The Journal channel, divided into eight episodes:
- In the Q3 2021 earnings call, Facebook CEO Mark Zuckerberg discussed the recent leaks, characterizing them as coordinated efforts to paint a false picture of his company by selectively leaking documents.[45]
- According to a leaked internal email seen by The New York Times, Facebook asked its employees to "preserve internal documents and communications since 2016", a practice called a legal hold. The email continues: "As is often the case following this kind of reporting, a number of inquiries from governments and legislative bodies have been launched into the company's operations."[46]
- In December 2021, news broke on The Wall Street Journal pointing to Meta's lobbying efforts to divide US lawmakers and "muddy the waters" in Congress, to hinder regulation following the 2021 whistleblower leaks.[47] Facebook's lobbyist team in Washington suggested to Republican lawmakers that the whistleblower "was trying to help Democrats," while the narrative told to Democratic staffers was that Republicans "were focused on the company's decision to ban expressions of support for Kyle Rittenhouse," The Wall Street Journal reported. According to the article, the company's goal was to "muddy the waters, divide lawmakers along partisan lines and forestall a cross-party alliance" against Facebook (now Meta) in Congress.[48]

URL: https://www.washingtonpost.com/technology/2021/10/25/what-are-the-facebook-papers/
- This article was published more than 1 year ago
- A personal decision by Facebook CEO Mark Zuckerberg leads to a crackdown on dissent in Vietnam. Measures to suppress hateful, deceptive content are lifted after the American presidential election in 2020, as pro-Trump groups disputing the legitimacy of the election experience “meteoric” growth. A dummy test account on Facebook in India is flooded with violent anti-Muslim propaganda — which remains visible for weeks on the real account of a frightened Muslim college student in northern India.
- A trove of internal Facebook documents reveals that the social media giant has privately and meticulously tracked real-world harms exacerbated by its platforms, ignored warnings from its employees about the risks of their design decisions and exposed vulnerable communities around the world to a cocktail of dangerous content.
- Disclosed to the U.S. Securities and Exchange Commission by whistleblower Frances Haugen, the Facebook Papers were provided to Congress in redacted form by Haugen’s legal counsel. The redacted versions were reviewed by a consortium of news organizations, including The Washington Post, which obtained additional internal documents and conducted interviews with dozens of current and former Facebook employees.
- A mix of presentations, research studies, discussion threads and strategy memos, the Facebook Papers provide an unprecedented view into how executives at the social media giant weigh trade-offs between public safety and their own bottom line. Some of the documents were first reported by the Wall Street Journal.
- Here are key takeaways from The Post’s investigation:
- Haugen references Zuckerberg’s public statements at least 20 times in her SEC complaints, asserting that the CEO’s unique degree of control over Facebook forces him to bear ultimate responsibility for a litany of societal harms caused by the company’s relentless pursuit of growth.
- The documents also show that Zuckerberg’s public statements are often at odds with internal company findings.
- For example, Zuckerberg testified last year before Congress that the company removes 94 percent of the hate speech it finds before a human reports it. But in internal documents, researchers estimated that the company was removing less than 5 percent of all hate speech on Facebook.
- Facebook spokeswoman Dani Lever denied that Zuckerberg “makes decisions that cause harm” and dismissed the findings, saying they are “based on selected documents that are mischaracterized and devoid of any context.”
- The case against Mark Zuckerberg: How Facebook’s CEO chose growth and free speech over safety
- During the run-up to the 2020 U.S. presidential election, the social media giant dialed up efforts to police content that promoted violence, misinformation and hate speech. But after Nov. 6, Facebook rolled back many of the dozens of measures aimed at safeguarding U.S. users. A ban on the main Stop the Steal group didn’t apply to the dozens of look-alike groups that popped up in what the company later concluded was a “coordinated” campaign, documents show.
- By the time Facebook tried to reimpose its “break the glass” measures, it was too late: A pro-Trump mob was storming the U.S. Capitol.
- Facebook officials said they planned exhaustively for the election and its aftermath, anticipated the potential for post-election violence, and always expected the challenges to last through the inauguration of President Biden on Jan. 20.
- Inside Facebook, Jan. 6 violence fueled anger, regret over missed warning signs
- For all Facebook’s troubles in North America, its problems with hate speech and misinformation are dramatically worse in the developing world. Documents show that Facebook has meticulously studied its approach abroad, and is well aware that weaker moderation in non-English-speaking countries leaves the platform vulnerable to abuse by bad actors and authoritarian regimes.
- According to one 2020 summary, the vast majority of its efforts against misinformation — 84 percent — went toward the United States, the documents show, with just 16 percent going to the “Rest of World,” including India, France and Italy.
- Though Facebook considers India a top priority, activating large teams to engage with civil society groups and protect elections, the documents show that Indian users experience Facebook without critical guardrails common in English-speaking countries.
- Facebook’s Lever said the company has made “progress,” with “global teams with native speakers reviewing content in over 70 languages along with experts in humanitarian and human rights issues.”
- “We’ve hired more people with language, country and topic expertise,” Lever said, adding that Facebook has “also increased the number of team members with work experience in Myanmar and Ethiopia to include former humanitarian aid workers, crisis responders and policy specialists.”
- How Facebook neglected the rest of the world, fueling hate speech and violence in India
- Zuckerberg has said the company does not design its products to persuade people to spend more time on them. But dozens of documents suggest the opposite.
- The company exhaustively studies potential policy changes for their effects on user engagement and other factors key to corporate profits. Amid this push for user attention, Facebook abandoned or delayed initiatives to reduce misinformation and radicalization.
- One 2019 report tracking a dummy account set up to represent a conservative mother in North Carolina found that Facebook’s recommendation algorithms led her to QAnon, an extremist ideology that the FBI has deemed a domestic terrorism threat, in just five days. Still, Facebook allowed QAnon to operate on its site largely unchecked for another 13 months.
- “We have no commercial or moral incentive to do anything other than give the maximum number of people as much of a positive experience as possible,” Facebook’s Lever said, adding that the company is “constantly making difficult decisions.”
- Starting in 2017, Facebook’s algorithm gave emoji reactions like “angry” five times the weight as “likes,” boosting these posts in its users’ feeds. The theory was simple: Posts that prompted lots of reaction emoji tended to keep users more engaged, and keeping users engaged was the key to Facebook’s business.
- The company’s data scientists eventually confirmed that “angry” reaction, along with “wow” and “haha,” occurred more frequently on “toxic” content and misinformation.
- Last year, when Facebook finally set the weight on the angry reaction to zero, users began to get less misinformation, less “disturbing” content and less “graphic violence,” company data scientists found. Lever said that the company continues to work on its understanding of negative experience to reduce its spread.
- Five points for anger, one for a ‘like’: How Facebook’s formula fostered rage and misinformation
- Elizabeth Dwoskin, Shibani Mahtani, Cat Zakrzewski, Craig Timberg, Will Oremus and Jeremy Merrill contributed to this report.
- A previous version of this article incorrectly described the content of congressional testimony by Facebook's CEO, Mark Zuckerberg. Zuckerberg testified that the company removes 94 percent of the hate speech it finds before a human reports it, not just that it removes 94 percent of the hate speech it finds. The article has been corrected.
- The Facebook Papers are a set of internal documents that were provided to Congress in redacted form by Frances Haugen’s legal counsel. The redacted versions were reviewed by a consortium of news organizations, including The Washington Post.
- The trove of documents show how Facebook CEO Mark Zuckerberg has, at times, contradicted, downplayed or failed to disclose company findings on the impact of its products and platforms.
- The documents also provided new details of the social media platform’s role in fomenting the storming of the U.S. Capitol. An investigation by ProPublica and The Washington Post found that Facebook groups swelled with at least 650,000 posts attacking the legitimacy of Joe Biden’s victory between Election Day and Jan. 6.
- Facebook engineers gave extra value to emoji reactions, including ‘angry,’ pushing more emotional and provocative content into users’ news feeds.
- Read more from The Post’s investigation:
- Key takeaways from the Facebook Papers
- Frances Haugen took thousands of Facebook documents. This is how she did it.
- How Facebook neglected the rest of the world, fueling hate speech and violence in India
- How Facebook shapes your feed

URL: https://www.washingtonpost.com/technology/2021/10/26/facebook-angry-emoji-algorithm/
- This article was published more than 1 year ago
- Five years ago, Facebook gave its users five new ways to react to a post in their news feed beyond the iconic “like” thumbs-up: “love,” “haha,” “wow,” “sad” and “angry.”
- Behind the scenes, Facebook programmed the algorithm that decides what people see in their news feeds to use the reaction emoji as signals to push more emotional and provocative content — including content likely to make them angry. Starting in 2017, Facebook’s ranking algorithm treated emoji reactions as five times more valuable than “likes,” internal documents reveal. The theory was simple: Posts that prompted lots of reaction emoji tended to keep users more engaged, and keeping users engaged was the key to Facebook’s business.
- Facebook’s own researchers were quick to suspect a critical flaw. Favoring “controversial” posts — including those that make users angry — could open “the door to more spam/abuse/clickbait inadvertently,” a staffer, whose name was redacted, wrote in one of the internal documents. A colleague responded, “It’s possible.”
- The warning proved prescient. The company’s data scientists confirmed in 2019 that posts that sparked angry reaction emoji were disproportionately likely to include misinformation, toxicity and low-quality news.
- Lawmakers’ latest idea to fix Facebook: Regulate the algorithm
- That means Facebook for three years systematically amped up some of the worst of its platform, making it more prominent in users’ feeds and spreading it to a much wider audience. The power of the algorithmic promotion undermined the efforts of Facebook’s content moderators and integrity teams, who were fighting an uphill battle against toxic and harmful content.
- The internal debate over the “angry” emoji and the findings about its effects shed light on the highly subjective human judgments that underlie Facebook’s news feed algorithm — the byzantine machine-learning software that decides for billions of people what kinds of posts they’ll see each time they open the app. The deliberations were revealed in disclosures made to the Securities and Exchange Commission and provided to Congress in redacted form by the legal counsel of whistleblower Frances Haugen. The redacted versions were reviewed by a consortium of news organizations, including The Washington Post.
- “Anger and hate is the easiest way to grow on Facebook,” Haugen told the British Parliament on Monday.
- In several cases, the documents show Facebook employees on its “integrity” teams raising flags about the human costs of specific elements of the ranking system — warnings that executives sometimes heeded and other times seemingly brushed aside. Employees evaluated and debated the importance of anger in society: Anger is a “core human emotion,” one staffer wrote, while another pointed out that anger-generating posts might be essential to protest movements against corrupt regimes.
- An algorithm such as Facebook’s, which relies on sophisticated, opaque machine-learning techniques to generate its engagement predictions, “can sound mysterious and menacing,” said Noah Giansiracusa, a math professor at Bentley University in Massachusetts and author of the book “How Algorithms Create and Prevent Fake News.” “But at the end of the day, there’s one number that gets predicted — one output. And a human is deciding what that number is.”
- Outage and whistleblower testimony renew focus on dangers of Facebook’s global reach
- Facebook spokesperson Dani Lever said: “We continue to work to understand what content creates negative experiences, so we can reduce its distribution. This includes content that has a disproportionate amount of angry reactions, for example.”
- The weight of the angry reaction is just one of the many levers that Facebook engineers manipulate to shape the flow of information and conversation on the world’s largest social network — one that has been shown to influence everything from users’ emotions to political campaigns to atrocities.
- How Facebook shapes your feed
- Facebook takes into account numerous factors — some of which are weighted to count a lot, some of which count a little and some of which count as negative — that add up to a single score that the news feed algorithm generates for each post in each user’s feed, each time they refresh it. That score is in turn used to sort the posts, deciding which ones appear at the top and which appear so far down that you’ll probably never see them. That single all-encompassing scoring system is used to categorize and sort vast swaths of human interaction in nearly every country of the world and in more than 100 languages.
- Facebook doesn’t publish the values its algorithm puts on different kinds of engagement, let alone the more than 10,000 “signals” that it has said its software can take into account in predicting each post’s likelihood of producing those forms of engagement. It often cites a fear of giving people with bad intentions a playbook to explain why it keeps the inner workings under wraps.
- Facebook’s levers rely on signals most users wouldn’t notice, like how many long comments a post generates, or whether a video is live or recorded, or whether comments were made in plain text or with cartoon avatars, the documents show. It even accounts for the computing load that each post requires and the strength of the user’s Internet signal. Depending on the lever, the effects of even a tiny tweak can ripple across the network, shaping whether the news sources in your feed are reputable or sketchy, political or not, whether you saw more of your real friends or more posts from groups Facebook wanted you to join, or if what you saw would be likely to anger, bore or inspire you.
- Beyond the debate over the angry emoji, the documents show Facebook employees wrestling with tough questions about the company’s values, performing cleverly constructed analyses. When they found that the algorithm was exacerbating harms, they advocated for tweaks they thought might help. But those proposals were sometimes overruled.
- When boosts, like those for emoji, collided with “deboosts” or “demotions” meant to limit potentially harmful content, all that complicated math added up to a problem in protecting users. The average post got a score of a few hundred, according to the documents. But in 2019, a Facebook data scientist discovered there was no limit to how high the ranking scores could go.
- If Facebook’s algorithms thought a post was bad, Facebook could cut its score in half, pushing most of instances of the post way down in users’ feeds. But a few posts could get scores as high as a billion, according to the documents. Cutting an astronomical score in half to “demote” it would still leave it with a score high enough to appear at the top of the user’s feed.
- “Scary thought: civic demotions not working,” one Facebook employee noted.
- How years of privacy controversies finally caught up with Facebook
- The culture of experimentation ran deep at Facebook, as engineers pulled levers and measured the results. An experiment in 2012 that was published in 2014 sought to manipulate the emotional valence of posts shown in users’ feeds to be more positive or more negative, and then observed whether their own posts changed to match those moods, raising ethical concerns, The Post reported at the time. Another, reported by Haugen to Congress this month, involved turning off safety measures for a subset of users as a comparison to see if the measures worked at all.
- A previously unreported set of experiments involved boosting some people more frequently into the feeds of some of their randomly chosen friends — and then, once the experiment ended, examining whether the pair of friends continued communication, according to the documents. A researcher hypothesized that, in other words, Facebook could cause relationships to become closer.
- In 2017, Facebook was trying to reverse a worrying decline in how much people were posting and talking to each other on the site, and the emoji reactions gave it five new levers to pull. Each emotional reaction was worth five likes at the time. The logic was that a reaction emoji signaled the post had made a greater emotional impression than a like; reacting with an emoji took an extra step beyond the single click or tap of the like button. But Facebook was coy with the public as to the importance it was placing on these reactions: The company told Mashable in 2017 that it was weighting them just “a little more than likes.”
- The move was consistent with a pattern, highlighted in the documents, in which Facebook set the weights very high on new features it was trying to encourage users to adopt. By training the algorithm to optimize for those features, Facebook’s engineers all but ensured they’d be widely used and seen. Not only that, but anyone posting on Facebook with the hope of reaching a wide audience — including publishers and political actors — would inevitably catch on that certain types of posts were working better than others.
- At one point, CEO Mark Zuckerberg even encouraged users in a public reply to a user’s comment to use the angry reaction to signal they disliked something, although that would make Facebook show similar content more often.
- Replies to a post, which signaled a larger effort than the tap of a reaction button, were weighted even higher, up to 30 times as much as a like. Facebook had found that interaction from a user’s friends on the site would create a sort of virtuous cycle that pushed users to post even more. The Wall Street Journal reported last month on how Facebook’s greater emphasis on comments, replies to comments and replies to re-shares — part of a metric it called “meaningful social interactions” — further incentivized divisive political posts. (That article also mentioned the early weight placed on the angry emoji, though not the subsequent debates over its impact.)
- The goal of that metric is to “improve people’s experience by prioritizing posts that inspire interactions, particularly conversations, between family and friends,” Lever said.
- New whistleblower claims Facebook allowed hate, illegal activity to go unchecked
- The first downgrade to the angry emoji weighting came in 2018, when Facebook cut it to four times the value of a like, keeping the same weight for all of the emotions.
- But it was apparent that not all emotional reactions were the same. Anger was the least used of the six emoji reactions, at 429 million clicks per week, compared with 63 billion likes and 11 billion “love” reactions, according to a 2020 document. Facebook’s data scientists found that angry reactions were “much more frequent” on problematic posts: “civic low quality news, civic misinfo, civic toxicity, health misinfo, and health antivax content,” according to a document from 2019. Its research that year showed the angry reaction was “being weaponized” by political figures.
- In April 2019, Facebook put in place a mechanism to “demote” content that was receiving disproportionately angry reactions, although the documents don’t make clear how or where that was used, or what its effects were.
- By July, a proposal began to circulate to cut the value of several emoji reactions down to that of a like, or even count them for nothing. The “angry” reaction, along with “wow” and “haha,” occurred more frequently on “toxic” content and misinformation. In another proposal, from late 2019, “love” and “sad” — apparently called “sorry” internally — would be worth four likes, because they were safer, according to the documents.
- The proposal depended on Facebook higher-ups being “comfortable with the principle of different values for different reaction types,” the documents said. This would have been an easy fix, the Facebook employee said, with “fewer policy concerns” than a technically challenging attempt to identify toxic comments.
- But at the last minute, the proposal to expand those measures worldwide was nixed.
- “The voice of caution won out by not trying to distinguish different reaction types and hence different emotions,” a staffer later wrote.
- Later that year, as part of a debate over how to adjust the algorithm to stop amplifying content that might subvert democratic norms, the proposal to value angry emoji reactions less was again floated. Another staffer proposed removing the button altogether. But again, the weightings remained in place.
- Finally, last year, the flood of evidence broke through the dam. Additional research had found that users consistently didn’t like it when their posts received “angry” reactions, whether from friends or random people, according to the documents. Facebook cut the weight of all the reactions to one and a half times that of a like.
- How Facebook is trying to stop its own algorithms from doing their job
- That September, Facebook finally stopped using the angry reaction as a signal of what its users wanted and cut its weight to zero, taking it out of the equation, the documents show. Its weight is still zero, Facebook’s Lever said. At the same time, it boosted “love” and “sad” to be worth two likes.
- It was part of a broader fine-tuning of signals. For example, single-character comments would no longer count. Until that change was made, a comment just saying “yes” or “.” — tactics often used to game the system and appear higher in the news feed — had counted as 15 times the value of a like.
- “Like any optimization, there’s going to be some ways that it gets exploited or taken advantage of,” Lars Backstrom, a vice president of engineering at Facebook, said in an emailed statement. “That’s why we have an integrity team that is trying to track those down and figure out how to mitigate them as efficiently as possible.”
- But time and again, Facebook made adjustments to weightings after they had caused harm. Facebook wanted to encourage users to stream live video, which it favored over photo and text posts, so its weight could go as high as 600 times. That had helped cause “ultra-rapid virality for several low quality viral videos,” a document said. Live videos on Facebook played a big role in political events, including both the racial justice protests last year after the killing of George Floyd and the riot at the U.S. Capitol on Jan. 6.
- Immediately after the riot, Facebook frantically enacted its “Break the Glass” measures on safety efforts it had previously undone — including to cap the weight on live videos at only 60. Facebook didn’t respond to requests for comment about the weighting on live videos.
- When Facebook finally set the weight on the angry reaction to zero, users began to get less misinformation, less “disturbing” content and less “graphic violence,” company data scientists found. As it turned out, after years of advocacy and pushback, there wasn’t a trade-off after all. According to one of the documents, users’ level of activity on Facebook was unaffected.
- An experiment that sought to manipulate the emotional valence of posts shown in users’ feeds to be more positive or more negative, and then observed whether their own posts changed to match those moods, took place in 2012, not 2014. It was published in 2014. This article has been corrected.
- The Facebook Papers are a set of internal documents that were provided to Congress in redacted form by Frances Haugen’s legal counsel. The redacted versions were reviewed by a consortium of news organizations, including The Washington Post.
- The trove of documents show how Facebook CEO Mark Zuckerberg has, at times, contradicted, downplayed or failed to disclose company findings on the impact of its products and platforms.
- The documents also provided new details of the social media platform’s role in fomenting the storming of the U.S. Capitol. An investigation by ProPublica and The Washington Post found that Facebook groups swelled with at least 650,000 posts attacking the legitimacy of Joe Biden’s victory between Election Day and Jan. 6.
- Facebook engineers gave extra value to emoji reactions, including ‘angry,’ pushing more emotional and provocative content into users’ news feeds.
- Read more from The Post’s investigation:
- Key takeaways from the Facebook Papers
- Frances Haugen took thousands of Facebook documents. This is how she did it.
- How Facebook neglected the rest of the world, fueling hate speech and violence in India
- How Facebook shapes your feed

URL: https://www.wsj.com/articles/facebook-algorithm-change-zuckerberg-11631654215
- WSJ Membership
- Customer Service
- Tools & Features
- Ads
- More
- Dow Jones Products

URL: https://www.dailymail.co.uk/news/article-10132759/Facebooks-algorithm-promoted-toxic-hateful-content.html
- By Mark Duell for MailOnline
- Published:  11:27 EDT, 26 October 2021   |  Updated:  15:46 EDT, 26 October 2021
- 
- 463
- View  comments
- 
- Facebook's algorithm promoted 'toxic and hateful' content by giving five points to posts with emojis including 'angry and sad' and only one for those that received likes, leaked documents claimed today.
- The firm's algorithm, which decides what people see on a newsfeed, was allegedly programmed to use the reaction emoji as a sign to push more provocative content.
- The five emojis of 'love,' 'haha,' 'wow,' 'sad' and 'angry' were launched five years ago to give users an alternative way to react to content aside from the traditional 'like'.
- But a ranking algorithm meant emoji reactions were treated as five times more valuable than 'likes', according to internal papers revealed by the Washington Post.
- This idea behind this was that high numbers of reaction emojis on posts were keeping users more engaged - a crucial element to Facebook's business model.
- But it meant content that created strong reactions such as hate and anger were shown to more people than more benign posts that people merely 'liked' - amplifying online arguments.
- The five Facebook emojis of 'love,' 'haha,' 'wow,' 'sad' and 'angry' were launched five years ago to give users an alternative way to react to content aside from the normal 'like'
- And the company's own researchers and scientists found that posts prompting angry reactions were far more likely to include misinformation and low-quality news.
- One staffer allegedly wrote that favouring 'controversial' posts such as those making people angry could open 'the door to more spam/abuse/clickbait inadvertently'.
- Another is said to have replied: 'It's possible'. In 2019, its data scientists confirmed the link between posts sparking the angry emoji and toxicity on its platform.
- This means Facebook stands accused of promoting the worst parts of its site for three years - making it more prominent and seeing it reach a much bigger audience.
- It would have also had a negative effect on the work of its content moderators who were trying to reduce the amount of toxic and harmful posts being seen by users.
- Facebook whistleblower Frances Haugen told MPs yeterday that the firm is 'unquestionably' making online hate worse because it is programmed to prioritise extreme content
- The discussions between staff were revealed in papers given to the Securities and Exchange Commission and provided to Congress by the lawyers of Frances Haugen.
- Facebook profits shot higher as the number of daily active users on its site and apps hit 1.93billion on average in September.
- This was 6 per cent up on last year.
- Around 3.6billion people used Facebook or one of its other platforms - which include WhatsApp and Instagram - last month.
- Facebook's profits shot 17 per cent higher to £6.7billion in the third quarter amid the jump in users.
- But the company's revenues fell short of Wall Street forecasts as Apple's new privacy rules hit sales.
- Since April, Apple has required all apps to ask users if they want to be tracked, which has made it harder for advertisers to target the right audiences. It said Apple's new regime would continue to hit business for the rest of the year.
- Facebook's total revenue - most of which comes from advertising - rose to £21billion in the third quarter.
- This was £400million below expectations - though it was more than a third higher than the same period of last year when companies had put their marketing budgets on ice during the pandemic.
- The whistleblower said in London just yesterday that Facebook was 'unquestionably' making online hate worse because it is programmed to prioritise extreme content.
- Miss Haugen told MPs and peers that bosses at the firm were guilty of 'negligence' in not accepting how the workings of their algorithm were damaging society.
- The American data scientist claimed the tech giant was 'subsidising hate' because its business model made it cheaper to run angry and divisive adverts.
- She said there was 'no doubt' the platform's systems would drive more violent events because its most extreme content is targeted at the most impressionable people.
- Miss Haugen also issued a stark warning to parents that Instagram, owned by Facebook, may never be safe for children as its own research found it turned them into addicts.
- She also told the joint committee on the draft Online Safety Bill that it was a 'critical moment for the UK to stand up' and improve social media.
- The Bill will impose a duty of care on social media companies to protect users from harmful content and give watchdog Ofcom the power to fine them up to 10 per cent of their global turnover.
- Facebook is currently battling a crisis after Miss Haugen, a former product manager at the firm, leaked thousands of internal documents that revealed its inner workings.
- Its founder Mark Zuckerberg has previously rejected her claims, saying her attacks on the company were 'misrepresenting' the work it does.
- Yesterday the committee highlighted how the tech giant had previously claimed it removes 97 per cent of hateful posts on the platform.
- But leaked research showed its own staff estimated that it only took down posts that generated around 3 to 5 per cent of hate speech and 0.6 per cent of content that breached its rules on violence and incitement.
- Facebook founder Mark Zuckerberg (pictured) has previously rejected the claims made by Miss Haugen, saying her attacks on the company were 'misrepresenting' the work it does
- Asked about hate speech, Miss Haugen said: 'Unquestionably it is making hate worse.'
- She said Facebook was 'very good at dancing with data' to make it seem as though it was on top of the problem but was reluctant to sacrifice even a 'slither of profit' to make the platform safer.
- The committee also heard how Facebook's research found that 10 to 15 per cent of ten-year-olds were on the platform – despite the minimum age being 13.
- Lord Black of Brentwood noted that the Bill exempts legitimate news publishers from its scope, but that there is no obligation for Facebook and other platforms to carry such journalism as they would have to observe the codes of the regulator.
- AI would effectively be making these decisions, he said, and asked if Miss Haugen trusted AI to make these types of judgment.
- Miss Haugen said the Bill should not treat a 'random blogger' the same way as a recognised news source as this would dilute users' access to high quality news on the platform.
- She said: 'I'm very concerned that if you just exempted across the board you will make the regulations ineffective.'
- She further warned that 'any system where the solution is AI is a system that's going to fail'.
- The thumbs up 'Like' logo is shown on a sign at Facebook's offices in Menlo Park, California
- In response to the Washington Post report about emojis, a Facebook spokesman told MailOnline today: 'We continue to work to understand what content creates negative experiences, so we can reduce its distribution. This includes content that has a disproportionate amount of angry reactions, for example.'
- Mr Zuckerberg also spoke about the issue on October 6, saying: 'The argument that we deliberately push content that makes people angry for profit is deeply illogical.
- 'We make money from ads, and advertisers consistently tell us they don't want their ads next to harmful or angry content. And I don't know any tech company that sets out to build products that make people angry or depressed. The moral, business and product incentives all point in the opposite direction.'
- And in reponse to yesterday's hearing on the Draft Online Safety Bill, a Facebook spokesman said today: 'Contrary to what was discussed at the hearing, we've always had the commercial incentive to remove harmful content from our sites.
- 'People don't want to see it when they use our apps and advertisers don't want their ads next to it. That's why we've invested $13billion and hired 40,000 people to do one job: keep people safe on our apps.
- 'As a result we've almost halved the amount of hate speech people see on Facebook over the last three quarters - down to just 0.05 per cent of content views.
- 'While we have rules against harmful content and publish regular transparency reports, we agree we need regulation for the whole industry so that businesses like ours aren't making these decisions on our own. The UK is one of the countries leading the way and we're pleased the Online Safety Bill is moving forward.'
- And finally, on the subject of safety, misinformation and harmful content, a Facebook spokesman said: 'Every day our teams have to balance protecting the ability of billions of people to express themselves openly with the need to keep our platform a safe and positive place.
- 'We continue to make significant improvements to tackle the spread of misinformation and harmful content. To suggest we encourage bad content and do nothing is just not true.'
- Published by Associated Newspapers Ltd
- Part of the Daily Mail, The Mail on Sunday & Metro Media Group

URL: https://www.poynter.org/commentary/2021/the-most-damning-facebook-story-yet/

URL: https://theweek.com/facebook/1006422/facebook-reportedly-gave-the-angry-emoji-5-times-as-much-weight-as-a-like
- The Facebook papers
- DENIS CHARLET/AFP via Getty Images
- Facebook reportedly sparked internal concern and debate after tweaking its algorithm to make reactions — including anger — five times more important than "likes."
- The company made this change to its algorithm giving emoji reactions five times the weight of likes in 2017, The Washington Post reported, citing company documents. The idea was to boost content that sparked engagement and interaction from users, but "Facebook's own researchers were quick to suspect a critical flaw," the Post writes. As one staffer warned, this could lead to a "higher ratio of controversial than agreeable content" in users' news feeds, opening "the door to more spam/abuse/clickbait inadvertently." Another Facebook staffer at the time reportedly acknowledged this was "possible."
- Facebook data scientists by 2019 determined posts that earn angry emojis were more likely to include misinformation, toxicity, and low quality news, meaning "Facebook for three years systematically amped up some of the worst of its platform, making it more prominent in users' feeds and spreading it to a much wider audience," the Post writes. In 2018, Facebook reportedly cut the weight of the anger emoji to four times that of likes. The company eventually gave the anger emoji a weight of zero, which it continues to have today, while the "love" and "sad" emojis became worth two likes.
- This was the latest report to come out of a series of documents provided to Congress by former Facebook employee Frances Haugen, who has accused Facebook of prioritizing profits over users' safety. A Facebook spokesperson told the Post that "we continue to work to understand what content creates negative experiences, so we can reduce its distribution," including "content that has a disproportionate amount of angry reactions." Read more at The Washington Post.
- The Week™ is part of Future plc, an international media group and leading digital publisher. Visit our corporate site at https://futureplc.comThe Week™ is a registered trade mark.© Future US, Inc. Full 7th Floor, 130 West 42nd Street, New York, NY 10036. All rights reserved.

URL: https://thehill.com/policy/technology/578548-facebook-formula-gave-anger-five-times-weight-of-likes-documents-show
- Facebook’s formula made it so posts that people reacted angrily to were given more weight in what users would see than posts that users reacted to with just a simple push of the “like” icon, according to documents seen by The Washington Post.
- The documents showed when Facebook introduced new reaction emojis to their platform, it changed what content was more heavily promoted toward users.
- In 2017, Facebook introduced emojis people could use to react to a person’s post beyond the simple “like” button. A person was able to react with emojis that correlated to “angry,” “sad,” “haha,” “love” and “wow.”
- If a person reacted with an emoji instead of the “like” button, the Facebook algorithm would see the post as five times more valuable and push similar content.
- The documents showed Facebook staff immediately were skeptical of the plan, with one staffer saying it could open “the door to more spam/abuse/clickbait inadvertently,” according to The Post.
- The documents were presented by Frances Haugen, a former Facebook employee and whistleblower, to the Securities and Exchange Commission and Congress.
- The documents showed the subject of anger was highly debated in the company with executives appearing to sometimes take employees’ concerns into consideration and seemingly ignoring those concerns at other times.
- Facebook CEO Mark Zuckerberg even encouraged users to use the angry face emoji to react to posts they didn’t like, without the users knowing it would push more content they didn’t like.
- There was a proposal to cut down the value of reaction emojis in 2019 but the idea was cut at the last moment, the documents showed.
- “The voice of caution won out by not trying to distinguish different reaction types and hence different emotions,” a staffer said.
- The weight of emojis was finally cut to only one and a half times greater than a “like” back in 2020.
- “Like any optimization, there’s going to be some ways that it gets exploited or taken advantage of,” Lars Backstrom, a vice president of engineering at Facebook, told The Washington Post. “That’s why we have an integrity team that is trying to track those down and figure out how to mitigate them as efficiently as possible.”
- The Hill has reached out to Facebook for comment.
- Copyright 2023 Nexstar Media Inc. All rights reserved. This material may not be published, broadcast, rewritten, or redistributed.
- THE HILL 1625 K STREET, NW SUITE 900 WASHINGTON DC 20006 | 202-628-8500 TEL | 202-628-8503 FAX
- © 1998 - 2023 Nexstar Media Inc. | All Rights Reserved.

URL: https://nymag.com/intelligencer/2021/10/what-was-leaked-in-the-facebook-papers.html
- 
- Things you buy through our links may earn Vox Media a commission.
- A consortium of media organizations are reporting revelations from a trove of leaked internal documents from Facebook. Most of the company documents were provided to Congress, the Securities and Exchange Commission, and to a consortium of news organizations by lawyers representing Facebook whistleblower Frances Haugen. She recently testified before Congress about a range of troubling issues and policies at the social-media giant and has filed at least eight whistleblower complaints — about the company putting profit over public safety — with the SEC. In addition, the Washington Post has reported that a new Facebook whistleblower, who is a former employee like Haugen, has submitted a sworn affidavit to the SEC that makes similar allegations. Facebook, which is getting ready to announce a company name change, has been pushing back against all reports. The company has denied that it values profit over public safety, emphasized the effectiveness of its safeguards, and claimed the leaked documents present a cherry-picked, negative view of the company’s internal operations. Below, a guide to the latest revelations from the leaked Facebook papers.
- .
- Facebook founder and CEO Mark Zuckerberg announced on Monday that the company was undertaking a long-term “retooling” to make engaging younger users its “north star.” As our Verge colleague Alex Heath explains, the leaked documents detail why:
- Teenage users of the Facebook app in the US had declined by 13 percent since 2019 and were projected to drop 45 percent over the next two years, driving an overall decline in daily users in the company’s most lucrative ad market. Young adults between the ages of 20 and 30 were expected to decline by 4 percent during the same timeframe. Making matters worse, the younger a user was, the less on average they regularly engaged with the app. The message was clear: Facebook was losing traction with younger generations fast. The “aging up issue is real,” the researcher wrote in an internal memo. They predicted that, if “increasingly fewer teens are choosing Facebook as they grow older,” the company would face a more “severe” decline in young users than it already projected.
- And young adults really don’t like Facebook:
- “Most young adults perceive Facebook as a place for people in their 40s and 50s,” according to [a March presentation to the company’s chief product officer, Chris Cox.] “Young adults perceive content as boring, misleading, and negative. They often have to get past irrelevant content to get to what matters.” It added that they “have a wide range of negative associations with Facebook including privacy concerns, impact to their wellbeing, along with low awareness of relevant services.”
- 
- The March presentation to Cox showed that, in the US, “teen acquisition is low and regressing further.” Account registrations for users under 18 were down 26 percent from the previous year in the app’s five top countries. For teens already on Facebook, the company was continuing to “see lower or worsening levels of engagement compared to older cohorts.” Messages sent by teens were down 16 percent from the previous year, while messages sent by users aged 20–30 were flat.
- Heath notes that there are warning signs for Instagram, too:
- Instagram was doing better with young people, with full saturation in the US, France, the UK, Japan, and Australia. But there was still cause for concern. Posting by teens had dropped 13 percent from 2020 and “remains the most concerning trend,” the researchers noted, adding that the increased use of TikTok by teens meant that “we are likely losing our total share of time.”
- The company also estimated that teenagers spend two to three times more time on TikTok than Instagram.
- .
- Some individuals who operate multiple Facebook accounts (which the company calls Single User Multiple Accounts, or SUMAs) have been responsible for a lot of the most divisive and harmful content on Facebook. But as Politico reports, the leaked documents indicate the company failed to address the problem after identifying it:
- [A] significant swath of [SUMAs] spread so many divisive political posts that they’ve mushroomed into a massive source of the platform’s toxic politics, according to internal company documents and interviews with former employees. While plenty of SUMAs are harmless, Facebook employees for years have flagged many such accounts as purveyors of dangerous political activity. Yet, the company has failed to crack down on SUMAs in any comprehensive way, the documents show. That’s despite the fact that operating multiple accounts violates Facebook’s community guidelines.
- 
- Company research from March 2018 said accounts that could be SUMAs were reaching about 11 million viewers daily, or about 14 percent of the total U.S. political audience. During the week of March 4, 2018, 1.6 million SUMA accounts made political posts that reached U.S. users.
- .
- One of Trump’s most inflammatory social media posts came on May 28, 2020, when he issued a warning to those protesting George Floyd’s murder in Minneapolis that “when the looting starts the shooting starts!” The AP reports that Facebook’s automated software determined with almost 90% certainty that the president had violated its rules. But Trump’s post, and account, stayed up, even as the company found that things began rapidly deteriorating on Facebook immediately after his message:
- The internal analysis shows a five-fold increase in violence reports on Facebook, while complaints of hate speech tripled in the days following Trump’s post. Reports of false news on the platform doubled. Reshares of Trump’s message generated a “substantial amount of hateful and violent comments,” many of which Facebook worked to remove. Some of those comments included calls to “start shooting these thugs” and “f—- the white.”
- On May 29, CEO Mark Zuckerberg wrote on his Facebook page that Trump had not violated Facebook’s policies, since he did not “cause imminent risk of specific harms or dangers spelled out in clear policies.” The company told the AP that its software is not always correct, and that humans are more reliable judges.
- .
- The Wall Street Journal notes that there has been contentious internal debate about the far right’s use of Facebook, and that political considerations loom large within company management:
- The documents reviewed by the Journal didn’t render a verdict on whether bias influences its decisions overall. They do show that employees and their bosses have hotly debated whether and how to restrain right-wing publishers, with more-senior employees often providing a check on agitation from the rank and file. The documents viewed by the Journal, which don’t capture all of the employee messaging, didn’t mention equivalent debates over left-wing publications.
- 
- Other documents also reveal that Facebook’s management team has been so intently focused on avoiding charges of bias that it regularly places political considerations at the center of its decision making.
- .
- Politico reports that the internal documents show that in late 2020, Facebook researchers concluded that the company’s efforts to moderate hate speech in the Middle East were failing, and not without consequence:
- Only six percent of Arabic-language hate content was detected on Instagram before it made its way onto the photo-sharing platform owned by Facebook. That compared to a 40 percent takedown rate on Facebook. Ads attacking women and the LGBTQ community were rarely flagged for removal in the Middle East. In a related survey, Egyptian users told the company they were scared of posting political views on the platform out of fear of being arrested or attacked online. …
- 
- In Afghanistan, where 5 million people are monthly users, Facebook employed few local-language speakers to moderate content, resulting in less than one percent of hate speech being taken down. Across the Middle East, clunky algorithms to detect terrorist content incorrectly deleted non-violent Arabic content 77 percent of the time, harming people’s ability to express themselves online and limiting the reporting of potential war crimes. In Iraq and Yemen, high levels of coordinated fake accounts — many tied to political or jihadist causes — spread misinformation and fomented local violence, often between warring religious groups.
- .
- The leaked documents reveal more information about Facebook training its platform algorithms to boost engagement in 2017 by promoting posts that provoked emotional responses. The effort was an attempt to reverse a decline in how much users were posting and communicating on the site. Per the Washington Post:
- Facebook programmed the algorithm that decides what people see in their news feeds to use the reaction emoji as signals to push more emotional and provocative content — including content likely to make them angry. Starting in 2017, Facebook’s ranking algorithm treated emoji reactions as five times more valuable than “likes,” internal documents reveal.
- 
- Facebook for three years systematically amped up some of the worst of its platform, making it more prominent in users’ feeds and spreading it to a much wider audience. The power of the algorithmic promotion undermined the efforts of Facebook’s content moderators and integrity teams, who were fighting an uphill battle against toxic and harmful content.
- The “angry” emoji itself has also prompted internal controversy.
- .
- The Washington Post also makes note of new information revealed in the leaked documents regarding Facebook’s efforts to research manipulating users’ emotions:
- The culture of experimentation ran deep at Facebook, as engineers pulled levers and measured the results. An experiment in 2014 sought to manipulate the emotional valence of posts shown in users’ feeds to be more positive or more negative, and then watch to see if the posts changed to match, raising ethical concerns, The Post reported at the time. Another, reported by [whistleblower Frances] Haugen to Congress this month, involved turning off safety measures for a subset of users as a comparison to see if the measures worked at all.
- 
- A previously unreported set of experiments involved boosting some people more frequently into the feeds of some of their randomly chosen friends — and then, once the experiment ended, examining whether the pair of friends continued communication, according to the documents. A researcher hypothesized that, in other words, Facebook could cause relationships to become closer.
- .
- The Associated Press reports that according to the leaked documents, last March, as the U.S. vaccine rollout was picking up steam, Facebook employees researched ways to counter anti-vaccine claims on the platform, but the solutions they suggested weren’t implemented either quickly or at all:
- By altering how posts about vaccines are ranked in people’s newsfeeds, researchers at the company realized they could curtail the misleading information individuals saw about COVID-19 vaccines and offer users posts from legitimate sources like the World Health Organization.
- “Given these results, I’m assuming we’re hoping to launch ASAP,” one Facebook employee wrote, responding to the internal memo about the study. Instead, Facebook shelved some suggestions from the study. Other changes weren’t made until April. When another Facebook researcher suggested disabling comments on vaccine posts in March until the platform could do a better job of tackling anti-vaccine messages lurking in them, that proposal was ignored.
- And Facebook had already been struggling to detect and address user comments expressing opposition or hesitancy toward the vaccines:
- [C]ompany employees admitted they didn’t have a handle on catching those comments. And if they did, Facebook didn’t have a policy in place to take the comments down. The free-for-all was allowing users to swarm vaccine posts from news outlets or humanitarian organizations with negative comments about vaccines.
- 
- “Our ability to detect (vaccine hesitancy) in comments is bad in English — and basically non-existent elsewhere,” another internal memo posted on March 2 said.
- .
- The New York Times reports that according to internal documents, the company has scrutinized some of its core features and how they could cause harm:
- What researchers found was often far from positive. Time and again, they determined that people misused key features or that those features amplified toxic content, among other effects. In an August 2019 internal memo, several researchers said it was Facebook’s “core product mechanics” — meaning the basics of how the product functioned — that had let misinformation and hate speech flourish on the site.
- 
- “The mechanics of our platform are not neutral,” they concluded.
- The Times adds that while the internal documents do not reveal how Facebook acted in response to the research, most of the platform’s core experience has remained the same, and “Many significant modifications to the social network were blocked in the service of growth and keeping users engaged, some current and former executives said.”
- .
- Our Verge colleague Casey Newton explains that one theme that stands out from the leaked documents is “the significant variation in content moderation resources afforded to different countries based on criteria that are not public or subject to external review”:
- Brazil, India, and the United States were placed in “tier zero,” the highest priority. Facebook set up “war rooms” to monitor the network continuously. They created dashboards to analyze network activity and alerted local election officials to any problems. Germany, Indonesia, Iran, Israel, and Italy were placed in tier one. They would be given similar resources, minus some resources for enforcement of Facebook’s rules and for alerts outside the period directly around the election. In tier two, 22 countries were added. They would have to go without the war rooms, which Facebook also calls “enhanced operations centers.”
- 
- The rest of the world was placed into tier three. Facebook would review election-related material if it was escalated to them by content moderators. Otherwise, it would not intervene.
- .
- The Associated Press reports that the internal documents reveal that Facebook did not dedicate the necessary resources to tackle hate speech and incitements to violence in numerous countries around the world — and knew it:
- An examination of the files reveals that in some of the world’s most volatile regions, terrorist content and hate speech proliferate because the company remains short on moderators who speak local languages and understand cultural contexts. And its platforms have failed to develop artificial-intelligence solutions that can catch harmful content in different languages. …
- 
- “The root problem is that the platform was never built with the intention it would one day mediate the political speech of everyone in the world,” said Eliza Campbell, director of the Middle East Institute’s Cyber Program. “But for the amount of political importance and resources that Facebook has, moderation is a bafflingly under-resourced project.”
- .
- Reports CNN:
- The company has known about human traffickers using its platforms in this way since at least 2018, the documents show …
- 
- Facebook documents describe women trafficked in this way being subjected to physical and sexual abuse, being deprived of food and pay, and having their travel documents confiscated so they can’t escape. Earlier this year, an internal Facebook report noted that “gaps still exist in our detection of on-platform entities engaged in domestic servitude” and detailed how the company’s platforms are used to recruit, buy and sell what Facebook’s documents call “domestic servants.”
- .
- In 2019, Apple threatened to remove Facebook and Instagram from its app store, citing the reported sale and trade of women as maids on Facebook in the Middle East. CNN reports that according to internal documents, Facebook employees then “rushed to take down problematic content and make emergency policy changes avoid what they described as a ‘potentially severe’ consequence for the business.” The Associated Press adds that Facebook acknowledged internally that it was “under-enforcing on confirmed abusive activity.” And it’s a still a problem, per the AP:
- Facebook’s crackdown seems to have had a limited effect. Even today, a quick search for “khadima,” or “maids” in Arabic, will bring up accounts featuring posed photographs of Africans and South Asians with ages and prices listed next to their images. That’s even as the Philippines government has a team of workers that do nothing but scour Facebook posts each day to try and protect desperate job seekers from criminal gangs and unscrupulous recruiters using the site.
- .
- When Facebook’s CEO was hit with an ultimatum from Vietnam’s autocratic government — censor posts from anti-government pages or be forced to cease operations in the country — he placated the autocrats. The Washington Post reports that ahead of an election in the country, Zuckerberg personally gave the okay to bend to government demands. As a result, “Facebook significantly increased censorship of ‘anti-state’ posts, giving the government near-total control over the platform, according to local activists and free-speech advocates.” The country has strict rules around dissent in social media, and its authorities often detain and prosecute citizens who run afoul of them.
- Zuckerberg’s decision illustrates how Facebook’s stated commitment to free speech shifts drastically across countries. It is also illustrative of the crucial role the social network plays in disseminating information around the world — a reality often overlooked in the conversation around its American operations.
- .
- In 2019 and 2020, a researcher at Facebook created fictitious user accounts on the platform in order to test how the company’s recommendation systems fed misinformation and polarizing content. One test user, created in the summer of 2019, was a conservative mother named Carol Smith from North Carolina who expressed an interest in politics, parenting, and Christianity. Within two days, Facebook was already recommending QAnon groups to the woman. That continued even after the test user did not follow the suggested groups. In a report titled “Carol’s Journey to QAnon,” the researcher concluded that Facebook ultimately supplied “a barrage of extreme, conspiratorial, and graphic content.” Facebook has since banned QAnon groups from the platform, but NBC News reports: “The body of research consistently found Facebook pushed some users into ‘rabbit holes,’ increasingly narrow echo chambers where violent conspiracy theories thrived. People radicalized through these rabbit holes make up a small slice of total users, but at Facebook’s scale, that can mean millions of individuals.”
- The researcher left the company in 2020, citing Facebook’s slow response to the rise of QAnon as a reason in her exit letter.
- .
- On November 5, 2020, a Facebook employee alerted colleagues that election misinformation had proliferated in comments responding to posts and that the worst of these messages were being amplified to the tops of comment threads. On November 9, a Facebook data scientist informed his colleagues that about 10 percent of all U.S. views of political content on the platform were of content alleging there had been election fraud — as much as one in every 50 views on Facebook at the time. He added that there was “also a fringe of incitement to violence” in the content, according to the New York Times.
- .
- Facebook dismantled some of the safeguards it had put in place to counter misinformation ahead of and immediately following the 2020 election, according to the leaked documents. The New York Times reports that three former employees said that Facebook, in part concerned about user backlash, began winding down some of those safeguards in November. It also disbanded a 300-person “Civic Integrity” team in early December just as the Stop the Steal movement was gaining more and more momentum, including on Facebook. Some Stop the Steal Facebook groups enjoyed record-breaking growth compared with any other group on the platform up to then, and it was apparent that group organizers were actively trying to get around Facebook’s moderation efforts.
- The documents, including a company postmortem analysis, indicate that Facebook failed to address the movement as a whole and thus didn’t do all it could have to counter the spread of Stop the Steal on the platform. Facebook was then left scrambling to implement emergency measures on January 6 when the movement became an insurrection at the U.S. Capitol.
- .
- On January 6, after Facebook CEO Mark Zuckerberg and CTO Mike Schroepfer posted notes condemning the Capitol riot on the company’s internal discussion platform, some employees responded with outrage. Among their comments:
- .
- As the New York Times emphasized in its report on Friday:
- What the documents do not offer is a complete picture of decision-making inside Facebook. Some internal studies suggested that the company struggled to exert control over the scale of its network and how quickly information spread, while other reports hinted that Facebook was concerned about losing engagement or damaging its reputation. Yet what was unmistakable was that Facebook’s own employees believed the social network could have done more, according to the documents.
- .
- Facebook’s largest national user base is in India, where 340 million people use one of the company’s social-media platforms. But the New York Times reports that the leaked documents “provide stark evidence of one of the most serious criticisms levied by human rights activists and politicians against the world-spanning company: It moves into a country without fully understanding its potential impact on local culture and politics, and fails to deploy the resources to act on issues once they occur.” One leaked document indicated that only 13 percent of Facebook’s global budget for time spent classifying misinformation was set aside for markets beyond the U.S., despite the fact that 90 percent of Facebook’s user base is abroad. (Facebook told the Times those figures were incomplete.)
- According to the documents, Facebook has struggled to address the spread of misinformation, hate speech (including anti-Muslim content), and celebrations of violence on the platform in India. The company’s efforts have been hampered by a lack of resources, a lack of expertise in the country’s numerous languages, and other problems like the use of bots linked to some of India’s political groups. As one stark example, a Facebook researcher, who ran an experiment in 2019 where a test user in India followed all the recommendations made by the platform’s algorithms, later said in a report, “Following this test user’s News Feed, I’ve seen more images of dead people in the past three weeks than I’ve seen in my entire life total.”
- The Washington Post reported that a team from Facebook traveled to the European Union and heard critiques from politicians about a change to the site’s algorithm that was made in 2018. They said that the adjustment had changed politics “for the worse,” according to an April 2019 document. The team noted particular concerns in Poland where political party members believed that Facebook was contributing to a “social civil war” where negative politicking received more weight and attention on the platform:
- In Warsaw, the two major parties — Law and Justice and the opposition Civic Platform — accused social media of deepening the country’s political polarization, describing the situation as “unsustainable,” the Facebook report said.
- 
- “Across multiple European countries, major mainstream parties complained about the structural incentive to engage in attack politics,” the report said. “They see a clear link between this and the outsize influence of radical parties on the platform.”
- This post has been updated.
- By submitting your email, you agree to our Terms and Privacy Notice and to receive email correspondence from us.
- Things you buy through our links may earn Vox Media a commission.
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 

URL: https://www.inputmag.com/culture/facebook-weighed-emoji-reactions-much-heavier-than-a-like
- Facebook
- 5x
- The past weight of a single "Angry" emoji versus one "Like."
- The Washington Post
- Are you tired of dunking on Facebook yet? Good, ‘cause we sure as hell aren’t, either. A new report courtesy of The Washington Post reveals that, for years following their debut in 2017, Facebook weighted “emoji” reactions to user posts five times more than that of a single “like.”
- The decision is easy enough to understand — anything that prompts people to opt for a pointedly emotional response versus a neutral-to-positive “like” often results in more time spent on Facebook, thus more time giving the company valuable user data and potential ad revenues. Of course, a problem surfaced almost immediately, explains the WaPo:
- Did this happen for billions of users? You better believe it. Did Facebook do anything to change that? Barely.
- An (eventual) change in approach — Roughly four years after their introduction and outsized weight metric, Facebook finally adjusted how emoji responses factor into users’ News Feed algorithms. “Angry” responses are now a net-zero effect, with “Love” and “Sad” now worth that of two “Likes.” Facebook didn’t change this because of the obviously negative effects on society, though... no, it made the switch after receiving an inordinate amount of evidence showing that its users just “didn’t like” seeing them.
- Manipulating societal emotion — The entire WaPo piece is well worth a read, if nothing else for the reminder that, back in 2012, Facebook engineers were straight-up experimenting with societal emotions. “An experiment in 2012 that was published in 2014 sought to manipulate the emotional valence of posts shown in users’ feeds to be more positive or more negative, and then observed whether their own posts changed to match those moods, raising ethical concerns,” the article recounts. Extrapolate that across millions upon millions of users, and... yeah. It’s creepy as hell.
- You should delete your account if you haven’t yet, by the way.

URL: https://www.niemanlab.org/2021/10/more-internal-documents-show-how-facebooks-algorithm-prioritized-anger-and-posts-that-triggered-it/
- As if there wasn’t enough Facebook news to digest already, another deep dive from The Washington Post this morning revealed that Facebook engineers changed the company’s algorithm to prioritize and elevate posts that elicited emoji reactions — many of which were rolled out in 2017. More specifically, the ranking algorithm treated reactions such as “angry,” “love,” “sad,” and “wow” as five times more valuable than traditional “likes” on the social media platform.
- The problem with this plan for engagement: Other posts also likely to yield similar reactions were more likely to show up, only these posts were likely to also contain misinformation, spam, or forms of clickbait. One Facebook staffer, whose name was redacted in a dump of documents shared with the Securities and Exchange Commission by whistleblower and former Facebook employee Frances Haugen, had warned that this might happen; they were proven right.
- According to the Post, “The company’s data scientists confirmed in 2019 that posts that sparked angry reaction emoji were disproportionately likely to include misinformation, toxicity and low-quality news.”
- More on that:
- That means Facebook for three years systematically amped up some of the worst of its platform, making it more prominent in users’ feeds and spreading it to a much wider audience. The power of the algorithmic promotion undermined the efforts of Facebook’s content moderators and integrity teams, who were fighting an uphill battle against toxic and harmful content.
- This isn’t the first time that “anger” has reared its ugly head as a useful metric. Back in 2017, a report found that hyper-political publishers were especially adept at provoking the anger of their readers. And in 2019, another report found some of the effects of Facebook’s change to prioritizing meaningful interactions:
- Facebook introduced the suite of “reaction” emojis in response to a decline in people talking to each other on the social platform, according to the report. Giving the reactions five times the value of a single like was Facebook’s effort to signal that “the post had made a greater emotional impression than a like; reacting with an emoji took an extra step beyond the single click or tap of the like button.”
- 
- Mark Zuckerberg acknowledges that reactions can be used to indicate dislike.
- Members of Facebook’s integrity teams raised concerns about the amplification of “anger” as a societal emotion, the documents reviewed by the Post show, but managers had a mixed record when it came to responding to these concerns.
- 
- A screenshot showing a staffer raising the question, “Quick question to play devil’s advocate: will weighting Reactions 5x stronger than Likes lead to News Feed having a higher ratio of controversial than agreeable content?”
- According to the latest documents, even efforts to counteract this effect — when they were actually implemented — produced less-than-desirable results. For instance, even if Facebook employees tried to manipulate the score of a high-ranking post to get it to show up less often things often didn’t work out as planned.
- If Facebook’s algorithms thought a post was bad, Facebook could cut its score in half, pushing most of instances of the post way down in users’ feeds. But a few posts could get scores as high as a billion, according to the documents. Cutting an astronomical score in half to “demote” it would still leave it with a score high enough to appear at the top of the user’s feed.
- “Scary thought: civic demotions not working,” one Facebook employee noted.
- The Post’s story details Facebook’s different attempts at dialing down this effect of amplifying reaction-driven posts.
- When Facebook finally set the weight on the angry reaction to zero, users began to get less misinformation, less “disturbing” content and less “graphic violence,” company data scientists found. As it turned out, after years of advocacy and pushback, there wasn’t a trade-off after all. According to one of the documents, users’ level of activity on Facebook was unaffected.
- Facebook’s reaction to this latest finding linking its algorithm and the prioritizing “anger” and posts that tend to invoke that emotion in users: “We continue to work to understand what content creates negative experiences, so we can reduce its distribution. This includes content that has a disproportionate amount of angry reactions, for example,” Facebook spokesperson Dani Lever told the Post.
- Cite this articleHide citations
- CLOSE
- MLA
- Chakradhar, Shraddha. "More internal documents show how Facebook’s algorithm prioritized anger and posts that triggered it." Nieman Journalism Lab. Nieman Foundation for Journalism at Harvard, 26 Oct. 2021. Web. 26 May. 2023.
- APA
- Chakradhar, S. (2021, Oct. 26). More internal documents show how Facebook’s algorithm prioritized anger and posts that triggered it. Nieman Journalism Lab. Retrieved May 26, 2023, from https://www.niemanlab.org/2021/10/more-internal-documents-show-how-facebooks-algorithm-prioritized-anger-and-posts-that-triggered-it/
- Chicago
- Chakradhar, Shraddha. "More internal documents show how Facebook’s algorithm prioritized anger and posts that triggered it." Nieman Journalism Lab. Last modified October 26, 2021.  Accessed May 26, 2023. https://www.niemanlab.org/2021/10/more-internal-documents-show-how-facebooks-algorithm-prioritized-anger-and-posts-that-triggered-it/.
- Wikipedia
- {{cite web
    | url = https://www.niemanlab.org/2021/10/more-internal-documents-show-how-facebooks-algorithm-prioritized-anger-and-posts-that-triggered-it/
    | title = More internal documents show how Facebook’s algorithm prioritized anger and posts that triggered it
    | last = Chakradhar
    | first =  Shraddha
    | work = [[Nieman Journalism Lab]]
    | date = 26 October 2021
    | accessdate = 26 May 2023
    | ref = {{harvid|Chakradhar|2021}}
}}
- To promote and elevate the standards of journalism
- Covering thought leadership in journalism
- Pushing to the future of journalism
- Exploring the art and craft of story
- The Nieman Journalism Lab is a collaborative attempt to figure out how quality journalism can survive and thrive in the Internet age.
- It’s a project of the Nieman Foundation for Journalism at Harvard University.

URL: https://fortune.com/2021/10/26/facebook-zuckerberg-why-were-so-afraid/
- On the surface — and maybe this is close to how the company wants to be seen — Facebook is merely a repository, a digital reflection of the human experience. All our pictures are there, but so are our reactions: I like this, this makes me sad/angry/jealous. In some ways, the criticism of Facebook is a criticism of who we are, and the realization that the ugliest parts of being human still break through the most curated social media profile.
- But the recent revelations about Facebook’s inner workings, furnished by whistleblower Frances Haugen, show us how distorted that reflection really is. In the words of one staffer, “we are not a neutral entity.” Thanks to the widening consortium of news outlets that are digging through thousands of pages of partially-redacted material, it’s now possible to get a deeper sense of what’s happening in the black box that is Facebook’s brain.
- One story I’m going to highlight here is from today’s Washington Post. For all the wonkiness that Facebook algorithms allegedly employ, for all the Stanford computer science degrees that are apparently required to understand what’s happening behind the scenes, this story boils down one scary aspect of how Facebook works, and all it requires is kindergarten-level arithmetic.
- Five is greater than one.
- That’s the basic formula behind the spread of anger through the social network. Why? Because the anger emoji  that people use to react to Facebook posts is five times more potent than a simple “like” in its algorithm. That means that posts that instill more anger from readers — regardless of whether or not it’s true — will then get shown on more people’s feeds.
- (Before I get any emails about this, yes, I know it’s more complicated than that, and that other factors, like comment length and time spent also play a role).
- It’s a deliberate choice that biases strong reactions, no matter what they are, rather than lukewarm ones, to keep people coming back, spending more time on the site, being exposed to ads and, eventually spending their money. That’s how Mark Zuckerberg’s company made $29 billion in revenue last quarter — and it was a disappointment.
- The revelations keep pointing to one thing that is, at heart, what’s so scary about Facebook. It’s the idea that our emotions aren’t our own, and we have less control over our very thoughts and feelings than we think we do. BuzzFeed reporter Joseph Bernstein channels Hannah Arendt — the German philosopher who coined the phrase “the banality of evil” — on this idea by pointing out that it’s in a giant social media company’s interests to sell the idea that, hey, our ad systems are so good they can nearly destroy democracy in a few years. Wouldn’t you want to use an engine that powerful to get people to buy jeans?
- That may be true. And perhaps, as Arendt points out, people can’t change their political opinions as easily as they can be persuaded to buy soap. But isn’t that actually an argument better suited to the anger-baiting stories that appear on Facebook, rather the distribution machine that Zuckerberg created? Misleading stories, propaganda, clickbait —all these can make us feel something like anger, even for a little while, even if it’s alien to our nature. What’s scary is how we keep coming back to the site that hosts it, how it can feel inescapable, even when we know we don’t want it.
- Kevin T. Dugan@kevintdugan
- Big beautiful meme stock. Digital World Acquisition, the blank-check company that’s bringing Donald Trump’s forthcoming social media gambit to the public markets, tumbled 28% at one point in trading today. The company, which has mysteriously little by way of solid plans or metrics, has become the latest stock to ignite the passions of day-trading Redditors who previously pumped up GameStop, AMC, and other so-called “meme stocks.” The company’s whipsawed through the markets, jumping from less than $10 a share to over $130, and is now back to the $67.
- TikTok talk. Michael Beckerman, head of public policy at TikTok, denied during a Congressional hearing that the company gives data to China — after years of scrutiny over its parent company’s relationship with Xi Jinping’s government, according to Reuters. The social media app, owned by Chinese company ByteDance, was once at the center of a standoff between the Trump Administration and China, with the previous U.S. administration pushing TikTok to divest part of its operations to Microsoft, and then Oracle, in deals that never materialized.
- Net neutrality on a moving train. The Biden administration appointed Jessica Rosenworcel to lead the Federal Communications Commission, a sign that the White House is gearing up to fight to reimplement net neutrality rules and expand broadband access, according to The Verge. Rosenworcel, who will be the first FCC chair, is a long-time advisor at the agency, and has been a supporter of keeping net neutrality, which blocks Internet service providers from charging more for certain content.
- Congratulations, Elon Musk. The Tesla CEO rocketed past Jeff Bezos yesterday as the world's wealthiest individual, with a personal fortune of $289 billion. According to Bloomberg, Musk's holdings are now greater than the market cap of ExxonMobil or Nike. I suppose if he wanted to to buy either of those companies he would have, uh, have the funding secured.
- The surge came after Hertz said it would buy 100,000 Tesla Model 3s — a pretty large order considering that Tesla has delivered fewer than 2 million cars since 2016. While this is unlikely to make renting a car any less of a pain, it made Tesla's stock go vroom by about 13% on Monday, before inching even higher today.
- Tesla is, as the New York Times points out, worth about as much as every other major car company in the world, combined. Wedbush Securities analyst Dan Ives says that Tesla's battery tech and its factories support this, but the strong brand, inextricable from Musk’s personality and salesmanship, is also a major factor.
- And this is really what it comes down to, isn’t it? Musk has a track record of hot air — the “funding secured” tweet, the fake solar panels. He’s even gotten in on the joke, doing the fake robot thing. But really, what it comes down to is that he sells an image of a viable future through technological progress. People want to buy it.
- No matter how bad the Facebook whistleblower allegations get, Mark Zuckerberg remains untouchable by David Meyer
- Facebook’s damaging document leak is expected to continue for 6 more weeks by Sophie Mellor
- GM is adding 40,000 electric vehicle charging stations in the U.S. and Canada by Chris Morris
- UBS is making huge profits off billionaires—but it won’t touch crypto by Christiaan Hetzner
- Adobe sees NFTs and the metaverse as opportunities for collaboration and growth by Dina Bass and Bloomberg
- Some of these stories require a subscription to access. Thank you for supporting our journalism.
- Ground control to Major Jeff. Billionaires rushing to get to the lowest band of “outer space” have been met with a healthy amount of skepticism about the overall plans for their operations. Are these space companies much more than vanity projects if it’s really just a race to say who got there first?
- So a new “space business park,” to be built by Jeff Bezos’s Blue Origin, sounds like it has enough functional use to answer those criticisms. Will it be enough to silence the doubters? Of course not! And why should it? First, we need to figure out, what exactly is space business, anyway?
- 
- This is the web version of Data Sheet, a daily newsletter on the business of tech. Sign up to get it delivered free to your inbox.
- © 2023 Fortune Media IP Limited. All Rights Reserved. Use of this site constitutes acceptance of our Terms of Use and Privacy Policy | CA Notice at Collection and Privacy Notice | Do Not Sell/Share My Personal Information | Ad Choices 
FORTUNE is a trademark of Fortune Media IP Limited, registered in the U.S. and other countries. FORTUNE may receive compensation for some links to products and services on this website. Offers may be subject to change without notice.
S&P Index data is the property of Chicago Mercantile Exchange Inc. and its licensors. All rights reserved. Terms & Conditions. Powered and implemented by Interactive Data Managed Solutions.

URL: https://mashable.com/article/facebook-reactions-news-feed
- Facebook's reactions just turned one and the company is finally revealing how its revamped like button impacts your News Feed.
- It turns out the company weighs reactions -- love, haha, wow, sad and angry -- over "likes" in determining what content to surface in your News Feed.
- "Over the past year we've found that if people leave a Reaction on a post, it is an even stronger signal that they'd want to see that type of post than if they left a Like on the post," a Facebook spokesperson said in a statement. "So we are updating News Feed to weigh reactions a little more than Likes when taking into account how relevant the story is to each person."
- In other words: While "liking" a post signals to Facebook that its something you're interested in and want to see more of, responding with any other sentiment -- even a "negative" one like angry -- sends an even stronger signal.
- Facebook notes that currently all reactions are weighted the same, so News Feed's algorithms won't be influenced based on whether you react with "love" or "sad" or "angry."
- While it's not surprising that Facebook would adjust News Feed to account for reactions -- after all, responding with a reaction takes an extra step than just tapping "like" -- it's interesting that all reactions are treated equally. At least for now. Facebook says that "love" is far and away the most popular of all reactions, accounting for more than half of all reactions shared on the service.
- The update also raises the question of whether data about reactions could be used for advertising or other purposes outside of News Feed. (Facebook tells its advertisers(opens in a new tab) that all reactions are weighted the same as a "like.") Some privacy advocates have expressed concern that the feature could be used to gather data on users.
- One artist even went so far as to create a dedicated Chrome extension(opens in a new tab) that randomly selects a reaction each time you hit the "like" button in order to hide your true feelings from Facebook.
- Regardless of how you feel about Facebook's reactions, though, the update serves as a good reminder that just about every action you take on the social network influences your News Feed in some way.
- And the best way to signal to Facebook that you don't like something is to simply ignore it (don't forget you can tweak your News Feed settings, too.)
- More in
Facebook
- Karissa was Mashable's Senior Tech Reporter, and is based in San Francisco. She covers social media platforms, Silicon Valley, and the many ways technology is changing our lives. Her work has also appeared in Wired, Macworld, Popular Mechanics, and The Wirecutter. In her free time, she enjoys snowboarding and watching too many cat videos on Instagram. Follow her on Twitter @karissabe.

URL: https://www.forbes.com/sites/amitchowdhry/2017/03/02/facebook-confirms-emoji-reactions-affects-your-news-feed/
- Facebook Reactions in the mobile News Feed / Screenshot Credit: Amit Chowdhry
- In February 2016, Facebook added emoji “Reactions” to the News Feed after more than a year of building an alternative to the trademark “Like” button. And recently, Facebook has confirmed that using emoji “Reactions” actually influences the way your News Feed appears. In fact, Facebook weighs the emoji “Reactions” more than “Likes” to determine which content should appear towards the top of your News Feed.
- “Over the past year we've found that if people leave a Reaction on a post, it is an even stronger signal that they'd want to see that type of post than if they left a Like on the post,” said Facebook in a statement via Mashable. “So we are updating News Feed to weigh reactions a little more than Likes when taking into account how relevant the story is to each person.”
- Facebook’s emoji “Reactions” appear when you push down the “Like” button on the mobile app or you hover the mouse over the icon on the desktop version. The six animated emoji “Reactions” that Facebook added are Like, Love, Haha, Wow, Sad and Angry.
- However, Facebook said that all of the “Reactions” are weighted the same as of right now. So using a Love Reaction on certain types of content will not make it appear more than a Haha. Interestingly Facebook said that the “Love” button has been the most popular of all the Reactions thus far, making up over half the Reactions used on the social network.
- In terms of advertising, Facebook “Reactions” are being treated the same as Likes for ad delivery. So “Loves” do not carry any extra weight than “Likes” for ad auctions on the social network. But the popularity of “Reactions” could be a potentially lucrative opportunity if Facebook builds a feature that allows advertisers to target users based on their emotional responses. This could help drive brand engagement and enable advertisers to learn what types of content their followers would enjoy. That is why it seems very likely that Facebook is working on developing this type of advertising feature.
- Around the time that emoji “Reactions” launched, Facebook acknowledged that users were often put in the position to “like” a post about a death without distinction from how they would “like” an engagement photo. “We kept hearing from people that they didn't have a way to express empathy,” said Facebook product manager Sammi Krug in an interview with Forbes’ Kathleen Chaykowski last year. According to AdWeek, Facebook “Reactions” hit a total of 300 billion times used at the one year mark of the feature launch. Christmas Day 2016 was the day that the most Reactions were used on Facebook. And the 10 countries where Reactions are used the most are:
- 1.) Mexico
- 2.) Chile
- 3.) Suriname
- 4.) Greece
- 5.) Paraguay
- 6.) Costa Rica
- 7.) Belize
- 8.) U.S.
- 9.) Brazil
- 10.) Uruguay
- What are your thoughts about Facebook's News Feed being affected by the emoji "Reactions?" Please leave a comment!
- 

- Facebook downranking system failure
- Amazon US own brand search engine rigging
- Page info Type: IncidentPublished: October 2021Last updated: January 2022
