- Released: January 2021
- Can you improve this page?Share your insights with us
- DALL-E is a software programme that automatically generates images from natural-language text descriptions (or 'prompts'). Trained on text-image pairs culled from the Internet, DALL-E claims to create 'realistic imagery and art' in multiple styles and compositions.
- Developed by OpenAI and first revealed in January 2021, DALL-E uses a modified version of large language model GTP-3 to generate images. DALLE-2, which generates more diverse, higher resolution images faster, was released in May 2022.
- DALL-E has been praised by researchers and commentators for the ease with which it makes it possible to create highly realistic, if surprising and weird, images and artwork at high speed.
- Others, however, have pointed out the software's technical limitations, and ethical and legal risks, including:
- Accuracy: DALLE's ability to produce illogical and incomprehensible content, especially when presented with longer prompts.
- Bias: DALL-E exhibits and reinforces biases, including gender, racial and cultural stereotyping.
- Copyright: DALL-E produces content that copies existing artwork, and is reckoned to abuse copyright and trademarks.
- Employment: DALL-E's will kill the careers of artists, graphic designers, animators, anime cartoonists, food photographers and others.
- Environment: Generative models like DALL-E typically consume huge amounts of energy.
- Mis/disinformation: Having stopped users uploading and editing human facial images in order to minimise the generation of deepfakes, OpenAI'S decision to reintroduce this ability has fueled concerns that it is much easier to use DALL-E to generate and spread mis and disinformation. UC Berkeley researcher Henry Farid reckons DALL-E 'could be disinformation on steroids'.
- Privacy: OpenAI is trained on photographs and other images images publicly available on the internet without consent. Furthermore, the company's decision to reintroduce the ability to upload third-party faces is seen to potentially damage the privacy of people whose consent may not have been obtained.
- Safety: DALL-E can produce offensive or explicit content, as well as content that can be construed as harrassment or bullying.
- These limitations and risks broadly reflect those published by OpenAI upon DALL-E's launch and DALLE-2 upgrade.
- Some commentators complain that OpenAI's refusal to let third parties assess its algorithm makes it difficult to understand how it works, and how its risks can be managed.
- Given the variety and nature of the risks of DALL-E, and its potential negative impacts, OpenAI's decision to restrict user access to DALL-E has mostly been welcomed, even if some users complain that Stable Diffusion, Midjourney and other image generation tools are open to everyone and can be used with few, if any, restrictions.
- In July 2022, OpenAI announced DALL-E 2 would be made available to up to one million users as part of a large-scale beta test. An API for the system was released in November 2022.
- Operator: OpenAI; MicrosoftDeveloper: OpenAICountry: USASector: TechnologyPurpose: Generate images Technology: NLP/text analysis; Computer vision; Text-to-image; Neural network; Deep learning Issue: Accuracy/reliability; Bias/discrimination - race, ethnicity, gender; Copyright; Employment - jobs; Environment;  Mis/disinformation; Privacy; Safety Transparency: Governance; Black box; Marketing; Privacy
URL: https://openai.com/blog/dall-e/
- Illustration: Justin Jay Wang
- We’ve trained a neural network called DALL·E that creates images from text captions for a wide range of concepts expressible in natural language.
- DALL·E is a 12-billion parameter version of GPT-3 trained to generate images from text descriptions, using a dataset of text–image pairs. We’ve found that it has a diverse set of capabilities, including creating anthropomorphized versions of animals and objects, combining unrelated concepts in plausible ways, rendering text, and applying transformations to existing images.
- See also: DALL·E 2, which generates more realistic and accurate images with 4x greater resolution.
- GPT-3 showed that language can be used to instruct a large neural network to perform a variety of text generation tasks. Image GPT showed that the same type of neural network can also be used to generate images with high fidelity. We extend these findings to show that manipulating visual concepts through language is now within reach.
- Like GPT-3, DALL·E is a transformer language model. It receives both the text and the image as a single stream of data containing up to 1280 tokens, and is trained using maximum likelihood to generate all of the tokens, one after another. [^footnote-1]
- This training procedure allows DALL·E to not only generate an image from scratch, but also to regenerate any rectangular region of an existing image that extends to the bottom-right corner, in a way that is consistent with the text prompt.
- 
- We recognize that work involving generative models has the potential for significant, broad societal impacts. In the future, we plan to analyze how models like DALL·E relate to societal issues like economic impact on certain work processes and professions, the potential for bias in the model outputs, and the longer term ethical challenges implied by this technology.
- We find that DALL·E is able to create plausible images for a great variety of sentences that explore the compositional structure of language. We illustrate this using a series of interactive visuals in the next section. The samples shown for each caption in the visuals are obtained by taking the top 32 of 512 after reranking with CLIP, but we do not use any manual cherry-picking, aside from the thumbnails and standalone images that appear outside.[^footnote-2]
- We test DALL·E’s ability to modify several of an object’s attributes, as well as the number of times that it appears.
- Click to edit text prompt or view more AI-generated images
- a pentagonal green click. a green clock in the shape of a pentagon.
- We find that DALL·E can render familiar objects in polygonal shapes that are sometimes unlikely to occur in the real world. For some objects, such as “picture frame” and “plate,” DALL·E can reliably draw the object in any of the polygonal shapes except heptagon. For other objects, such as “manhole cover” and “stop sign,” DALL·E’s success rate for more unusual shapes, such as “pentagon,” is considerably lower.For several of the visuals in this post, we find that repeating the caption, sometimes with alternative phrasings, improves the consistency of the results.
- a cube made of porcupine. a cube with the texture of a porcupine.
- We find that DALL·E can map the textures of various plants, animals, and other objects onto three dimensional solids. As in the preceding visual, we find that repeating the caption with alternative phrasing improves the consistency of the results.
- a collection of glasses is sitting on a table
- We find that DALL·E is able to draw multiple copies of an object when prompted to do so, but is unable to reliably count past three. When prompted to draw nouns for which there are multiple meanings, such as “glasses,” “chips,” and “cups” it sometimes draws both interpretations, depending on the plural form that is used.
- Simultaneously controlling multiple objects, their attributes, and their spatial relationships presents a new challenge. For example, consider the phrase “a hedgehog wearing a red hat, yellow gloves, blue shirt, and green pants.” To correctly interpret this sentence, DALL·E must not only correctly compose each piece of apparel with the animal, but also form the associations (hat, red), (gloves, yellow), (shirt, blue), and (pants, green) without mixing them up [^footnote-3]
- We test DALL·E’s ability to do this for relative positioning, stacking objects, and controlling multiple attributes.
- a small red block sitting on a large green block
- We find that DALL·E correctly responds to some types of relative positions, but not others. The choices “sitting on” and “standing in front of” sometimes appear to work, “sitting below,” “standing behind,” “standing left of,” and “standing right of” do not. DALL·E also has a lower success rate when asked to draw a large object sitting on top of a smaller one, when compared to the other way around.
- a stack of 3 cubes. a red cube is on the top, sitting on a green cube. the green cube is in the middle, sitting on a blue cube. the blue cube is on the bottom.
- We find that DALL·E typically generates an image with one or two of the objects having the correct colors. However, only a few samples for each setting tend to have exactly three objects colored precisely as specified.
- an emoji of a baby penguin wearing a blue hat, red gloves, green shirt, and yellow pants
- We find that DALL·E typically generates an image with two or three articles of clothing having the correct colors. However, only a few of the samples for each setting tend to have all four articles of clothing with the specified colors.
- While DALL·E does offer some level of controllability over the attributes and positions of a small number of objects, the success rate can depend on how the caption is phrased. As more objects are introduced, DALL·E is prone to confusing the associations between the objects and their colors, and the success rate decreases sharply. We also note that DALL·E is brittle with respect to rephrasing of the caption in these scenarios: alternative, semantically equivalent captions often yield no correct interpretations.
- We find that DALL·E also allows for control over the viewpoint of a scene and the 3D style in which a scene is rendered.
- an extreme close-up view of a capybara sitting in a field
- We find that DALL·E can draw each of the animals in a variety of different views. Some of these views, such as “aerial view” and “rear view,” require knowledge of the animal’s appearance from unusual angles. Others, such as “extreme close-up view,” require knowledge of the fine-grained details of the animal’s skin or fur.
- a capybara made of voxels sitting in a field
- We find that DALL·E is often able to modify the surface of each of the animals according to the chosen 3D style, such as “claymation” and “made of voxels,” and render the scene with plausible shading depending on the location of the sun. The “x-ray” style does not always work reliably, but it shows that DALL·E can sometimes orient the bones within the animal in plausible (though not anatomically correct) configurations.
- To push this further, we test DALL·E’s ability to repeatedly draw the head of a well-known figure at each angle from a sequence of equally spaced angles, and find that we can recover a smooth animation of the rotating head.
- a photograph of a bust of homer
- We prompt DALL·E with both a caption describing a well-known figure and the top region of an image showing a hat drawn at a particular angle. Then, we ask DALL·E to complete the remaining part of the image given this contextual information. We do this repeatedly, each time rotating the hat a few more degrees, and find that we are able to recover smooth animations of several well-known figures, with each frame respecting the precise specification of angle and ambient lighting.
- DALL·E appears to be able to apply some types of optical distortions to scenes, as we see with the options “fisheye lens view” and “a spherical panorama.” This motivated us to explore its ability to generate reflections.
- a plain white cube looking at its own reflection in a mirror. a plain white cube gazing at itself in a mirror.
- We prompt DALL·E with both a caption describing a well-known figure and the top region of an image showing a hat drawn at a particular angle. Then, we ask DALL·E to complete the remaining part of the image given this contextual information. We do this repeatedly, each time rotating the hat a few more degrees, and find that we are able to recover smooth animations of several well-known figures, with each frame respecting the precise specification of angle and ambient lighting.
- The samples from the “extreme close-up view” and “x-ray” style led us to further explore DALL·E’s ability to render internal structure with cross-sectional views, and external structure with macro photographs.
- a cross-section view of a walnut
- We find that DALL·E is able to draw the interiors of several different kinds of objects.
- a macro photograph of brain coral
- We find that DALL·E is able to draw the fine-grained external details of several different kinds of objects. These details are only apparent when the object is viewed up close.
- The task of translating text to images is underspecified: a single caption generally corresponds to an infinitude of plausible images, so the image is not uniquely determined. For instance, consider the caption “a painting of a capybara sitting on a field at sunrise.” Depending on the orientation of the capybara, it may be necessary to draw a shadow, though this detail is never mentioned explicitly. We explore DALL·E’s ability to resolve underspecification in three cases: changing style, setting, and time; drawing the same object in a variety of different situations; and generating an image of an object with specific text written on it.
- a painting of a capybara sitting in a field at sunrise
- We find that DALL·E is able to render the same scene in a variety of different styles, and can adapt the lighting, shadows, and environment based on the time of day or season.
- a stained glass window with an image of a blue strawberry
- We find that DALL·E is able to flexibly adapt the representation of the object based on the medium on which it is being drawn. For “a mural,” “a soda can,” and “a teacup,” DALL·E must change how it draws the object based on the angle and curvature of the drawing surface. For “a stained glass window” and “a neon sign,” it must alter the appearance of the object from how it usually appears.
- a store front that has the word ‘openai’ written on it. a store front that has the word ‘openai’ written on it. a store front that has the word ‘openai’ written on it. ‘openai’ store front.
- We find that DALL·E is able to draw the fine-grained external details of several different kinds of objects. These details are only apparent when the object is viewed up close.
- With varying degrees of reliability, DALL·E provides access to a subset of the capabilities of a 3D rendering engine via natural language. It can independently control the attributes of a small number of objects, and to a limited extent, how many there are, and how they are arranged with respect to one another. It can also control the location and angle from which a scene is rendered, and can generate known objects in compliance with precise specifications of angle and lighting conditions.
- Unlike a 3D rendering engine, whose inputs must be specified unambiguously and in complete detail, DALL·E is often able to “fill in the blanks” when the caption implies that the image must contain a certain detail that is not explicitly stated.
- Next, we explore the use of the preceding capabilities for fashion and interior design.
- a male mannequin dressed in an orange and black flannel shirt
- We explore DALL·E’s ability to render male mannequins in a variety of different outfits. When prompted with two colors, e.g., “an orange and white bomber jacket” and “an orange and black turtleneck sweater,” DALL·E often exhibits a range of possibilities for how both colors can be used for the same article of clothing.DALL·E also seems to occasionally confuse less common colors with other neighboring shades. For example, when prompted to draw clothes in “navy,” DALL·E sometimes uses lighter shades of blue, or shades very close to black. Similarly, DALL·E sometimes confuses “olive” with shades of brown or brighter shades of green.
- a female mannequin dressed in a black leather jacket and gold pleated skirt
- We explore DALL·E’s ability to render female mannequins in a variety of different outfits. We find that DALL·E is able to portray unique textures such as the sheen of a “black leather jacket” and “gold” skirts and leggings. As before, we see that DALL·E occasionally confuses less common colors, such as “navy” and “olive,” with other neighboring shades.
- a living room with two white armchairs and a painting of the colosseum. the painting is mounted above a modern fireplace.
- We explore DALL·E’s ability to generate images of rooms with several details specified. We find that it can generate paintings of a wide range of different subjects, including real-world locations such as “the colosseum” and fictional characters like “yoda.” For each subject, DALL·E exhibits a variety of interpretations. While the painting is almost always present in the scene, DALL·E sometimes fails to draw the fireplace or the correct number of armchairs.
- a loft bedroom with a white bed next to a nightstand. there is a fish tank beside the bed.
- We explore DALL·E’s ability to generate bedrooms with several details specified. Despite the fact that we do not tell DALL·E what should go on top of the nightstand or shelf beside the bed, we find that it sometimes decides to place the other specified object on top. As before, we see that it often fails to draw one or more of the specified objects.
- The compositional nature of language allows us to put together concepts to describe both real and imaginary things. We find that DALL·E also has the ability to combine disparate ideas to synthesize objects, some of which are unlikely to exist in the real world. We explore this ability in two instances: transferring qualities from various concepts to animals, and designing products by taking inspiration from unrelated concepts.
- a snail made of harp. a snail with the texture of a harp.
- We find that DALL·E can generate animals synthesized from a variety of concepts, including musical instruments, foods, and household items. While not always successful, we find that DALL·E sometimes takes the forms of the two objects into consideration when determining how to combine them. For example, when prompted to draw “a snail made of harp,” it sometimes relates the pillar of the harp to the spiral of the snail’s shell.In a previous section, we saw that as more objects are introduced into the scene, DALL·E is liable to confuse the associations between the objects and their specified attributes. Here, we see a different sort of failure mode: sometimes, rather than binding some attribute of the specified concept (say, “a faucet”) to the animal (say, “a snail”), DALL·E just draws the two as separate items.
- an armchair in the shape of an avocado. an armchair imitating an avocado.
- In the preceding visual, we explored DALL·E’s ability to generate fantastical objects by combining two unrelated ideas. Here, we explore its ability to take inspiration from an unrelated idea while respecting the form of the thing being designed, ideally producing an object that appears to be practically functional. We found that prompting DALL·E with the phrases “in the shape of,” “in the form of,” and “in the style of” gives it the ability to do this.When generating some of these objects, such as “an armchair in the shape of an avocado”, DALL·E appears to relate the shape of a half avocado to the back of the chair, and the pit of the avocado to the cushion. We find that DALL·E is susceptible to the same kinds of mistakes mentioned in the previous visual.
- In the previous section, we explored DALL·E’s ability to combine unrelated concepts when generating images of real-world objects. Here, we explore this ability in the context of art, for three kinds of illustrations: anthropomorphized versions of animals and objects, animal chimeras, and emojis.
- an illustration of a baby daikon radish in a tutu walking a dog
- We find that DALL·E is sometimes able to transfer some human activities and articles of clothing to animals and inanimate objects, such as food items. We include “pikachu” and “wielding a blue lightsaber” to explore DALL·E’s ability to incorporate popular media.We find it interesting how DALL·E adapts human body parts onto animals. For example, when asked to draw a daikon radish blowing its nose, sipping a latte, or riding a unicycle, DALL·E often draws the kerchief, hands, and feet in plausible locations.
- a professional high quality illustration of a giraffe turtle chimera. a giraffe imitating a turtle. a giraffe made of turtle.
- We find that DALL·E is sometimes able to combine distinct animals in plausible ways. We include “pikachu” to explore DALL·E’s ability to incorporate knowledge of popular media, and “robot” to explore its ability to generate animal cyborgs. Generally, the features of the second animal mentioned in the caption tend to be dominant.We also find that inserting the phrase “professional high quality” before “illustration” and “emoji” sometimes improves the quality and consistency of the results.
- a professional high quality emoji of a lovestruck cup of boba
- We find that DALL·E is sometimes able to combine distinct animals in plausible ways. We include “pikachu” to explore DALL·E’s ability to incorporate knowledge of popular media, and “robot” to explore its ability to generate animal cyborgs. Generally, the features of the second animal mentioned in the caption tend to be dominant.We also find that inserting the phrase “professional high quality” before “illustration” and “emoji” sometimes improves the quality and consistency of the results.
- GPT-3 can be instructed to perform many kinds of tasks solely from a description and a cue to generate the answer supplied in its prompt, without any additional training. For example, when prompted with the phrase “here is the sentence ‘a person walking his dog in the park’ translated into French:”, GPT-3 answers “un homme qui promène son chien dans le parc.” This capability is called zero-shot reasoning. We find that DALL·E extends this capability to the visual domain, and is able to perform several kinds of image-to-image translation tasks when prompted in the right way.
- the exact same cat on the top as a sketch on the bottom
- We find that DALL·E is able to apply several kinds of image transformations to photos of animals, with varying degrees of reliability. The most straightforward ones, such as “photo colored pink” and “photo reflected upside-down,” also tend to be the most reliable, although the photo is often not copied or reflected exactly. The transformation “animal in extreme close-up view” requires DALL·E to recognize the breed of the animal in the photo, and render it up close with the appropriate details. This works less reliably, and for several of the photos, DALL·E only generates plausible completions in one or two instances.Other transformations, such as “animal with sunglasses” and “animal wearing a bow tie,” require placing the accessory on the correct part of the animal’s body. Those that only change the color of the animal, such as “animal colored pink,” are less reliable, but show that DALL·E is sometimes capable of segmenting the animal from the background. Finally, the transformations “a sketch of the animal” and “a cell phone case with the animal” explore the use of this capability for illustrations and product design.
- the exact same teapot on the top with ’gpt’ written on it on the bottom
- We find that DALL·E is able to apply several different kinds of image transformations to photos of teapots, with varying degrees of reliability. Aside from being able to modify the color of the teapot (e.g., “colored blue”) or its pattern (e.g., “with stripes”), DALL·E can also render text (e.g., “with ‘gpt’ written on it”) and map the letters onto the curved surface of the teapot in a plausible way. With much less reliability, it can also draw the teapot in a smaller size (for the “tiny” option) and in a broken state (for the “broken” option).
- We did not anticipate that this capability would emerge, and made no modifications to the neural network or training procedure to encourage it. Motivated by these results, we measure DALL·E’s aptitude for analogical reasoning problems by testing it on Raven’s progressive matrices, a visual IQ test that saw widespread use in the 20th century.
- a sequence of geometric shapes.
- Rather than treating the IQ test a multiple-choice problem as originally intended, we ask DALL·E to complete the bottom-right corner of each image using argmax sampling, and consider its completion to be correct if it is a close visual match to the original.DALL·E is often able to solve matrices that involve continuing simple patterns or basic geometric reasoning, such as those in sets B and C. It is sometimes able to solve matrices that involve recognizing permutations and applying boolean operations, such as those in set D. The instances in set E tend to be the most difficult, and DALL·E gets almost none of them correct.For each of the sets, we measure DALL·E’s performance on both the original images, and the images with the colors inverted. The inversion of colors should pose no additional difficulty for a human, yet does generally impair DALL·E’s performance, suggesting its capabilities may be brittle in unexpected ways.
- We find that DALL·E has learned about geographic facts, landmarks, and neighborhoods. Its knowledge of these concepts is surprisingly precise in some ways and flawed in others.
- a photo of the food of china
- We test DALL·E’s understanding of simple geographical facts, such as country flags, cuisines, and local wildlife. While DALL·E successfully answers many of these queries, such as those involving national flags, it often reflects superficial stereotypes for choices like “food” and “wildlife,” as opposed to representing the full diversity encountered in the real world.
- a photo of alamo square, san francisco, from a street at night
- We find that DALL·E is sometimes capable of rendering semblances of certain locations in San Francisco. For locations familiar to the authors, such as San Francisco, they evoke a sense of déjà vu—eerie simulacra of streets, sidewalks and cafes that remind us of very specific locations that do not exist.
- a photo of san francisco’s golden gate bridge
- We can also prompt DALL·E to draw famous landmarks. In fact, we can even dictate when the photo was taken by specifying the first few rows of the sky. When the sky is dark, for example, DALL·E recognizes it is night, and turns on the lights in the buildings.
- In addition to exploring DALL·E’s knowledge of concepts that vary over space, we also explore its knowledge of concepts that vary over time.
- a photo of a phone from the 20s
- We find that DALL·E has learned about basic stereotypical trends in design and technology over the decades. Technological artifacts appear to go through periods of explosion of change, dramatically shifting for a decade or two, then changing more incrementally, becoming refined and streamlined.
- DALL·E is a simple decoder-only transformer that receives both the text and the image as a single stream of 1280 tokens—256 for the text and 1024 for the image—and models all of them autoregressively. The attention mask at each of its 64 self-attention layers allows each image token to attend to all text tokens. DALL·E uses the standard causal mask for the text tokens, and sparse attention for the image tokens with either a row, column, or convolutional attention pattern, depending on the layer. We provide more details about the architecture and training procedure in our paper.
- Text-to-image synthesis has been an active area of research since the pioneering work of Reed et. al,[^reference-1] whose approach uses a GAN conditioned on text embeddings. The embeddings are produced by an encoder pretrained using a contrastive loss, not unlike CLIP. StackGAN[^reference-3] and StackGAN++[^reference-4] use multi-scale GANs to scale up the image resolution and improve visual fidelity. AttnGAN[^reference-5] incorporates attention between the text and image features, and proposes a contrastive text-image feature matching loss as an auxiliary objective. This is interesting to compare to our reranking with CLIP, which is done offline. Other work[^reference-2][^reference-6][^reference-7] incorporates additional sources of supervision during training to improve image quality. Finally, work by Nguyen et. al[^reference-8] and Cho et. al[^reference-9] explores sampling-based strategies for image generation that leverage pretrained multimodal discriminative models.
- Similar to the rejection sampling used in VQVAE-2, we use CLIP to rerank the top 32 of 512 samples for each caption in all of the interactive visuals. This procedure can also be seen as a kind of language-guided search[^reference-16], and can have a dramatic impact on sample quality.
- an illustration of a baby daikon radish in a tutu walking a dog [caption 1, best 8 of 2048]
- Reranking the samples from DALL·E using CLIP can dramatically improve consistency and quality of the samples.

URL: https://openai.com/dall-e-2/
- DALL·E 2 is an AI system that can create realistic images and art from a description in natural language.
- 
- DALL·E 2 can create original, realistic images and art from a text description. It can combine concepts, attributes, and styles.
- DALL·E 2 can create original, realistic images and art from a text description. It can combine concepts, attributes, and styles.
- DALL·E 2 can expand images beyond what’s in the original canvas, creating expansive new compositions.
- DALL·E 2 can make realistic edits to existing images from a natural language caption. It can add and remove elements while taking shadows, reflections, and textures into account.
- DALL·E 2 can take an image and create different variations of it inspired by the original.
- In January 2021, OpenAI introduced DALL·E. One year later, our newest system, DALL·E 2, generates more realistic and accurate images with 4x greater resolution.
- DALL·E 1
- DALL·E 2
- preferred for caption matching
- preferred for photorealism
- Our hope is that DALL·E 2 will empower people to express themselves creatively. DALL·E 2 also helps us understand how advanced AI systems see and understand our world, which is critical to our mission of creating AI that benefits humanity.
- Research Advancements
- Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen
- 
- Engineering, Design, Product, and Prototyping
- Jeff Belgum, Dave Cummings, Jonathan Gordon, Chris Hallacy, Shawn Jain, Joanne Jang, Fraser Kelton, Vishal Kuo, Joel Lehman, Rachel Lim, Bianca Martin, Evan Morikawa, Rajeev Nayak, Glenn Powell, Krijn Rijshouwer, David Schnurr, Maddie Simens, Kenneth Stanley, Felipe Such, Chelsea Voss, Justin Jay Wang
- 
- Comms, Policy, Legal, Ops, Safety, and Security
- Steven Adler, Lama Ahmad, Miles Brundage, Kevin Button, Che Chang, Fotis Chantzis, Derek Chen, Frances Choi, Steve Dowling, Elie Georges, Shino Jomoto, Aris Konstantinidis, Gretchen Krueger, Andrew Mayne, Pamela Mishkin, Bob Rotsted, Natalie Summers, Dave Willner, Hannah Wong
- 
- Acknowledgments
- Thanks to those who helped with and provided feedback on this release: Sandhini Agarwal, Sam Altman, Chester Cho, Peter Hoeschele, Jacob Jackson, Jong Wook Kim, Matt Knight, Jason Kwon, Anna Makanju, Katie Mayer, Bob McGrew, Luke Miller, Mira Murati, Adam Nace, Hyeonwoo Noh, Cullen O’Keefe, Long Ouyang, Michael Petrov, Henrique Ponde de Oliveira Pinto, Alec Radford, Girish Sastry, Pranav Shyam, Aravind Srinivas, Ilya Sutskever, Preston Tuggle, Arun Vijayvergiya, Peter Welinder

URL: https://en.wikipedia.org/wiki/DALL-E
- 
- DALL-E (stylized as DALL·E) and DALL-E 2 are deep learning models developed by OpenAI to generate digital images from natural language descriptions, called "prompts". DALL-E was revealed by OpenAI in a blog post in January 2021, and uses a version of GPT-3[1] modified to generate images. In April 2022, OpenAI announced DALL-E 2, a successor designed to generate more realistic images at higher resolutions that "can combine concepts, attributes, and styles".[2]
- OpenAI has not released source code for either model. On 20 July 2022, DALL-E 2 entered into a beta phase with invitations sent to 1 million waitlisted individuals;[3] users can generate a certain number of images for free every month and may purchase more.[4] Access had previously been restricted to pre-selected users for a research preview due to concerns about ethics and safety.[5][6] On 28 September 2022, DALL-E 2 was opened to anyone and the waitlist requirement was removed.[7]
- In early November 2022, OpenAI released DALL-E 2 as an API, allowing developers to integrate the model into their own applications. Microsoft unveiled their implementation of DALL-E 2 in their Designer app and Image Creator tool included in Bing and Microsoft Edge. CALA and Mixtiles are among other early adopters of the DALL-E 2 API.[8] The API operates on a cost per image basis, with prices varying depending on image resolution. Volume discounts are available to companies working with OpenAI’s enterprise team.[9]
- The software's name is a portmanteau of the names of animated robot Pixar character WALL-E and the Spanish surrealist artist Salvador Dalí.[10][1]
- The first generative pre-trained transformer (GPT) model was initially developed by OpenAI in 2018,[11] using a Transformer architecture. The first iteration, GPT-1,[12] was scaled up to produce GPT-2 in 2019;[13] in 2020 it was scaled up again to produce GPT-3, with 175 billion parameters.[14][1][15] DALL-E's model is a multimodal implementation of GPT-3[16] with 12 billion parameters[1] which "swaps text for pixels", trained on text-image pairs from the Internet.[17] DALL-E 2 uses 3.5 billion parameters, a smaller number than its predecessor.[18]
- DALL-E was developed and announced to the public in conjunction with CLIP (Contrastive Language-Image Pre-training).[17] CLIP is a separate model based on zero-shot learning that was trained on 400 million pairs of images with text captions scraped from the Internet.[1][17][19] Its role is to "understand and rank" DALL-E's output by predicting which caption from a list of 32,768 captions randomly selected from the dataset (of which one was the correct answer) is most appropriate for an image. This model is used to filter a larger initial list of images generated by DALL-E to select the most appropriate outputs.[10][17]
- DALL-E 2 uses a diffusion model conditioned on CLIP image embeddings, which, during inference, are generated from CLIP text embeddings by a prior model.[18]
- DALL-E can generate imagery in multiple styles, including photorealistic imagery, paintings, and emoji.[1] It can "manipulate and rearrange" objects in its images,[1] and can correctly place design elements in novel compositions without explicit instruction. Thom Dunn writing for BoingBoing remarked that "For example, when asked to draw a daikon radish blowing its nose, sipping a latte, or riding a unicycle, DALL-E often draws the handkerchief, hands, and feet in plausible locations."[20] DALL-E showed the ability to "fill in the blanks" to infer appropriate details without specific prompts, such as adding Christmas imagery to prompts commonly associated with the celebration,[21] and appropriately-placed shadows to images that did not mention them.[22] Furthermore, DALL-E exhibits a broad understanding of visual and design trends.[citation needed]
- DALL-E can produce images for a wide variety of arbitrary descriptions from various viewpoints[23] with only rare failures.[10] Mark Riedl, an associate professor at the Georgia Tech School of Interactive Computing, found that DALL-E could blend concepts (described as a key element of human creativity).[24][25]
- Its visual reasoning ability is sufficient to solve Raven's Matrices (visual tests often administered to humans to measure intelligence).[26][27]
- Given an existing image, DALL-E 2 can produce "variations" of the image as individual outputs based on the original and edit the image to modify or expand upon it. DALL-E 2's "inpainting" and "outpainting" use context from an image to fill in missing areas using a medium consistent with the original, following a given prompt. For example, this can be used to insert a new subject into an image, or expand an image beyond its original borders.[28] According to OpenAI, "Outpainting takes into account the image’s existing visual elements — including shadows, reflections, and textures — to maintain the context of the original image."[29]
- DALL-E 2's reliance on public datasets influences its results and leads to algorithmic bias in some cases, such as generating higher numbers of men than women for requests that do not mention gender.[30] DALL-E 2's training data was filtered to remove violent and sexual imagery, but this was found to increase bias in some cases such as reducing the frequency of women being generated.[31] OpenAI hypothesize that this may be because women were more likely to be sexualized in training data which caused the filter to influence results.[31] In September 2022, OpenAI confirmed to The Verge that DALL-E invisibly inserts phrases into user prompts to address bias in results; for instance, "black man" and "Asian woman" are inserted into prompts that do not specify gender or race.[32]
- A concern about DALL-E 2 and similar image generation models is that they could be used to propagate deepfakes and other forms of misinformation.[33][34] As an attempt to mitigate this, the software rejects prompts involving public figures and uploads containing human faces.[35] Prompts containing potentially objectionable content are blocked, and uploaded images are analyzed to detect offensive material.[36] A disadvantage of prompt-based filtering is that it is easy to bypass using alternative phrases that result in a similar output. For example, the word "blood" is filtered, but "ketchup" and "red liquid" are not.[37][36]
- Another concern about DALL-E 2 and similar models is that they could cause technological unemployment for artists, photographers, and graphic designers due to their accuracy and popularity.[38][39]
- DALL-E 2's language understanding has limits. It is sometimes unable to distinguish "A yellow book and a red vase" from "A red book and a yellow vase" or "A panda making latte art" from "Latte art of a panda".[40] It generates images of "an astronaut riding a horse" when presented with the prompt "a horse riding an astronaut".[41] It also fails to generate the correct images in a variety of circumstances. Requesting more than three objects, negation, numbers, and connected sentences may result in mistakes, and object features may appear on the wrong object.[23] Additional limitations include handling text - which, even with legible lettering, almost invariably results in dream-like gibberish - and its limited capacity to address scientific information, such as astronomy or medical imagery.[42]
- Most coverage of DALL-E focuses on a small subset of "surreal"[17] or "quirky"[24] outputs. DALL-E's output for "an illustration of a baby daikon radish in a tutu walking a dog" was mentioned in pieces from Input,[43] NBC,[44] Nature,[45] and other publications.[1][46][47] Its output for "an armchair in the shape of an avocado" was also widely covered.[17][25]
- ExtremeTech stated "you can ask DALL-E for a picture of a phone or vacuum cleaner from a specified period of time, and it understands how those objects have changed".[21] Engadget also noted its unusual capacity for "understanding how telephones and other objects change over time".[22]
- According to MIT Technology Review, one of OpenAI's objectives was to "give language models a better grasp of the everyday concepts that humans use to make sense of things".[17]
- Wall Street investors have had a positive reception of DALL-E 2, with some firms thinking it could represent a turning point for a future multi-trillion dollar industry. OpenAI has already received over $1 billion  in funding from Microsoft and Khosla Ventures.[48]
- Japan's anime community has had a negative reaction to DALL-E 2 and similar models.[49][50][51] Two arguments are typically presented by artists against the software. The first is that AI art is not art because it is not created by a human with intent. "The juxtaposition of AI-generated images with their own work is degrading and undermines the time and skill that goes into their art. AI-driven image generation tools have been heavily criticized by artists because they are trained on human-made art scraped from the web."[3] The second is the trouble with copyright law and data text-to-image models are trained on. OpenAI has not released information about what dataset(s) were used to train DALL-E 2, inciting concern from some that the work of artists has been used for training without permission. Copyright laws surrounding these topics are inconclusive at the moment.[4]
- There have been several attempts to create open-source implementations of DALL-E.[52][53] Released in 2022 on Hugging Face's Spaces platform, Craiyon (formerly DALL-E Mini until a name change was requested by OpenAI in June 2022) is an AI model based on the original DALL-E that was trained on unfiltered data from the Internet. It attracted substantial media attention in mid-2022, after its release due to its capacity for producing humorous imagery.[54][55][56]

URL: https://github.com/openai/dalle-2-preview
- Use Git or checkout with SVN using the web URL.
- Work fast with our official CLI.
      Learn more about the CLI.
- Please
                sign in
                to use Codespaces.
- If nothing happens, download GitHub Desktop and try again.
- If nothing happens, download GitHub Desktop and try again.
- If nothing happens, download Xcode and try again.
- Your codespace will open once ready.
- There was a problem preparing your codespace, please try again.

URL: https://github.com/openai/dalle-2-preview/blob/main/system-card.md
- Summary
- Below, we summarize initial findings on potential risks associated with DALL·E 2, and mitigations aimed at addressing those risks as part of the ongoing Preview of this technology. We are sharing these findings in order to enable broader understanding of image generation and modification technology and some of the associated risks, and to provide additional context for users of the DALL·E 2 Preview.
- Without sufficient guardrails, models like DALL·E 2 could be used to generate a wide range of deceptive and otherwise harmful content, and could affect how people perceive the authenticity of content more generally. DALL·E 2 additionally inherits various biases from its training data, and its outputs sometimes reinforce societal stereotypes.
- The DALL·E 2 Preview involves a variety of mitigations aimed at preventing and mitigating related risks, with limited access being particularly critical as we learn more about the risk surface.
- Content warning
- Introduction
- System Components
- Model
- Restrictions
- Policies and enforcement
- Risk assessment process
- Early work
- External red teaming
- Probes and evaluations
- Explicit content
- Bias and representation
- Harassment, bullying, and exploitation
- Disinformation
- Copyright and Trademarks
- Economic
- Relation to existing
technologies
- Future work
- Contributors
- Glossary of terms
- Last updated: April 11, 2022
- This document takes inspiration from the concepts of model
cards and system
cards
in providing information about the DALL·E 2 Preview, an image generation
demo OpenAI is releasing to trusted users for non-commercial purposes.
This document often takes the system level of analysis, with that system
including non-model mitigations such as access controls, prompt and
image filters, and monitoring for abuse. This is an assessment of the
system as of April 6, 2022, referred to in this document as the "DALL·E
2 Preview," with the underlying generative model being referred to as "DALL·E 2."
- This document builds on the findings of internal as well as
external researchers, and is intended to be an early investigation of
this platform and the underlying model. We specifically focus on risks
rather than benefits. Thus, we do not aim to provide a well-rounded
sense of the overall effects of image generation technologies.
Additionally, the models in question completed training relatively
recently and the majority of the risk assessment period (described in Risk assessment process below) probed earlier models. As such, this analysis is intended to be preliminary and to be read and used as such.
We are excited to support further research informed by remaining
questions around how to deploy these models safely, equitably, and
successfully.
- The document proceeds as follows. First, we describe different facets of
the DALL·E 2 Preview system, beginning with model functionality, then
covering input filtering and policies related to access, use, and
content. Second, we summarize the processes conducted internally and
externally to generate the analysis presented here. Third, we describe a
range of risk-oriented probes and evaluations conducted on DALL·E 2,
covering bias and representation; dis- and mis-information; explicit
content; economic effects; misuse involving hate, harassment, and
violence; and finally, copyright and memorization. Fourth, we discuss
how DALL·E 2 compares with, and might be combined with, existing
technologies. Fifth and finally, we describe future work that could shed
further light on some of the risks and mitigations discussed.
- DALL·E 2 is an artificial intelligence model that takes a text prompt
and/or existing image as an input and generates a new image as an
output. DALL·E 2 was developed by researchers at OpenAI to understand
the capabilities and broader implications of multimodal generative
models. In order to help us and others better understand how image
generation models can be used and misused, OpenAI is providing access to
a subset of DALL·E 2's capabilities1 via the DALL·E 2 Preview.
- DALL·E 2 builds on DALL·E 1
(Paper | Model
Card),
increasing the level of resolution, fidelity, and overall photorealism
it is capable of producing. DALL·E 2 is also trained to have new
capabilities compared to DALL·E 1.
- In addition to generating images based on text description prompts
("Text to Image"), DALL·E 2 can modify existing images as prompted using
a text description ("Inpainting"). It can also take an existing image as
an input and be prompted to produce a creative variation on it
("Variations").
- DALL·E 2 was trained on pairs of images and their corresponding captions. Pairs were drawn from a combination of publicly available sources and sources that we licensed.
- We have made an effort to filter the most explicit content from the
training data for DALL·E 2.2 This filtered explicit content
includes graphic sexual and violent content as well as images of some
hate symbols.3 The filtering was informed by but distinct from
earlier, more aggressive filtering (removing all images of people) that
we performed when building
GLIDE, a distinct model that
we published several months ago. We performed more aggressive filtering
in that context because a small version of the model was intended to be
open sourced. It is harder to prevent an open source model from being
used for harmful purposes than one that is only exposed through a
controlled interface, not least due to the fact that a model, once open
sourced, can be modified and/or be combined with other third party
tools.4
- We conducted an internal audit of our filtering of sexual content to see
if it concentrated or exacerbated any particular biases in the training
data. We found that our initial approach to filtering of sexual content
reduced the quantity of generated images of women in general, and we
made adjustments to our filtering approach as a result.
- For additional resources on DALL·E 2 and the DALL·E 2 Preview, see:
- DALL·E 2 Landing Page
- DALL·E 2 Paper
- For additional resources on DALL·E 1 and Glide, see:
- DALL·E 1: Paper, Model Card, Blog post
- GLIDE: Paper, code and weights
- Within the DALL·E 2 Preview, filters on inputs (i.e. text prompts for
"Text to Image" and Inpainting) and on uploads (i.e. images for
Inpainting or Variations) seek to prevent users from using the Preview
for the following types of prompts and uploads:
- Those with strong safety concerns attached (e.g. sexualized or suggestive images of children, violent content, explicitly political content, and toxic content).
- Places where the only meaning of the content would constitute a violation of our content policy (i.e. the violation does not depend on the context in which that content is shared).
- Prompts related to use cases we do not support at this time (e.g. we only support English language prompts at this time).
- Prompts in areas where model behavior is not robust or may be misaligned due to pre-training filtering (e.g. as a result of pre-training filters, we cannot confidently allow generation of images related to common American hate symbols, even in cases where the user intended to appropriately contextualize such symbols and not to endorse them).
- A non-goal at this stage was catching:
- Using filters in this way has a few known deficiencies:
- The filters do not fully capture actions that violate our Terms of Use. This partially stems from the fact that there are many examples of misuse that are directly tied to the context in which content is shared, more than the content itself (e.g. many seemingly innocuous images can be exploited by information operations, as discussed in the Disinformation section below).
- The filters on prompts and uploaded images also work independently so the filters do not refuse cases where the prompt and image are independently neutral but, when considered in combination, may constitute prompting for misuse (e.g. the prompt "a woman" and an image of a shower in Inpainting).
- Input classifiers have the capacity to potentially introduce or amplify bias, e.g. insofar as it may lead to erasure of certain groups. Here, we have aimed to err on the side of avoiding bias that may be introduced by prompt classification, though this may make some of the model's harmful biases more visible. That is, false positives can cause harm to minority groups by silencing their voices or opportunities. This may extend to true positives as well – e.g. we know that the model produces particularly biased or sexualized results in response to prompts requesting images of women and that these results are likely to be "harmful" in certain cases; however, filtering of all images of women would cause problems of its own. In addition, commonly used methods for mitigating such content have been found to work less well for marginalized groups (Sap et al., 2019), further motivating a holistic, contextual approach to mitigation at the system level, including mitigations at the level of system access.
- For the most part, our input filters aim to reduce cases where either the generated content or the input content is necessarily a violation of our content policy (details below).
- At present, the prompt filters do not cover prompts that are likely to lead to displays of harmful bias, or the holistic generation of people or children.
- Because our filtering approach is imperfect, a key component of our current mitigation strategy is limiting system access to trusted users, with whom we directly reinforce the importance of following our use case guidelines (see discussion in Policies and enforcement).
- Beyond limitations on the types of content that can be generated, we also limit the rate at which users may interact with the DALL·E 2 system. In addition to the above, we have put in place rate limits (e.g. limits on the number of prompts or images a user submits or generates per minute or simultaneously).
- The primary purposes of rate limits at this stage are to help identify anomalous use and to limit the possibility of at-scale abuse.
- At this stage we are not allowing programmatic access to the model by non-OpenAI employees.
- Accesss is currently gained via a waitlist -- ensuring trust by monitoring adherence to our content policy and terms.
- Access mitigations have limitations. For example, the power
to control use of a particular generated image diminishes the moment an
image leaves the platform. Because trust declines the second images are
shared off the platform – where affected parties may include not just
direct users of the site but also anyone who may view that content when
it is shared – we are carefully tracking use during this period.
Further, restricting access means access to the DALL•E 2 Preview is not
granted in an inclusive way, which may preferentially benefit certain
groups.
- By expanding access, we aim to get as much signal as possible on the exact
vectors of risk from the platform. We will support this through ongoing
access for researchers and experts who will help inform our
understanding of the effectiveness of mitigations as well as the
limitations of the model (see more in the Contributions section below).
In addition to that, we are pleased to support longer term research on
our models via the Researcher Access Program
which will allow us to give some researchers access to the underlying
model.
- Use of the DALL·E 2 Preview is subject to the use case and content
policies we outline below and which can be read in full here.
- The intended use of the DALL·E 2 Preview at this time is for personal,
non-commercial exploration and research purposes by people who are
interested in understanding the potential uses of these capabilities.
This early access is intended to help us better understand benefits and
risks associated with these capabilities, and further adjust our
mitigations. Other uses are explicitly out of scope for the DALL·E 2
Preview, though findings from the Preview period may inform our
understanding of the mitigations required for enabling other future
uses.
- While we are highly uncertain which commercial and non-commercial use
cases might get traction and be safely supportable in the longer-term,
plausible use cases of powerful image generation and modification
technologies like DALL·E 2 include education (e.g. illustrating and
explaining concepts in pedagogical contexts), art/creativity (e.g. as a
brainstorming tool or as one part of a larger workflow for artistic
ideation), marketing (e.g. generating variations on a theme or "placing"
people/items in certain contexts more easily than with existing tools),
architecture/real estate/design (e.g. as a brainstorming tool or as one
part of a larger workflow for design ideation), and research (e.g.
illustrating and explaining scientific concepts).
- In addition to instituting the above access and use policies, we have instituted a similar set of content policies to those we have previously developed for our API, and are enforcing these content policies as part of our portfolio of mitigations for the DALL·E 2 Preview.
- That said, while there are many similarities between image generation and text generation, we did need to address new concerns from the addition of images and the introduction of multimodality itself (i.e. the intersection of image and text).
- To address these concerns, we expanded categories of interest to include shocking content; depictions of illegal activity; and content regarding public and personal health. We also adapted existing policies to cover visual analogues of prohibited text (e.g. explicit and hateful content) as well as text-image pairs which are violative of our policies when considered in combination even if they are not individually.
- Some particularly important policies governing use the DALL·E 2 Preview
are the following:
- Disclosure of role of AI: Users are asked to clearly indicate that images are AI-generated - or which portions of them are - by attributing to OpenAI when sharing, whether in public or private. In addition to asking users to disclose the role of AI, we are exploring other measures for image provenance and traceability.
- Respect the rights of others: Users are asked to respect the rights of others, and in particular, are asked not to upload images of people without their consent (including public figures), or images to which they do not hold appropriate usage rights. Individuals who find that their images have been used without their consent can report the violation to the OpenAI Support team (support@openai.com) as outlined in the content policy. Issues of consent are complex and are further discussed in the subsections on Consent.
- Use for non-commercial purposes: As this is an experimental research platform, users are not allowed to use generated images for commercial purposes. For example, users may not license, sell, trade, or otherwise transact on these image generations in any form, including through related assets such as NFTs. Users also may not serve these image generations to others through a web application or through other means of third-parties initiating a request.
- Each generated image includes a signature in the lower right corner, with the goal of indicating when DALL·E 2 helped generate a certain image. We recognize that this alone does not help to prevent a bad actor, and is easily circumvented by methods such as cropping an image.
- Our policies are enforced via monitoring and human review. In addition,
at this stage of the DALL·E 2 Preview, any user can flag content that is
sensitive for additional review.
- Non-users / third parties who find that their images have been used
without their consent or that violate other areas of the content
policies can report the suspected violation to the OpenAI Support team
(support@openai.com) as outlined in
the content policy, which is publicly available and discoverable by
users and non-users both. A limitation of this reporting mechanism is that it assumes an individual would know that the image was generated by DALL·E 2, and would therefore know to contact OpenAI about their concerns. We are continuing to explore watermarks and other image provenance techniques to aid this.
- We are not currently sharing more details about our processes for detecting and responding to incidents in part to make these policies more difficult to evade. Penalties for policy violation include disabling of accounts.
- 
- Beginning in 2021, several staff at OpenAI have been exploring risks
associated with image generation systems, and potential mitigations for
those risks. This effort grew over time as momentum grew around an
effort to build DALL·E 2 and the DALL·E 2 Preview. Some early results of
that research were reported in Nichol, Dhariwal, and Ramesh et al.
(2021) and informed data-level
interventions for DALL·E 2.
- Additionally, since 2021 a variety of Slackbots exposing model
capabilities, and other internal prototypes of interfaces to those
models, have been available to OpenAI staff, enabling asynchronous,
intermittent exploration of model capabilities by around 200 people.
Informal findings from this work, and more formal analyses conducted by
staff, informed the high-level plan for the DALL·E 2 Preview and its
associated mitigations, and these plans were and will be further
fine-tuned over time in response to internal and external findings to
date. We expect to further adjust our thinking as we consider broadening
access to a small number of trusted users.
- Starting in February 2022, OpenAI began recruiting external experts to
provide feedback on the DALL·E 2 Preview. We described this process as
"red teaming" in line with the definition given in Brundage, Avin,
Wang, Belfield, and Krueger et. al
(2020), "a structured effort to
find flaws and vulnerabilities in a plan, organization, or technical
system, often performed by dedicated 'red teams' that seek to adopt an
attacker's mindset and methods."
- OpenAI reached out to researchers and industry professionals, primarily
with expertise in bias, disinformation, image generation, explicit
content, and media studies, to help us gain a more robust
understanding of the DALL·E 2 Preview and the risk areas of potential
deployment plans. Participants in the red team were chosen based on
areas of prior research or experience in the risk areas identified from
our internal analyses, and therefore reflect a bias towards groups with
specific educational and professional backgrounds (e.g., PhD's or
significant higher education or industry experience). Participants
also have ties to English-speaking, Western countries (U.S., Canada, U.K.)
in part due to compensation restrictions. This background likely
influenced both how they interpreted particular risks and how they
probed politics, values, and the default behavior of the model. It is
also likely that our sourcing of researchers privileges risks that have
received weight in academic communities and by AI firms.
- Participation in this red teaming process is not an endorsement of the
deployment plans of OpenAI or OpenAI's policies. Because of the very
early nature of this engagement with models that had not been publicly
released, as well as the sensitive nature of the work, red teaming
participants were required to sign an NDA. OpenAI offered compensation
to all red teaming participants for their time spent on this work.
- Participants interacted with different versions of the Preview as it
developed. The underlying model shifted between when they completed the
primary red teaming stage (March 9th, 2022 - March 28th, 2022) and the
DALL·E 2 model underlying the system today. We have started to apply
techniques and evaluation methods developed by red-teamers to the system
design for the DALL-E 2 Preview. Our planned mitigations have also
evolved during this period, including changes to our filtering
strategies, limiting the initial release to only trusted users, and
additional monitoring.
- Participants in the red teaming process received access to the DALL·E 2
Preview and model in 3 primary ways:
- Advisory conversations about the model, system, and their area(s) of expertise. This includes preliminary discussions, access to a Slack channel with OpenAI and other participants in the red teaming process, and group debrief sessions hosted by OpenAI.
- Generating "Text to Image" prompts for OpenAI to run in bulk on the backend, bypassing prompt filters and accelerating analysis.
- Direct access to the Preview site to test all functionalities including "Text to Image Generation", Inpainting, and Variations, with availability of features varying over the course of the red teaming period.
- The first model was available from March 9th, 2022 to March 28th, 2022
- The second model and the Variations feature were available after March 28th, 2022
- Not all participants in the red teaming had access to every feature or Preview access for the full duration, due to competitive considerations relevant to a small number of participants.
- Participants in the red teaming process joined a Slack channel to share
findings collaboratively with each other and OpenAI staff, as well as to
ask continued questions about the Preview and red team process. All
participants were asked to document their prompts, findings, and any
notes so that their analyses could be continuously applied as the
Preview evolved. Participants were invited to group debrief sessions
hosted by OpenAI to discuss their findings with the OpenAI team. Their
observations, final reports, and prompts are inputs into this document, and helped to inform changes to our mitigation plan.
- The red teaming process will be ongoing even after the initial
deployment of the DALL·E 2 Preview, and we will support longer term
research via OpenAI's Researcher Access Program.
- The DALL·E 2 Preview allows generation of images that, depending on the
prompt, parameters, viewer, and context in which the image is viewed,
may be harmful or may be mistaken as authentic photographs or
illustrations. In order to better measure and mitigate the risk of harms
the DALL·E 2 Preview presents, we conducted a series of primarily
qualitative probes and evaluations in areas such as bias and
representation, explicit content, and disinformation, as outlined below.
- Despite the pre-training filtering, DALL·E 2 maintains the ability to
generate content that features or suggests any of the following:
nudity/sexual content, hate, or violence/harm. We refer to these
categories of content using the shorthand "explicit" in this document,
in the interest of brevity. Whether something is explicit depends on
context. Different individuals and groups hold different views on what
constitutes, for example, hate speech (Kocoń et al.,
2021).
- Explicit content can originate in the prompt, uploaded image, or
generation and in some cases may only be identified as such via the
combination of one or more of these modalities. Some prompts requesting
this kind of content are caught with prompt filtering in the DALL·E 2
Preview but this is currently possible to bypass with descriptive or
coded words.
- Some instances of explicit content are possible for us to predict in
advance via analogy to the language domain, because OpenAI has deployed
language generation technologies previously. Others are difficult to
anticipate, as discussed further below. We continue to update our input
(prompt and upload) filters in response to cases identified via internal
and external red teaming, and leverage a flagging system built
into the user interface of the DALL·E 2 Preview.
- We use "spurious content" to refer to explicit or suggestive content
that is generated in response to a prompt that is not itself explicit or
suggestive, or indicative of intent to generate such content. If the
model were prompted for images of toys and instead generated images of
non-toy guns, that generation would constitute spurious content.
- We have to date found limited instances of spurious explicit content on
the DALL·E 2 model that is live as of April 6, 2022, though significantly more
red teaming of this is needed to be confident that spurious content is
minimal.
- An interesting cause of spurious content is what we informally refer to
as "reference collisions": contexts where a single word may reference
multiple concepts (like an eggplant emoji), and an unintended concept is
generated. The line between benign collisions (those without malicious
intent, such as "A person eating an eggplant") and those involving
purposeful collisions (those with adversarial intent or which are more
akin to visual synonyms, such as "A person putting a whole eggplant into
her mouth") is hard to draw and highly contextual. This example would
rise to the level of "spurious content" if a clearly benign example –
"A person eating eggplant for dinner" contained phallic imagery in the
response.
- In qualitative evaluations of previous models (including those made
available for external red teaming), we found that places where the
model generated with less photorealistic or lower fidelity generations
were often perceived as explicit. For instance, generations with less-photorealistic women often suggested nudity. So far we have not found
these cases to be common in the latest version of DALL·E 2.
- Visual synonyms and visual synonym judgment have been studied by
scholars in fields such as linguistics to refer to the ability to judge
which of two visually presented words is most similar in meaning to a
third visually-presented word. The term "visual synonym" has also been
used previously in the context of AI scholarship to refer to
"independent visual words that nonetheless cover similar appearance"
(Gavves et al.,
2012),
and by scholars constructing a contextual "visual synonym dictionary" in
order to show synonyms for visual words, i.e. words which have similar
contextual distributions (Tang et al.,
2011).
- Here, we use the term "visual synonym" to refer to the use of prompts
for things that are visually similar to objects or concepts that are
filtered, e.g. ketchup for blood. While the pre-training filters do
appear to have stunted the system's ability to generate explicitly
harmful content in response to requests for that content, it is still
possible to describe the desired content visually and get similar
results. To effectively mitigate these we would need to train prompt
classifiers conditioned on the content they lead to as well as explicit
language included in the prompt.
- Another way visual synonyms can be operationalized is through the use of images of dolls, mannequins, or other anthropomorphic representations. Images of dolls or other coded language might be used to bypass filtering to create violent, hateful, or explicit imagery.
- Use of DALL·E 2 has the potential to harm individuals and groups by reinforcing stereotypes, erasing or denigrating them, providing them with disparately low quality performance, or by subjecting them to indignity. These behaviors reflect biases present in DALL·E 2 training data and the way in which the model is trained. While the deeply contextual nature of bias makes it difficult to measure and mitigate the actual downstream harms resulting from use of the DALL·E 2 Preview (i.e. beyond the point of generation), our intent is to provide concrete illustrations here that can inform users and affected non-users even at this very initial preview stage.
- In addition to biases present in the DALL·E 2 model, the DALL·E 2
Preview introduces its own sets of biases, including: how and for whom
the system is designed; which risks are prioritized with associated
mitigations; how prompts are filtered and blocked; how uploads are
filtered and blocked; and how access is prioritized (among others).
Further bias stems from the fact that the monitoring tech stack and
individuals on the monitoring team have more context on, experience
with, and agreement on some areas of harm than others. For example, our
safety analysts and team are primarily located in the U.S. and English
language skills are one of the selection criteria we use in hiring them,
so they are less well equipped to analyze content across international
contexts or even some local contexts in the U.S.
- The default behavior of the DALL·E 2 Preview produces images that tend to overrepresent people who are White-passing and Western concepts generally. In some places it over-represents generations of people who are female-passing (such as for the prompt: “a flight attendant” ) while in others it over-represents generations of people who are male-passing (such as for the prompt: “a builder”). In some places this is representative of stereotypes (as discussed below) but in others the pattern being recreated is less immediately clear.
- For example, when prompted with “wedding,” it tends to assume Western wedding traditions, and to default to heterosexual couples. This extends to generations that don’t include any depictions of individuals or groups, such as generations from prompts such as “restaurant” or “home” which tend to depict Western settings, food serving styles, and homes.
- With added capabilities of the model (Inpainting and Variations), there may be additional ways that bias can be exhibited through various uses of these capabilities. Wang et al. (2020), and Steed and Caliskan (2021) have previously conducted social bias analyses on related topics of image classification models and visual datasets, and Cho et al. (2022) propose methods for quantitative evaluation of social biases for Text to Image generative models.
- Some of these researchers, and others with whom we worked as part of the red teaming period, analyzed earlier iterations of the DALL·E 2 Preview and the underlying model and found significant bias in how the model represents people and concepts, both in what the model generates when a prompt is “underspecified” and potentially fits a vast array of images (e.g. the “CEO” example above), and in what the model generates when a prompt is hyper-specified (see further discussion below on disparate performance). Recent mitigations have partially addressed the issue of underspecified prompts requesting images of humans.
- We are in the early stages of quantitatively evaluating DALL·E 2’s biases, which is particularly challenging at a system level, due to the filters discussed above, and due to model changes. Additionally, it remains to be seen to what extent our evaluations or other academic benchmarks will generalize to real-world use, and academic benchmarks (and quantitative bias evaluations generally) have known limitations. Cho et al., creators of DALL-Eval, compared an April 1, 2022 checkpoint of DALL·E 2 to minDALL-E. They found that the April 1 DALL·E 2 checkpoint exhibited more gender bias and racial bias than minDALL-E (i.e. tending to generate images of male-passing people more often and White-passing people more often, with both models having very strong tendencies toward generating images labeled as male and Hispanic by CLIP). This could reflect differences in the underlying datasets (minDALL-E is trained on Conceptual Captions data), a difference in the models’ sizes or training objectives, or other factors, which more research would be needed in order to disentangle.
- Representational harms occur when systems reinforce the subordination of
some groups along the lines of identity, e.g. stereotyping or
denigration, as compared to allocative harms, which occur when a system
allocates or withholds a certain opportunity or resource (Jacobs et
al., 2020, and
Blodgett et al, 2020).
- DALL·E 2 tends to serve completions that suggest stereotypes, including race and gender stereotypes. For example, the prompt “lawyer” results disproportionately in images of people who are White-passing and male-passing in Western dress, while the prompt “nurse” tends to result in images of people who are female-passing.
- As noted above, not only the model but also the manner in which it is
deployed and in which potential harms are measured and mitigated have the
potential to create harmful bias, and a particularly concerning example
of this arises in DALL·E 2 Preview in the context of pre-training data
filtering and post-training content filter use, which can result in some
marginalized individuals and groups, e.g. those with disabilities and
mental health conditions, suffering the indignity of having their
prompts or generations filtered, flagged, blocked, or not generated in
the first place, more frequently than others. Such removal can have
downstream effects on what is seen as available and appropriate in
public discourse.
- Image generation models may produce different quality generations when producing different concepts, where we consider diversity of responses, photorealism, aesthetic quality, and conceptual richness as different dimensions of “quality.”
- Earlier versions of DALL·E seemed to be worse at producing high quality images on concepts that are further outside of its training distribution. We have had more difficulty finding evidence of such disparate realism in the released version of the DALL·E 2 Preview, though we do see evidence that typical outputs tend to more often involve some demographics, which we discussed above under Defaults and assumptions and Stereotypes but can also be thought of as a form of disparate performance.
- “Person-first” and specific language can help improve performance and mitigate disparities (e.g. “a person who is female and is a CEO leading a meeting”) by removing diversity of responses as an input into “quality.” Additionally, small differences in prompts can have a disproportionate impact on the quality of responses, as the example below comparing “CEO” and “a CEO” demonstrates.
- Moreover, this disparity in the level of specification and steering needed to produce certain concepts is, on its own, a performance disparity bias. It places the burden of careful specification and adaptation on marginalized users, while enabling other users to enjoy a tool that, by default, feels customized to them. In this sense, it is not dissimilar to users of a voice recognition system needing to alter their accents to ensure they are better understood.
- Targeted harassment, bullying, or exploitation of individuals is a
principal area of concern for deployment of image generation models
broadly and Inpainting in particular.
- Inpainting – especially combined with the ability to upload images –
allows for a high degree of freedom in modifying images of people and their
visual context. While other image editing tools are able to achieve
similar outcomes, Inpainting affords greater speed, scale, and efficiency.
Many photo editing tools also require potentially costly access and/or a
particular skill set to achieve photorealistic outcomes. Cheaper and more accessible
options than photo editing exist, for instance tools that allow for
simple face swapping may offer speed and efficiency, but over a much
more narrow set of capabilities and often with the ability to clearly
trace provenance of the given images.
- In qualitative evaluations, we find that the system, even with current
mitigations in place, can still be used to generate images that may be
harmful in particular contexts and difficult for any reactive response
team to identify and catch.5 This underscores the importance of
access controls and further investment in more robust mitigations, as
well as tight monitoring of how capabilities with a high capacity for
misuse – e.g. Inpainting on images of people – are being used and
shared in practice.
- Some examples of this that could only be clear as policy violations in
context include:
- Modifying clothing: adding or removing religious items of clothing (yarmulke, hijab)
- Adding specific food items to pictures: adding meat to an image of an individual who is vegetarian
- Adding additional people to an image: inpainting a person into an image holding hands with the original subject (e.g. someone who is not their spouse)
- Such images could then be used to either directly harass or bully an
individual, or to blackmail or exploit them.
- It is important to note that our mitigations only apply to our Inpainting system. Open-ended generation may be combined with third-party tools to swap in private individuals, therefore bypassing any Inpainting restrictions we have in place. Inpainting can also be combined with other image transformations (such as “zooming out” of an image prior to uploading it) in order to make it easier to “place” a subject in a scene.
- DALL·E 2 currently has a very limited ability to render legible text. When it does, text may sometimes be nonsensical and could be misinterpreted. It’s important to track this capability as it develops, as image generative models may eventually develop novel text generation capabilities via rendering text.
- Qualifying something as harassment, bullying, exploitation, or
disinformation targeted at an individual requires understanding
distribution and interpretation of the image. Because of this, it may be
difficult for mitigations (including content policies, prompt and image
filtering, and human in the loop review) to catch superficially
innocuous uses of Inpainting that then result in the spread of harmful
dis- or misinformation.
- Our Terms of Use require that users both (a) obtain consent before
uploading any one else's picture or likeness, and (b) have ownership and
rights to the given uploaded image. We remind users of this at upload
time and third parties can report violations of this policy as described
in the Monitoring section above.
- While users are required to obtain consent for use of anyone else's
image or likeness in Inpainting, there are larger questions to be
answered about how people who may be represented in the training data
may be replicated in generations and about the implications of
generating likenesses of particular people.
- OpenAI has made efforts to implement model-level technical mitigations that ensure that DALL·E 2 Preview
cannot be used to directly generate exact matches for any of the images
in its training data. However, the models
may still be able to compose aspects of real images and identifiable
details of people, such as clothing and backgrounds.
- Even if DALL·E 2 Preview cannot literally generate exact images of
people, it may be possible to generate a similar likeness to someone in
the training data. Previous literature (Webster et al.,
2021) has demonstrated that
many faces produced by a different model class –  generative adversarial networks (or “GANs”)   – bear a
striking resemblance to actual people who appear in the training data.
More work is needed to understand the impacts of DALL·E 2 being used to
generate conceivably recognizable people in addition to the impacts of the harassment and
disinformation vectors discussed above.
- Generations from models like DALL·E 2 could be used to intentionally
mislead or misinform subjects, and could potentially empower information
operations and disinformation campaigns.6 Indeed, outputs from some
GANs have been used for such purposes
already. The efficacy of using generated content in service of an
information operation is a function of multiple factors: the model's
capabilities, the cost-effectiveness of using generated content for any
such operation, mitigations (such as the ability to trace the provenance
of images back to DALL·E 2), and existing trust in information systems
(Hwang 2020).
- Existing tools powered by generative models have been used to generate
synthetic profile pictures in disinformation campaigns.7 Like these
tools, DALL·E 2 can create photorealistic images of people. However,
DALL·E 2's understanding of language allows more flexibility and
steerability in composing novel images from natural language, which
could have important applications to information operations.8 In the
following table, we non-exhaustively list some potential applications of
Text to Image Generation, Inpainting, and Variations to information operations:
- These capabilities could be used to create fake account infrastructure
or spread harmful content. It's unclear to what extent the effectiveness
of DALL·E 2 is better than those of reasonable alternative tools;
however, the wide surface area of the system's capabilities means that
any provision of access to them requires caution.
- It is often possible to generate images of public figures using large-scale image generation systems, because such figures tend to be well-represented in public datasets, causing the model to learn representations of them.
- We modified the training process to limit the DALL·E 2 model’s ability to memorize faces from the training data, and find that this limitation is helpful in preventing the model from faithfully reproducing images of celebrities and other public figures.
- However, intervening at the level of a model’s internal knowledge – e.g. by masking public individuals – is not always effective. These interventions can make it more difficult to generate harmful outputs, but do not guarantee that it is impossible: the methods we discussed previously to Inpaint private individuals in harmful or defamatory contexts could also be applied to public individuals. Uploading images into the system (as distinct from the model) allows injection of new knowledge, which malicious users could potentially use in order to generate harmful outputs.
- Of course, dis- and misinformation need not include images of people.
Indeed we expect that people will be best able to identify outputs as
synthetic when tied to images or likenesses they know well (e.g. that
image of the President looks a little off). DALL·E 2
can, however, potentially be used to generate images that could be used as
evidence of news reports which could, in turn, be misused in an information operations
campaign. This may be especially important during crisis response
(Starbird, Dailey, Mohamed, Lee, and Spiro
2018).
- Beyond the direct consequences of a generated or modified image that is
used for harmful purposes, the very existence of believable synthetic
images can sway public opinion around news and information sources.
Simply knowing that an image of quality X could be faked may reduce
credibility of all images of quality X. Scholars have named this
phenomenon, in which deep fakes make it easier for disinformants to
avoid accountability for things that are in fact true, the "liar's
dividend" (Citron and Chesney,
2019).
Research by Christian Vaccari and Andrew Chadwick shows that people are
more likely to feel uncertain than misled by deepfakes, and as a result
have a reduced level of trust in news on social media (Vaccari,
Chadwick
2020).
- The challenges with deciding to label or disclose AI generated content
also have an impact on trust in information systems generally (Shane,
2020).
The implied truth effect is one possible consideration - for example,
news headlines that have warning labels attached increase the likelihood
of people perceiving unlabeled content as true even if it is not
(Pennycook et. al,
2020).
Another similar consideration is the tainted truth effect, where
corrections start to make people doubt other, true information (Freeze
et. al,
2021).
Our content policies require the disclosure of the role of AI when
sharing the generations, and we are still evaluating other image
provenance techniques while taking into account the effect of labeled AI
generated content.
- Finally, even if the Preview itself is not directly harmful, its
demonstration of the potential of this technology could motivate various
actors to increase their investment in related technologies
and tactics.
- The model can generate known entities including trademarked logos and
copyrighted characters. OpenAI will evaluate different approaches to
handle potential copyright and trademark issues, which may include
allowing such generations as part of "fair use" or similar concepts,
filtering specific types of content, and working directly with
copyright/trademark owners on these issues.
- Though DALL·E 2 is for exclusively non-commercial purposes today, it may eventually have significant economic implications. The model may increase the efficiency of performing some tasks like photo editing or production of stock photography which could displace jobs of designers, photographers, models, editors, and artists. At the same time it may make possible new forms of artistic production, by performing some tasks quickly and cheaply.
- As mentioned above, the model both underrepresents certain concepts and people and its knowledge is limited by its training set. This means that if commercial use is eventually allowed, groups and intellectual property that are represented in or by the model may feel the economic benefits and harms more acutely than those that are not, e.g., if access to the model is given for an application to retouch photos but the model is shown to not work as well on dark skin as it does on light skin.
- Finally, access to the model is currently given to a limited number of users, many of whom are selected from OpenAI employees’ networks. While commercial use is not currently allowed, simply having access to an exclusive good can have indirect effects and real commercial value. For example, people may establish online followings based on their use of the technology, or develop and explore new ideas that have commercial value without using generations themselves. Moreover, if commercial access is eventually granted, those who have more experience using and building with the technology may have first mover advantage – for example, they may have more time to develop better prompt engineering techniques.
- We do not provide robust comparisons with existing photo editing
software, but this is an exciting area for future work, and essential to
comprehensively understanding the impact of systems like this at large
scale.
- Anecdotally and informally, we believe that DALL·E 2, and similar image
generation models and systems, may accelerate both the positive and negative uses
associated with generating visual content. A reason for this
acceleration is that these systems can "encapsulate" multimodal knowledge which is similar in some ways to that which resides in human brains, and work at a faster-than-human
pace. In principle any image generated by DALL·E 2 could have been drawn
by hand, edited from existing images using tools,
or recreated with hired models and photographers; this speed (and
cost) differential is a difference in degree that may add up to a
difference in kind.
- In addition to side-by-side comparisons, it is important to consider how
new image generation technologies can be combined with previous ones.
Even if images from tools like the DALL·E 2 Preview are not immediately usable for
harmful contexts, they may be combined with other photo editing and
manipulation tools to increase the believability or fidelity of
particular images. Even low-fidelity images can be used as
disinformation, for example if someone claims they were taken with a cell
phone camera, perhaps with the addition of blur. Moreover it is
important to consider what impacts deployments such as this will have on
wider norms related to image generation and modification technologies.
- Given these considerations, and our expectation that this class of
technologies will continue to advance rapidly, we recommend that
stakeholders consider not just the capabilities of the image generation
model in front of them but the larger context in which these images may
be used and shared, both today and down the line.
- More work is needed to understand the model and potential impacts of its
deployment. We lay out a few areas of additional work below. This is not
intended to be exhaustive but rather to highlight the breadth and depth
of work still outstanding.
- One particularly important area for future
work is assessment and analysis of downstream impacts after the point of
generation, and the ways in which the lives and experiences of real
people are impacted by the use of DALLE 2 Preview. A full impact
assessment would evaluate the effectiveness of mitigations and
critically assess our procedural rules.
- Another area for future work is the analysis of different modes of use.
For example, we have done only preliminary red teaming of uses such as
visual question answering, sentence completion or story continuation,
and preliminary findings point to these and other less explored modes of
use as an important risk area. In addition, while we have done some
light red teaming of Variations, there is yet more to uncover, including
in analysis of in particular through "iterative variations" or
repeatedly giving the feature its own outputs. DALL·E 2 has the
potential to change the way in which the creation or modification of
visual content is directed, and even to be used as a new instrument or
creative medium.
- DALL·E 2 and successor models have the potential to be used in systems
that enable a user to generate not only images but entire multimodal
experiences or "worlds," or to lower the cost of high-fidelity immersive
experience; and the potential impacts of this is another avenue for
future work. And finally, this research direction has potentially
far-reaching implications for both disinformation and economics/labor markets, which is one of the reasons we are pursuing long-term
research agendas in these areas.
- Primary authors of this document: Pamela Mishkin, Lama Ahmad, Miles Brundage, Gretchen Krueger, Girish Sastry
- Primary researchers and developers of DALL·E 2: Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen
- Contributors to the DALL·E 2 Preview: Steven Adler, Sandhini Agarwal, Chaitra Agvekar, Lama Ahmad, Sam Altman, Jeff Belgum, Miles Brundage, Kevin Button, Che Chang, Fotis Chantzis, Derek Chen, Mark Chen, Frances Choi, Casey Chu, Dave Cummings, Prafulla Dhariwal, Steve Dowling, Tyna Eloundou, Juston Forte, Elie Georges, Jonathan Gordon, Reggie Hall, Chris Hallacy, Peter Hoeschele, Shawn Jain, Raf Jakubanis, Joanne Jang, Shino Jomoto, Fraser Kelton, Jong Wook Kim, Matt Knight, Aris Konstantinidis, Gretchen Krueger, Vishal Kuo, Loren Kwan, Jason Kwon, Joel Lehman, Rachel Lim, Anna Makanju, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Luke Miller, Pamela Mishkin, Evan Morikawa, Mira Murati, Adam Nace, Rajeev Nayak, Alex Nichol, Hyeonwoo Noh, Cullen O'Keefe, Long Ouyang, Michael Petrov, Henrique Ponde de Oliveira Pinto, Glenn Powell, Alec Radford, Aditya Ramesh, Adam Rhodes, Krijn Rijshouwer, Bob Rotsted, Mario Saltarelli, Girish Sastry, David Schnurr, Sarah Shoker, Pranav Shyam, Maddie Simens, Katarina Slama, Aravind Srinivas, Kenneth Stanley, Natalie Staudacher, Felipe Such, Natalie Summers, Ilya Sutskever, Chelsea Voss, Justin Wang, Peter Welinder, David Willner, Austin Wiseman, Hannah Wong
- Participation in this red teaming process is not an endorsement of the
deployment plans of OpenAI or OpenAI's policies.
- With thanks to:
- Mohit Bansal, Vladimir Barash, Ana Carolina D N S Da Hora, Jaemin Cho, Lia Coleman,
Victor do Nascimento Silva, Shahar Edgerton Avin, Zivvy Epstein, Matthew
Groh, Vipul Gupta, Jack Hessel, Liwei Jiang, Yennie Jun, Ximing Lu, Sam
Manning, Micaela Mantegna, Alex Newhouse, Marcelo Rinesi, Hannah Rose
Kirk, Maarten Sap, Neerja Thakkar, Angelina Wang, Abhay Zala
- Please use the following BibTeX entry:
- Specifically, OpenAI provides access to Text to Image
generation, Inpainting (as defined in the text), and a capability
OpenAI calls Variations. Each of these features is made available
in restricted form (with input filters, rate limits, and other
mitigations). Hence this type of access, like API-based access, is
not equivalent to full model access, and lacks some transparency
properties possible with open source models, while providing more
assurances against certain kinds of (especially large scale) abuse. ↩
- We discuss our use of the term "explicit" and some of the
implications of filtering for such content in the section on
Explicit content. ↩
- For DALL·E 2, our filtering procedure involved using classifiers
to filter sexual content and violent content. We also filtered
training set images with captions that mentioned hate symbols such
as those common among white supremacist groups in the United States.
Training data was collected and labeled in-house by OpenAI
researchers. Our sources included the Hate on Display™ Hate
Symbols Database from the
Anti-Defamation League. ↩
- We note also that there are risks attached to open-sourcing even a
filtered model, such as accelerating other actors, allowing others
to potentially fine-tune the model for a particular specific use
case (including person generation), and allowing for non-person
generation associated risks. ↩
- Creation of this content does not require an intentionally
malicious user to misuse the system. For example, consider the case
of someone intending a generation to be received in jest or
intending a generation to only be shared in private. Third party
assessment of harm in these cases can be difficult, if not
impossible, without an intimate understanding of the context of the
shared image. ↩
- "Information operations and warfare, also known as influence
operations, includes the collection of tactical information about an
adversary as well as the dissemination of propaganda in pursuit of a
competitive advantage over an opponent" from RAND's page on
Information
Operations.
- The term is also
used
by social media companies. ↩
- For example, this 2019
campaign
used synthetic profile pictures.
This
is an example of a Twitter network (not officially attributed) in
2021. ↩
- While the full extent of these implications are unknown, AI and
the Future of Disinformation
Campaigns
discusses how AI can plug into the killchain of disinformation. ↩

URL: https://openai.com/blog/dall-e-now-available-in-beta/
- We’ll invite 1 million people from our waitlist over the coming weeks. Users can create with DALL·E using free credits that refill every month, and buy additional credits in 115-generation increments for $15.
- Illustration: Ruby Chen & Justin Jay Wang × DALL·E
- DALL·E, the AI system that creates realistic images and art from a description in natural language, is now available in beta. Today we’re beginning the process of inviting 1 million people from our waitlist over the coming weeks.
- Every DALL·E user will receive 50 free credits during their first month of use and 15 free credits every subsequent month. Each credit can be used for one original DALL·E prompt generation — returning four images — or an edit or variation prompt, which returns three images.
- DALL·E allows users to create quickly and easily, and artists and creative professionals are using DALL·E to inspire and accelerate their creative processes. We’ve already seen people use DALL·E to make music videos for young cancer patients, create magazine covers, and bring novel concepts to life.
- Other features include:
- In this first phase of the beta, users can buy additional DALL·E credits in 115-credit increments (460 images[^1]) for $15 on top of their free monthly credits. One credit is applied each time a prompt is entered and a user hits “generate” or “variations.”
- As we learn more and gather user feedback, we plan to explore other options that will align with users’ creative processes.
- Starting today, users get full usage rights to commercialize the images they create with DALL·E, including the right to reprint, sell, and merchandise. This includes images they generated during the research preview.
- Users have told us that they are planning to use DALL·E images for commercial projects, like illustrations for children’s books, art for newsletters, concept art and characters for games, moodboards for design consulting, and storyboards for movies.
- Prior to making DALL·E available in beta, we’ve worked with researchers, artists, developers, and other users to learn about risks and have taken steps to improve our safety systems based on learnings from the research preview, including:
- We hope to make DALL·E as accessible as possible. Artists who are in need of financial assistance will be able to apply for subsidized access. Please fill out this interest form if you’d like to be notified once more details are available.
- We are excited to see what people create with DALL·E and look forward to users’ feedback during this beta period.

URL: https://openai.com/blog/dall-e-api-now-available-in-public-beta/
- Starting today, developers can begin building apps with the DALL·E API.
- Illustration: Ruby Chen × DALL·E
- Developers can now integrate DALL·E directly into their apps and products through our API. More than 3 million people are already using DALL·E to extend their creativity and speed up their workflows, generating over 4 million images a day. Developers can start building with this same technology in a matter of minutes.
- DALL·E’s flexibility allows users to create and edit original images ranging from the artistic to the photorealistic. DALL·E excels at following natural language descriptions so users can plainly describe what they want to see. As our research evolves, we will continue to bring the state of the art into the API, including advances in image quality, latency, scalability, and usability.
- Incorporating the trust & safety lessons we’ve learned while deploying DALL·E to 3 million artists and users worldwide, developers can ship with confidence knowing that built-in mitigations—like filters for hate symbols and gore—will handle the challenging aspects of moderation. As a part of OpenAI’s commitment to responsible deployment, we will continue to make trust & safety a top priority so that developers can focus on building.
- We’ve worked closely with a few early customers who have already built DALL·E into their apps and products across a variety of use cases.
- Microsoft is bringing DALL·E to a new graphic design app called Designer, which helps users create professional quality social media posts, invitations, digital postcards, graphics, and more.
- Microsoft is also integrating DALL·E in Bing and Microsoft Edge with Image Creator, allowing users to create images if web results don’t return what they’re looking for.
- CALA is the world’s first fashion and lifestyle operating system. CALA unifies the entire design process—from product ideation all the way through e-commerce enablement and order fulfillment—into a single digital platform. Powered by DALL·E, CALA’s new artificial intelligence tools will allow users to generate new design ideas from natural text descriptions or uploaded reference images.
- Mixtiles is a fast-growing photo startup. They use software and an easy hanging experience to help millions of people create beautiful photo walls. Mixtiles uses the DALL·E API to create and frame emotionally resonating artwork, by guiding users through a creative process that captures childhood memories, dream destinations, and more.
- We’re excited to see what our customers will do with DALL·E and what creative ideas they’ll come up with.
- DALL·E joins GPT-3, Embeddings, and Codex in our API platform, adding a new building block that developers can use to create novel experiences and applications. All API customers can use the DALL·E API today.

URL: https://arxiv.org/abs/2210.12889
- Help | Advanced Search
- arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
- Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
- Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.
- 
- arXiv Operational Status 
                    Get status notifications via
                    email
                    or slack

URL: https://arxiv.org/abs/2202.04053
- Help | Advanced Search
- arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
- Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
- Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.
- 
- arXiv Operational Status 
                    Get status notifications via
                    email
                    or slack

URL: https://arxiv.org/abs/2211.03759
- Help | Advanced Search
- arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
- Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
- Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.
- 
- arXiv Operational Status 
                    Get status notifications via
                    email
                    or slack

URL: https://www.vox.com/future-perfect/23023538/ai-dalle-2-openai-bias-gpt-3-incentives
- We use cookies and other tracking technologies to improve your browsing experience on our site, show personalized content and targeted ads, analyze site traffic, and understand where our audiences come from. To learn more or opt-out, read our Cookie Policy. Please also read our Privacy Notice and Terms of Use, which became effective December 20, 2019.
- By choosing I Accept, you consent to our use of cookies and other tracking technologies.
- OpenAI’s DALL-E 2 is incredible at turning text into images. It also highlights the problem of AI bias — and the need to change incentives in the industry.
- Finding the best ways to do good.
- You may have seen some weird and whimsical pictures floating around the internet recently. There’s a Shiba Inu dog wearing a beret and black turtleneck. And a sea otter in the style of “Girl with a Pearl Earring” by the Dutch painter Vermeer. And a bowl of soup that looks like a monster knitted out of wool.
- These pictures weren’t drawn by any human illustrator. Instead, they were created by DALL-E 2, a new AI system that can turn textual descriptions into images. Just write down what you want to see, and the AI draws it for you — with vivid detail, high resolution, and, arguably, real creativity.
- Sam Altman, the CEO of OpenAI — the company that created DALL-E 2 — called it “the most delightful thing to play with we’ve created so far … and fun in a way I haven’t felt from technology in a while.”
- That’s totally true: DALL-E 2 is delightful and fun! But like many fun things, it’s also very risky.
- There are the obvious risks — that people could use this type of AI to make everything from pornography to political deepfakes, or the possibility that it’ll eventually put some human illustrators out of work. But there is also the risk that DALL-E 2 — like so many other cutting-edge AI systems — will reinforce harmful stereotypes and biases, and in doing so, accentuate some of our social problems.
- As is typical for AI systems, DALL-E 2 has inherited biases from the corpus of data used to train it: millions of images scraped off the internet and their corresponding captions. That means for all the delightful images that DALL-E 2 has produced, it’s also capable of generating a lot of images that are not delightful.
- For example, here’s what the AI gives you if you ask it for an image of lawyers:
- Meanwhile, here’s the AI’s output when you ask for a flight attendant:
- OpenAI is well aware that DALL-E 2 generates results exhibiting gender and racial bias. In fact, the examples above are from the company’s own “Risks and Limitations” document, which you’ll find if you scroll to the bottom of the main DALL-E 2 webpage.
- OpenAI researchers made some attempts to resolve bias and fairness problems. But they couldn’t really root out these problems in an effective way because different solutions result in different trade-offs.
- For example, the researchers wanted to filter out sexual content from the training data because that could lead to disproportionate harm to women. But they found that when they tried to filter that out, DALL-E 2 generated fewer images of women in general. That’s no good, because it leads to another kind of harm to women: erasure.
- OpenAI is far from the only artificial intelligence company dealing with bias problems and trade-offs. It’s a challenge for the entire AI community.
- “Bias is a huge industry-wide problem that no one has a great, foolproof answer to,” Miles Brundage, the head of policy research at OpenAI, told me. “So a lot of the work right now is just being transparent and upfront with users about the remaining limitations.”
- In February, before DALL-E 2 was released, OpenAI invited 23 external researchers to “red team” it — engineering-speak for trying to find as many flaws and vulnerabilities in it as possible, so the system could be improved. One of the main suggestions the red team made was to limit the initial release to only trusted users.
- To its credit, OpenAI adopted this suggestion. For now, only about 400 people (a mix of OpenAI’s employees and board members, plus hand-picked academics and creatives) get to use DALL-E 2, and only for non-commercial purposes.
- That’s a change from how OpenAI chose to deploy GPT-3, a text generator hailed for its potential to enhance our creativity. Given a phrase or two written by a human, it can add on more phrases that sound uncannily human-like. But it’s shown bias against certain groups, like Muslims, whom it disproportionately associates with violence and terrorism. OpenAI knew about the bias problems but released the model anyway to a limited group of vetted developers and companies, who could use GPT-3 for commercial purposes.
- Last year, I asked Sandhini Agarwal, a researcher on OpenAI’s policy team, whether it makes sense that GPT-3 was being probed for bias by scholars even as it was released to some commercial actors. She said that going forward, “That’s a good thing for us to think about. You’re right that, so far, our strategy has been to have it happen in parallel. And maybe that should change for future models.”
- The fact that the deployment approach has changed for DALL-E 2 seems like a positive step. Yet, as DALL-E 2’s “Risks and Limitations” document acknowledges, “even if the Preview itself is not directly harmful, its demonstration of the potential of this technology could motivate various actors to increase their investment in related technologies and tactics.”
- And you’ve got to wonder: Is that acceleration a good thing, at this stage? Do we really want to be building and launching these models now, knowing it can spur others to release their versions even quicker?
- Some experts argue that since we know there are problems with the models and we don’t know how to solve them, we should give AI ethics research time to catch up to the advances and address some of the problems, before continuing to build and release new tech.
- Helen Ngo, an affiliated researcher with the Stanford Institute for Human-Centered AI, says one thing we desperately need is standard metrics for bias. A bit of work has been done on measuring, say, how likely certain attributes are to be associated with certain groups. “But it’s super understudied,” Ngo said. “We haven’t really put together industry standards or norms yet on how to go about measuring these issues” — never mind solving them.
- OpenAI’s Brundage told me that letting a limited group of users play around with an AI model allows researchers to learn more about the issues that would crop up in the real world. “There’s a lot you can’t predict, so it’s valuable to get in contact with reality,” he said.
- That’s true enough, but since we already know about many of the problems that repeatedly arise in AI, it’s not clear that this is a strong enough justification for launching the model now, even in a limited way.
- Brundage also noted another motivation at OpenAI: competition. “Some of the researchers internally were excited to get this out in the world because they were seeing that others were catching up,” he said.
- That spirit of competition is a natural impulse for anyone involved in creating transformative tech. It’s also to be expected in any organization that aims to make a profit. Being first out of the gate is rewarded, and those who finish second are rarely remembered in Silicon Valley.
- As the team at Anthropic, an AI safety and research company, put it in a recent paper, “The economic incentives to build such models, and the prestige incentives to announce them, are quite strong.”
- But it’s easy to see how these incentives may be misaligned for producing AI that truly benefits all of humanity. Rather than assuming that other actors will inevitably create and deploy these models, so there’s no point in holding off, we should ask the question: How can we actually change the underlying incentive structure that drives all actors?
- The Anthropic team offers several ideas. One of their observations is that over the past few years, a lot of the splashiest AI research has been migrating from academia to industry. To run large-scale AI experiments these days, you need a ton of computing power — more than 300,000 times what you needed a decade ago — as well as top technical talent. That’s both expensive and scarce, and the resulting cost is often prohibitive in an academic setting.
- So one solution would be to give more resources to academic researchers; since they don’t have a profit incentive to commercially deploy their models quickly the same way industry researchers do, they can serve as a counterweight. Specifically, countries could develop national research clouds to give academics access to free, or at least cheap, computing power; there’s already an existing example of this in Compute Canada, which coordinates access to powerful computing resources for Canadian researchers.
- The Anthropic team also recommends exploring regulation that would change the incentives. “To do this,” they write, “there will be a combination of soft regulation (e.g., the creation of voluntary best practices by industry, academia, civil society, and government), and hard regulation (e.g., transferring these best practices into standards and legislation).”
- Although some good new norms have been adopted voluntarily within the AI community in recent years — like publishing “model cards,” which document a model’s risks, as OpenAI did for DALL-E 2 — the community hasn’t yet created repeatable standards that make it clear how developers should measure and mitigate those risks.
- “This lack of standards makes it both more challenging to deploy systems, as developers may need to determine their own policies for deployment, and it also makes deployments inherently risky, as there’s less shared knowledge about what ‘safe’ deployments look like,” the Anthropic team writes. “We are, in a sense, building the plane as it is taking off.”
- Explanatory journalism is a public good
- At Vox, we believe that everyone deserves access to information that helps them understand and shape the world they live in. That's why we keep our work free.   Support our mission and help keep Vox free for all by making a financial contribution to Vox today.
- $95/year
- $120/year
- $250/year
- $350/year
- We accept credit card, Apple Pay, and
              

                Google Pay. You can also contribute via

URL: https://www.technologyreview.com/2021/01/05/1015754/avocado-armchair-future-ai-openai-deep-learning-nlp-gpt3-computer-vision-common-sense/
- OpenAI has extended GPT-3 with two new models that combine NLP with image recognition to give its AI a better understanding of everyday concepts.
- With GPT-3, OpenAI showed that a single deep-learning model could be trained to use language in a variety of ways simply by throwing it vast amounts of text. It then showed that by swapping text for pixels, the same approach could be used to train an AI to complete half-finished images. GPT-3 mimics how humans use words; Image GPT-3 predicts what we see.
- Now OpenAI has put these ideas together and built two new models, called DALL·E and CLIP, that combine language and images in a way that will make AIs better at understanding both words and what they refer to.
- “We live in a visual world,” says Ilya Sutskever, chief scientist at OpenAI. “In the long run, you’re going to have models which understand both text and images. AI will be able to understand language better because it can see what words and sentences mean.”
- For all GPT-3’s flair, its output can feel untethered from reality, as if it doesn’t know what it’s talking about. That’s because it doesn’t. By grounding text in images, researchers at OpenAI and elsewhere are trying to give language models a better grasp of the everyday concepts that humans use to make sense of things.
- DALL·E and CLIP come at this problem from different directions. At first glance, CLIP (Contrastive Language-Image Pre-training) is yet another image recognition system. Except that it has learned to recognize images not from labeled examples in curated data sets, as most existing models do, but from images and their captions taken from the internet. It learns what’s in an image from a description rather than a one-word label such as “cat” or “banana.”
- The AI is the largest language model ever created and can generate amazing human-like text on demand but won't bring us closer to true intelligence.
- CLIP is trained by getting it to predict which caption from a random selection of 32,768 is the correct one for a given image. To work this out, CLIP learns to link a wide variety of objects with their names and the words that describe them. This then lets it identify objects in images outside its training set. Most image recognition systems are trained to identify certain types of object, such as faces in surveillance videos or buildings in satellite images. Like GPT-3, CLIP can generalize across tasks without additional training. It is also less likely than other state-of-the-art image recognition models to be led astray by adversarial examples, which have been subtly altered in ways that typically confuse algorithms even though humans might not notice a difference.
- Instead of recognizing images, DALL·E (which I’m guessing is a WALL·E/Dali pun) draws them. This model is a smaller version of GPT-3 that has also been trained on text-image pairs taken from the internet. Given a short natural-language caption, such as “a painting of a capybara sitting in a field at sunrise” or “a cross-section view of a walnut,” DALL·E generates lots of images that match it: dozens of capybaras of all shapes and sizes in front of orange and yellow backgrounds; row after row of walnuts (though not all of them in cross-section).
- The results are striking, though still a mixed bag. The caption “a stained glass window with an image of a blue strawberry” produces many correct results but also some that have blue windows and red strawberries. Others contain nothing that looks like a window or a strawberry. The results showcased by the OpenAI team in a blog post have not been cherry-picked by hand but ranked by CLIP, which has selected the 32 DALL·E images for each caption that it thinks best match the description.
- “Text-to-image is a research challenge that has been around a while,” says Mark Riedl, who works on NLP and computational creativity at the Georgia Institute of Technology in Atlanta. “But this is an impressive set of examples.”
- To test DALL·E’s ability to work with novel concepts, the researchers gave it captions that described objects they thought it would not have seen before, such as “an avocado armchair” and “an illustration of a baby daikon radish in a tutu walking a dog.” In both these cases, the AI generated images that combined these concepts in plausible ways.
- The armchairs in particular all look like chairs and avocados. “The thing that surprised me the most is that the model can take two unrelated concepts and put them together in a way that results in something kind of functional,” says Aditya Ramesh, who worked on DALL·E. This is probably because a halved avocado looks a little like a high-backed armchair, with the pit as a cushion. For other captions, such as “a snail made of harp,” the results are less good, with images that combine snails and harps in odd ways.
- DALL·E is the kind of system that Riedl imagined submitting to the Lovelace 2.0 test, a thought experiment that he came up with in 2014. The test is meant to replace the Turing test as a benchmark for measuring artificial intelligence. It assumes that one mark of intelligence is the ability to blend concepts in creative ways. Riedl suggests that asking a computer to draw a picture of a man holding a penguin is a better test of smarts than asking a chatbot to dupe a human in conversation, because it is more open-ended and less easy to cheat.
- “The real test is seeing how far the AI can be pushed outside its comfort zone,” says Riedl.
- “The ability of the model to generate synthetic images out of rather whimsical text seems very interesting to me,” says Ani Kembhavi at the Allen Institute for Artificial Intelligence (AI2), who has also developed a system that generates images from text. “The results seems to obey the desired semantics, which I think is pretty impressive.” Jaemin Cho, a colleague of Kembhavi’s, is also impressed: “Existing text-to-image generators have not shown this level of control drawing multiple objects or the spatial reasoning abilities of DALL·E,” he says.
- Yet DALL·E already shows signs of strain. Including too many objects in a caption stretches its ability to keep track of what to draw. And rephrasing a caption with words that mean the same thing sometimes yields different results. There are also signs that DALL·E is mimicking images it has encountered online rather than generating novel ones.
- “I am a little bit suspicious of the daikon example, which stylistically suggests it may have memorized some art from the internet,” says Riedl. He notes that a quick search brings up a lot of cartoon images of anthropomorphized daikons. “GPT-3, which DALL·E is based on, is notorious for memorizing,” he says.
- Still, most AI researchers agree that grounding language in visual understanding is a good way to make AIs smarter.
- “The future is going to consist of systems like this,” says Sutskever. “And both of these models are a step toward that system.”
- “I have suddenly switched my views on whether these things are going to be more intelligent than us.”
- The narrative around cheating students doesn’t tell the whole story. Meet the teachers who think generative AI could actually make learning better.
- Hinton will be speaking at EmTech Digital on Wednesday.
- Large language models are full of security vulnerabilities, yet they’re being embedded into tech products on a vast scale.
- Discover special offers, top stories,
            upcoming events, and more.
- Thank you for submitting your email!
- It looks like something went wrong.
- We’re having trouble saving your preferences.
                Try refreshing this page and updating them one
                more time. If you continue to get this message,
                reach out to us at
                customer-service@technologyreview.com with a list of newsletters you’d like to receive.
- 
- © 2023 MIT Technology Review

URL: https://techcrunch.com/2022/07/22/commercial-image-generating-ai-raises-all-sorts-of-thorny-legal-issues/
- This week, OpenAI granted users of its image-generating AI system, DALL-E 2, the right to use their generations for commercial projects, like illustrations for children’s books and art for newsletters. The move makes sense, given OpenAI’s own commercial aims — the policy change coincided with the launch of the company’s paid plans for DALL-E 2. But it raises questions about the legal implications of AI like DALL-E 2, trained on public images around the web, and their potential to infringe on existing copyrights.
- DALL-E 2 “trained” on approximately 650 million image-text pairs scraped from the internet, learning from that dataset the relationships between images and the words used to describe them. But while OpenAI filtered out images for specific content (e.g. pornography and duplicates) and implemented additional filters at the API level, for example for prominent public figures, the company admits that the system can sometimes create works that include trademarked logos or characters. See:
- SpongeBob SquarePants shopping for a TV in Best Buy pic.twitter.com/9TMUhVAllU
- — Brian Cronin 🌎🌍🌏🌡📈 (@briangcronin) July 12, 2022
- 
- “OpenAI will evaluate different approaches to handle potential copyright and trademark issues, which may include allowing such generations as part of ‘fair use’ or similar concepts, filtering specific types of content, and working directly with copyright [and] trademark owners on these issues,” the company wrote in an analysis published prior to DALL-E 2’s beta release on Wednesday.
- It’s not just a DALL-E 2 problem. As the AI community creates open source implementations of DALL-E 2 and its predecessor, DALL-E, both free and paid services are launching atop models trained on less-carefully filtered datasets. One, Pixelz.ai, which rolled out an image-generating app this week powered by a custom DALL-E model, makes it trivially easy to create photos showing various Pokémon and Disney characters from movies like Guardians of the Galaxy and Frozen.
- When contacted for comment, the Pixelz.ai team told TechCrunch that they’ve filtered the model’s training data for profanity, hate speech and “illegal activities” and block users from requesting those types of images at generation time. The company also said that it plans to add a reporting feature that will allow people to submit images that violate the terms of service to a team of human moderators. But where it concerns intellectual property (IP), Pixelz.ai leaves it to users to exercise “responsibility” in using or distributing the images they generate — grey area or no.
- “We discourage copyright infringement both in the dataset and our platform’s terms of service,” the team told TechCrunch. “That being said, we provide an open text input and people will always find creative ways to abuse a platform.”
- An image of Rocket Racoon from Disney’s/Marvel’s Guardians of the Galaxy, generated by Pixelz.ai’s system. Image Credits: Pixelz.ai
- An image of Rocket Racoon from Disney’s/Marvel’s Guardians of the Galaxy, generated by Pixelz.ai’s system. Image Credits: Pixelz.ai
- Bradley J. Hulbert, a founding partner at law firm MBHB and an expert in IP law, believes that image-generating systems are problematic from a copyright perspective in several aspects. He noted that artwork that’s “demonstrably derived” from a “protected work” — i.e. a copyrighted character — has generally been found by the courts to be infringing, even if additional elements were added. (Think an image of a Disney princess walking through a gritty New York neighborhood.) In order to be shielded from copyright claims, the work must be “transformative” — in other words, changed to such a degree that the IP isn’t recognizable.
- “If a Disney princess is recognizable in an image generated by DALL-E 2, we can safely assume that The Walt Disney Co. will likely assert that the DALL-E 2 image is a derivative work and an infringement of its copyrights on the Disney princess likeness,” Hulbert told TechCrunch via email. “A substantial transformation is also a factor considered when determining whether a copy constitutes ‘fair use.’ But, again, to the extent a Disney princess is recognizable in a later work, assume that Disney will assert later work is a copyright infringement.”
- Of course, the battle between IP holders and alleged infringers is hardly new, and the internet has merely acted as an accelerant. In 2020, Warner Bros. Entertainment, which owns the right to film depictions of the Harry Potter universe, had certain fan art removed from social media platforms including Instagram and Etsy. A year earlier, Disney and Lucasfilm petitioned Giphy to take down GIFs of “Baby Yoda.”
- But image-generating AI threatens to vastly scale the problem by lowering the barrier to entry. The plights of large corporations aren’t likely to garner sympathy (nor should they), and their efforts to enforce IP often backfire in the court of public opinion. On the other hand, AI-generated artwork that infringes on, say, an independent artist’s characters could threaten a livelihood.
- The other thorny legal issue around systems like DALL-E 2 pertains to the content of their training datasets. Did companies like OpenAI violate IP law by using copyrighted images and artwork to develop their system? It’s a question that’s already been raised in the context of Copilot, the commercial code-generating tool developed jointly by OpenAI and GitHub. But unlike Copilot, which was trained on code that GitHub might have the right to use for the purpose under its terms of service (according to one legal analysis), systems like DALL-E 2 source images from countless public websites.
- Ladies and gentleman, I got my Dall-E 2 invite! 😁😁 here are some stills from Homer Simpson in Stranger Things before I start tweeting the amazing stuff #dalle2 pic.twitter.com/PHPI6n9yJk
- — limb0wl 🦉👾 (@limb0wl) July 5, 2022
- 
- As Dave Gershgorn points out in a recent feature for The Verge, there isn’t a direct legal precedent in the U.S. that upholds publicly available training data as fair use.
- One potentially relevant case involves a Lithuanian company called Planner 5D. In 2020, the firm sued Meta (then Facebook) for reportedly stealing thousands of files from Planner 5D’s software, which were made available through a partnership with Princeton to contestants of Meta’s 2019 Scene Understanding and Modeling challenge for computer vision researchers. Planner 5D claimed Princeton, Meta and Oculus, Meta’s VR-focused hardware and software division, could have benefited commercially from the training data that was taken from it.
- The case isn’t scheduled to go to trial until March 2023. But last April, the U.S. district judge overseeing the case denied motions by then-Facebook and Princeton to dismiss Planner 5G’s allegations.
- Unsurprisingly, rightsholders aren’t swayed by the fair use argument. A spokesperson for Getty Images told IEEE Spectrum in an article that there are “big questions” to be answered about “the rights to the imagery and the people, places, and objects within the imagery that [models like DALL-E 2] were trained on.” Association of Illustrators CEO Rachel Hill, who was also quoted in the piece, brought up the issue of compensation for images in training data.
- Hulbert believes it’s unlikely a judge will see the copies of copyrighted works in training datasets as fair use — at least in the case of commercial systems like DALL-E 2. He doesn’t think it’s out of the question that IP holders could come after companies like OpenAI at some point and demand that they license the images used to train their systems.
- Welp, didn’t take long for DALL-E to pivot to paid model and DOR has summed up pretty succinctly how that’s a problem pic.twitter.com/8NLSwS437L
- — Dan Kelly (@dananthonykelly) July 22, 2022
- 
- “The copies … constitute infringement of the copyrights of the original authors. And infringers are liable to the copyright owners for damages,” he added. “[If] DALL-E (or DALL-E 2) and its partners make a copy of a protected work, and the copy was neither approved by the copyright owner nor fair use, the copying constitutes copyright infringement.”
- Interestingly, the U.K. is exploring legislation that would remove the current requirement that systems trained through text and data mining, like DALL-E 2, be used strictly for non-commercial purposes. While copyright holders could still ask for payment under the proposed regime by putting their works behind a paywall, it would make the U.K.’s policy one of the most liberal in the world.
- The U.S. seems unlikely to follow suit, given the lobbying power of IP holders in the U.S. The issue seems likely to play out in a future lawsuit instead. But time will tell.

URL: https://www.npr.org/2022/07/20/1112331013/dall-e-ai-art-beta-test
- Bobby Allyn
- DALL-E2, the AI image tool, generated these images of a giraffe shopping in a grocery store.
                
                    
                    Image generated by AI/DALL-E2 /OpenAI
                    
                
hide caption
- When the Silicon Valley research lab OpenAI unveiled DALL-E earlier this year, it dazzled the internet.
- The tool is seen as one of the most advanced artificial intelligence systems for creating images in the world. Type a description, and DALL-E instantly produces professional-looking art or hyperrealistic photographs.
- "It's incredibly powerful," said Hany Farid, a digital forensics expert at the University of California, Berkeley. "It takes the deepest, darkest recesses of your imagination and renders it into something that is eerily pertinent."
- DALL-E — a name meant to evoke the Pixar film WALL-E and the Surrealist painter Salvador Dalí — is not available to the public. It has been used only by a vetted group of testers — mostly researchers, academics, journalists and artists.
- But on Wednesday, OpenAI announced it would invite more people to the party. The company says it plans to let in up to 1 million people from its waitlist over the coming weeks, as it moves from its research phase into its beta stage.
- It is unclear if DALL-E will ever be fully available to the public, but the expansion is expected to be a significant test for the platform, with many researchers watching out for how the technology will be abused.
- An image generated by OpenAI's DALL-E2 with the prompt "Shrek's older lobbyist brother visits Washington."
                
                    
                    Image generated by AL-DALL-E2
                    
                
hide caption
- An image generated by OpenAI's DALL-E2 with the prompt "Shrek's older lobbyist brother visits Washington."
- OpenAI has kept DALL-E closely guarded out of fear that bad actors could use the powerful tool to spread disinformation. Imagine someone trying to use it to fabricate images of the war in Ukraine, or creating realistic images of natural disasters that never occurred.
- On top of that, generating an image with the platform is so energy intensive that company officials worried its servers would melt down if too many people tried to use it at once.
- An image generated by OpenAI's DALL-E2 with the prompt: "A photograph of three chihuahuas sitting on a yellow cab in New York City"
                
                    
                    Image generated by AI/DALL-E2
                    
                
hide caption
- An image generated by OpenAI's DALL-E2 with the prompt: "A photograph of three chihuahuas sitting on a yellow cab in New York City"
- The exclusivity created buzz, as droves of people tried to get their hands on the cutting-edge technology, the latest version of which is called DALL-E2.
- The company started a waitlist, which quickly ballooned. The excitement also spurred a free imitation, DALL-E mini. Its renderings, while far less impressive, helped to turn AI image generation into a hobby for some. Recently, DALL-E mini changed its name to Craiyon to avoid confusion. It is not affiliated with OpenAI.
- A image generated by DALLE-2 with the prompt "man with an ambiguous expression holding a martini in the style of Picasso"
                
                    
                    Image generated by AI/DALL-E2
                    
                
hide caption
- A image generated by DALLE-2 with the prompt "man with an ambiguous expression holding a martini in the style of Picasso"
- Joanne Jang, the product manager of DALL-E, says the company is still refining its content rules, which now prohibit what you might expect: making violent, pornographic and hateful content. It also bans images depicting ballot boxes and protests, or any image that "may be used to influence the political process or to campaign."
- DALL-E also bans depictions of real people, and it anticipates establishing more guardrails as its researchers learn how users interact with the system.
- A photo created by OpenAI's DALL-E2 using the prompt: "Painting of Sasquatch driving a convertible in the style of Salvador Dalí.
                
                    
                    Image generated by AI/DALL-E2
                    
                
hide caption
- A photo created by OpenAI's DALL-E2 using the prompt: "Painting of Sasquatch driving a convertible in the style of Salvador Dalí.
- "Right now we think that there are a lot of unknown unknowns that we would like to have a better handle on," Jang said. "We expect to ramp up and rapidly invite more and more people as we gain better confidence."
- Experts say while image creation algorithms have existed for some time, the speed, precision and breadth of DALL-E represents a remarkable advancement in the field.
- "What DALL-E is doing is capturing some element of human imagination. It's not actually that different than how humans can read a book and imagine things, but it's being able to capture that intelligence with an algorithm," said Phillip Isola, a computer science professor at MIT who previously worked at Open AI but is no longer affiliated. "Of course, there are plenty of concerns about how this kind of technology can be misused."
- An image created by DALL-E2 with the prompt: "Photograph of a young boy and his Golden Retriever in the woods of Montana on a foggy day."
                
                    
                    Image generated by AI/DALL-E2
                    
                
hide caption
- An image created by DALL-E2 with the prompt: "Photograph of a young boy and his Golden Retriever in the woods of Montana on a foggy day."
- OpenAI was founded in 2015 by Elon Musk, who left the board three years later, and Sam Altman, a protégé of Peter Thiel, who was an early investor. It is financially backed by Microsoft and competing in the race to develop the best AI technology against Google, Amazon and Facebook. They are all also building AI tools using similar systems.
- Using DALL-E is simple: You type it what you want to see and seconds later a panel of four images appears.
- The possibilities seem endless. You can ask for images that look like photographs or the work of Picasso; images that look like 3D renderings; photos conveying an aesthetic like "post-apocalyptic" or "cyberpunk."
- An image created with DALL-E2 with the prompt: "Retro vaporwave cyberpunk dinosaur wearing a tough jacket, character design, hyper detailed, art station," which was conceived by visual artist Don Allen III.
                
                    
                    Image generated by AI/DALL-E2
                    
                
hide caption
- DALL-E will not produce anything if a description of an image violates its content rules. Instead, it will warn users that their accounts could be suspended if they repeatedly try to break the system's rules.
- NPR was granted access to DALL-E2 in its research phase. Users were limited to making 50 images a day to mitigate the power strain on the company's servers.
- In its beta phase announced on Wednesday, OpenAI will allow people to create 50 images during their first month for free. After that, they may create 15 a month. Once that limit is reached, people can pay $15 for another 115 images.
- DALL-E, its researchers like to say, rewards specificity: the more precise a description, the better the image, even abstract ideas can produce surprisingly vivid results.
- "It's not often that we get to give users a product experience that feels like magic," Jang said.
- An image created with DALL-E2 with the prompt: "Film still, squirrel playing cards with a hat on, low angle, shot from below, worm's eye view"
                
                    
                    Image generated by AI/DALL-E2
                    
                
hide caption
- An image created with DALL-E2 with the prompt: "Film still, squirrel playing cards with a hat on, low angle, shot from below, worm's eye view"
- "DALL-E knows a lot about everything, so the deeper your knowledge of the requisite jargon, the more detailed the results," wrote London-based art curator and programmer Guy Parsons, who put together an 81-page prompt book for the system.
- Image generated using DALL-E2 with the prompt: "Still image of a fancy salmon dinner."
                
                    
                    Image generated by AI/DALL-E2
                    
                
hide caption
- Image generated using DALL-E2 with the prompt: "Still image of a fancy salmon dinner."
- Jang with OpenAI said while many of the people testing the service have used it for digital art and design, she has heard from Alzheimer's researchers who say it could be used to help people with the disease regain memories. Surgeons have used it to show patients what their bodies might look like after surgery. And it has helped chefs dream up new dishes and even engaged couples ideas for marriage rings.
- An image generated with DALL-E2 with the prompt: "A photo of a gold ring with the letters 'BA' engraved."
                
                    
                    Image generated by AI/DALL-E2
                    
                
hide caption
- An image generated with DALL-E2 with the prompt: "A photo of a gold ring with the letters 'BA' engraved."
- DALL-E does not rely on the internet for its raw data. Instead, its researchers have fed the system more than 650 million images that the company has licensed, though company officials are tight-lipped about the details.
- DALL-E's algorithm is trained on the thousands of images it has ingested and text captions associated with the images, and it makes rapid-fire associations. The art it creates is not a mishmash of many images. Rather, it is a unique image based on a sophisticated AI model known as a "neural network," because it makes connections in ways that mimic the human brain.
- "It's kind of like showing a child hundreds of millions of flash cards," Jang said. "And if you show a child enough flash cards, or multiple images of people doing yoga, for instance, and tell them that it's yoga, at some point, they'll learn that yoga involves certain poses, a yoga mat, relaxed Zen impressions.
- "That's how DALL-E has learned about concepts and how concepts relate to each other," she said.
- Some academic researchers have criticized OpenAI for keeping its dataset secret, saying that makes it impossible for outside experts to assess what might be driving harmful stereotypes that start appearing in the images — which is something researchers at the company say they are trying to combat internally.
- A view of the recent image results on OpenAI's DALL-E2 when searching for "nurse."
                
                    
                    Image generated by AI/DALL-E2
                    
                
hide caption
- A view of the recent image results on OpenAI's DALL-E2 when searching for "nurse."
- Generating an image for "lawyer" returned images of mostly white men during the early stages of the technology. When a user tried to generate an image for "nurse," DALL-E depicted only women. Jang also said they noticed when sexual images were removed from the dataset, there was a significant drop in the representation of women in DALL-E images.
- Now, Jang says, the algorithm has been recalibrated so that there is more gender and racial representation in its image results.
- There are ways to evade some of DALL-E's content rules. For example, images of blood are banned. But users can generate similar images by typing "red liquid."
- A image generated with DALL-E2 with the prompt: "A man in with red liquid on his face."
                
                    
                    Image generated by AI/DALL-E2
                    
                
hide caption
- A image generated with DALL-E2 with the prompt: "A man in with red liquid on his face."
- The company says it is monitoring the tool and will terminate the accounts of those trying to abuse the technology.
- Farid, the Berkeley researcher, said with 1 million new users joining, deeply troubling examples are really just a matter of time.
- "This could be disinformation on steroids," he said. "People are going to find ways around the rules."
- Sponsor Message
- Become an NPR sponsor

URL: https://www.bloomberg.com/opinion/articles/2022-04-21/openai-project-risks-bias-without-more-scrutiny
- To continue, please click the box below to let us know you're not a robot.
- Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our Terms of
                Service and Cookie Policy.
- For inquiries related to this message please contact
            our support team and provide the reference ID below.

URL: https://www.dezeen.com/2022/04/21/openai-dall-e-2-unseen-images-basic-text-technology/
- Research company OpenAI has developed a programme that can turn simple text instructions into high-quality images.
- Named DALL-E 2, the programme uses artificial intelligence (AI) to create realistic images or artworks from a text description written in natural language.
- The descriptions can be quite complex, incorporating actions, art styles and multiple subjects. Some of the examples on OpenAI's blog include "an astronaut lounging in a tropical resort in space in a vaporwave style" and "teddy bears working on new AI research underwater with 1990s technology".
- DALL-E 2 builds on OpenAI's previous tool, DALL-E, which launched in January 2021. The new iteration produces more astonishing results, thanks to higher-resolution imagery, greater textual comprehension, faster processing and some new capabilities.
- Named after the Pixar robot WALL-E and the artist Salvador Dalí, DALL-E is a type of neural network – a computing system loosely modelled on the connected neurons in a biological brain.
- The neural network has been trained on images and their text descriptions to understand the relationship between objects.
- "Through deep learning it not only understands individual objects like koala bears and motorcycles but learns from relationships between objects," said OpenAI.
- "And when you ask DALL-E for an image of a koala bear riding a motorcycle, it knows how to create that or anything else with a relationship to another object or action."
- DALL-E 2 provides several image alternatives for each text prompt. An additional capability added with DALL-E 2 is to use the same natural language descriptions to edit and retouch existing photos.
- This feature, which OpenAI calls "in-painting", works like a more sophisticated version of Photoshop's content-aware fill, realistically adding or removing elements from a selected section of the image while taking into account shadows, reflections and textures.
- For instance, the examples on the OpenAI blog show a sofa added to various spots in a photograph of an empty room.
- OpenAI says that the DALL-E project not only allows people to express themselves visually but also helps researchers understand how advanced AI systems see and understand our world.
- "This is a critical part of developing AI that's useful and safe," said OpenAI.
- Originally founded as a non-profit by high-profile technology figures including Elon Musk, OpenAI is dedicated to developing AI for long-term positive human impact and curbing its potential dangers.
- To that end, DALL-E 2 is not currently being made available to the public. OpenAI identifies the application could be dangerous if it were used to create deceptive content, similar to current "deepfakes", or otherwise harmful imagery.
- It also recognises that AI inherits biases from its training and so can end up reinforcing social stereotypes.
- While OpenAI refines its safety measures, DALL-E is only shared with a select few users for testing. Already, there is a content policy barring users from making any violent or hate imagery, as well as anything "not G-rated" or any political content.
- This is enforced by filters and both automated and human monitoring systems.
- DALL-E's ability to generate such images in the first place would be limited. All explicit or violent content was removed from its training data, so it has had blissfully little exposure to these concepts.
- OpenAI was started by Musk, Y Combinator's Sam Altman and other backers in late 2015, although Musk has since resigned from the board. In 2019 it transitioned to being a for-profit company, apparently to secure more funding, although its parent company remains a non-profit.
- One of OpenAI's other projects is Dactyl, which involved training a robot hand to nimbly manipulate objects using human-like movements it taught itself.
- Our most popular newsletter, formerly known as Dezeen Weekly. Sent every Thursday and featuring a selection of the best reader comments and most talked-about stories. Plus occasional updates on Dezeen’s services and breaking news.
- Sent every Tuesday and containing a selection of the most important news highlights. Plus occasional updates on Dezeen’s services and breaking news.
- A daily newsletter containing the latest stories from Dezeen.
- Daily updates on the latest design and architecture vacancies advertised on Dezeen Jobs. Plus occasional news.
- Weekly updates on the latest design and architecture vacancies advertised on Dezeen Jobs. Plus occasional news.
- News about our Dezeen Awards programme, including entry deadlines and announcements. Plus occasional updates.
- News from Dezeen Events Guide, a listings guide covering the leading design-related events taking place around the world. Plus occasional updates.
- We will only use your email address to send you the newsletters you have requested. We will never give your details to anyone else without your consent. You can unsubscribe at any time by clicking on the unsubscribe link at the bottom of every email, or by emailing us at [email protected].
- For more details, please see our privacy notice.
- 
- You will shortly receive a welcome email so please check your inbox.
- You can unsubscribe at any time by clicking the link at the bottom of every newsletter.
- "The traditional boundaries between client and architect may ...
- Less than a week left to enter Dezeen's £1,000 AI artwork ...
- "What happened to the concept of human beings collaborating ...
- Dezeen Agenda features "world's first AI designer"
- This week Dezeen launched an AI artwork competition
- Dezeen launches AI artwork competition for AI editorial ...
- Studio Snoop presents "world's first AI designer" at Milan ...
- Dezeen and Autodesk present live talk on role of data and AI
- Visit our comments page | Read our
comments policy
- Subscribe to our newsletters
- Our most popular newsletter, formerly known as Dezeen Weekly. Sent every Thursday and featuring a selection of the best reader comments and most talked-about stories. Plus occasional updates on Dezeen’s services and breaking news.
- Sent every Tuesday and containing a selection of the most important news highlights. Plus occasional updates on Dezeen’s services and breaking news.
- A daily newsletter containing the latest stories from Dezeen.
- Daily updates on the latest design and architecture vacancies advertised on Dezeen Jobs. Plus occasional news.
- Weekly updates on the latest design and architecture vacancies advertised on Dezeen Jobs. Plus occasional news.
- News about our Dezeen Awards programme, including entry deadlines and announcements. Plus occasional updates.
- News from Dezeen Events Guide, a listings guide covering the leading design-related events taking place around the world. Plus occasional updates.
- We will only use your email address to send you the newsletters you have requested. We will never give your details to anyone else without your consent. You can unsubscribe at any time by clicking on the unsubscribe link at the bottom of every email, or by emailing us at [email protected].
- For more details, please see our privacy notice.
- You will shortly receive a welcome email so please check your inbox.
- You can unsubscribe at any time by clicking the link at the bottom of every newsletter.
- Please share your location to continue.
- Check our help guide for more info.
- Please share your location to continue.
- Check our help guide for more info.
- Please share your location to continue.
- Check our help guide for more info.
- Please share your location to continue.
- Check our help guide for more info.

URL: https://www.vox.com/recode/23405149/ai-art-dall-e-colonialism-artificial-intelligence
- We use cookies and other tracking technologies to improve your browsing experience on our site, show personalized content and targeted ads, analyze site traffic, and understand where our audiences come from. To learn more or opt-out, read our Cookie Policy. Please also read our Privacy Notice and Terms of Use, which became effective December 20, 2019.
- By choosing I Accept, you consent to our use of cookies and other tracking technologies.
- DALL-E and other models keep making art that ignores traditions from the rest of the world.
- In late September, OpenAI made its DALL-E 2 AI art generator widely available to the public, allowing anyone with a computer to make one of those striking, slightly bizarre images that seem to be floating around the internet more and more these days. DALL-E 2 is by no means the first AI art generator to open to the public (the competing AI art models Stable Diffusion and Midjourney also launched this year), but it comes with a strong pedigree: Its cousin, the text-generating model known as GPT-3 — itself the subject of much intrigue and multiple gimmicky stories — was also developed by OpenAI.
- Last week, Microsoft announced it would be adding AI-generated art tools — powered by DALL-E 2 — to its Office software suite, and in June DALL-E 2 was used to design the cover of Cosmopolitan magazine. The most techno-utopian proponents of AI-generated art say it provides a democratization of art for the masses; the cynics among us would argue it’s copying human artists and threatening to end their careers. Either way, it seems clear that AI art is here, and its potential has only just begun to be explored.
- Naturally, I decided to try it.
- Get the best of Vox technology coverage, from essential reporting on Silicon Valley to the latest news about media, policy, and beyond.
- Check your inbox for a welcome email.
- Oops. Something went wrong. Please enter a valid email and try again.
- As I scrolled through examples of DALL-E’s work for inspiration (I had determined that my first attempt ought to be a masterpiece), it seemed to me that AI-generated art didn’t have any particular aesthetic other than, maybe, being a bit odd. There were pigs wearing sunglasses and floral shirts while riding motorcycles, raccoons playing tennis, and Johannes Vermeer’s Girl With a Pearl Earring, tweaked ever so slightly so as to replace the titular girl with a sea otter. But as I kept scrolling, I realized there is one unifying theme underlying every piece: AI art, more often than not, looks like Western art.
- “All AI is only backward-looking,” said Amelia Winger-Bearskin, professor of AI and the Arts at the University of Florida’s Digital Worlds Institute. “They can only look at the past, and then they can make a prediction of the future.”
- For an AI model (also known as an algorithm), the past is the data set it has been trained on. For an AI art model, that data set is art. And much of the fine art world is dominated by white, Western artists. This leads to AI-generated images that look overwhelmingly Western. This is, frankly, a little disappointing: AI-generated art, in theory, could be an incredibly useful tool for imagining a more equitable vision of art that looks very different from what we have come to take for granted. Instead, it stands to simply perpetuate the colonial ideas that drive our understanding of art today.
- To be clear, models like DALL-E 2 can be asked to generate art in the style of any artist; asking for an image with the modifier “Ukiyo-e,” for example, will create works that mimic Japanese woodblock prints and paintings. But users must include those modifiers; they are rarely, if ever, the default.
- Winger-Bearskin has seen the limits of AI art firsthand. When one of her students used images generated by Stable Diffusion to make a video of a nature scene, she realized the twilight backgrounds put out by the AI model looked oddly similar to the scenes painted by Disney animators in the 1950s and ‘60s — which themselves had been inspired by the French Rococo movement. “There are a lot of Disney films, and what he got back was something we see a lot of,” Winger-Bearskin told Recode. “There are so many things missing in those datasets. There are millions of night scenes from all over the world that we would never see.”
- AI bias is a notoriously difficult problem. Left unchecked, algorithms can perpetuate racist and sexist biases, and that bias extends to AI art as well: as Sigal Samuel wrote for Future Perfect in April, previous versions of DALL-E would spit out images of white men when asked to depict lawyers, for example, and depict all flight attendants as women. OpenAI has been working to mitigate these effects, fine-tuning its model to try to weed out stereotypes, though researchers still disagree on whether those measures have worked.
- But even if they work, the problem of artistic style will persist: If DALL-E manages to depict a world free of racist and sexist stereotypes, it would still do so in the image of the West.
- “You can’t fine-tune a model to be less Western if your dataset is mostly Western,” Yilun Du, a PhD student and AI researcher at MIT, told Recode. AI models are trained by scraping the internet for images, and Du thinks models made by groups based in the United States or Europe are likely predisposed to Western media. Some models made outside the United States, like ERNIE-ViLG, which was developed by the Chinese tech company Baidu, do a better job generating images that are more culturally relevant to their place of origin, but they come with issues of their own; as the MIT Technology Review reported in September, ERNIE-ViLG is better at producing anime art than DALL-E 2 but refuses to make images of Tiananmen Square.
- Because AI is backward-looking, it’s only able to make variations of images it has seen before. That, Du says, is why an AI model is unable to create an image of a plate sitting on top of a fork, even though it should conceivably understand each aspect of the request. The model has simply never seen an image of a plate on top of a fork, so it spits out images of forks on top of plates instead.
- Injecting more non-Western art into an existing dataset wouldn’t be a very helpful solution, either, because of the overwhelming prevalence of Western art on the internet. “It’s kind of like giving clean water to a tree that was fed with contaminated water for the last 25 years,” said Winger-Bearskin. “Even if it’s getting better water now, the fruit from that tree is still contaminated. Running that same model with new training data does not significantly change it.”
- Instead, creating a better, more representative AI model would require creating it from scratch — which is what Winger-Bearskin, who is a member of the Seneca-Cayuga Nation of Oklahoma and an artist herself, does when she uses AI to create art about the climate crisis.
- That’s a time-consuming process. “The hardest thing is making the data set,” said Du. Training an AI art generator requires millions of images, and Du said it would take months to create a data set that’s equally representative of all the art styles that can be found around the world.
- If there’s an upside to the artistic bias inherent in most AI art models, perhaps it’s this: Like all good art, it exposes something about our society. Many modern art museums, Winger-Bearskin said, give more space to art made by people from underrepresented communities than they did in the past. But this art still only makes up a small fraction of what exists in museum archives.
- “An artist’s job is to talk about what’s going on in the world, to amplify issues so we notice them,” said Jean Oh, an associate research professor at Carnegie Mellon University’s Robotics Institute. AI art models are unable to provide commentary of their own — everything they produce is at the behest of a human — but the art they produce creates a sort of accidental meta-commentary that Oh thinks is worthy of notice. “It gives us a way to observe the world the way it is structured, and not the perfect world we want it to be.”
- That’s not to say that Oh believes more equitable models shouldn’t be created — they are important for circumstances where depicting an idealized world is helpful, like for children’s books or commercial applications, she told Recode — but rather that the existence of the imperfect models should push us to think more deeply about how we use them. Instead of simply trying to eliminate the biases as though they don’t exist, Oh said, we should take the time to identify and quantify them in order to have constructive discussions about their impacts and how to minimize them.
- “The main purpose is to help human creativity,” Oh said, who’s researching ways to create more intuitive human-AI interactions. “People want to blame the AI. But the final product is our responsibility.”
- This story was first published in the Recode newsletter. Sign up here so you don’t miss the next one!
- Explanatory journalism is a public good
- At Vox, we believe that everyone deserves access to information that helps them understand and shape the world they live in. That's why we keep our work free.   Support our mission and help keep Vox free for all by making a financial contribution to Vox today.
- $95/year
- $120/year
- $250/year
- $350/year
- We accept credit card, Apple Pay, and
              

                Google Pay. You can also contribute via
- Each week, we explore unique solutions to some of the world's biggest problems.
- Check your inbox for a welcome email.
- Oops. Something went wrong. Please enter a valid email and try again.

URL: https://www.engadget.com/dall-e-generative-ai-tracking-data-privacy-160034656.html
- In 1917, Marcel Duchamp submitted a sculpture to the Society of Independent Artists under a false name. Fountain was a urinal, bought from a toilet supplier, with the signature R. Mutt on its side in black paint. Duchamp wanted to see if the society would abide by its promise to accept submissions without censorship or favor. (It did not.) But Duchamp was also looking to broaden the notion of what art is, saying a ready-made object in the right context would qualify. In 1962, Andy Warhol would twist convention with Campbell’s Soup Cans, 32 paintings of soup cans, each one a different flavor. Then, as before, the debate raged about if something mechanically produced – a urinal, or a soup can (albeit hand-painted by Warhol) – counted as art, and what that meant.
- Now, the debate has been turned upon its head, as machines can mass-produce unique pieces of art on their own. Generative Artificial Intelligences (GAIs) are systems which create pieces of work that can equal the old masters in technique, if not in intent. But there is a problem, since these systems are trained on existing material, often using content pulled from the internet, from us. Is it right, then, that the AIs of the future are able to produce something magical on the backs of our labor, potentially without our consent or compensation?
- 
- The most famous GAI right now is DALL-E 2, Open AI’s system for creating “realistic images and art from a description in natural language.” A user could enter the phrase “teddy bears shopping for groceries in the style of Ukiyo-e,” and the model will produce pictures in that style. Similarly, ask for the bears to be shopping in Ancient Egypt and the images will look more like dioramas from a museum depicting life under the Pharaohs. To the untrained eye, some of these pictures look like they were drawn in 17th-century Japan, or shot at a museum in the 1980s. And these results are coming despite the technology still being at a relatively early stage.
- Open AI recently announced that DALL-E 2 would be made available to up to one million users as part of a large-scale beta test. Each user will be able to make 50 generations for free during their first month of use, and then 15 for every subsequent month. (A generation is either the production of four images from a single prompt, or the creation of three more if you choose to edit or vary something that’s already been produced.) Additional 115-credit packages can be bought for $15, and the company says more detailed pricing is likely to come as the product evolves. Crucially, users are entitled to commercialize the images produced with DALL-E, letting them print, sell or otherwise license the pictures borne from their prompts.
- These systems did not, however, develop an eye for a good picture in a vacuum, and each GAI has to be trained. Artificial Intelligence is, after all, a fancy term for what is essentially a way of teaching software how to recognize patterns. “You allow an algorithm to develop that can be improved through experience,” said Ben Hagag, head of research at Darrow, an AI startup looking to improve access to justice. “And by experience I mean examining and finding patterns in data.” “We say to the [system] ‘take a look at this dataset and find patterns,” which then go on to form a coherent view of the data at hand. “The model learns as a baby learns,” he said, so if a baby looked at a 1,000 pictures of a landscape, it would soon understand that the sky – normally oriented across the top of the image – would be blue while land is green.
- Hagag cited how Google built its language model by training a system on several gigabytes of text, from the dictionary to examples of the written word. “The model understood the patterns, how the language is built, the syntax and even the hidden structure that even linguists find hard to define,” Hagag said. Now that model is sophisticated enough that “once you give it a few words, it can predict the next few words you’re going to write.” In 2018, Google’s Ajit Varma told The Wall Street Journal that its smart reply feature had been trained on “billions of Gmail messages,” adding that initial tests saw options like ‘I Love You’ and ‘Sent from my iPhone’ offered up since they were so commonly seen in communications.
- Developers who do not have the benefit of access to a data set as vast as Google’s need to find data via other means. “Every researcher developing a language model first downloads Wikipedia then adds more,” Hagag said. He added that they are likely to pull down any, and every, piece of available data that they can find. The sassy tweet you sent a few years ago, or that sincere Facebook post, may have been used to train someone’s language model, somewhere. Even Open AI uses social media posts with WebText, a dataset which pulls text from outbound Reddit links which received at least three karma, albeit with Wikipedia references removed.
- Guan Wang, CTO of Huski, says that the pulling down of data is “very common.” “Open internet data is the go-to for the majority of AI model training nowadays,” he said. And that it’s the policy of most researchers to get as much data as they can. “When we look for speech data, we will get whatever speech we can get,” he added. This policy of more data-is-more is known to produce less than ideal results, and Ben Hagag cited Riley Newman, former head of data science at Airbnb, who said “better data beats more data,” but Hagag notes that often, “it’s easier to get more data than it is to clean it.”
- DALL-E may now be available to a million users, but it’s likely that people’s first experience of a GAI is with its less-fancy sibling. Craiyon, formerly DALL-E Mini, is the brainchild of French developer Boris Dayma, who started work on his model after reading Open AI’s original DALL-E paper. Not long after, Google and the AI development community HuggingFace ran a hackathon for people to build quick-and-dirty machine learning models. “I suggested, ‘Hey, let’s replicate DALL-E. I have no clue how to do that, but let’s do it,” said Dayma. The team would go on to win the competition, albeit with a rudimentary, rough-around-the-edges version of the system. “The image [it produced] was clear. It wasn’t great, but it wasn’t horrible,” he added. But unlike the full-fat DALL-E, Dayma’s team was focused on slimming the model down so that it could work on comparatively low-powered hardware.
- Dayma’s original model was fairly open about which image sets it would pull from, often with problematic consequences. “In early models, still in some models, you ask for a picture – for example mountains under the snow,” he said, “and then on top of it, the Shutterstock or Alamy watermark.” It’s something many AI researchers have found, with GAIs being trained on those image libraries public-facing image catalogs, which are covered in anti-piracy watermarks.
- Dayma said that the model had erroneously learned that high-quality landscape images typically had a watermark from one of those public photo libraries, and removed them from his model. He added that some early results also output not-safe-for-work responses, forcing him to make further refinements to his initial training set. Dayma added that he had to do a lot of the sorting through the data himself, and said that “a lot of the images on the internet are bad.”
- Got sent some moody Russian ruDall-E GAN images last week from my dev piotr, that had shutterstock logos generated in them, oh how we laughed....now looks like the real Dall-E is doing the same... pic.twitter.com/6A2yLFHelw
- But it’s not just Dayma who has noticed the regular appearance of a Shutterstock watermark, or something a lot like it, popping up in AI-generated art. Which begs the question, are people just ripping off Shutterstock’s public-facing library to train their AI? It appears that one of the causes is Google, which has indexed a whole host of watermarked Shutterstock images as part of its Conceptual Captions framework. Delve into the data, and you’ll see a list of image URLs which can be used to train your own AI model, thousands of which are from Shutterstock. Shutterstock declined to comment on the practice for this article.
- Several results from the bigger GAN models, like StyleGAN are even able to recreate the watermark on images from certain websites, namely @Shutterstock It looks like hardly anyone doing ML really cares about privacy or copyright at the moment pic.twitter.com/ADrKzzOzMH
- A Google spokesperson said that they don’t “believe this is an issue for the datasets we’re involved with.” They also quoted from this Creative Commons report, saying that “the use of works to train AI should be considered non-infringing by default, assuming that access to the copyright works was lawful at the point of input.” That is despite the fact that Shutterstock itself expressly forbids visitors to its site from using “any data mining, robots or similar data and/or image gathering and extraction methods in connection with the site or Shutterstock content.”
- https://t.co/j6uDEuFgMnYou got to love how the GAN has the shutterstick watermark trained it and tries hard to put it into the image. Also apparently a certain subset of images of horses have all the shutterstock address placed in the same position on the bottom. pic.twitter.com/I7iW1kcuYz
- Alex Cardinell, CEO at AI startup Article Forge, says that he sees no issue with models being trained on copyrighted texts, “so long as the material itself was lawfully acquired and the model does not plagiarize the material.” He compared the situation to a student reading the work of an established author, who may “learn the author’s styles or patterns, and later find applicable places to reuse those concepts.” He added that so long as a model isn’t “copying and pasting from their training data,” then it simply repeats a pattern that has appeared since the written word began.
- Dayma says that, at present, hundreds of thousands, if not millions of people are playing with his system on a daily basis. That all incurs a cost, both for hosting and processing, which he couldn’t sustain from his own pocket for very long, especially since it remains a “hobby.” Consequently, the site runs ads at the top and bottom of its page, between which you’ll get a grid of nine surreal images. “For people who use the site commercially, we could always charge for it,” he suggested. But he admitted his knowledge of US copyright law wasn’t detailed enough to be able to discuss the impact of his own model, or others in the space. This is the situation that Open AI also perhaps finds itself dealing with given that it is now allowing users to sell pictures created by DALL-E.
- The legal situation is not a particularly clear one, especially not in the US, where there have been few cases covering Text and Data Mining, or TDM. This is the technical term for the training of an AI by plowing through a vast trove of source material looking for patterns. In the US, TDM is broadly covered by Fair Use, which permits various forms of copying and scanning for the purposes of allowing access. This isn’t, however, a settled subject, but there is one case that people believe sets enough of a precedent to enable the practice.
- Authors Guild v. Google (2015) was brought by a body representing authors, which accused Google of digitizing printed works that were still held under copyright. The initial purpose of the work was, in partnership with several libraries, to catalog and database the texts to make research easier. Authors, however, were concerned that Google was violating copyright, and even if it wasn’t making the text of a still-copyrighted work available publicly, it was prohibited from scanning and storing it in the first place. Eventually, the Second Circuit ruled in favor of Google, saying that digitizing copyright-protected work did not constitute copyright infringement.
- Rahul Telang is Professor of Information Systems at Carnegie Mellon University, and an expert in digitization and copyright. He says that the issue is “multi-dimensional,” and that the Google Books case offers a “sort of precedent” but not a solid one. “I wish I could tell you there was a clear answer,” he said, “but it’s a complicated issue,” especially around works that may or may not be transformative. And until there is a solid case, it’s likely that courts will apply the usual tests for copyright infringement, around if a work supplants the need for the original, and if it causes economic harm to the original rights holder. Telang believes that countries will look to loosen restrictions on TDM wherever possible in order to boost domestic AI research.
- The US Copyright Office says that it will register an “original work of authorship, provided that the work was created by a human being.” This is due to the old precedent that the only thing worth copyrighting is “the fruits of intellectual labor,” produced by the “creative powers of the mind.” In 1991, this principle was affirmed by a case of purloined listings from one phone book company by another. The Supreme Court held that while effort may have gone into the compilation of a phone book, the information contained therein was not an original work, created by a human being, and so therefore couldn’t be copyrighted. It will be interesting to see if there are any challenges made to users trying to license or sell a DALL-E work for this very reason.
- Rob Holmes, a private investigator who works on copyright and trademark infringement with many major tech companies and fashion brands, believes that there is a reticence across the industry to pursue a landmark case that would settle the issue around TDM and copyright. “Legal departments get very little money,” he said. “All these different brands, and everyone’s waiting for the other brand, or IP owner, to begin the lawsuit. And when they do, it’s because some senior VP or somebody at the top decided to spend the money, and once that happens, there’s a good year of planning the litigation.” That often gives smaller companies plenty of time to either get their house in order, get big enough to be worth a lawsuit or go out of business.
- “Setting a precedent as a sole company costs a lot of money,” Holmes said, but brands will move fast if there’s an immediate risk to profitability. Designer brand Hermés, for instance, is suing an artist named Mason Rothschild, who is producing MetaBirkins NFTs. These are styled images on a design reminiscent of Hermés’ famous Birkin handbag, something the French fashion house says is nothing more than an old-fashioned rip-off. This, too, is likely to have ramifications for the industry as it wrestles with philosophical questions of what work is sufficiently transformational as to prevent an accusation of piracy.
- Artists are also able to upload their own work to DALL-E and then generate recreations in their own style. I spoke to one artist, who asked not to be named or otherwise described for fear of being identified and suffering reprisals. They showed me examples of their work alongside recreations made by DALLl-E, which while crude, were still close enough to look like the real thing. They said that, on this evidence alone, their livelihood as a working artist is at risk, and that the creative industries writ large are “doomed.”
- Article Forge CEO Alex Cardinell says that this situation, again, has historical precedent with the industrial revolution. He says that, unlike then, society has a collective responsibility to “make sure that anyone who is displaced is adequately supported.” And that anyone in the AI space should be backing a “robust safety net,” including “universal basic income and free access to education,” which he says is the “bare minimum” a society in the midst of such a revolution should offer.
- 
- AIs are already in use. Microsoft, for instance, partnered with OpenAI to harness GPT-3 as a way to build code. In 2021, the company announced that it would integrate the system into its low-code app-development platform to help people build apps and tools for Microsoft products. Duolingo uses the system to improve people’s French grammar, while apps like Flowrite employs it to help make writing blog posts and emails easier and faster. Midjourney, a DALL-E 2-esque GAI for art, which has recently opened up its beta, is capable of producing stunning illustrated art – with customers charged between $10-50 a month if they wish to produce more images or use those pictures commercially.
- For now, that’s something Craiyon doesn't necessarily need to worry about, since the resolution is presently so low. “People ask me ‘why is the model bad on faces’, not realizing that the model is equally good – or bad – at everything,” Dayma said. “It’s just that, you know, when you draw a tree, if the leaves are messed up you don’t care, but when the faces or eyes are, we put more attention on it.” This will, however, take time both to improve the model, and to improve the accessibility of computing power capable of producing the work. Dayma believes that despite any notion of low quality, any GAI will need to be respectful of “the applicable laws,” and that it shouldn’t be used for “harmful purposes.”
- And artificial intelligence isn’t simply a toy, or an interesting research project, but something that has already caused plenty of harm. Take Clearview AI, a company that scraped several billion images, including from social media platforms, to build what it claims is a comprehensive image recognition database. According to The New York Times, this technology was used by billionaire John Catsimatidis to identify his daughter’s boyfriend. BuzzFeed News reported that Clearview has offered access not just to law enforcement – its supposed corporate goal – but to a number of figures associated with the far right. The system has also proved less than reliable, with The Times reporting that it has led to a number of wrongful arrests.
- Naturally, the ability to synthesize any image without the need for a lot of photoshopping should raise alarm. Deepfakes, a system that uses AI to replace someone’s face in a video has already been used to produce adult content featuring celebrities. As quickly as companies making AIs can put in guardrails to prevent adult-content prompts, it’s likely that loopholes will be found. And as open-source research and development becomes more prevalent, it’s likely that other platforms will be created with less scrupulous aims. Not to mention the risk of this technology being used for political ends, given the ease of creating fake imagery that could be used for propaganda purposes.
- Of course, Duchamp and Warhol may have stretched the definitions of what art can be, but they did not destroy art in and of itself. It would be a mistake to suggest that automating image generation will inevitably lead to the collapse of civilization. But it’s worth being cautious about the effects on artists, who may find themselves without a living if it’s easier to commission a GAI to produce something for you. Not to mention the implication for what, and how, these systems are creating material for sale on the backs of our data. Perhaps it is time that we examined if it’s necessary to implement a way of protecting our material – something equivalent to Do Not Track – to prevent it being chewed up and crunched through the AI sausage machine.

URL: https://www.washingtonpost.com/business/openai-project-risks-bias-without-more-scrutiny/2022/04/21/4876513a-c13d-11ec-b5df-1fba61a66c75_story.html
- Politics
- Opinions
- War in Ukraine
- Investigations
- Climate
- Well+Being
- Tech
- Lifestyle
- World
- D.C., Md. & Va.
- Sports

URL: https://www.vice.com/en/article/g5vbx9/dall-e-is-now-generating-realistic-faces-of-fake-people
- OpenAI’s machine learning tool DALL-E has generated a lot of buzz lately for its ability to generate bizarrely specific images from text prompts. Now, after a recent change to the AI model’s internal use policies, OpenAI is allowing researchers to share generated images of photorealistic human faces belonging to nonexistent people.
- According to an email sent to DALL-E testers on Tuesday, users are now allowed to share realistic face photos created by the system after developers put in place safeguards designed to prevent the creation of deepfake images.
- “This is due to two new safety measures designed to minimize the risk of DALL·E being used to create deceptive content,” reads the email, which was shared with Motherboard. Specifically, the system now automatically “rejects attempts to create the likeness of any public figures, including celebrities,” and also blocks users from uploading images of human faces in order to generate similar faces. Previously, the system’s safeguards only prevented users from making images of political figures.
- Researchers have already begun sharing some early examples, and the results are… well, pretty weird.
- DALL-E is still in closed testing, and OpenAI normally keeps a tight lid on what types of generated results testers can share publicly. Meanwhile, smaller-scale volunteer projects like DALL-E Mini have already given the general public the ability to create AI-generated images and memes from text prompts—albeit with much lower quality results.
- Needless to say, the ability to generate realistic human faces raises all kinds of ethical questions, even if they don’t belong to real humans.
- AI ethics researchers have warned that massive-scale machine learning systems like DALL-E can harm marginalized people through deeply embedded biases that can’t be easily engineered out. OpenAI’s own researchers have also admitted that DALL-E frequently reproduces racist and sexist stereotypes when certain words are included in text prompts. And other AI models from Facebook and Google haven’t fared much better.
- While AI engineers say they’re doing their best to create safeguards that prevent abuse, it’s likely we’ve only just begun to see what large AI systems like DALL-E are capable of—and what types of harm they might cause.

URL: https://www.wsj.com/articles/think-of-any-image-then-ask-an-ai-art-generator-for-it-the-results-are-amazingand-terrifying-11666179308
- WSJ Membership
- Customer Service
- Tools & Features
- Ads
- More
- Dow Jones Products

URL: https://www.theverge.com/2022/9/21/23364696/getty-images-ai-ban-generated-artwork-illustration-copyright
- By  James Vincent, a senior reporter who has covered AI, robotics, and more for eight years at The Verge.
- Getty Images has banned the upload and sale of illustrations generated using AI art tools like DALL-E, Midjourney, and Stable Diffusion. It’s the latest and largest user-generated content platform to introduce such a ban, following similar decisions by sites including Newgrounds, PurplePort, and FurAffinity.
- Getty Images CEO Craig Peters told The Verge that the ban was prompted by concerns about the legality of AI-generated content and a desire to protect the site’s customers.
- “There are real concerns with respect to the copyright of outputs from these models”
- “There are real concerns with respect to the copyright of outputs from these models and unaddressed rights issues with respect to the imagery, the image metadata and those individuals contained within the imagery,” said Peters. Given these concerns, he said, selling AI artwork or illustrations could potentially put Getty Images users at legal risk. “We are being proactive to the benefit of our customers,” he added.
- The creators of AI image generators say the technology is legal, but that’s no guarantee this status won’t be contested. Software like Stable Diffusion is trained on copyrighted images scraped from the web, including personal art blogs, news sites, and stock photo sites like Getty Images. The act of scraping is legal in the US, and it seems the output of the software is covered by “fair use” doctrine. But fair use provides weaker protection to commercial activity like selling pictures, and some artists whose work has been scraped and imitated by companies making AI image generators have called for new laws to regulate this domain.
- Peters refused to say whether Getty Images has received legal challenges over its sale of AI-generated content. He said such content was “extremely limited” on the platform and repeated his assertion that the company was only introducing this policy to “avoid risk to [customers’] reputation, brand and bottom line.”
- One of Getty Images’ biggest competitors, Shutterstock, seems to be limiting some searches for AI content but hasn’t yet introduced specific policies banning the material. Other platforms have removed AI imagery for reasons other than protecting customers. Furry-focused social art site FurAffinity, for example, said it banned AI artwork because it undermines the work of human artists.
- Some art platforms have banned AI to support human artists
- “AI and machine learning applications (DALL-E, Craiyon) sample other artists’ work to create content. That content generated can reference hundreds, even thousands of pieces of work from other artists to create derivative images,” said FurAffinity’s mods. “Our goal is to support artists and their content. We don’t believe it’s in our community’s best interests to allow AI generated content on the site.”
- When asked if AI-generated content was a threat to the livelihood of illustrators and photographers who sell their work on Getty Images, Peters suggested that these tools were just the latest example of technology expanding the amount of available imagery.
- “The world is already awash in imagery. Digital cameras generated an exponential growth in imagery given the reduced cost and simplicity of capture, transmission and use. The introduction of the smartphone and social media took this to all new levels, with trillions of images taken and posted,” said Peters. “Our business has never been about the ease of creating imagery or the resulting volume. It is about connecting and cutting through.”
- However, actually removing AI content may be difficult. Peters says Getty Images will rely on users to identify and report such images, and that it’s working with C2PA (the Coalition for Content Provenance and Authenticity) to create filters. However, no automated filter will be wholly reliable, and it’s not clear how easy Getty Images will find it to enforce its new ban.
- As of this morning, a quick search on the site for “AI-generated art” reveals plenty of content for sale.
- Update, Wed 21st September, 11:18AM ET: Story updated to clarify the description of fair use.
- / Sign up for Verge Deals to get deals on products we've tested sent to your inbox daily.
- The Verge is a vox media network
- © 2023 Vox Media, LLC. All Rights Reserved

URL: https://venturebeat.com/business/openai-will-dall-e-2-kill-creative-careers/
- Join top executives in San Francisco on July 11-12, to hear how leaders are integrating and optimizing AI investments for success. Learn More
- 
- Last week, OpenAI announced it would expand beta access to DALL-E 2, its powerful image-generating AI solution, to over one million users via a paid subscription model. It also offered those users full usage rights to commercialize the images they create with DALL-E, including the right to reprint, sell, and merchandise.
- The announcement sent the tech world buzzing, but it mostly amounted to gleeful Twitter feeds filled with the results of random DALL-E prompts like “steampunk Jesus DMT trip under an electron microscope” to a “dark wizard using a magical smartphone to cast spells.” But a variety of questions, one leading to the next, seem to linger beneath the surface.
- For one thing, what does the commercial use of DALL-E’s AI-powered imagery mean for creative industries and workers – from graphic designers and video creators to PR firms, advertising agencies and marketing teams? Should we imagine the wholesale disappearance of, say, the illustrator?
- According to OpenAI, the answer is no. DALL-E is a tool that “enhances and extends the creative process,” an OpenAI spokesperson told VentureBeat. Much like an artist would look at different artworks for inspiration, DALL-E can help an artist with coming up with creative concepts.
- Transform 2023
- Join us in San Francisco on July 11-12, where top executives will share how they have integrated and optimized AI investments for success and avoided common pitfalls.
- 
- “What we’ve heard from artists and users to date is that it takes human direction to generate a good representation of the idea,” the spokesperson said.
- But how can someone who uses DALLE-2 to create an image attest that it is their own work? After all, the person using DALLE-2 is simply entering a prompt. How can the results of that prompt be their own? If they are allowed to sell those works commercially, are they really the artist?
- OpenAI insists that DALL-E creates original images, saying: Similar to how we learn as kids, DALL-E 2 has learned the relationship between images and the text used to describe them. As an example, DALL-E can learn what the city of Paris looks like from photos of Paris, including the Eiffel Tower and the Seine river. If you give DALL-E 2 the prompt “Paris,” it will generate a unique, original image of Paris based on what it has learned about the city.
- Overall, the creatives who VentureBeat reached out to seem to be taking the appearance of DALL-E on the scene in stride – and exploring the tool’s potential to boost productivity and efficiency, and take advantage of its creative assistance.
- “For enterprise clients, this technology can provide a vehicle to get from idea to concept and then help to refine the concept much faster,” said Andy Martinus, global head of innovation at London-based public relations firm, Team Lewis.
- However, he emphasized that it can’t replace the ideas or creative direction.
- “For artists and marketers, while there might be skepticism initially, there is also an opportunity,” he said. “Creators can use the tool to build out their initial ideas and to create variations of an existing design or idea, [which] provides a greater level of creative control.”
- Meghan Goetz, director of marketing at digital agency Crowd Favorite, points out that enterprise brand clients often have strict brand guides and user personas that require the in-house marketing and design team.
- “For these teams, DALL-E can be utilized to create new, unique or custom stock media, which could be great for campaigns that require specific design styles,” she said. “It could be a great tool for prototyping or inspiration for design assets, while modifying and editing images can be a great way to utilize these tools to save time and money.”
- Expanded access to DALL-E is an opportunity for designers to update their workflows, said Juan Pablo Madrid, senior director of design innovation at New Orleans-based creative agency Online Optimism. He said he considers it similar to the widely adopted AI-powered algorithms that have simplified image processing in tools like Adobe Photoshop.
- “Some examples I have seen from other designers are using DALL-E 2 to create photorealistic mockups of brand materials or creating original blog post images,” he added.
- But while the commercial use of DALL-E 2 may expand creativity and provide artists with more options in creating and expanding their markets, it may create more competition in the creative space, cautioned Baruch Labunski, CEO at Rank Secure.
- “There is going to be a flood of creative work stemming from this, which can be good or bad, depending on your agency and your location,” he said.
- The advantage, he explained, is for small marketing agencies or small businesses that could, with the subscription, produce highly professional imaging that makes them more competitive with larger firms or businesses while keeping costs in check. It would also open up more opportunities for freelancers because they could also compete.
- The disadvantages, however, are increased competition in the creative space that could drive down the prices of creative work and marketing, especially in larger, urban areas where there is already heavy competition.
- “I don’t see it limiting jobs in the space,” he said. “I see it as creating more jobs and that will also mean more competition.
- According to OpenAI’s spokesperson, user feedback found that full usage rights are what creators want. OpenAI, however, retains ownership of the original image “primarily so that we can better enforce our content policy.”
- But creative workers find the issues around ownership and copyright to be unclear.
- “Commercially, there are questions to be answered around ownership of the imagery that tools like DALL-E 2 create – is the image owned by DALL-E 2 or by the creative that directed it?” said Martinus. “If [OpenAI] owns it, do you buy the usage rights, and can others use the image as well, as with stock images? Could this be a longer-term alternative to stock imagery?”
- Goetz agrees that ownership rights seem “a bit hazy,” pointing out that when it comes to working for specific brands, “they tend to avoid any uncertainty when it comes to image and design asset licenses.”
- Madrid said he would be hesitant to adopt tools like DALL-E 2 for high-value client work, “… considering that users do not have an exclusive copyright to any image generations and cannot, therefore, transfer it to a client. So, I would not advise ad agencies to consider getting rid of their designers unless they’re prepared for potential legal battles over produced work.”
- However, he suggests they might want to reevaluate pricey stock photo subscriptions.
- “The price-point and ability to create virtually any image from a text prompt is pretty attractive,” he said.
- OpenAI points out that artists and creative professionals are already using DALL-E in a wide range of projects.
- “Our hope is that DALL-E can be used by artists, designers and photographers as a tool to help with the creative process,” the OpenAI spokesperson said. “We have seen AI be a good tool for people in the creative space. For example, as photo editing software has become more powerful and accessible, it has allowed more people to enter the photography field. In recent years, we’ve also seen artists use AI to create new kinds of art.”
- Martinus emphasized that DALL-E 2 and other tools should not be seen as a threat to the creative field. “People tend to ‘hack’ tools and use them for tasks beyond their original intention,” he said. “I expect the same with DALL-E 2. People will use it, but differently than we expect.”
- Overall, Goetz added that she is not seeing full adoption of these tools – yet.
- “Many clients and projects demand the expertise and experience of the human aspect when it comes to final production,” she said.
- VentureBeat's mission is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact. Discover our Briefings.
- Want must read news straight to your inbox?
- © 2023 VentureBeat. All rights reserved.

URL: https://newatlas.com/computers/dall-e-2-ai-art/
- The greatest artistic tool ever built, or a harbinger of doom for entire creative industries? OpenAI's second-generation DALL-E 2 system is slowly opening up to the public, and its text-based image generation and editing abilities are awe-inspiring.
- The pace of progress in the field of AI-powered text-to-image generation is positively frightening. The generative adversarial network, or GAN, first emerged in 2014, putting forth the idea of two AIs in competition with one another, both "trained" by being shown a huge number of real images, labeled to help the algorithms learn what they're looking at. A "generator" AI then starts to create images, and a "discriminator" AI tries to guess if they're real images or AI creations.
- At first, they're evenly matched, both being absolutely terrible at their jobs. But they learn; the generator is rewarded if it fools the discriminator, and the discriminator is rewarded if it correctly picks the origin of an image. Over millions and billions of iterations – each taking a matter of seconds – they improve to the point where humans start struggling to tell the difference.
- They learn in their own way, completely undirected by their programmers; each AI develops its own understanding about what a horse is, completely untethered from the reality we understand. All it knows or cares about is its job: either fooling the other AI or not getting fooled, based on its own individual and completely mysterious methods of analyzing and creating data.
- A stern maltese dog dressed as a judge in court, oil painting#dalle #dalle2 @images_ai pic.twitter.com/8Yt4hjLf2u
- This leads to the famously weird disconnects from reality that have been the hallmark of such systems to date. Think Deepdream's strange obsession with dogs and eyes, or the rampant and beautiful surrealism of systems like Botto, the AI/human NFT art collaboration.
- Thus far, these algorithms have been fascinating amusements. DALL-E 2, on the other hand, makes it crystal clear just how disruptive this technology will be – not five or 10 years in the future, but the minute its doors are flung open to the public. Just look at the video below, and imagine how much time and money you'd need to budget to make this using non-artificial intelligence.
- DALL-E 2 represents a step change in AI image generation technology. It understands natural-language prompts much better than anything that's come before, allowing an unprecedented level of control over subjects, styles, techniques, angles, backgrounds, locations, actions, attributes and concepts – and it generates images of extraordinary quality. If you tell it you want photo-realism, for example, it'll happily let you direct its lens and aperture choices.
- “fisheye lens closeup of a bunny wearing a sombrero” #dalle #dalle2 pic.twitter.com/QnIN3anEuo
- Given a high-quality prompt, it will generate dozens of options for you in seconds, each at a level of quality that would take a human photographer, painter, digital artist or illustrator hours to produce. It's some kind of art director's dream; a smorgasbord of visual ideas in an instant, without having to pay creatives, models or location fees.
- You can also generate different versions – either versions of something DALL-E has generated for you, or of something you've uploaded. It'll create its own understanding of the subject, the composition, the style, the color palette and the conceptual meaning of the image, and generate a series of original pieces that echo the look, the feel and the content of the original, but each adds its own twist.
- And DALL-E 2 can now also make edits, in a way that makes Adobe's insanely powerful but notoriously unapproachable Photoshop software feel like a relic of the past. No level of education is required. You can paint out a splodge in a chair and say "put a cat there." You can tell DALL-E to "make it sunset," "put her in a neon-lit cyberpunk atrium," or "remove the bicycle." It understands things like reflections, and will update those accordingly.
- You can stick an image in, and ask the AI to expand it outward to a wider frame of view. Each time, it'll give you a few different options, and if you don't like them, you can just run the same instruction again or get more specific in your prompting. Effectively, you can continue zooming out on an image indefinitely, and people are already using this to extraordinary creative effect.
- A house in the middle of a beautiful lush field with mountains in the background 🏡🏔 #dalle2 #dalle pic.twitter.com/bitMvnRz0e
- I used DALL·E 2's inpainting feature to make a version of Eames Powers of Ten, leaving the prompts in to explain each of the 57 steps. #DALLE @OpenAI #AI pic.twitter.com/lP11NP2q41
- These capabilities – which just scratch the surface of what it can do – make DALL-E 2 an absolutely revolutionary image editor. It feels like this technology can do just about anything.
- Well, within limits. OpenAI has designed DALL-E 2 to refuse to create images of celebrities or public figures. It also won't accept image uploads "containing realistic faces," and it does its best not to generate images of real people, instead tweaking things in an interesting way that tends to look somewhat like the actual person, but also very clearly not. Mind you, given the sophistication of deepfake and image editing software, we don't imagine it'll take a ton of effort to take a DALL-E image and stick the head of your choice on it.
- #dallE AI is close...but also totally not close LOL. Artificial Intelligence generated art my friends. #beyonce pic.twitter.com/VNvTA6Kdpl
- The system wont generate porn, or gore, or political content – and indeed, the data used to train it excludes these types of images. And, unless you specify racial or demographic information in your prompts, the system "generates images of people that more accurately reflect the diversity of the world's population," in the hopes of pre-empting some of the racial bias AI systems frequently suffer from due to skewed training data.
- DALL-E 2 is currently in beta, with a waitlist for interested parties. Over the coming weeks, a million accounts will be welcomed in, each with 50 free credits to use the system and a further 15 credits each month. Additional credits will cost $15 per 115 credits – and each credit will bring you back four images for a prompt or instruction. It's at once an incredible democratization of visual creativity, and a knife to the heart of anyone who's spent years or decades refining their artistic techniques in the hope of making a living from them.
- “A drawing from an old encyclopediadepicting a very unusual creature thatlooks like Mike Wazowski”#dalle2 #dalleMini #art pic.twitter.com/gLgnUXQV42
- OpenAI explicitly says users "get full rights to commercialize the images they create with DALL-E, including the right to reprint, sell, and merchandise." But there are still some fascinating legal grey areas yet to be fully explored here, given that everything these AIs know about art, they've learned by analyzing the works of other, human creators.
- If this latest piece of software looks amazing, it's worth remembering that it's still a very early version of this kind of technology. DALL-E 2, its contemporaries and its descendants will continue their evolution at a breakneck pace that's only likely to accelerate.
- Where to from here? Well, why not video? As processing power and storage continues to expand, it's easy to imagine systems like this should eventually be capable of generating moving images, too. Adobe's already embedded AI-enhanced video editing capabilities into its pro-level After Effects software, but we're yet to see any DALL-E style creativity in video as yet.
- How long will be be before we see an entire short film, written, directed, soundtracked and made entirely by AI systems? And then, after that point, how long until they start being worth watching?
- beautiful woman in a jellyfish inspired translucent spacesuit #dalle2 #dalle #digitalart #AIart #AiArtwork pic.twitter.com/PoBgj5rFin
- What about other forms of graphic design? Can DALL-E do logos? Website templates? Business cards? Will it evolve to auto-generate catalogs, posters, brochures, book covers and everything else a designer currently makes a living from? Probably. Indeed, if you're young and interested in art or design, you'd probably best become an expert at getting the best out of these emerging tools, because in a few short years, whether you like it or not, this might be what the gig looks like.
- Presumably, alternative AI image generators will soon begin to spring up without the ethical and moral boundaries that OpenAI has drawn around DALL-E. Cans of worms will be opened. Noses will be put out of joint. DALL-E shows a glimpse of a future that's fundamentally different, and this kind of upheaval is never painless.
- Check out a short video below.
- Source: OpenAI

URL: https://www.cosmopolitan.com/lifestyle/a40314356/dall-e-2-artificial-intelligence-cover/
- We may earn commission from links on this page, but we only recommend products we love. Promise.
- The technology behind DALL-E 2 is already reshaping the world as you know it—perhaps most literally with this magazine cover you’re looking at. Are you ready for what comes next?
- 
- n a Monday afternoon in June of the year 2022 AD, six women on Zoom type increasingly bizarre descriptions into a search field.
- The group, composed of editors from Cosmopolitan, members of artificial-intelligence research lab OpenAI, and a digital artist—Karen X. Cheng, the first “real-world” person granted access to the computer system they’re all using—are working together, with this system, to try to create the world’s first magazine cover designed by artificial intelligence.
- Sure, there have been other stabs. AI has been around since the 1950s, and many publications have experimented with AI-created images as the technology has lurched and leaped forward over the past 70 years. Just last week, The Economist used an AI bot to generate an image for its report on the state of AI technology and featured that image as an inset on its cover.
- This Cosmo cover is the first attempt to go the whole nine yards.
- But the portal-to-another-universe-earring thing isn’t working. “It looks like Mary Poppins,” says Mallory Roynon, creative director of Cosmopolitan, who appears unruffled by the fact that she’s directing an algorithm to assist with one of the more important functions of her job. (Nor should she be ruffled—more on that later.)
- Back to something more basic then.  Cheng types a fresh request into the text box: “1960s fashionable woman close up, encyclopedia-style illustration.” The AI thinks for 20 seconds. And then: Six high-quality illustrations of women, each unique, appear on the screen.
- Six images that didn’t exist until right now.
- This technology is a creation of OpenAI called DALL-E 2. It’s an artificial intelligence that takes verbal requests from users and then, through its knowledge of hundreds of millions of images across all of human history, creates its own images—pixel by pixel—that are entirely new. Type “bear playing a violin on a stage” and DALL-E will make it for you, in almost any style you want. You can depict your ursine virtuoso “in watercolor,” “in the style of van Gogh,” or “in synthwave,” a style the Cosmo team favors for perhaps obvious reasons.
- The results are shockingly good, which is why, since its limited release in April, DALL-E 2 has inspired both awe and trepidation from the people who have seen what it can do. The Verge declared that DALL-E “Could Power a Creative Revolution.” The Studio, a YouTube channel by tech reviewer Marques Brownlee, wondered, “Can AI Replace Our Graphic Designer?”
- By the end of the Zoom meeting, a cover is close. It’s taken less than an hour. This is wild to witness. And, yes, a little scary. And it raises serious questions far beyond the scope of magazine design: about art, about ethics, about our future.
- Watching it work though? It makes your jaw drop.
- ALL-E’s creators don’t like to anthropomorphize it, and for good reason—contemplating AI as an autonomous entity freaks people out. Just see the recent news about Google engineer Blake Lemoine, who was put on probation for claiming that his conversations with the company’s AI chatbot, LaMDA, proved it had a soul and should need to grant engineers permission before being experimented on. Most independent experts, as well as Google itself, were quick to dismiss the idea, pointing out that if AI seems human, it’s only because of the massive amounts of data that humans have fed it.
- In fact, this kind of AI is fundamentally designed to imitate us. DALL-E is powered by a neural network, a type of algorithm that mimics the workings of the human brain. It “learns” what objects are and how they relate to each other by analyzing images and their human-written captions. DALL-E product manager Joanne Jang says it’s like showing a kid flash cards: If DALL-E sees a lot of pictures of koalas captioned “koala,” it learns what a koala looks like. And if you type “koala riding motorcycle,” DALL-E draws on what it knows about koalas, motorcycles, and the concept of riding to put together a logical interpretation. This understanding of relationships can be keen and contextual: Type “Darth Vader on a Cosmopolitan magazine cover” and DALL-E doesn’t just cut and paste a photo of Darth; it dresses him in a gown and gives him hot-pink lipstick.
- All this represents a major breakthrough in AI, says Drew Hemment, a researcher and lead for the AI & Arts program at the Alan Turing Institute in London. “It is phenomenal, what they have achieved,” he says. There are many following suit: Last month, Google released a similar AI called Imagen, and a comparable generator called Midjourney, which The Economist used for its aforementioned cover image, was released in beta around the same time as DALL-E 2. There’s even a DALL-E “light,” now called Craiyon, made by the open-source community for public use.
- That said, the technology is far from perfect. DALL-E is still in what OpenAI calls a “preview” phase, being released to just a thousand users a week as engineers continue to make tweaks. If you ask for something the model hasn’t seen before, for example, it’ll provide its best guess, which can be wacky. Despite the generally high quality of the images it renders, areas requiring finer details often turn out blurry or abstract. Perhaps most problematically, the majority of the people it renders, due to the biased data sets it’s seen, are white. And perhaps most surprisingly, it has a hard time figuring out how many fingers humans are supposed to have—to the machine, the number of fingers seems as arbitrary as the number of leaves on a tree.
- But DALL-E is imperfect also by design. It’s intentionally bad at rendering photorealistic faces, instead generating wonky eyes or twisted lips on purpose in an effort to protect against the tech being used to make deepfakes or pornographic images, which disproportionately harm women.
- These land mines are part of the reason OpenAI is releasing DALL-E slowly, so they can observe user behavior and refine its system of safeguards against misuse. For now, those safeguards include removing sexually explicit images from those hundreds of millions of images used to train the model, prohibiting and flagging the use of hate speech, and instituting a human review process. DALL-E also has a content policy that asks users to adhere to ethical guidelines, like not sharing any photorealistic faces DALL-E may accidentally generate and not removing the multicolored signature in the bottom right corner that indicates an image was made by AI. And there’s an ongoing effort to make the data set less biased and more diverse; in the month Cosmo spent poking around, results already started to yield more representative subjects.
- Despite DALL-E’s limitations, intentional and otherwise, its small but growing number of users are forging ahead, posting images on social media at a fever pitch lately—playing around with DALL-E and its knockoffs and sharing thousands of their results, like this and this and this. OpenAI does eventually plan to monetize all this interest by charging users for access to its interface and intends to carefully position it as an artist’s tool, not her replacement—a “creative copilot,” as OpenAI’s Jang puts it. Codex, another of OpenAI’s innovations, writes software based on normal-language directives as opposed to coding lingo and has streamlined and democratized parts of the software development process as a result. In the same way, Jang says she sees DALL-E 2 streamlining essential parts of the creative process like mood-boarding and conceptualizing.
- Experts I spoke to generally agreed that while fears of AI replacing visual artists are not totally unfounded, the technology will also create new opportunities and possibly entire new art forms. Independent UK-based AI art curator Luba Elliott says she also hopes it can bring more women to the field of AI-generated art, where they’re less represented.
- Cheng, the digital artist working with Cosmo, used DALL-E to make a music video for Nina Simone’s “Feeling Good” and is now using it to design a dress that bursts into geometric shapes when it’s viewed through an augmented reality filter. A video director by trade, Cheng says that in the past, she’s been limited as a visual artist because she can’t draw. “Now I have the power of all these different kinds of artists,” she says. DALL-E has become part of her day-to-day workflow and has drastically sped up her creative process.
- “But I don’t want to sugarcoat it either,” Cheng wrote in an Instagram caption accompanying her music video. “With AI, we are about to enter a period of massive change in all fields, not just art. Many people will lose their jobs. At the same time, there will be an explosion of creativity and possibility and new jobs being created—many that we can’t even imagine right now.”
- touches almost every part of our lives, from the electronic systems in our cars to the TikTok filters that give us Pamela Anderson eyebrows to our increasingly polarized social feeds and the proliferation of fake news. While AI itself is not new, “it is now a very powerful technology,” says Eduardo Alonso, director of the Artificial Intelligence Research Centre at City, University of London, and “we are starting to consider the ethical and legal impacts of what we are doing.” But technology tends to be a step ahead of the law, he says, so until the law catches up, it’s on the industry itself to set a code of conduct.
- OpenAI’s stated mission is to work toward creating an artificial general intelligence (an AGI) that accomplishes two things: first, the ability to perform any task, not just the ones it’s explicitly asked. That’s why tech like DALL-E is such a big step—it’s an attempt at giving AGI the sense of sight. “For an AGI to fully understand the world, it needs vision,” says Jang. “Up to now, we’ve taught it to be good at reasoning, but now it can look at things and we can incorporate visual reasoning.” This could pave the path for other senses, too, so that one day, Jang says, an AGI could process all the things a human can process. The second, and even loftier, goal? To create an AGI that “benefits all of humanity,” and the experts I spoke with seem to believe the company is genuinely committed to deploying AI responsibly. But others say that any AGI could have dangerous or even catastrophic consequences, like becoming a surveillance tool for authoritarian governments or becoming the operating system that enables autonomous weapons systems.
- Ultimately, because a true AGI doesn’t yet exist, we still don’t and can’t know—but every day, whether we’re ready or not, we’re closer to finding out.
- ack in the virtual conference room, the Cosmo and OpenAI group is tooling around with the cocktail cover idea, trying to put various miniature objects into the glass, like a sailboat or a tiny woman on a pool float. But the vision seems almost too surreal for DALL-E, which appears confused—“a woman taking a bubble bath in a martini glass” just generates a creepy face floating beneath the surface of the liquid.
- Then DALL-E suggests a new idea everyone loves: putting a goldfish in the glass. It almost feels like the AI and the humans are riffing off each other.
- An hour in, the team has stalled. The martini glass images look too clip-art-y to make for a satisfying Cosmo cover, and the deadline is nigh. (My deadline is nigher still, and I find myself wishing an AI would write my story.) When the group signs off, the fate of the cover feels uncertain.
- The next morning, though, an email attachment in my inbox: an image of a decidedly feminine, decidedly fearless astronaut in an extraterrestrial landscape, striding toward the reader. It’s DALL-E’s interpretation of Cheng’s prompt from overnight, “wide-angle shot from below of a female astronaut with an athletic feminine body walking with swagger toward camera on Mars in an infinite universe, synthwave digital art,” and it’s stunning.
- The image encapsulates the reasons OpenAI wanted to work with Cosmo and Cosmo wanted to work with OpenAI—reasons Natalie Summers, an OpenAI communications rep who also runs its Artist Access program, put best in email after seeing the cover: “I believe there will be women who see this and a door will open for them to consider going into the AI and machine learning fields—or even just to explore how AI tools can enhance their work and their lives. Women will be better equipped to lead in this next chapter of what it means to coexist with, and determine the course of, increasingly powerful technology. That badass woman astronaut is how I feel right now: swaggering on into a future I am excited to be a part of.”
- Members of the team futz with the image in DALL-E over the next 24 hours—Cheng uses an impressive experimental feature, not yet available to users, that draws on the context of the image to “extend” it to the correct cover proportions—and by the next day, Cosmo has a cover.
- bserving this process, I think,This sure is a lot of human effort for an AI-generated magazine cover.
- My initial takeaway is that DALL-E truly is an artist’s tool—one that can’t create without the artist. Which might ultimately be the point.
- At a family wedding in the midst of all this, I met a renowned backdrop painter, Sarah Oliphant. Over a 45-year career, Oliphant—an artist in the classical sense, one who paints with a brush and draws with a pen—has painted backdrops for some of the world’s most famous photographers, including Annie Liebovitz and Patrick Demarchelier. I told her about DALL-E and asked what did she think? When AI can write poetry and make art, what did that say about…well, art?
- “All art is borrowed,” she said. “Every single thing that’s ever done in art—we’re all just mimicking and forging and copying the human experience.” Everything we’ve ever seen that’s been meaningful to us, she said, becomes the inspiration we draw from when we’re creating art.
- A data set, if you will.
- She showed me a painting she made as a wedding gift, a fantastically bizarre and lavish creation that depicted the groom as a plump baby sitting in a bird’s nest in a mystical forest, surrounded by sumptuous cakes and mischievous fairies, each of which has the face of the bride. To paint it, Oliphant referenced a photo of the man as a baby, a book of Victorian fairy paintings, and a photo of a bird’s nest—as well as, probably, every baby, picture of a fairy, and bird’s nest she’s seen in her life.
- But what about the fact that DALL-E can generate art almost instantaneously? Does that make a difference?
- Not to Oliphant. “It’s what art evokes in the viewer that makes it valuable, not how long it took,” she said. This painting took her nine months, but “if the computer can generate a piece of art that I look at and I’m overwhelmed by its beauty or what it evokes, or I see it as intrinsically fascinating, then that’s just as valuable.”
- So she didn’t feel threatened?
- Oliphant laughed. “I’d say to that computer, good luck,” she said. “Okay, computer, you try to paint a backdrop as beautiful as I can. Go for it.”
- A few days after our conversation, I type into DALL-E’s text box: “baby wearing flower wreath in a mystical forest at night, surrounded by fairies and cakes.” I wait. I realize: I’m nervous.
- When the images populate 20 seconds later, I actually say “whoa” out loud. They’re eerily similar in mood and composition to Oliphant’s, and all the elements are there…the baby’s rosy cheeks, the gossamer fairy wings. But they’re unsettling to see after the original, and I realize why: DALL-E is representing a different data set, different experiences, a different worldview.
- To call it a tool understates its capabilities. It’s not merely a paintbrush that an artist can wield to express her whims directly—if it were, she’d be able to paint a woman taking a bubble bath in a martini glass. Instead, it brings something of its own.
- I save one of the better versions of the baby with his head tilted skyward, hands outreached, and enlarge it on my screen. I lean in and search the image, trying to distinguish which part was imagined by the human and which part by the machine.
- 
- This article has been updated to reflect DALL-E Mini’s name change to Craiyon.
- Gloria Liu is a freelance journalist in Golden, Colorado.
- These 10 Cloud Bed Dupes Are Soooo Dreamy
- Your Weekly Tarot Card Reading, by Zodiac Sign
- Your Horoscope for the Week of May 28
- How to Make Pregnancy Work at Work
- Plan Your Wedding Seating by Zodiac Sign
- Parachute’s Memorial Day Sales Are *So* Good
- Grab Your Sunnies, We’re Going Pool-Float Shopping
- Stop What You're Doing. Apple Deals Have Arrived.
- Tip: Use Our Discount @ Saatva’s Memorial Day Sale
- ATTN: These Summer Salads Will Change Your Life
- MDW Furniture Sales, You Say? Sign Me the Hell Up
- IG Caption Inspo for Every Summer Pic You Post
- A Part of Hearst Digital Media
- We may earn commission from links on this page, but we only recommend products we back.
- ©2023 Hearst Magazine Media, Inc. All Rights Reserved.

URL: https://www.nytimes.com/2022/04/06/technology/openai-images-dall-e.html
- Please enable JS and disable any ad blocker

URL: https://www.nytimes.com/2022/08/24/technology/ai-technology-progress.html
- Please enable JS and disable any ad blocker

URL: https://www.businessinsider.com/ai-image-generators-artists-copying-style-thousands-images-2022-10
- Jump to
- 
- 
- 
- Greg Rutkowski is an artist with a distinctive style: He's known for creating fantasy scenes of dragons and epic battles that fantasy games like Dungeons and Dragons have used.
- He said it used to be "really rare to see a similar style to mine on the internet."
- Yet if you search for his name on Twitter, you'll see plenty of images in his exact style — that he didn't make.
- Rutkowski has become one of the most popular names in AI art, despite never having used the technology himself.
- People are creating thousands of artworks that look like his using programs called AI-image generators, which use artificial intelligence to create original artwork in minutes or even seconds after a user types in a few words as directions.
- Rutkowski's name has been used to generate around 93,000 AI images on one image generator, Stable Diffusion — making him a far more popular search term than Picasso, Leonardo Da Vinci, and Vincent van Gogh in the program.
- "I feel like something's happening that I can't control," Rutkowski, who is based in Poland, told Insider. "My name is being used a lot to generate AI images, along with the names of other working artists."
- AI-image generators create images that are unique, rather than collages pulled from stock images.
- A user simply types words describing what they'd like to see, referred to as "prompts," into a search bar. It's a bit like searching Google Images, except the results are brand-new artworks created using the text in the user's search terms as instructions.
- One of the most common prompts is to use the name of an artist to create something mimicking their style.
- "People are pretending to be me," Rutkowski said. "I'm very concerned about it; it seems unethical."
- Simon Stålenhag, an artist and designer based in Sweden, told Insider that although he isn't against AI-generated art in principle, he does take issue with how some people are using the new technology.
- "People are selling prints made by AI that have my name in the title," he said. "Something like — 'Rusty Robot in a field in the style of Simon Stålenhag' — which is a super aggressive way of using this technology."
- He's seen people be hostile when they share an AI image in his style on social media. "People have tagged me and said that they're gonna make me lose my job or something like that, they're really harsh and aggressive," he said.
- He believes AI-image generators are "not in the hands of artists right now. It's in the hands of early adopters of tech."
- Rutkowski, who uses both digital tools and classic oil on canvas for his work, is worried that this explosion in imitation art means his style — which has seen him land deals with Sony and Ubisoft — might lose its value.
- "We work for years on our portfolio," Rutkowski said. "Now suddenly someone can produce tons of images with these generators and sign them with our name."
- "The generators are being commercialized right now, so you don't know exactly what the final output will be of your name being used over the years," he said.
- "Maybe you and your style will be excluded from the industry because there'll be so many artworks in that style that yours won't be interesting anymore."
- More and more consumers are using AI-image generators.
- OpenAI, which Elon Musk cofounded in 2015, made its DALL-E image generator open to the public in September. Before lifting the waitlist, OpenAI said the program already had more than 1.5 million users.
- Liz DiFiore, the president of the Graphic Artist Guild, an organization that supports designers, illustrators, and photographers across the US, said the ease with which AI can copy styles could cause financial fallout for artists.
- "Artists spend a lot of time throughout their career, and make a lot of income, on being able to license their images and being sought after specifically for their style," she said.
- "So if an AI is copying an artist's style and a company can just get an image generated that's similar to a popular artist's style without actually going to artists to pay them for that work, that could become an issue."
- US copyright law only protects artists against the reproduction of their actual artworks — not from someone else mimicking their style.
- Some of the most popular AI-image generators — which include DALL-E, Midjourney, and Stable Diffusion — have policies in place to prevent consumers from using their products in certain ways. OpenAI, for example, prohibits the use of images of celebrities or politicians.
- All three programs block users from creating "harmful content" by filtering things like nudity and gore.
- Insider asked representatives from DALL-E, Midjourney, and Stable Diffusion if they have any measures in place to prevent images being created that mimic the style of working artists.
- A representative for Stable Diffusion said the company was working on an opt-out system for artists who don't want AI programs to be trained on their work.
- The spokesperson added that an artist's name "is only one component of a diverse set of instructions to the AI model that creates a unique style that is different from an individual artist's style."
- Representatives for Open AI did not specify any measures in place to protect living artists but said the company would seek artists' perspectives as it expanded access to DALL-E.
- Midjourney didn't respond to Insider's questions.
- AI-image generators "train" by learning from large sets of images and captions. Representatives from OpenAI said both publicly available sources and images licensed by the company make up DALL-E's training data.
- Representatives for Stable Diffusion said the program uses web crawls to gather information and images.
- Rutkowski thinks living artists should have been excluded from the databases that train the generators.
- "I'm not against the AI overall, I think it's a good technology. But I think they should have excluded artists' names from the program," he said.
- Another designer and illustrator, RJ Palmer, dubbed the generators actively "anti-artist" on Twitter because he said they are "explicitly trained on current working artists."
- Artists can check if their work has been used to train AI programs on a website called Have I Been Trained, which the German artist Mat Dryhurst and the American sound artist Holly Herndon created.
- The pair have been working on tools to help artists opt-out of AI data-training sets. The website filters through around 5.8 billion images that are in the dataset Stable Diffusion and Midjourney use to train their programs.
- Other artists feel they should have been asked for consent for their images to be scraped for the data used to train AI generators.
- Stålenhag said it would have been nice to be asked if he could be included in the training data, but said it was an inevitable consequence of putting art on the internet.
- "I see it as being very similar to how artists already work," he told Insider.
- "We do copy other people's ideas and styles and designs, and we take stuff," he said, noting that he also doesn't think AI art is good enough quality to be a "threat" at present.
- "There's a hype around AI that I think is weird because I just don't think that it's very good," he said. "I don't see it as a threat because the visuals are not as good as what artists can create."
- It's unclear whether copyright laws will protect the new artwork that AI programs generate.
- "Copyright issues around AI is probably one of the biggest areas that we are focused on," DiFiore said, adding that it is still "a very gray area."
- Some stock-image libraries, such as Getty Images, have refused to carry AI-generated artwork due to the uncertainty around copyright and commercial use.
- A spokesperson for the US Copyright Office told Insider that works generated only by artificial intelligence lacked the human authorship necessary to support a copyright claim.
- They said the office would not "knowingly grant registration to a work that was claimed to have been created solely by machine with artificial intelligence."
- But it's unclear whether a person entering search prompts into a program to create an AI artwork counts as a human-AI collaboration.
- Representatives for Stable Diffusion said while images created can be used for commercial offerings, the company wasn't able to say whether the images would be copyrightable. They added this decision was up to individual nations at the legislative level.
- Representatives for OpenAI said they think images generated by their programs can be copyrighted for commercial reasons.
- A spokesperson from OpenAI said: "When DALL-E is used as a tool that assists human creativity, we believe that the images are copyrightable. DALL-E users have full rights to commercialize and distribute the images they create as long as they comply with our content policy."
- They added that "copyright law has adapted to new technology in the past and will need to do the same with AI-generated content."
- Despite reservations, the technology's potential also excites many artists.
- Giles Christopher, a London-based commercial photographer specializing in food and drink, uses DALL-E and other AI-image generators to experiment with portraits and create artificial backgrounds for some of his commercial shots.
- "I've come out with images that you wouldn't question are photographs," he said. "Some of the arguments I've had from photographers are that the images are looking too good."
- He thinks the genie is out of the bottle when it comes to AI, and that artists should look for ways to include it into their work.
- "I have friends in the industry who will storm out of the room if I even bring up using AI," he said.
- But he's keeping an open mind. "I'm still on the fence. It's like keeping your enemies close," Christopher said.
- Read next

URL: https://eandt.theiet.org/content/articles/2022/11/full-power-behind-ai-s-green-ambitions/
- Image credit: Dreamstime
- By Chris Edwards
- Published Thursday, November 10, 2022
- No one really knows how much energy artificial intelligence is using, but the need to find out is getting more urgent.
- It’s bonanza time in the land of computing, especially if you have bet large on artificial intelligence (AI). Opening his company’s autumn technology conference, Nvidia CEO Jensen Huang claimed: “Computing is advancing at incredible speeds. The engine propelling this rocket is accelerated computing, and its fuel is AI.”
- Huang has good reason to be optimistic about the future of AI-driven computing. A decade ago, researchers at the Swiss research institute IDSIA took the deep-learning concepts developed by a small group led by Geoffrey Hinton, professor of computer science at the University of Toronto, and found they could use the parallel computing units sitting inside graphics processing units (GPUs) originally developed to run 3D games to speed up the processing. After training the deep neural network on road signs, they found the model could spot such tiny hints in shapes that it could read almost completely bleached surfaces.
- Not only did the IDSIA work help how much deep learning would propel AI development, it underlined how important specialised accelerators would be in driving the revolution that followed – and, to an increasing extent, Nvidia’s revenues. The company’s top-end GPUs are now designed explicitly for speeding-up AI models running in data centres rather than first-person shooters. GTC (Nvidia’s GPU Technology Conference) was not short of speakers promoting AI and high-speed computing as a way of improving life and combating climate change. The GPU-maker is far from alone: it has become something of an article of faith that information and computing technologies (ICT) will deliver overall savings in energy consumption through better planning.
- Peter Herweck, CEO of Aveva, was typical of many executives in the ICT sector when he characterised the situation earlier this year for the World Economic Forum (WEF): “The key to unlocking a net-zero future in industry is transforming the way industrial teams work through digitalisation. Higher efficiency and more ambitious sustainability objectives are enabled today by technologies that provide real-time data to optimise and better automate industrial processes and energy management.”
- The reasoning is simple: the more you know about a process, the more you can optimise it. AI delivers a mechanism for improving this understanding by finding many more patterns in data that lead to efficiency improvements. Two years ago, research by Ricardo Vinuesa, associate professor at KTH Royal Institute of Technology, Stockholm, and co-workers identified numerous situations where AI and similar data-driven algorithms could help meet close to 80 per cent of the 170 targets of the 2030 Agenda for Sustainable Development. At the same time, he warns that meeting a third of those targets could just as easily be made harder through the expansion of AI.
- Examples of positive effects are not difficult to find, particularly in industrial control. An example that many have used over the past couple of decades is that of motors, where direct digital control works out far less wasteful than just letting basic AC motors keep spinning and using gearboxes to control the speed of conveyors and grinders.
- Fine-grained control can extend across an entire facility. Industrial giant Siemens is keen to build entire digital twins of factories and to use AI to manage processes, and this year signed a partnership with Nvidia to help that process along. Nvidia itself has demonstrated digital twins of warehouses, using a combination of AI and simulation to calculate the best layouts for shelves and to change in real-time how conveyor robots move around the space so that they do not accidentally block each other.
- Though it can achieve energy savings in the mechanical systems it helps control, AI’s rampant growth raises questions of how big some of the savings could be to society once the energy consumption of the part-cloud, part-local ICT systems are factored in. One major problem is working out just how much the computing consumes in the first place.
- One eye-watering estimate hit the headlines in 2019 when researcher Emma Strubell and colleagues at the University of Massachusetts, Amherst found the development of one of the largest neural networks of the time would emit the same amount of carbon dioxide as five petrol cars over their entire lifetimes from some 650MWh of electricity. It was a statistic that has been repeated many times since in keynotes and talks on the environmental cost of AI, though often with one key detail missing.
- Google engineers argued in a paper published in March this year that the headline figure that often gets quoted does not distinguish between the extensive rounds of training performed on different variants of a model before selecting one that shows enough promise to be trained fully, though the Strubell paper did identify this difference. Even then, the energy cost of targeted training can still easily be counted in megawatt-hours.
- At first the compute requirements for neural-network training were fairly modest. Even today, the kinds of models used for simple computer vision applications can remain fairly light in terms of energy. In their experiments to test the energy demands of AI, a team from the Allen Institute for AI in Seattle, working with Strubell and others, found even a fairly large version of the DenseNet neural network introduced in 2017 for computer vision took about 40Wh of energy to train in about half an hour, about the same as recharging 10 mobile phones.
- The numbers shoot up dramatically for the huge models that have become famous for their apparent ability to understand written text and to connect words to images. Researchers at Stanford University’s Institute for Human-Centred AI regard these natural-language processing (NLP) engines as so important they decided to call them ‘foundation models’.
- Based on the Transformer structure originally developed by Google Brain computer scientists, these networks can take days to train even on arrays of top-end GPUs. BERT-small, which is considerably smaller than one of the headline-grabbing neural networks like OpenAI’s GPT-3 or DALL-E, took the Allen Institute team a day and half to train on eight Nvidia V100 GPUs, for a total energy cost a thousand times higher than DenseNet, at 37kWh.
- The Allen Institute group was unable to commit the resources to test the training of a larger six-billion-parameter model, which is about 30 times smaller than GPT-3, but estimated it would need 103.5MWh to complete the job. OpenAI reported that it needed close to 1.3GWh to train GPT-3.
- Working against this rising demand for energy is the improved efficiency of accelerators designed specifically for machine learning. A 2020 paper by OpenAI claimed algorithmic advances and the use of accelerators on AlexNet, a predecessor of DenseNet, cut the computational cost of training by a factor of 44 over a period of seven years. Similarly, in a paper published in the spring, Google Brain engineers pointed to the advances made in dedicated neural-network accelerators such as the company’s own Tensor Processing Units. They argued their one-trillion-parameter GLaM language model took about a third of the energy needed to train GPT-3. The resulting demand was still equivalent to the daily electricity supply to a small town.
- Though more common than searching for a deep-learning architecture, a factor in training’s favour is that it is not an everyday process. Some applications such as automated driving may call for regular, even daily updates of models that are then downloaded to a fleet of vehicles. In many cases, the re-training will likely be a more restricted and less energy-intensive process and will be amortised across a large fleet.
- Similarly, Nvidia wants to sell customers access to its own supersized NLP engine, Megatron-BERT, but it does not expect customers to fully train it. Foundation models benefit from being amenable to a fine-tuning process for specific tasks that consumes far less data and compute time than the original training. Typically, this process involves 10 per cent of the energy that goes into the initial training.
- Most of the time a neural network model will be used for inferencing: analysing inputs based on what it has already been trained on. Amazon Web Services claimed in 2020 that inference accounted for 90 per cent of the infrastructure costs of AI. However, comparatively little research has gone into the energy usage of AI inferencing, though results have shown consumption to be highly variable.
- “In our carbon-footprint characterisation, inference dominates universal language models’ overall carbon footprint, whereas for deep-learning recommendation tasks, the footprints between training and inference are a roughly equal split,” Meta research scientist Carole-Jean Wu said at the MLSys conference in the summer.
- One big contributor to differences in energy lies in model accuracy, as a study by post-doctoral researcher Fernando Martínez-Plumed at the Valencian Research Institute for AI (VRAIN) and colleagues found in 2021. The top-performing model of 2012 for computer vision needed around two billion floating-point operations (2GFLOPs) to perform a single pass, though this has a lower accuracy than the best available models today. The older model had a 60 per cent chance of being correct when presented with a test image. A much larger, 90 per cent accurate model from 2021 needed more than 3,000GFLOPs. A modern model with 80 per cent accuracy reduced that overhead to 100GFLOPs.
- As with training, successive generations of accelerator have helped prevent inferencing energy growing too far and have provided ways to deploy complex models more cheaply. Stripped-down data formats such as a 16bit operation designed specifically for neural networks in place of the conventional 32bit IEEE-standard format could push efficiency measured in billions of floating-point operations per second per watt (GFLOPS/W) to 1,000, compared to 100 or less for the full-precision calculations, according to the VRAIN analysis. On top of that, overall hardware efficiency has improved thanks to hardware tweaks. Even at the same precision, accelerators and GPUs provide 10 times more GFLOPS/W in 2021 compared to those available in 2011.
- Higher level restructuring pays off even more. Wu outlined at MLSys the process the company’s AI operations use to try to cut down the number of cycles each model needs. Some of these include looking again at the structure of foundation models and slashing the amount of memory they need, which translates into large power savings. She said the technique was not designed to minimise the carbon footprint but has provided a way to make savings.
- The VRAIN researchers found that, thanks to the use of acceleration and other shortcuts, models that make it into production and so are far more likely to be replicated around the world in thousands or millions of instances tend to be more efficient than those that grab the headlines immediately. But savings can come with a sting in the tail.
- “Improved efficiency can translate into more uses. This is also known as Jevons’ paradox: efficiency improvements can encourage higher uses, leading to even higher resource consumption overall. So, despite the fact that we can achieve higher performance efficiency, the overall footprint of machine-learning tasks continues to rise over time,” says Wu.
- Image credit: Dreamstime
- More efficient hardware may just push AI into supporting various forms of Jevons’ paradox. The rise of autonomous vehicles could make transport more efficient and less polluting, a process that will be helped by electrification. But it can lead to increased usage and even substitution for other possibly less polluting modes of transport, some researchers argue.
- In 2020, ETH Zurich academic Vlad Coroama and Daniel Pargman, who works at KTH, came up with the phrase “skill rebound” to describe this form of Jevons’ paradox: a situation where people locked out of using something find themselves able to use it once it has been automated. With self-driving vehicles, children and the elderly could call up a car they cannot drive today, as could people who want to work while they travel to their destination. Road usage and even air travel, made possible by a new generation of personal aircraft, could easily increase, potentially displacing more energy-efficient but individually less convenient choices. How much so is hard to gauge right now, as is the overall contribution of ICT to global energy use and carbon footprint.
- A 2021 study by a team at Lancaster University estimated ICT’s contribution to global greenhouse gas emissions to be between 2 and 3 per cent, possibly up to 4 per cent. Some believe this proportion could easily reach 20 per cent in the coming years. But, in reality, no one yet knows. The Lancaster researchers and others have called for much greater use of measurement and transparency by large ICT users, and some teams have released tools to help the process.
- “We need to encourage the ICT industry to address its own emissions and other environmental impacts,” says Lancaster University lecturer Kelly Widdicks. “And we need to take a more cohesive approach to this, considering the full lifecycle and all scopes of emission rather than the current focus on efficiencies and use-phase emissions. Exact measurement is very difficult to do and rebound effects make ICT’s impacts even more tricky to estimate, but the sector mustn’t delay efforts to reduce its impacts by trying to get accurate numeric values on its emissions.”
- Sign up to the E&T News e-mail to get great stories like this delivered to your inbox every day.
- England, Didcot, Oxfordshire
- £31931 - £44166 per annum
- England, Hampshire, Basingstoke / Berkshire, England, Reading
- £27960 - £65000 per annum
- Contact us
- © 2023 The Institution of Engineering and Technology. The Institution of Engineering and Technology is registered as a Charity in England & Wales (no 211014) and Scotland (no SC038698).

URL: https://spectrum.ieee.org/openai-dall-e-2
- IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our Privacy Policy.
- OpenAI’s text-to-image generator still struggles with text, science, faces, and bias
- IEEE Spectrum queried DALL-E 2 for an image of “a technology journalist writing an article about a new AI system that can create remarkable and strange images.” In response, it sent back only pictures of men.
- In April, the artificial intelligence research lab OpenAI revealed DALL-E 2, the successor to 2021’s DALL-E. Both AI systems can generate astounding images from natural-language text descriptions; they’re capable of producing images that look like photos, illustrations, paintings, animations, and basically any other art style you can put into words. DALL-E 2 upped the ante with better resolution, faster processing, and an editor function that lets the user make changes within a generated image using only text commands, such as “replace that vase with a plant” or “make the dog’s nose bigger.” Users can also upload an image of their own and then tell the AI system how to riff on it.
- The world’s initial reactions to DALL-E 2 were amazement and delight. Any combination of objects and creatures could be brought together within seconds; any art style could be mimicked; any location could be depicted; and any lighting conditions could be portrayed. Who wouldn’t be impressed at the sight, for example, of a parrot flipping pancakes in the style of Picasso? There were also ripples of concern, as people cataloged the industries that could easily be disrupted by such a technology.
- OpenAI has not released the technology to the public, to commercial entities, or even to the AI community at large. “We share people’s concerns about misuse, and it’s something that we take really seriously,” OpenAI researcher 
	Mark Chen tells IEEE Spectrum.But the company did invite select people to experiment with DALL-E 2 and allowed them to share their results with the world. That policy of limited public testing stands in contrast to Google’s policy with its own just-released text-to-image generator, Imagen. When unveiling the system, Google announced that it would not be releasing code or a public demo due to risks of misuse and generation of harmful images. Google has released a handful of very impressive images but hasn’t shown the world any of the problematic content to which it had alluded.
- 
- That makes the images that have come out from the early DALL-E 2 experimenters more interesting than ever. The results that have emerged over the last few months say a lot about the limits of today’s deep-learning technology, giving us a window into what AI understands about the human world—and what it totally doesn’t get.
- OpenAI kindly agreed to run some text prompts from 
	Spectrum through the system. The resulting images are scattered through this article.
- Spectrum asked for "a Picasso-style painting of a parrot flipping pancakes," and DALL-E 2 served it up.
	OpenAI
- DALL-E 2 was trained on approximately 650 million image-text pairs scraped from the Internet, according to 
	the paper that OpenAI posted to ArXiv. From that massive data set it learned the relationships between images and the words used to describe them. OpenAI filtered the data set before training to remove images that contained obvious violent, sexual, or hateful content. “The model isn’t exposed to these concepts,” says Chen, “so the likelihood of it generating things it hasn’t seen is very, very low.” But the researchers have clearly stated that such filtering has its limits and have noted that DALL-E 2 still has the potential to generate harmful material.
- Once this “encoder” model was trained to understand the relationships between text and images, OpenAI paired it with a decoder that generates images from text prompts using a process called diffusion, which begins with a random pattern of dots and slowly alters the pattern to create an image. Again, the company integrated certain filters to keep generated images in line with its 
	content policy and has pledged to keep updating those filters. Prompts that seem likely to produce forbidden content are blocked and, in an attempt to prevent deepfakes, it can't exactly reproduce faces it has seen during its training. Thus far, OpenAI has also used human reviewers to check images that have been flagged as possibly problematic.
- Because of DALL-E 2’s clear potential for misuse, OpenAI initially granted access to only a few hundred people, mostly AI researchers and artists. Unlike the lab’s language-generating model, 
	GPT-3, DALL-E 2 has not been made available for even limited commercial use, and OpenAI hasn’t publicly discussed a timetable for doing so. But from browsing the images that DALL-E 2 users have created and posted on forums such as Reddit, it does seem like some professions should be worried. For example, DALL-E 2 excels at food photography, at the type of stock photos used for corporate brochures and websites, and with illustrations that wouldn’t seem out of place on a dorm room poster or a magazine cover.
- 
- Spectrum asked for a “New Yorker-style cartoon of an unemployed panda realizing her job eating bamboo has been taken by a robot.” OpenAI
- Here’s DALL-E 2’s response to the prompt: “An overweight old dog looks delighted that his younger and healthier dog friends have remembered his birthday, in the style of a greeting card.”OpenAI
- Spectrum reached out to a few entities within these threatened industries. A spokesperson for Getty Images, a leading supplier of stock photos, said the company isn’t worried. “Technologies such a DALL-E are no more a threat to our business than the two-decade reality of billions of cellphone cameras and the resulting trillions of images,” the spokesperson said. What’s more, the spokesperson said, before models such as DALL-E 2 can be used commercially, there are big questions to be answered about their use for generating deepfakes, the societal biases inherent in the generated images, and “the rights to the imagery and the people, places, and objects within the imagery that these models were trained on.” The last part of that sounds like a lawsuit brewing.
- Rachel Hill, CEO of the 
	Association of Illustrators, also brought up the issues of copyright and compensation for images’ use in training data. Hill admits that “AI platforms may attract art directors who want to reach for a fast and potentially lower-price illustration, particularly if they are not looking for something of exceptional quality.” But she still sees a strong human advantage: She notes that human illustrators help clients generate initial concepts, not just the final images, and that their work often relies “on human experience to communicate an emotion or opinion and connect with its viewer.” It remains to be seen, says Hill, whether DALL-E 2 and its equivalents could do the same, particularly when it comes to generating images that fit well with a narrative or match the tone of an article about current events.
- To gauge its ability to replicate the kinds of stock photos used in corporate communications, Spectrum asked for “a multiethnic group of blindfolded coworkers touching an elephant.”OpenAI
- For all DALL-E 2’s strengths, the images that have emerged from eager experimenters show that it still has a lot to learn about the world. Here are three of its most obvious and interesting bugs.
- Text: It’s ironic that DALL-E 2 struggles to place comprehensible text in its images, given that it’s so adept at making sense of the text prompts that it uses to generate images. But users have discovered that asking for any kind of text usually results in a mishmash of letters. The AI blogger Janelle Shane had fun asking the system to create corporate logos and observing the resulting mess. It seems likely that a future version will correct this issue, however, particularly since OpenAI has plenty of text-generation expertise with its GPT-3 team. “Eventually a DALL-E successor will be able to spell Waffle House, and I will mourn that day,” Shane tells Spectrum. “I’ll just have to move on to a different method of messing with it.”
- To test DALL-E 2’s skills with text, Spectrum riffed on the famous Magritte painting that has the French words “Ceci n’est pas une pipe” below a picture of a pipe. Spectrum asked for the words “This is not a pipe” beneath a picture of a pipe. OpenAI
- Science: You could argue that DALL-E 2 understands some laws of science, since it can easily depict a dropped object falling or an astronaut floating in space. But asking for an anatomical diagram, an X-ray image, a mathematical proof, or a blueprint yields images that may be superficially right but are fundamentally all wrong. For example, Spectrum asked DALL-E 2 for an “illustration of the solar system, drawn to scale,” and got back some very strange versions of Earth and its far too many presumptive interplanetary neighbors—including our favorite, Planet Hard-Boiled Egg. “DALL-E doesn’t know what science is. It just knows how to read a caption and draw an illustration,” explains OpenAI researcher Aditya Ramesh, “so it tries to make up something that’s visually similar without understanding the meaning.”
- Spectrum asked for “an illustration of the solar system, drawn to scale,” and got back a very crowded and strange collection of planets, including a blobby Earth at lower left and something resembling a hard-boiled egg at upper left.OpenAI
- Faces: Sometimes, when DALL-E 2 tries to generate photorealistic images of people, the faces are pure nightmare fodder. That’s partly because, during its training, OpenAI introduced some deepfake safeguards to prevent it from memorizing faces that appear often on the Internet. The system also rejects uploaded images if they contain realistic faces of anyone, even nonfamous people. But an additional issue, an OpenAI representative tells Spectrum, is that the system was optimized for images with a single focus of attention. That’s why it’s great at portraits of imaginary people, such as this nuanced portrait produced when Spectrum asked for “an astronaut gazing back at Earth with a wistful expression on her face,” but pretty terrible at group shots and crowd scenes. Just look what happened when Spectrum asked for a picture of seven engineers gathered around a whiteboard.
- This image shows DALL-E 2’s skill with portraits. It also shows that the system’s gender bias can be overcome with careful prompts. This image was a response to the prompt “an astronaut gazing back at Earth with a wistful expression on her face.”OpenAI
- When DALL-E 2 is asked to generate pictures of more than one human at a time, things fall apart. This image of “seven engineers gathered around a white board” includes some monstrous faces and hands. OpenAI
- Bias: We’ll go a little deeper on this important topic. DALL-E 2 is considered a multimodal AI system because it was trained on images and text, and it exhibits a form of multimodal bias. For example, if a user asks it to generate images of a CEO, a builder, or a technology journalist, it will typically return images of men, based on the image-text pairs it saw in its training data.
- Spectrum queried DALL-E 2 for an image of “a technology journalist writing an article about a new AI system that can create remarkable and strange images.” This image shows one of its responses; the others are shown at the top of this article. OpenAI
- OpenAI asked external researchers who work in this area to serve as a “red team” before DALL-E 2’s release, and their insights helped inform OpenAI’s write-up on 
	the system’s risks and limitations. They found that in addition to replicating societal stereotypes regarding gender, the system also over-represents white people and Western traditions and settings. One red team group, from the lab of Mohit Bansal at the University of North Carolina, Chapel Hill, had previously created a system that evaluated the first DALL-E for bias, called DALL-Eval, and they used it to check the second iteration as well. The group is now investigating the use of such evaluation systems earlier in the training process—perhaps sampling data sets before training and seeking additional images to fix problems of underrepresentation or using bias metrics as a penalty or reward signal to push the image-generating system in the right direction.
- Chen notes that a team at OpenAI has already begun experimenting with “machine-learning mitigations” to correct for bias. For example, during DALL-E 2’s training the team found that removing sexual content created a data set with more males than females, which caused the system to generate more images of males. “So we adjusted our training methodology and up-weighted images of females so they’re more likely to be generated,” Chen explains. Users can also help DALL-E 2 generate more diverse results by specifying gender, ethnicity, or geographical location using prompts such as “a female astronaut” or “a wedding in India.”
- But critics of OpenAI say the overall trend toward training models on massive uncurated data sets should be questioned. 
	Vinay Prabhu, an independent researcher who co-authored a 2021 paper about multimodal bias, feels that the AI research community overvalues scaling up models via “engineering brawn” and undervalues innovation. “There is this sense of faux claustrophobia that seems to have consumed the field where Wikipedia-based data sets spanning [about] 30 million image-text pairs are somehow ad hominem declared to be ‘too small’!” he tells Spectrum in an email.
- Prabhu champions the idea of creating smaller but “clean” data sets of image-text pairs from such sources as Wikipedia and e-books, including textbooks and manuals. “We could also launch (with the help of agencies like UNESCO for example) a global drive to contribute images with descriptions according to W3C’s 
	best practices and whatever is recommended by vision-disabled communities,” he suggests.
- The DALL-E 2 team says they’re eager to see what faults and failures early users discover as they experiment with the system, and they’re already thinking about next steps. “We’re very much interested in improving the general intelligence of the system,” says Ramesh, adding that the team hopes to build “a deeper understanding of language and its relationship to the world into DALL-E.” He notes that OpenAI’s text-generating GPT-3 has a surprisingly good understanding of common sense, science, and human behavior. “One aspirational goal could be to try to connect the knowledge that GPT-3 has to the image domain through DALL-E,” Ramesh says.
- As users have worked with DALL-E 2 over the past few months, their initial awe at its capabilities changed fairly quickly to bemusement at its quirks. As one experimenter put it in a 
	blog post, “Working with DALL-E definitely still feels like attempting to communicate with some kind of alien entity that doesn’t quite reason in the same ontology as humans, even if it theoretically understands the English language.” One day, maybe, OpenAI or its competitors will create something that approximates human artistry. For now, we’ll appreciate the marvels and laughs that come from an alien intelligence—perhaps hailing from Planet Hard-Boiled Egg.
- This article appears in the August 2022 print issue as “DALL-E 2’s Failures Show the Limits of AI.”
- Eliza Strickland is a senior editor at IEEE Spectrum, where she covers AI, biomedical engineering, and other topics. She holds a master’s degree in journalism from Columbia University.

URL: https://www.theregister.com/2022/05/08/in_brief_ai/

URL: https://www.theguardian.com/technology/2022/may/04/techscape-openai-dall-e-2
- In this week’s newsletter: Dall-E 2 can conjure vivid pictures of dogs in berets to astronauts playing basketball. It also represents every major ethical concern there is about AI.
- AI progress comes in fits and starts. You hear nothing for months and then, suddenly, the limits of what seems possible are burst asunder. April was one of those months, with two major new releases in the field stunning onlookers.
- The first was Google’s PaLM, a new language model (the same basic type of AI as the famous GPT series) that shows a pretty stunning ability to comprehend and parse complex statements – and explain what it’s doing in the process. Take this simple comprehension question from the company’s announcement:
- Prompt: Which of the following sentences makes more sense? 1. I studied hard because I got an A on the test. 2. I got an A on the test because I studied hard.
- Model Response: I got an A on the test because I studied hard.
- Or this:
- Prompt: Q: A president rides a horse. What would have happened if the president had ridden a motorcycle? 1. She or he would have enjoyed riding the horse. 2. They would have jumped a garden fence. 3. She or he would have been faster. 4. The horse would have died.
- Model Response: She or he would have been faster.
- These are the sorts of questions that computers have historically struggled with, that require a fairly broad understanding of basic facts about the world before you can begin tackling the statement in front of you. (For another example, try parsing the famous sentence “time flies like an arrow, fruit flies like a banana”).
- So poor Google that, less than a week later, its undeniable achievements with PaLM were overshadowed by a far more photogenic release from OpenAI, the formerly Musk-backed research lab that spawned GPT and its successors. The lab showed off Dall-E 2 (as in, a hybrid of Wall-E and Dalí), an image generation AI with the ability to take text descriptions in natural language and spit out alarmingly detailed images.
- A picture is worth a thousand words, so here’s a short book about Dall-E 2, with the pictures accompanied by the captions that generated them.
- From the official announcement, “An astronaut playing basketball with cats in space in a watercolor style”:
- And “A bowl of soup as a planet in the universe as a 1960s poster”:
- From the academic paper going into detail about how Dall-E 2 works, “a shiba inu wearing a beret and black turtleneck”:
- And “a teddy bear on a skateboard in times square”:
- Not all the prompts have to be in conversational English, and throwing in a bunch of keywords can help tune what the system does. In this case, “artstation” is the name of an illustration social network, and Dall-E is effectively being told “make these images as you’d expect to see them on artstation”. And so:
- “panda mad scientist mixing sparkling chemicals, artstation”
- “a dolphin in an astronaut suit on saturn, artstation”
- The system can do more than simple generation, though. It can produce variations on a theme, effectively by looking at an image, describing it itself, and then creating more images based on that description. Here’s what it gets from Dalí’s famous The Persistence of Memory, for instance:
- And it can create images that are a blend of two, in a similar way. Here’s Starry Night merging with two dogs:
- It can also use one image as an anchor and then modify it with a text description. Here we see a “photo of a cat” becoming “an anime drawing of a super saiyan cat, artstation”:
- These images are all, of course, cherrypicked. They are the best, most compelling examples of what the AI can produce. OpenAI has not, despite its name, opened up access to Dall-E 2 to all, but it has allowed a few people to play with the model, and is taking applications for a waiting list in the meantime.
- Dave Orr, a Google AI staffer, is one lucky winner, and published a critical assessment: “One thing to be aware of when you see amazing pictures that DE2 generates, is that there is some cherrypicking going on. It often takes a few prompts to find something awesome, so you might have looked at dozens of images or more.”
- Orr’s post also highlights the weaknesses of the system. Despite being a sibling to GPT, for instance, Dall-E 2 can’t really do writing; it focuses on looking right, rather than reading right, leading to images like this, caption “a street protest in belfast”:
- There’s one last load of images to look at, and it’s a much less rosy one. OpenAI published a detailed document on the “Risks and Limitations” of the tool, and when laid out in one large document, it’s positively alarming. Every major concern from the past decade of AI research is represented somewhere.
- Take bias and stereotypes: ask Dall-E for a nurse, and it will produce women. Ask it for a lawyer, it will produce men. A “restaurant” will be western; a “wedding” will be heterosexual:
- The system will also merrily produce explicit content, depicting nudity or violence, even though the team endeavoured to filter that out of its training material. “Some prompts requesting this kind of content are caught with prompt filtering in the DALL·E 2 Preview,” they say, but new problems are thrown up: the use of the 🍆 emoji, for instance, seems to have confused Dall-E 2, so that “‘A person eating eggplant for dinner’; contained phallic imagery in the response.”
- OpenAI also addresses a more existential problem: the fact that the system will happily generate “trademarked logos and copyrighted characters”. It’s not great on the face of it if your cool new AI keeps spitting out Mickey Mouse images and Disney has to send a stern word. But it also raises awkward questions about the training data for the system, and whether training an AI using images and text scraped off the public internet is, or should be, legal.
- Not everyone was impressed by OpenAI’s efforts to warn about the harms. “It’s not good enough to simply write reports about the risks of this technology. This is the AI lab equivalent of thoughts and prayers – without action it doesn’t mean anything,” says Mike Cook, a researcher in AI creativity. “It’s useful to read these documents and there are interesting observations in them … But it’s also clear that certain options – such as halting work on these systems – are not on the table. The argument given is that building these systems helps us understand risks and develop solutions, but what did we learn between GPT-2 and GPT-3? It’s just a bigger model with bigger problems.
- “You don’t need to build a bigger nuclear bomb to know we need disarmament and missile defence. You build a bigger nuclear bomb if you want to be the person who owns the biggest nuclear bomb. OpenAI wants to be a leader, to make products, to build licensable technology. They cannot stop this work for that reason, they’re incapable of it. So the ethics stuff is a dance, much like greenwashing and pinkwashing is with other corporations. They must be seen to make motions towards safety, while maintaining full speed ahead on their work. And just like greenwashing and pinkwashing, we must demand more and lobby for more oversight.”
- Almost a year on from the first time we looked at a cutting edge AI tool in this newsletter, the field hasn’t shown any signs of getting less contentious. And we haven’t even touched on the chance that AI could “go FOOM” and change the world. File that away for a future letter.
- If you want to read the complete version of the newsletter please subscribe to receive TechScape in your inbox every Wednesday.

URL: https://edition.cnn.com/2022/10/21/tech/artists-ai-images/
- Erin Hanson has spent years developing the vibrant color palette and chunky brushstrokes that define the vivid oil paintings for which she is known. But during a recent interview with her, I showed Hanson my attempts to recreate her style with just a few keystrokes.
- Using Stable Diffusion, a popular and publicly available open-source AI image generation tool, I had plugged in a series of prompts to create images in the style of some of her paintings of California poppies on an ocean cliff and a field of lupin.
- “That one with the purple flowers and the sunset,” she said via Zoom, peering at one of my attempts, “definitely looks like one of my paintings, you know?”
- With Hanson’s guidance, I then tailored another detailed prompt: “Oil painting of crystal light, in the style of Erin Hanson, light and shadows, backlit trees, strong outlines, stained glass, modern impressionist, award-winning, trending on ArtStation, vivid, high-definition, high-resolution.” I fed the prompt to Stable Diffusion; within seconds it produced three images.
- “Oh, wow,” she said as we pored over the results, pointing out how similar the trees in one image looked to the ones in her 2021 painting “Crystalline Maples.” “I would put that on my wall,” she soon added.
- Hanson, who’s based in McMinnville, Oregon, is one of many professional artists whose work was included in the data set used to train Stable Diffusion, which was released in August by London-based Stability AI. She’s one of several artists interviewed by CNN Business who were unhappy to learn that pictures of their work were used without someone informing them, asking for consent, or paying for their use.
- Once available only to a select group of tech insiders, text-to-image AI systems are becoming increasingly popular and powerful. These systems include Stable Diffusion, from a company that recently raised more than $100 million in funding, and DALL-E, from a company that has raised $1 billion to date.
- These tools, which typically offer some free credits before charging, can create all kinds of images with just a few words, including those that are clearly evocative of the works of many, many artists (if not seemingly created by the same artist). Users can invoke those artists with words such as “in the style of” or “by” along with a specific name. And the current uses for these tools can range from personal amusement to more commercial cases.
- In just months, millions of people have flocked to text-to-image AI systems and they are already being used to create experimental films, magazine covers and images to illustrate news stories. An image generated with an AI system called Midjourney recently won an art competition at the Colorado State Fair, and caused an uproar among artists.
- But as artists like Hanson have discovered that their work is being used to train AI, it raises an even more fundamental concern: that their own art is effectively being used to train a computer program that could one day cut into their livelihoods. Anyone who generates images with systems such as Stable Diffusion or DALL-E can then sell them (the specific terms regarding copyright and ownership of these images varies).
- “I don’t want to participate at all in the machine that’s going to cheapen what I do,” said Daniel Danger, an illustrator and print maker who learned a number of his works were used to train Stable Diffusion.
- The machines are far from magic. For one of these systems to ingest your words and spit out an image, it must be trained on mountains of data, which may include billions of images scraped from the internet, paired with written descriptions.
- Some services, including OpenAI’s DALL-E system, don’t disclose the datasets behind their AI systems. But with Stable Diffusion, Stability AI is clear about its origins. Its core dataset was trained on image and text pairs that were curated for their looks from an even more massive cache of images and text from the internet. The full-size dataset, known as LAION-5B was created by the German AI nonprofit LAION, which stands for “large-scale artificial intelligence open network.”
- This practice of scraping images or other content from the internet for dataset training isn’t new, and traditionally falls under what’s known as “fair use” — the legal principle in US copyright law that allows for the use of copyright-protected work in some situations. That’s because those images, many of which may be copyrighted, are being used in a very different way, such as for training a computer to identify cats.
- But datasets are getting larger and larger, and training ever-more-powerful AI systems, including, recently, these generative ones that anyone can use to make remarkable looking images in an instant.
- A few tools let anyone search through the LAION-5B dataset, and a growing number of professional artists are discovering their work is part of it. One of these search tools, built by writer and technologist Andy Baio and programmer Simon Willison, stands out. While it can only be used to search a small fraction of Stable Diffusion’s training data (more than 12 million images), its creators analyzed the art imagery within it and determined that, of the top 25 artists whose work was represented, Hanson was one of just three who is still alive. They found 3,854 images of her art included in just their small sampling.
- Stability AI founder and CEO Emad Mostaque told CNN Business via email that art is a tiny fraction of the LAION training data behind Stable Diffusion. “Art makes up much less than 0.1% of the dataset and is only created when deliberately called by the user,” he said.
- But that’s slim comfort to some artists.
- Danger, whose artwork includes posters for bands like Phish and Primus, is one of several professional artists who told CNN Business they worry that AI image generators could threaten their livelihoods.
- He is concerned that the images people produce with AI image generators could replace some of his more “utilitarian” work, which includes media like book covers and illustrations for articles published online.
- “Why are we going to pay an artist $1,000 when we can have 1,000 [images] to pick from for free?” he asked. “People are cheap.”
- Meta is using AI to generate videos from just a few words
- Tara McPherson, a Pittsburgh-based artist whose work is featured on toys, clothing and in films such as the Oscar-winning “Juno,” is also concerned about the possibility of losing out on some work to AI. She feels disappointed and “taken advantage of” for having her work included in the dataset behind Stable Diffusion without her knowledge, she said.
- “How easy is this going to be? How elegant is this art going to become?,” she asked. “Right now it’s a little wonky sometimes but this is just getting started.”
- While the concerns are real, the recourse is unclear. Even if AI-generated images have a widespread impact — such as by changing business models — it doesn’t necessarily mean they’re violating artists’ copyrights, according to Zahr Said, a law professor at the University of Washington. And it would be prohibitive to license every single image in a dataset before using it, she said.
- “You can actually feel really sympathetic for artistic communities and want to support them and also be like, there’s no way,” she said. “If we did that, it would essentially be saying machine learning is impossible.”
- McPherson and Danger mused about the possibility of putting watermarks on their work when posting it online to safeguard the images (or at least make them look less appealing). But McPherson said when she’s seen artist friends put watermarks across their images online it “ruins the art, and the joy of people looking at it and finding inspiration in it.”
- If he could, Danger said he would remove his images from datasets used to train AI systems. But removing pictures of an artist’s work from a dataset wouldn’t stop Stable Diffusion from being able to generate images in that artist’s style.
- AI won an art contest, and artists are furious
- For starters, the AI model has already been trained. But also, as Mostaque said, specific artistic styles could still be called on by users because of OpenAI’s CLIP model, which was used to train Stable Diffusion to understand connections between words and images.
- Christoph Schuhmann, an LAION founder, said via email that his group thinks that truly enabling opting in and out of datasets will only work if all parts of AI models — of which there can be many — respect those choices.
- “A unilateral approach to consent handling will not suffice in the AI world; we need a cross-industry system to handle that,” he said.
- Partners Mathew Dryhurst and Holly Herndon, Berlin-based artists experimenting with AI in their collaborative work, are working to tackle these challenges. Together with two other collaborators, they have launched Spawning, making tools for artists that they hope will let them better understand and control how their online art is used in datasets.
- In September, Spawning released a search engine that can comb through the LAION-5B dataset, haveibeentrained.com, and in the coming weeks it intends to offer a way for people to opt out or in to datasets used for training. Over the past month or so, Dryhurst said, he’s been meeting with organizations training large AI models. He wants to get them to agree that if Spawning gathers lists of works from artists who don’t want to be included, they’ll honor those requests.
- Dryhurst said Spawning’s goal is to make it clear that consensual data collection benefits everyone. And Mostaque agrees that people should be able to opt out. He told CNN Business that Stability AI is working with numerous groups on ways to “enable more control of database contents by the community” in the future. In a Twitter thread in September, he said Stability is open to contributing to ways that people can opt out of datasets, “such as by supporting Herndon’s work on this with many other projects to come.”
- “I personally understand the emotions around this as the systems become intelligent enough to understand styles,” he said in an email to CNN Business.
- Schuhmann said LAION is also working with “various groups” to figure out how to let people opt in or out of including their images in training text-to-image AI models. “We take the feelings and concerns of artists very seriously,” Schuhmann said.
- Hanson, for her part, has no problem with her art being used for training AI, but she wants to be paid. If images are sold that were made with the AI systems trained on their work, artists need to be compensated, she said — even if it’s “fractions of pennies.”
- This could be on the horizon. Mostaque said Stability AI is looking into how “creatives can be rewarded from their work,” particularly as Stability AI itself releases AI models, rather than using those built by others. The company will soon announce a plan to get community feedback on “practical ways” to do this, he said.
- Theoretically, I may eventually owe Hanson some money. I’ve run that same “crystal light” prompt on Stable Diffusion many times since we devised it, so many in fact that my laptop is littered with trees in various hues, rainbows of sunlight shining through their branches onto the ground below. It’s almost like having my own bespoke Hanson gallery.
- Most stock quote data provided by BATS. US market indices are shown in real time, except for the S&P 500 which is refreshed every two minutes. All times are ET. Factset: FactSet Research Systems Inc. All rights reserved. Chicago Mercantile: Certain market data is the property of Chicago Mercantile Exchange Inc. and its licensors. All rights reserved. Dow Jones: The Dow Jones branded indices are proprietary to and are calculated, distributed and marketed by DJI Opco, a subsidiary of S&P Dow Jones Indices LLC and have been licensed for use to S&P Opco, LLC and CNN. Standard & Poor’s and S&P are registered trademarks of Standard & Poor’s Financial Services LLC and Dow Jones is a registered trademark of Dow Jones Trademark Holdings LLC. All content of the Dow Jones branded indices Copyright S&P Dow Jones Indices LLC and/or its affiliates. Fair value provided by IndexArb.com. Market holidays and trading hours provided by Copp Clark Limited.
- © 2023 Cable News Network. A Warner Bros. Discovery Company. All Rights Reserved.  CNN Sans ™ & © 2016 Cable News Network.

URL: https://www.theverge.com/2022/9/28/23376328/ai-art-image-generator-dall-e-access-waitlist-scrapped
- By  James Vincent, a senior reporter who has covered AI, robotics, and more for eight years at The Verge.
- OpenAI has scrapped the wait list for access to its text-to-image system DALL-E 2, meaning anyone can sign up to use the AI art generator immediately.
- The company unveiled the original DALL-E in January 2021, with the tool impressing both AI experts and the public with its ability to turn any text description (or prompt) into a unique image. Since then, a number of other text-to-image systems have been created that rival the speed and quality of DALL-E. Other systems, like Midjourney and Stable Diffusion, are much easier for anyone to access, drawing attention away from OpenAI’s own offering.
- Open access AI art image generators have drawn attention away from DALL-E
- OpenAI, which has received substantial funding from tech giant Microsoft, has been cautious about the public release of DALL-E. Experts note that the ability of text-to-image systems to produce nonconsensual nudes and photorealistic images is potentially damaging — fodder for harassment, propaganda, misinformation, and more. There are also issues of bias. Because text-to-images systems are trained on vast datasets of images scraped from the internet, they replicate unequal aspects of society. Ask a system to draw a picture of a CEO, for example, and it will typically produce an image of a white man.
- OpenAI has taken a number of measures to combat these effects, including filtering out sexual and violent imagery from its training data and refusing to generate images based on similarly explicit prompts. However, the company has also been criticized for what some see as an overly restrictive or clumsy approach to mitigating harm.
- Emad Mostaque, who helped develop rival text-to-image AI Stable Diffusion, said, for example, that it was an “asshole move” for OpenAI not to generate images from words like “Ukraine” and “Odesa.” (Presumably these terms are censored because of their potential to create misinformation during an ongoing war). Others have criticized the company’s attempts to fix bias as “hacky.” For example, DALL-E invisibly inserts phrases like “Black man” and “Asian woman” into user prompts that do not specify gender or ethnicity in order to nudge the system away from generating images of white people. (OpenAI confirmed to The Verge that it uses this method.) This does mitigate bias in DALL-E’s output, but some users have noted it also creates unwanted imagery that doesn’t match their instructions.
- OpenAI is confident it’s mitigated DALL-E’s potentially harmful effects
- In its blog post today, OpenAI said it was satisfied with the improvements it’s made to its safety systems and that this will help offset potential harms as DALL-E becomes more accessible. “In the past months, we have made our filters more robust at rejecting attempts to generate sexual, violent and other content that violates our content policy, and building new detection and response techniques to stop misuse,” said the company.
- The firm also said it’s testing an API for DALL-E that would allow companies to build their own apps and plug-ins using the system’s output. This would make it much easier for OpenAI to commercialize DALL-E’s output, potentially combining the system with tools used by illustrators and designers, for example.
- Anyone who signs up for access to DALL-E will get 50 credits free and then 15 more free credits every month after that. Each credit can be used to generate a single image, a variation of an image, or for “inpainting” and “outpainting” (editing the contents of an image or extending an image beyond its existing boundaries). Additional credits can be bought in blocks of 115 for $15. OpenAI says some 1.5 million DALL-E users are generating over 2 million images every day.
- / Sign up for Verge Deals to get deals on products we've tested sent to your inbox daily.
- The Verge is a vox media network
- © 2023 Vox Media, LLC. All Rights Reserved

URL: https://gizmodo.com/dall-e-ai-openai-deep-fakes-image-generators-1849557604
- OpenAI believes it’s ready to start letting DALL-E users edit images of real human faces, a possibility previously blocked over concerns of potential sexual and political deepfakes proliferating from the AI.
- In a letter to users on Monday spotted by TechCrunch and shared with Gizmodo, OpenAI said it would reintroduce the ability to upload and edit real human faces to its advanced AI image generator after building new detection and response techniques meant to prevent misuse and ultimately minimize, “the potential of harm.” Users are reportedly still barred from uploading images of people without their consent as well as images they don’t have legal rights to.
- “Many of you have told us that you miss using DALL-E to dream up outfits and hairstyles on yourselves and edit the backgrounds of family photos,” OpenAI said in an email. “A reconstructive surgeon told us that he’d been using DALL-E to help his patients visualize results. And filmmakers have told us that they want to be able to edit images of scenes with people to help speed up their creative processes.”
- OpenAI said it made its filters, “more robust” at spotting and rejecting attempts to generate sexual, violent, or political content, while simultaneously working to reduce “false flags.”
- DALL-E users wasted no time offering up their faces to the program. Here’s a few examples posted on Twitter.
- As AI image generators go, OpenAI has taken a relatively conservative route to realistic human faces, likely in order to avoid unintentionally facilitating the spread of deepfaked pornographic images and other graphic content already uploaded by less restrictive alternatives like Stability AI’s Stable Diffusion.
- Monday’s move builds off of previous experimentation with human-like faces at OpenAI. In June, the company said it would let researchers begin generating images of realistic looking humans belonging to non-real humans. Though these images don’t run into the same consent issues as authentic human faces since they don’t involve actual living people, OpenAI similarly put up guardrails to avoid DALL-E spitting out a cesspool of unwanted images or “deceptive content.”
- Deceptive or not, many of the recent images created by users and posted online are nonetheless pretty creepy, with the photorealistic images of fabricated people straddling right along the furthest edge of the uncanny valley.
- Updated 9/20/22 11:27 a.m. ET: Added email details from OpenAI.

URL: https://restofworld.org/2022/ai-backlash-anime-artists/
- On October 3, renowned South Korean illustrator Kim Jung Gi passed away unexpectedly at the age of 47. He was beloved for his innovative ink-and-brushwork style of manhwa, or Korean comic-book art, and famous for captivating audiences by live-drawing huge, intricate scenes from memory.
- Just days afterward, a former French game developer, known online as 5you, fed Jung Gi’s work into an AI model. He shared the model on Twitter as an homage to the artist, allowing any user to create Jung Gi-style art with a simple text prompt. The artworks showed dystopian battlefields and bustling food markets — eerily accurate in style, and, apart from some telltale warping, as detailed as Jung Gi’s own creations.
- The response was pure disdain. “Kim Jung Gi left us less than [a week ago] and AI bros are already ‘replicating’ his style and demanding credit. Vultures and spineless, untalented losers,” read one viral post from the comic-book writer Dave Scheidt on Twitter. “Artists are not just a ‘style.’ They’re not a product. They’re a breathing, experiencing person,” read another from cartoonist Kori Michele Handwerker.
- Far from a tribute, many saw the AI generator as a theft of Jung Gi’s body of work. 5you told Rest of World that he has received death threats from Jung Gi loyalists and illustrators, and asked to be referred to by his online pseudonym for safety.
- Generative AI might have been dubbed Silicon Valley’s “new craze,” but beyond the Valley, hostility and skepticism are already ramping up among an unexpected user base: anime and manga artists. In recent weeks, a series of controversies over AI-generated art — mainly in Japan, but also in South Korea — have prompted industry figures and fans to denounce the technology, along with the artists that use it.
- While there’s a long-established culture of creating fan art from copyrighted manga and anime, many are drawing a line in the sand where AI creates a similar artwork. Rest of World spoke to generative AI companies, artists, and legal experts, who saw this backlash as being rooted in the intense loyalty of anime and manga circles — and, in Japan, the lenient laws on copyright and data-scraping. The rise of these models isn’t just blurring lines around ownership and liability, but already stoking panic that artists will lose their livelihoods.
- “I think they fear that they’re training for something they won’t ever be able to live off because they’re going to be replaced by AI,” 5you told Rest of World.
- One of the catalysts is Stable Diffusion, a competitor to the AI art model Dall-E, which hit the market on August 22. Stability AI is open-source, which means that, unlike Dall-E, engineers can train the model on any image dataset to churn out almost any style of art they desire — no beta invite or subscription needed. 5you, for instance, pulled Jung Gi’s illustrations from Google Images without permission from the artist or publishers, which he then fed into Stable Diffusion’s service.
- In mid-October, Stability AI, the company behind Stable Diffusion, raised a reported $101 million dollars and earned about a $1 billion valuation. Looking for a cut of this market, AI startups are building off Stable Diffusion’s open-source code to launch more specialized and refined generators, including several primed for anime and manga art.
- “I think they fear that they’re training for something they won’t ever be able to live off of because they’re going to be replaced by AI.”
- Japanese AI startup Radius5 was one of the first companies to touch a nerve when, in August, it launched an art-generation beta called Mimic that targeted anime-style creators. Artists could upload their own work and customize the AI to produce images in their own illustration style; the company recruited five anime artists as test cases for the pilot.
- Almost immediately, on Mimic’s launch day, Radius5 released a statement that the artists were being targeted for abuse on social media. “Please refrain from criticizing or slandering creators,” the company’s CEO, Daisuke Urushihara, implored the swarm of Twitter critics. Illustrators decried the service, saying Mimic would cheapen the art form and be used to recreate artists’ work without their permission.
- And they were partly right. Just hours after the statement, Radius5 froze the beta indefinitely because users were uploading other artists’ work. Even though this violated Mimic’s terms of service, no restrictions had been built to prevent it. The phrase “AI学習禁止” (“No AI Learning”) lit up Japanese Twitter.
- A similar storm gathered around storytelling AI company NovelAI, which launched an image generator on October 3; Twitter rumors rapidly circulated that it was simply ripping human-drawn illustrations from the internet. Virginia Hilton, NovelAI’s community manager, told Rest of World that she thought the outrage had to do with how accurately the AI could imitate anime styles.
- “I do think that a lot of Japanese people would consider [anime] art a kind of export,” she told Rest of World. “Finding the capabilities of the [NovelAI] model, and the improvement over Stable Diffusion and Dall-E — it can be scary.” The company also had to pause the service for emergency maintenance. Its infrastructure buckled from a spike in traffic, largely from Japan and South Korea, and a hacking incident. The team published a blog post in Japanese to explain how it all works, while scrambling to hire friends to translate their Twitter and Discord posts.
- The ripple effect goes on. A Japanese artist was obliged to tweet screenshots showing layers of her illustration software to counter accusations that she was secretly using AI. Two of country’s most famous VTuber bands requested that millions of social media followers stop using AI in their fan art, citing copyright concerns if their official accounts republished the work. Pixiv has announced it will be launching tags to filter out AI-generated work in its search feature and in its popularity rankings.
- In effect, manga and anime are acting as an early testing ground for AI art-related ethics and copyright liability. The industry has long permitted the reproduction of copyrighted characters through doujinshi (fan-made publications), partly to stoke popularity of the original publications. Even the late Prime Minister Shinzo Abe once weighed in on the unlicensed industry, arguing it should be protected from litigation as a form of parody.
- Outside of doujinshi, Japanese law is ordinarily harsh on copyright violations. Even a user who simply retweets or reposts an image that violates copyright can be subject to legal prosecution. But with art generated by AI, legal issues only arise if the output is exactly the same, or very close to, the images on which the model is trained.
- “If the images generated are identical … then publishing [those images] may infringe on copyright,” Taichi Kakinuma, an AI-focused partner at the law firm Storia and a member of the economy ministry’s committee on contract guidelines for AI and data, told Rest of World. That’s a risk with Mimic, and similar generators built to imitate one artist. “Such [a result] could be generated if it is trained only with images of a particular author,” Kakinuma said.
- But successful legal cases against AI firms are unlikely, said Kazuyasu Shiraishi, a partner at the Tokyo-headquartered law firm TMI Associates, to Rest of World. In 2018, the National Diet, Japan’s legislative body, amended the national copyright law to allow machine-learning models to scrape copyrighted data from the internet without permission, which offers up a liability shield for services like NovelAI.
- Whether images are sold for profit or not is largely irrelevant to copyright infringement cases in the Japanese courts, said Shiraishi. But to many working artists, it’s a real fear.
- Haruka Fukui, a Tokyo-based artist who creates queer romance anime and manga, admits that AI technology is on track to transform the industry for illustrators like herself, despite recent protests. “There is a concern that the demand for illustrations will decrease and requests will disappear,” she told Rest of World. “Technological advances have both the benefits of cost reduction and the fear of fewer jobs.”
- Fukui has considered using AI herself as an assistive tool, but showed unease when asked if she would give her blessing to AI art generated using her work.
- “I don’t intend to consider legal action for personal use,” she said. “[But] I would consider legal action if I made my opinion known on the matter, and if money is generated,” she added. “If the artist rejects it, it should stop being used.”
- But the case of Kim Jung Gi shows artists may not be around to give their blessing. “You can’t express your intentions after death,” Fukui admits. “But if only you could ask for the thoughts of the family.”

- LAION image-text pairings datasets
- GPT-3 anti-Muslim bias
- Page info Type: SystemPublished: November 2022
