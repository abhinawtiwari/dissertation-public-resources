- Released: TBC
- Can you improve this page?Share your insights with us
- HireVue is a Salt Lake City-based company that uses video, gaming, and proprietary algorithms to assess job seekers, whose written answers, behaviour, intonation, and speech are fed into algorithms that assign them certain traits and qualities.
- The company is seen as amongst the top in its sector, and its products are used by 700+ customers in the US, UK, and elsewhere, including General Mills, Kraft, and Unilever.
- A number of HireVue's practices - notably its use of psychological inferences to determine people's ability and character based on facial data - have also proved controversial with academics, ethicists, regulators, and commentators.
- In November 2019, US privacy group Electronic Privacy Information Center (EPIC) filed a legal complaint (pdf) alleging HireVue's use of facial technologies and biometric data 'constitute unfair and deceptive trade practices' and that it produces results that are 'biased, unprovable, and not replicable'.
- It alleged HireVue's hiring algorithms are more likely to be biased by default, in contrast to it's marketing claims, that it's models fail to meet international standards on AI-based decision making, and that the company fails to give candidates access to their assessment scores or the training data, factors, logic, or techniques used to generate each algorithmic assessment.
- The complaint also set out that HireVue's claims that it 'does not use facial recognition technology' is misleading as it collects and analyses 'facial expressions' and 'facial movements' to measure job candidates’ 'cognitive ability,' 'emotional intelligence,' and 'social aptitudes.'
- Futhermore, EPIC accused HireVue of engaging in the 'intrusive collection and secret analysis of biometric data', thereby causing 'substantial privacy harms' to job candidates, that it’s assessment system causes 'substantial financial harms 'to job candidates, and that it's facial recognition software could be racially biased or improperly used to identify sexual orientation.
- In January 2021, HireVue announced that it had 'proactively' stopped using facial analysis in new assessments on the grounds that it's own internal research had 'demonstrated that recent advances in natural language processing had significantly increased the predictive power of language', meaning visual analysis 'no longer significantly added value to assessments.'
- In a response to WIRED, John Davisson, EPIC senior counsel, said 'I am surprised they are dropping this, as it was a keystone feature of the product they were marketing.' 'That is the source of a lot of concerns around biometric data collection, as well as these bold claims about being able to measure psychological traits, emotional intelligence, social attitudes, and things like that.'
- Alongside its statement on facial analysis, HireVue released the results of an algorithmic audit by O’Neil Risk Consulting & Algorithmic Auditing (ORCAA) that it concludes it's AI-based pre-built assessments used in hiring early career candidates do not demonstrate bias.
- However, Brookings Institution fellow Alex Engler pointed out in a Fast Company op-ed that HireVue mispresented both the claim that its termination of facial analysis was 'proactive' and that the audit concluded all of HireVue’s assessments were unbiased.
- The audit, he argued 'was narrowly focused on a specific use case, and it didn’t examine the assessments for which HireVue has been criticized, which include facial analysis and employee performance predictions.'
- HireVue claims that it 'leads the industry with commitment to transparent and ethical use of AI in hiring' have been undermined by the opaque nature of its products and selectively misleading marketing.
- In March 2022, HireVue released what it billed as a 'first of its kind' explainability statement (pdf) intended to explain clearly how it uses AI in its game-based and interview assessments.
- The Center for Democracy and Technology (CDC) said the statement 'sheds some useful light on how HireVue’s technology works, it is also incomplete in important respects', and 'suggests crucial deficiencies in the fairness and job-relatedness of HireVue’s approach to assessments.'
- Operator: Delta; General Electric; General Mills; Hilton; Kraft; UnileverDeveloper: HireVue
- Country: USA
- Sector: Business/professional services
- Purpose: Improve recruitment efficiency & effectiveness
- Technology: Facial analysis; Facial recognition; Behavioural analysis; NLP/text analysis Issue: Accuracy/reliability; Bias/discrimination - gender, disability; Privacy
- Transparency: Governance; Black box; Marketing
URL: https://www.hirevue.com/
- Candidates: Are you interviewing and need support?
- Modern Hire is now part of HireVue, solidifying HireVue’s position as the industry leader in transforming talent experiences. By adding Modern Hire to the HireVue portfolio, we continue to provide the best way for enterprises to engage, evaluate and select talent.
- Hiring speed matters when hiring hourly workers. Keep your candidates engaged with text-powered solutions that offer 24/7/365 day engagement while assessments identify the critical skills needed for success.
- 
- Learn More
- HireVue allows you to reach more students without traveling to more campuses. Reach this generation faster through text and video on their phones with solutions that support a full mobile hiring process. Candidates can text with recruiters and take interviews from any device–anytime and anywhere. Offer busy students flexibility during their candidate journey by offering on-demand and live video interviews and assessments.
- 
- Learn More
- Slow outdated processes will cost you top talent–and lost revenue. You need a hiring solution that allows your teams to move faster than the competition but doesn’t force them to sacrifice quality. HireVue’s solutions make recruiting and hiring teams more productive by consolidating steps and integrating seamlessly with your ATS. Reimagine how you recruit and hire with a faster process that allows you to focus on the skills that matter for success.
- 
- Learn More
- Better understand the person behind the code. Find talent that can meet deadlines and work well in teams–and also know how to code. Consistently identify qualified talent, include engineers, developers and data scientists, with coding assessments and challenges that are fully validated by a team of Industrial-Organizational Psychologists.
- 
- Learn More
- 30M+
- 46M+
- 6M+
- 90%
- HIREVUE
- RESOURCES
- WHY HIREVUE
- COMPANY
- © 2023 HireVue, Inc. All rights reserved.

URL: https://www.hirevue.com/press-release/hirevue-discontinues-facial-analysis-screening
- Candidates: Are you interviewing and need support?
- HIREVUE
- RESOURCES
- WHY HIREVUE
- COMPANY
- © 2023 HireVue, Inc. All rights reserved.

URL: https://apnews.com/press-release/globe-newswire/technology-political-issues-government-and-politics-artificial-intelligence-political-ethics-37d0fe232bcce86da20a5596cc840552
- 
- 
- 
- 
- 
- SALT LAKE CITY, Jan. 12, 2021 (GLOBE NEWSWIRE) -- HireVue, the global leader in video interviewing, assessments, chatbot and recruiting automation technology, today made two announcements to extend its leadership position defining the transparent and appropriate use of AI and software as part of the hiring process.
- HireVue released the results of an algorithmic audit by O’Neil Risk Consulting & Algorithmic Auditing (ORCAA). The audit concluded that “[HireVue] assessments work as advertised with regard to fairness and bias issues.” ORCAA’s CEO and founder, Cathy O’Neil, and her team audited an assessment used in hiring early career candidates (including from college campuses) as a representative use case that was developed with HireVue’s methodology. Moving forward, HireVue and ORCAA will continue to collaborate on ways to further enhance HireVue’s offerings. As a result, HireVue aims to set a high industry bar for candidate transparency and equality of access.
- Independently, early in 2020, HireVue proactively removed the visual analysis component from all of its new assessments. HireVue’s internal research demonstrated that recent advances in natural language processing had significantly increased the predictive power of language. With these advances, visual analysis no longer significantly added value to assessments. HireVue published a review of these supporting findings.
- “Democratizing the hiring process for candidates and employers is the foundation on which HireVue stands. Our news today reaffirms HireVue’s leadership role in defining the best, most appropriate use of AI in hiring, and our commitment to rigorous, ongoing measurement of the impact our software has on individual candidates, our customers, and society as a whole,” said Kevin Parker, Chairman and CEO of HireVue. “Along the way, we adhere to a strict set of ethical principles that ensures both employer and candidate are assured of a fair, diverse and inclusive hiring journey.”
- Ongoing Commitment to Fairness
- HireVue has a long, established track record delivering science-led ethical AI solutions for hiring:
- 
- 
- 
- 
- More details on our commitment to AI Ethics and standards can be found at www.hirevue.com/why-hirevue/ai-ethics
- For access to the ORCAA report visit https://www.hirevue.com/resources/orcaa-report
- About HireVue  HireVue is where hiring happens - transforming the way organizations discover, engage, and hire the best talent. Connecting companies and candidates anytime, anywhere, HireVue’s industry leading end-to-end hiring platform features video interviewing, assessments and conversational AI. HireVue has hosted more than 18 million video interviews and 114M chat-based candidate engagements for over 700 customers around the globe.
- Press Contact  Andrea Judson-Torres  Highwire PR  310-592-6341  hirevue@highwirepr.com
- 

URL: https://webapi.hirevue.com/wp-content/uploads/2022/03/HV_AI_Short-Form_Explainability_3152022.pdf

URL: https://epic.org/documents/in-re-hirevue/
- Consumer Cases
- 
- 
- 
- On November 6, 2019, EPIC filed a complaint with the Federal Trade Commission alleging that recruiting company HireVue has committed unfair and deceptive practices in violation of the FTC Act. EPIC charged that HireVue falsely denies it uses facial recognition. EPIC also said the company failed to comply with baseline standards for AI decision-making, such as the OECD AI Principles and the Universal Guidelines for AI. The company purports to evaluate a job applicant’s qualifications based upon their appearance by means of an opaque, proprietary algorithm.
- HireVue represents that it conducts video-based and game-based “pre-employment” assessments of job candidates on behalf of employers. These assessments employ facial recognition technology and proprietary algorithms. The company states that its algorithmic assessments will reveal the “cognitive ability,” “psychological traits,” “emotional intelligence,” and “social aptitudes” of job candidates HireVue states that it collects “tens of thousands of data points” from each video interview of a job candidate, including but not limited to a candidate’s “intonation,” “inflection,” and “emotions.” HireVue purportedly inputs these thousands of personal data points into “predictive algorithms” that allegedly determine each job candidate’s “employability.”
- According to HireVue, 10% to 30% of a candidate’s score is based on facial expressions and the remainder of the score is based on the language used. HireVue does not give candidates access to their assessment scores or the training data, factors, logic, or techniques used to generate each algorithmic assessment.
- HireVue markets its recruiting tools as a way to eliminate biases in the hiring process, but hiring algorithms are more likely to be biased by default. HireVue represents that it builds algorithmic models for employers based on data from top performers, a method which can perpetuate past hiring biases.
- Section 5 of the FTC Act (15 U.S.C. § 45) prohibits unfair and deceptive acts and practices and empowers the Commission to enforce the Act’s prohibitions. A company engages in a deceptive trade practice if it makes a representation to consumers yet “lacks a ‘reasonable basis’ to support the claims made[.]” A trade practice is unfair if it “causes or is likely to cause substantial injury to consumers which is not reasonably avoidable by consumers themselves and not outweighed by countervailing benefits to consumers or to competition.” In determining whether a trade practice is unfair, the Commission is expected to consider “established public policies.”
- HireVue has engaged in deceptive trade practices in violation of the FTC Act by falsely representing that it does not use facial recognition technology in its video interviews of candidates. As the FTC has established, the term “facial recognition technology” includes “technologies that merely detect basic human facial geometry; technologies that analyze facial geometry to predict demographic characteristics, expression, or emotions; and technologies that measure unique facial biometrics.”
- HireVue represents that it collects and analyzes “[f]acial expressions” and “facial movements” to measure job candidates’ “cognitive ability,” “emotional intelligence,” and “social aptitudes.” Yet HireVue also represents that it “does not use facial recognition technology[.]” Because HireVue lacks a reasonable basis for this claim, HireVue is engaged in a deceptive trade practice in violation of the FTC Act.
- HireVue has engaged in unfair trade practices in violation of the FTC Act by using biometric data and secret algorithms in a manner that causes substantial and widespread harm.
- HireVue, which performs job candidate assessments on behalf of 700-plus employers, claims to collect “tens of thousands” of biometric data points from job candidate interviews. These data points include (but are not limited to) a job candidate’s “intonation,” “inflection,” and “emotions.” HireVue inputs these personal data points into secret “predictive algorithms” that allegedly determine each job candidate’s “employability.” Companies then rely on HireVue’s assessments to determine whether to contract for the services of each job candidate.
- Because these algorithms are secret—even to HireVue itself, in some cases—it is impossible for job candidates to know how their personal data is being used or to consent to such uses. HireVue’s intrusive collection and secret analysis of biometric data thus causes substantial privacy harms to job candidates. HireVue’s assessment system also causes substantial financial harms to job candidates. Many job candidates are denied opportunities to contract with companies based on HireVue’s algorithmic assessments, and many of those same candidates are forced to expend significant resources to identify alternate contracting opportunities.
- Moreover, the injuries caused by HireVue’s use of biometric data and secret algorithms cannot be reasonably avoided. HireVue’s assessments are used by hundreds of major employers, and job candidates are not given an opportunity to opt out of or meaningfully challenge HireVue’s assessments.
- Nor are the harms caused by HireVue outweighed by countervailing benefits to consumers or to competition. HireVue has failed to demonstrate any legitimate purpose for the collection of job candidates’ biometric data or for the use of secret, unproven algorithms to assess the “cognitive ability,” “psychological traits,” “emotional intelligence,” and “social aptitudes” of job candidates. Other methods that accomplish the goal of evaluating job candidates are readily available and have long been in use. HireVue is therefore engaged in an unfair trade practice in violation of the FTC Act.
- EPIC’s complaint also alleges that HireVue has engaged in unfair trade practices by failing to meet minimal standards for AI-based decision-making set out in the OECD AI Principles or the recommended standards set out in the Universal Guidelines for Artificial Intelligence. The OECD Principles on Artificial Intelligence are “established public policies” within the meaning of the FTC Act (15 U.S.C. § 45(n)).
- In 2019, the member nations of the OECD, working also with many non-OECD members countries, promulgated the OECD Principles on Artificial Intelligence. The United States has endorsed the OECD AI Principles. EPIC’s complaint alleges that HireVue has violated the following principles:
- The Universal Guidelines for Artificial Intelligence (UGAI), a framework for AI governance based on the protection of human rights, were set out at the 2018 meeting of the International Conference on Data Protection and Privacy Commissioners in Brussels, Belgium. The UGAI have been endorsed by more than 250 experts and 60 organizations in 40 countries. EPIC’s complaint alleges that HireVue has violated the following principles:
- May 23, 2023
- May 18, 2023
- May 16, 2023
- May 23, 2023
- EPIC's work is funded by the support of individuals like you, who allow us to continue to protect privacy, open government, and democratic values in the information age.
- 1519 New Hampshire Avenue NW
- Washington, DC 20036
- Phone number: 202.483.1140
- Sign Up for EPIC Alerts
- © 1994 - 2023 EPIC, all rights reserved.

URL: https://epic.org/wp-content/uploads/privacy/ftc/hirevue/EPIC_FTC_HireVue_Complaint.pdf

URL: https://topclassactions.com/lawsuit-settlements/employment-labor/hirevue-class-action-alleges-company-collects-interview-candidates-biometric-data-without-informed-consent/
- Top Class Actions’s website and social media posts use affiliate links. If you make a purchase using 
	such links, we may receive a commission, but it will not result in any additional charges to you. 
	Please review our Affiliate Link Disclosure for more information.
- HireVue Inc. captures its users biometric identifying information without informed written consent, a new class action lawsuit alleges.
- Plaintiff Kristen Deyerler claims HireVue, an online video interview platform, improperly captures, collects, disseminates or otherwise uses the biometric data of users to “assess a potential job candidate through artificial intelligence.”
- Deyerler wants to represent an Illinois class of individuals whose biometric data was captured, collected, received by trade or otherwise obtained by HireVue’s video interview software.
- Deyerler claims HireVue uses “biometrically enabled software” that extracts biometric information from job candidates by using facial geometry scanning and tracking.
- HireVue’s alleged biometric data collection violates the Illinois Biometric Information Privacy Act (BIPA), the class action lawsuit alleges.
- Deyerler argues that, to be in compliance with BIPA, HireVue needed to first inform job candidates in writing that their biometric data was going to be collected or stored.
- Further, HireVue needed to receive a written release from the job candidates before they collected any of their biometric data in addition to publicly publishing how they planned to permanently destroy any biometric data collected.
- “BIPA’s requirements bestow a right to privacy in biometrics and a right to make an informed decision when electing whether to provide or withhold biometrics,” the class action lawsuit states.
- Deyerler argues that, in addition to the alleged BIPA violations, the software used by HireVue also causes biometric data collected by the company to be disseminated to third parties, including data storage vendors.
- “To this day, Plaintiff is unaware of the status of her biometrics obtained by Defendant,” the lawsuit states.
- Deyerler is demanding a jury trial and requesting injunctive relief along with statutory damages for herself and all class members.
- A separate class action lawsuit involving an alleged violation of BIPA was filed earlier this month against Pearson Education by a customer claiming the company improperly collected the biometric data of its test-taking customers.
- Have you had your biometric data improperly collected by HireVue? Let us know in the comments!
- The plaintiff is represented by David L. Gerbie, Timothy P. Kingsbury and Andrew T. Heldut of McGuire Law, P.C.
- The HireVue BIPA Class Action Lawsuit is Deyerler v. HireVue Inc., Case No. 2022CH00719, in the Circuit Court of Cook County.
- Check out our list of Class Action Lawsuits and Class Action Settlements you may qualify to join!
- Read About More Class Action Lawsuits & Class Action Settlements:
- Δ
- Please add me to the lawsuit list. I am fully aware of this company that has used me as their guinie pig for employment opportunities.
I am here in Utah where they are based.
- Add me please.
- Please add me. I live in Utah where this awful company is based out of. I have 2 separate instances where HireVue malfunctioned and did not capture my entire video.
- Add me , please
- Add me
- Yes add me
- Add me please
- yes. add me.  I applied for several jobs where hirevue was required.  i complained to management to process my application without use of hirevue but was excluded from job consideration.
- Add me
- Add me
- Your email address will not be published. By submitting your comment and contact information, you agree to receive marketing emails from Top Class Actions regarding this and/or similar lawsuits or settlements, and/or to be contacted by an attorney or law firm to discuss the details of your potential case at no charge to you if you qualify. Required fields are marked *
- Comment *
- Name *
- Email *
- 
- 
- Δ
- Δ
- Please add me to the lawsuit list. I am fully aware of this company that has used me as their guinie pig for employment opportunities.
I am here in Utah where they are based.
- Add me please.
- Please add me. I live in Utah where this awful company is based out of. I have 2 separate instances where HireVue malfunctioned and did not capture my entire video.
- Add me , please
- Add me
- Yes add me
- Add me please
- yes. add me.  I applied for several jobs where hirevue was required.  i complained to management to process my application without use of hirevue but was excluded from job consideration.
- Add me
- Add me
- Your email address will not be published. By submitting your comment and contact information, you agree to receive marketing emails from Top Class Actions regarding this and/or similar lawsuits or settlements, and/or to be contacted by an attorney or law firm to discuss the details of your potential case at no charge to you if you qualify. Required fields are marked *
- Comment *
- Name *
- Email *
- 
- 
- Δ
- Please note: Top Class Actions is not a settlement
		administrator or law firm. Top Class Actions is a legal news source
		that reports on class action lawsuits, class action settlements,
		drug injury lawsuits and product liability lawsuits. Top Class
		Actions does not process claims and we cannot advise you on the
		status of any class action settlement claim. You must contact the
		settlement administrator or your attorney for any updates regarding
		your claim status, claim form or questions about when payments are
		expected to be mailed out.
- 
- Δ
- @2023 Top Class Actions. All Rights Reserved.
                    
                        Privacy Policy                     |
                    
                        Terms and Conditions

URL: https://www.hirevue.com/resources/template/orcaa-report
- Candidates: Are you interviewing and need support?
- 
- Sharing your information helps us understand who is reading our research. The report you are downloading is being made available for review only.
- By downloading this document you acknowledge and agree this report is the sole and exclusive intellectual property of HireVue, Inc., and you agree you shall not use, copy, excerpt, reproduce, distribute, display, publish, etc. the contents of this report in whole, or in part, for any purpose not expressly authorized in writing by HireVue, Inc.
- By submitting your information, you agree to HireVue’s Privacy Policy.
- 
- HIREVUE
- RESOURCES
- WHY HIREVUE
- COMPANY
- © 2023 HireVue, Inc. All rights reserved.

URL: https://www.hirevue.com/blog/hiring/industry-leadership-new-audit-results-and-decision-on-visual-analysis
- Candidates: Are you interviewing and need support?
- January 12th,  2021
- Lindsey Zuloaga
- Artificial Intelligence, Science
- Today, HireVue is extending our leadership position defining the transparent and appropriate use of AI and software as part of the hiring process. Creating a level playing field for anyone seeking employment, reducing  bias and providing organizations with a more diverse talent pool is at the heart of HireVue’s mission.
- We are releasing the results of an algorithmic audit by O’Neil Risk Consulting & Algorithmic Auditing (ORCAA), and we are outlining our plans for additional audits to affirm the efficacy of our solutions.  We are committed to working with third-parties like ORCAA to collaborate on ideas to improve AI models and to leverage software to remove hiring bias and improve diversity.
- Separately, earlier this year, based on our research (published here) on the role of audio vs visual features’ in evaluating job candidates, and seeing significant improvements in natural language processing to date, we made the decision to remove visual analysis from our new assessment models. We will continue to conduct research in this area, establishing and adhering to best practices.
- ORCAA audit released as part of HireVue commitment to transparency
- Developing hiring solutions that are driven by science means meeting or exceeding the expectations of our own ethical AI principles, as well as legal standards across the globe (i.e. GDPR). In order to do that, HireVue retained third party auditors to evaluate our algorithms, IO Psychology, and the delivery of our assessments. The results of each will be released in the coming months on our Science page.
- Today, we are excited to share the results of our algorithmic audit with O’Neil Risk Consulting & Algorithmic Auditing (ORCAA). ORCAA’s CEO and founder, Cathy O'Neil, is a well-respected data scientist and author of the influential book, Algorithms of Math Destruction. Cathy was recently featured in the popular Netflix documentary “The Social Dilemma.” Cathy and her team audited a representative pre-built assessment used in hiring early career candidates (including from college campuses), seeing it as a complex and challenging use case that would provide valuable lessons about fairness.
- After evaluation, ORCAA concluded that, “The HireVue assessments work as advertised with regard to fairness and bias issues; ORCAA did not find any operational risks with respect to clients using them.” Together, HireVue and ORCAA came away with focus areas where improvements can be made to help increase transparency to candidates as well as formulated research questions that go above and beyond to ensure fairness and equal access.
- The audit with ORCAA represents countless hours of work on the part of internal teams, external stakeholders and subject matter experts. External participants included:
- In the conclusion of the ORCAA report, the authors suggest that, “Fairness work is a continual improvement process companies navigate – not a checklist provided by regulators or lawmakers.” We couldn’t agree more, and we look forward to working with customers, job-seekers, technology partners, ethicists, legal advisors, and society at large in the months and years ahead to define the best use of AI in hiring.
- For access to the ORCAA report visit https://www.hirevue.com/resources/orcaa-report
- Removing visual analysis from assessment models
- The goal of HireVue’s assessment models is to correlate an applicant’s interview to how they would perform in a specific role. These models are validated and tested continuously.
- HireVue research, conducted early this year, concluded that for the significant majority of jobs and industries, visual analysis has far less correlation to job performance than other elements of our algorithmic assessment. The predictive power of language has increased greatly with recent advances in natural language processing and consequently, our algorithms do not see significant additional predictive power when non-verbal data is added to language data. For that reason, we made the decision to not use any visual analysis in our pre-hire algorithms going forward. We recommend and hope that this decision becomes an industry standard.
- An ongoing commitment to fairness
- Our news today reaffirms HireVue’s leadership role in defining the best, most appropriate use of AI in hiring, and our commitment to rigorous, ongoing measurement of the impact our software has on individual candidates, our customers, and society as a whole.
- We follow established best practices in the field of IO Psychology and data privacy to ensure our assessments meet the strict standards of fairness from agencies such as the Equal Employment Opportunity Commision and the American Psychological Association (to name a few).
- In 2019 we became the first HR technology company to create an expert advisory board and publicly release a set of AI ethical principles. We undertook these steps because we know that transparency, concrete guideposts, and the input of experts is the best way to uphold our principles.  You can find more details about our commitment to AI Ethics and standards at www.hirevue.com/why-hirevue/ai-ethics
- We were also vocal in our support of the Illinois Artificial Intelligence Video Interview Act (AIVIA), and are open to ongoing involvement in conversations around the regulation and use of AI in hiring. These conversations only advance our goals of building tools that better society while safeguarding privacy and maintaining strict standards of data protection.
- Methods and approaches may change based on technological developments, but our commitment to leveling the playing field and eliminating bias for candidates and employers remains the same.
- HIREVUE
- RESOURCES
- WHY HIREVUE
- COMPANY
- © 2023 HireVue, Inc. All rights reserved.

URL: https://ainowinstitute.org/AI_Now_2019_Report.pdf
- The AI Now Institute produces diagnosis and actionable policy research to address the concentration of power in the tech industry.
- The AI Now Institute produces diagnosis and actionable policy research to address the concentration of power in the tech industry.

URL: https://www.washingtonpost.com/technology/2019/10/22/ai-hiring-face-scanning-algorithm-increasingly-decides-whether-you-deserve-job/
- This article was published more than 3 years ago
- An artificial intelligence hiring system has become a powerful gatekeeper for some of America’s most prominent employers, reshaping how companies assess their workforce — and how prospective employees prove their worth.
- Designed by the recruiting-technology firm HireVue, the system uses candidates’ computer or cellphone cameras to analyze their facial movements, word choice and speaking voice before ranking them against other applicants based on an automatically generated “employability” score.
- HireVue’s “AI-driven assessments” have become so pervasive in some industries, including hospitality and finance, that universities make special efforts to train students on how to look and speak for best results. More than 100 employers now use the system, including Hilton and Unilever, and more than a million job seekers have been analyzed.
- But some AI researchers argue the system is digital snake oil — an unfounded blend of superficial measurements and arbitrary number-crunching that is not rooted in scientific fact. Analyzing a human being like this, they argue, could end up penalizing nonnative speakers, visibly nervous interviewees or anyone else who doesn’t fit the model for look and speech.
- The system, they argue, will assume a critical role in helping decide a person’s career. But they doubt it even knows what it’s looking for: Just what does the perfect employee look and sound like, anyway?
- “It’s a profoundly disturbing development that we have proprietary technology that claims to differentiate between a productive worker and a worker who isn’t fit, based on their facial movements, their tone of voice, their mannerisms,” said Meredith Whittaker, a co-founder of the AI Now Institute, a research center in New York.
- “It’s pseudoscience. It’s a license to discriminate,” she added. “And the people whose lives and opportunities are literally being shaped by these systems don’t have any chance to weigh in.”
- Update: Prominent rights group files federal complaint against AI-hiring firm HireVue, citing “unfair and deceptive” practices
- Loren Larsen, HireVue’s chief technology officer, said that such criticism is uninformed and that “most AI researchers have a limited understanding” of the psychology behind how workers think and behave.
- Larsen compared algorithms’ ability to boost hiring outcomes with medicine’s improvement of health outcomes and said the science backed him up. The system, he argued, is still more objective than the flawed metrics used by human recruiters, whose thinking he called the “ultimate black box.”
- “People are rejected all the time based on how they look, their shoes, how they tucked in their shirts and how ‘hot’ they are,” he told The Washington Post. “Algorithms eliminate most of that in a way that hasn’t been possible before.”
- The AI, he said, doesn’t explain its decisions or give candidates their assessment scores, which he called “not relevant.” But it is “not logical,” he said, to assume some people might be unfairly eliminated by the automated judge.
- “When 1,000 people apply for one job,” he said, “999 people are going to get rejected, whether a company uses AI or not.”
- On Wednesday, a prominent rights group, the Electronic Privacy Information Center, filed an official complaint urging the Federal Trade Commission to investigate HireVue for “unfair and deceptive” practices. The system’s “biased, unprovable and not replicable" results, EPIC officials wrote, constitute a major threat to American workers’ privacy and livelihoods.
- Listen on Post Reports: Reporters Drew Harwell and Carolyn Y. Johnson on the rise of AI and the risks of bias that come with it
- The inscrutable algorithms have forced job seekers to confront a new kind of interview anxiety. Nicolette Vartuli, a University of Connecticut senior studying math and economics with a 3.5 GPA, said she researched HireVue and did her best to dazzle the job-interview machine. She answered confidently and in the time allotted. She used positive keywords. She smiled, often and wide.
- But when she didn’t get the investment banking job, she couldn’t see how the computer had rated her or ask how she could improve, and she agonized over what she had missed. Had she not looked friendly enough? Did she talk too loudly? What did the AI hiring system believe she had gotten wrong?
- “I feel like that’s maybe one of the reasons I didn’t get it: I spoke a little too naturally,” Vartuli said. “Maybe I didn’t use enough big, fancy words. I used ‘conglomerate’ one time.”
- HireVue said its system dissects the tiniest details of candidates’ responses — their facial expressions, their eye contact and perceived “enthusiasm” — and compiles reports companies can use in deciding whom to hire or disregard.
- Job candidates aren’t told their score or what little things they got wrong, and they can’t ask the machine what they could do better. Human hiring managers can use other factors, beyond the HireVue score, to decide which candidates pass the first-round test.
- The system, HireVue said, employs superhuman precision and impartiality to zero in on an ideal employee, picking up on telltale clues a recruiter might miss.
- Major employers with lots of high-volume, entry-level openings are increasingly turning to such automated systems to help find candidates, assess résumés and streamline hiring. The Silicon Valley start-up AllyO, for instance, advertises a “recruiting automation bot” that can text-message a candidate, “Are you willing to relocate?” And a HireVue competitor, the “digital recruiter” VCV, offers a similar system for use in phone interviews, during which a candidate’s voice and answers are analyzed by an “automated screening” machine.
- But HireVue’s prospects have cemented it as the leading player in the brave new world of semi-automated corporate recruiting. It says it can save employers a fortune on in-person interviews and quickly cull applicants deemed subpar. HireVue says it also allows companies to see candidates from an expanded hiring pool: Anyone with a phone and Internet connection can apply.
- Nathan Mondragon, HireVue’s chief industrial-organizational psychologist, told The Post the standard 30-minute HireVue assessment includes half a dozen questions but can yield up to 500,000 data points, all of which become ingredients in the person’s calculated score.
- The employer decides the written questions, which HireVue’s system then shows the candidate while recording and analyzing their responses. The AI assesses how a person’s face moves to determine, for instance, how excited someone seems about a certain work task or how they would behave around angry customers. Those “Facial Action Units,” Mondragon said, can make up 29 percent of a person’s score; the words they say and the “audio features” of their voice, like their tone, make up the rest.
- Wanted: The ‘perfect babysitter.’ Must pass AI scan for respect and attitude.
- “Humans are inconsistent by nature. They inject their subjectivity into the evaluations,” Mondragon said. “But AI can database what the human processes in an interview, without bias. … And humans are now believing in machine decisions over human feedback.”
- To train the system on what to look for and tailor the test to a specific job, the employer’s current workers filling the same job — “the entire spectrum, from high to low achievers” — sit through the AI assessment, Larsen said.
- Their responses, Larsen said, are then matched with a “benchmark of success” from those workers’ past job performance, like how well they had met their sales quotas and how quickly they had resolved customer calls. The best candidates, in other words, end up looking and sounding like the employees who had done well before the prospective hires had even applied.
- After a new candidate takes the HireVue test, the system generates a report card on their “competencies and behaviors,” including their “willingness to learn,” “conscientiousness & responsibility” and “personal stability,” the latter of which is defined by how well they can cope with “irritable customers or co-workers.”
- Those computer-estimated personality traits are then used to group candidates into high, medium and low tiers based on their “likelihood of success.” Employers can still pursue candidates ranked in the bottom tier, but several interviewed by The Post said they mostly focused on the ones the computer system liked best.
- The new way your boss can tell if you’re about to quit your job
- HireVue offers only the most limited peek into its interview algorithms, both to protect its trade secrets and because the company doesn’t always know how the system decides on who gets labeled a “future top performer.”
- The company has given only vague explanations when defining which words or behaviors offer the best results. For a call center job, the company says, “supportive” words might be encouraged, while “aggressive” ones might sink one’s score.
- HireVue said its board of expert advisers regularly reviews its algorithmic approach, but the company declined to make the system available for an independent audit. The company, Larsen said, is “exploring the use of an independent auditor right now, to see how that could work.”
- HireVue launched its AI assessment service in 2014 as an add-on to its video-interview software, which more than 700 companies have used for nearly 12 million interviews worldwide. The Utah-based company won’t disclose its revenue, the cost for employers or a full list of clients.
- The company said last month that the private-equity giant Carlyle Group would become its new majority investor, providing an undisclosed sum from an $18.5 billion fund. Patrick McCarter, a managing director at the investment firm — which uses HireVue’s video interviews internally and said it “will look to deploy AI-driven candidate assessments over time” — said the money would help the company expand to more employers and more specialized job openings, both in the United States and around the world.
- At the hotel giant Hilton International, thousands of applicants for reservation-booking, revenue management and call center positions have gone through HireVue’s AI system, and executives credit the automated interviews with shrinking their average hiring time from six weeks to five days.
- Sarah Smart, the company’s vice president of global recruitment, said the system has radically redrawn Hilton’s hiring rituals, allowing the company to churn through applicants at lightning speed. Hiring managers inundated with applicants can now just look at who the system ranked highly and filter out the rest: “It’s rare for a recruiter to need to go out of that range,” she said.
- At the consumer goods conglomerate Unilever, HireVue is credited with helping save 100,000 hours of interviewing time and roughly $1 million in recruiting costs a year. Leena Nair, the company’s chief human resource officer, said the system had also helped steer managers away from hiring only “mini-mes” who look and act just like them, boosting the company’s “diversity hires,” as she called them, by about 16 percent.
- “The more digital we become, the more human we become,” she added.
- Dane E. Holmes, the global head of human-capital management at HireVue client Goldman Sachs, wrote in the Harvard Business Review this spring that the banking giant’s roughly 50,000 video-interview recordings were “a treasure trove of data that will help us conduct insightful analyses."
- The investment bank said it uses HireVue’s video-interview system but not its computer-generated assessments. But Holmes said data from those videos could help the company figure out how candidates’ skills and backgrounds might correspond to how well they would work or how long they would stay at the firm. The company, he added, is also “experimenting with résumé-reading algorithms” that would help decide new hires’ departments and tasks.
- “Can I imagine a future in which companies rely exclusively on machines and algorithms to rate résumés and interviews? Maybe, for some,” he wrote. (The “human element” of recruiting, he pledged, would survive at Goldman Sachs.)
- Is your pregnancy app sharing your intimate data with your boss?
- HireVue’s expansion has also helped it win business from smaller groups such as Re:work, a Chicago nonprofit organization that trains unemployed local job seekers for careers in the tech industry. Shelton Banks, the group’s chief, said HireVue had proved to be an irreplaceable guide in assessing which candidates would be worth the effort.
- The nonprofit organization once allowed almost anyone into its intensive eight-week training program, but many burned out early. Now, every candidate goes through the AI assessment first, which ranks them on problem-solving and negotiation skills and helps the group determine who might have the most motivation, curiosity and grit.
- “Knowing where that person is at a starting place, when it comes to this person’s life,” Banks said, “can help us make more accurate assessments of the people we’re saying yes or no to.”
- But Lisa Feldman Barrett, a neuroscientist who studies emotion, said she is “strongly skeptical” that the system can really comprehend what it’s looking at. She recently led a team of four senior scientists, including an expert in “computer vision” systems, in assessing more than 1,000 published research papers studying whether the human face shows universal expressions of emotion and how well algorithms can understand them.
- The systems, they found, have become quite perceptive at detecting facial movements — spotting the difference, say, between a smile and a frown. But they’re still worryingly imprecise in understanding what those movements actually mean and woefully unprepared for the vast cultural and social distinctions in how people show emotion or personality.
- Look at scowling, Barrett said: A computer might see a person’s frown and furrowed brow and assume they’re easily angered — a red flag for someone seeking a sales associate job. But people scowl all the time, she said, “when they’re not angry: when they’re concentrating really hard, when they’re confused, when they have gas.”
- Luke Stark, a researcher at Microsoft’s research lab in Montreal studying emotion and AI — who spoke as an individual, not as a Microsoft employee — was similarly skeptical of HireVue’s ability to predict a worker’s personality from their intonations and turns of phrase.
- What happens when an algorithm labels you as mentally ill?
- Systems like HireVue, he said, have become quite skilled at spitting out data points that seem convincing, even when they’re not backed by science. And he finds this “charisma of numbers” really troubling because of the overconfidence employers might lend them while seeking to decide the path of applicants’ careers.
- The best AI systems today, he said, are notoriously prone to misunderstanding meaning and intent. But he worried that even their perceived success at divining a person’s true worth could help perpetuate a “homogenous” corporate monoculture of automatons, each new hire modeled after the last.
- The company, HireVue’s Larsen said, audits its performance data to look for potentially discriminatory hiring practices, known as adverse impacts, using “world-class bias testing” techniques. The company’s algorithms, he added, have been trained “using the most deep and diverse data set of facial action units available, which includes people from many countries and cultures.”
- HireVue’s growth, however, is running into some regulatory snags. In August, Illinois Gov. J.B. Pritzker (D) signed a first-in-the-nation law that will force employers to tell job applicants how their AI-hiring system works and get their consent before running them through the test. The measure, which HireVue said it supports, will take effect Jan. 1.
- State Rep. Jaime Andrade Jr. (D), who co-sponsored the bill, said he pushed the transparency law after learning how many job applicants were rejected at the AI stage of a job interview. He worried that spoken accents or cultural differences could end up improperly warping the results, and that people who declined to sit for the assessment could be unfairly punished by not being considered for the job.
- “What is the model employee? Is it a white guy? A white woman? Someone who smiles a lot?” he said. “What are the data points being used? There has to be some explanation, and there has to be consent.”
- As summer camps turn on facial recognition, parents demand: More smiles, please
- HireVue cautions candidates that there is no way to trick, cheat or hack the system, because it assesses tens of thousands of factors to assess a “unique set of personal competencies.” “Do what feels most natural to you,” the company says in an online guide.
- But roughly a dozen interviewees who have taken the AI test — including some who got the job — told The Post it felt alienating and dehumanizing to have to wow a computer before being deemed worthy of a company’s time.
- They questioned what would be done with the video afterward and said they felt uneasy about having to perform to unexplained AI demands. Several said they refused to do the interview outright because, in the words of one candidate, the idea “made my skin crawl.”
- Candidates said they have scrambled for ideas on how to maximize their worthiness before the algorithm’s eye, turning to the hundreds of videos and online handbooks suggesting, for instance, that they sit in front of a clean white wall, lest the background clutter dock their grade. “Glue some googly eyes to your webcam. It’ll make it easier to maintain eye contact,” one user on the message board Reddit suggested.
- Stark, the AI researcher, said these “folk theories of algorithms” were a natural response from people facing impenetrable AI systems with the power to decide their fate. The survival techniques could feel reassuring, he said, even if they were wrong: Pick the right words, use the right tone, put on a sufficiently happy face. “It’s a way of trying to give people confronting an opaque system they don’t understand some feeling of agency,” he said.
- But some HireVue interviewees questioned whether it was fair or even smart to judge a person’s workplace performance or personal abilities based on half an hour spent looking into a webcam. They also worried that people’s nerves about the odd nature of the exam might end up disqualifying them outright.
- Oregon became a testing ground for Amazon’s facial-recognition policing. But what if Rekognition gets it wrong?
- Emma Rasiel, an economics professor at Duke University who regularly advises students seeking jobs on Wall Street, said she has seen a growing number of students excessively unsettled about their upcoming HireVue test. The university’s economics department now offers a guide to HireVue interviews on its student resources website, including typical questions (“What does integrity mean to you?”) and behavioral tips (“Act natural, talk slowly!”).
- “It’s such a new and untried way of communicating who they are that it adds to their anxiety,” Rasiel said. “We’ve got an anxious generation, and now we’re asking them to talk to a computer screen, answering questions to a camera … with no real guidelines on how to make themselves look better or worse.”
- The mysterious demands can also push people’s angst into overdrive. When Sheikh Ahmed, a 25-year-old in Queens, applied for teller jobs at banks around New York, he said he received eight HireVue assessment offers, all scheduled for the same day.
- He studied guides on how to talk and act but found the hardest part was figuring out the camera angle: Too high, he worried, and he would look domineering; too low, and he would look shrunken and weak.
- Before his marathon of AI interviews, he put on a crisp dress shirt, a tie and pajama pants and went to his dad’s soundproof music studio, away from the family’s chirping society finch. He also turned off his air conditioning system, hoping the background noise wouldn’t mess up his score.
- He changed his answers slightly in each interview, in the hopes that the algorithm would find something it liked. But he found it exhausting and disheartening to boil down his life experience and worthiness into a computer-friendly sound bite.
- By the end, his mouth was dry, he was covered in sweat and he was paranoid he hadn’t made enough eye contact while worrying about the bird. A few weeks after the interviews, he said, he’s still waiting to hear whether he got a job.
- Correction: Due to incorrect information from Goldman Sachs, The Post misreported in an earlier version of this story that the investment bank used HireVue’s AI-driven assessment program. Goldman Sachs representatives said they only use HireVue’s video-interview system, not its automated assessments.

URL: https://www.washingtonpost.com/technology/2019/11/06/prominent-rights-group-files-federal-complaint-against-ai-hiring-firm-hirevue-citing-unfair-deceptive-practices/
- This article was published more than 3 years ago
- A prominent rights group is urging the Federal Trade Commission to take on the recruiting-technology company HireVue, arguing that the firm has turned to unfair and deceptive trade practices in its use of face-scanning technology to assess job candidates’ “employability.”
- The Electronic Privacy Information Center, known as EPIC, on Wednesday filed an official complaint calling on the FTC to investigate HireVue’s business practices, saying the company’s use of unproven artificial intelligence systems that scan people’s faces and voices constituted a wide-scale threat to American workers.
- HireVue’s “AI-driven assessments,” which more than 100 employers have used on a million-plus job candidates, use video interviews to analyze hundreds of thousands of data points related to a person’s speaking voice, word selection and facial movements. The system then creates a computer-generated estimate of the candidates’ skills and behaviors, including their “willingness to learn” and “personal stability.”
- Candidates aren’t told their scores, but employers can use those reports to decide whom to hire or disregard. The Utah-based company was the subject of a Washington Post report last month, in which AI researchers criticized its technology as “profoundly disturbing” and “opaque.”
- A face-scanning algorithm increasingly decides whether you deserve the job
- HireVue’s “intrusive collection and secret analysis of biometric data” causes substantial privacy and financial harms, EPIC officials wrote. And “because these algorithms are secret,” they added, “ ... it is impossible for job candidates to know how their personal data is being used or to consent to such uses.”
- The FTC declined to comment. HireVue did not respond to requests for comment.
- The complaint could for the first time throw a federal spotlight on a growing industry of tech firms that advertise automated systems they say can assess candidates’ résumés, divine people’s personalities and pinpoint problematic recruits. Critics say the systems are dehumanizing, invasive and built on flawed science that could perpetuate discriminatory hiring practices.
- The technology is also facing increasing pressure from lawmakers. In January, Illinois will enact a law forcing employers to tell job applicants and regulators how their AI video-interview systems work. Co-sponsors of the bill, approved in August by Gov. J.B. Pritzker (D), said they worried the systems could unfairly penalize candidates and hide biases in how they assess a “model employee.”
- Read the complaint
- Full PDF
- HireVue’s systems have become pervasive for employers because they can lower recruiting costs and speed up turnaround time for new hires. Some colleges now instruct students on how to impress the hidden algorithms: In the FTC filing, EPIC lawyers quote a guide from the University of Maryland business school, which tells interviewees, “Robots compare you against existing success stories; they don’t look for out-of-the-box candidates.”
- EPIC, based in Washington, has become one of the tech industry’s most renowned and effective watchdogs, helping shape U.S. policy around online privacy, civil liberties and domestic surveillance for nearly 25 years. The group has challenged tech giants and government agencies, including Facebook, Google and the National Security Agency, through consumer complaints, agency filings and federal lawsuits.
- EPIC urged the FTC to halt HireVue’s automatic scoring of job candidates and make public the algorithms and criteria used in analyzing people’s behavior. The technology is largely unregulated, but the FTC regularly enforces “unfair and deceptive acts or practices” statutes against companies found to be making claims to consumers without a “reasonable basis” in a way likely to “cause substantial injury.”
- Wanted: The ‘perfect babysitter.’ Must pass AI scan for respect and attitude.
- In its complaint, EPIC officials said HireVue’s AI-driven assessments produce results that are “biased, unprovable and not replicable.” The system, they argued, could unfairly score someone based on prejudices related to their gender, race, sexual orientation or neurological differences. HireVue says it uses “world-class bias testing” techniques to detect and prevent hiring discrimination.
- HireVue advertises that its technology does not use “facial recognition technology” because its systems do not attempt to identify people. But EPIC argued that HireVue’s assertion is misleading, and that the FTC has ruled the term applies to any “technologies that analyze facial geometry to predict demographic characteristics, expression or emotions.”
- EPIC also argued that HireVue had failed to meet international standards for AI systems set by the Organization for Economic Cooperation and Development and endorsed by the United States earlier this year. HireVue violated those principles, EPIC said, because its algorithmic assessments can’t be evaluated or “meaningfully challenged” by the job candidates they’ve assessed.
- This company is training artificial intelligence to scour California's forests for potential wildfires
- The company has not ensured the accuracy, reliability or validity of its computer-generated scores, the complaint added. It has also not “adequately evaluated whether the purpose, objectives, and benefits of its algorithmic assessments outweigh the risks.”

URL: https://fortune.com/2021/01/19/hirevue-drops-facial-monitoring-amid-a-i-algorithm-audit/
- This is the web version of Eye on A.I., Fortune’s weekly newsletter covering artificial intelligence and business. To get it delivered weekly to your in-box, sign up here.The journalist Malcolm Gladwell, on his podcast, “Revisionist History,” devoted a recent episode to his theory of “hiring nihilism.” It is Gladwell’s belief that people are so bad at predicting who will perform well at a given role—especially based on traditional screening criteria such as CVs and candidate interviews—that one should simply concede that all hiring is essentially arbitrary. Gladwell explained, when it came time to find a new assistant or hiring an accountant, he did so in explicitly arbitrary ways—picking whoever an acquaintance recommended or someone he met on the street, with only the most cursory of face-to-face conversation. Why waste time on a process that would ultimately produce a result no better than throwing darts?For decades, a segment of the tech industry has grown based on an acceptance of Gladwell’s premise—that humans are terrible at forecasting job performance—but an emphatic rejection of his resort to nihilism. Instead, these tech companies argue, with better screening tools (which, not coincidentally, these same companies happen to sell), this problem can fixed. Increasingly, artificial intelligence has been a part of what these firms are selling.
- A.I. offers the promise that there exists some hidden constellation of data, too complex or subtle for an H.R. executives or hiring managers to ever discern, that can predict which candidate will excel at a given role. In theory, the technology offers businesses the prospect of radically expanding the diversity of their candidate pool. In practice, though, critics warn, such software runs a high risk of reinforcing existing biases, making it harder for women, Black people and others from non-traditional backgrounds to get hired. What’s worse, it may cloak a process that remains as fundamentally arbitrary and biased as Gladwell argues in an ever more pseudoscientific veneer.HireVue is one of the leading companies in “hiretech”—its software allows companies to record videos of candidates answering a standard set of interview questions and then sort candidates based on those responses—and it has been the target of such criticism. In 2019, the nonprofit Electronic Privacy Information Center filed a complaint against the company with the Federal Trade Commission alleging that HireVue’s use of A.I. to assess job candidate’s video interviews constituted “unfair and deceptive trade practices.” The company says it’s not done anything illegal. But, partly in response to the criticism, HireVue announced last year that it had stopped using a candidate’s facial expressions in the video interviews as a factor its algorithms considered.
- This past week, the company also revealed the results of a third-party audit of its algorithms. The audit mostly gave HireVue good marks for its efforts to eliminate potential bias in its A.I. systems. But it also recommended several areas where the company could do more. For instance, it suggested the company investigate potential bias in the way the system assesses candidates with different accents. It also turns out that minority candidates are more likely to give very short answers to questions—one word responses or saying things such as “I don’t know”—which the system had difficulty scoring, resulting in these candidate interviews being disproportionately flagged for human reviewers.
- Lindsey Zuloaga, the company’s chief data scientist, told me that the most important factor in predicting whether a job candidate would succeed was the content of their answers to the interview questions. Nonverbal data didn’t provide much predictive power compared to the content of a candidate’s answers—in fact, in most cases, it contributed about 0.25% to a model’s predictive power, she says. Even when trying to assess candidates for a role with a lot of customer interaction, nonverbal attributes contributed just 4% to the model’s predictive accuracy. “When you put that in the context of the concerns people were having [about potential bias], it wasn’t worth the incremental value we might have been getting from it,” Kevin Parker, HireVue’s CEO, says.
- Parker says the company is “always looking for bias in the data that goes into the model” and that it had a policy of discarding datasets if using them led to a disparity in outcomes between groups based on things such as race, gender or age. He also notes that only about 20% of HireVue’s customers currently opt to use the predictive analytics feature of the software—the rest use humans to review the candidates’ videos—but that it’s becoming increasingly popular.HireVue’s audit was conducted by O’Neil Risk Consulting and Algorithmic Auditing (ORCAA), a firm founded by Cathy O’Neil, the mathematician best known for her 2016 book about algorithmic decision-making, Weapons of Math Destruction, and which is one of a growing handful of companies specializing in these kinds of assessments.Zuloaga says she was struck by the extent to which the ORCAA auditors sought out different types of people HireVue’s algorithms touched—from the job seekers themselves to the customers using the software to the data scientists helping to build the predictive models. One of the things that came across in the audit, she says, is that certain groups of job candidates may be more comfortable than others with entire idea of being interviewed by a piece of software and having a machine assess that interview—and so there may be some hidden selection bias built into all of HireVue’s data currently.ORCAA recommended HireVue do more to communicate to candidates exactly what the interview process will involve and how their answers will be screened. Zuloaga says that HireVue is also learning that minority candidates may need more explicit encouragement from the software in order to keep going through the interview process. She and Parker say the company is looking at ways to provide that.HireVue is among the first to engage a third party to conduct an algorithmic bias audit. And while PR damage control might have been part of the motivation—“this is a continuation of our focus on transparency,” Parker insists—it does make the company a pioneer. As A.I. gets adopted by more and more businesses, it is likely that such audits will become more commonplace. At least the audit reveals that HireVue is thinking hard about issues around A.I. ethics and bias and seems sincere in seeking to        address it. It’s an example other businesses should follow. It is also worth remembering that the alternative to using technology such as HireVue’s is not some utopian vision of rationality, empiricism and fairness—it is Gladwell’s hiring nihilism.And with that, here is the rest of this week’s A.I. news.Jeremy Kahn @jeremyakahnjeremy.kahn@fortune.com
- ***
- The societal reckoning over systemic racism continues to underscore the importance businesses must place on responsible A.I. All leaders are wrestling with thorny questions around liability and bias, exploring best practices for their company, and learning how to set effective industry guidelines on how to use the technology. Join us for our second interactive Fortune Brainstorm A.I. community conversation, presented by Accenture, on Tuesday, January 26, 2021 at 1:00–2:00 p.m. ET.
- U.S. creates National Artificial Intelligence office. In one of the Trump Administration's final acts, it has established the National Artificial Intelligence Initiative Office, which is part of the Office of Science and Technology Policy. The new office is charged with coordinating efforts across academia, government and industry to ensure the U.S. remains a leader in A.I. development and this development is in line with U.S. strategy. The announcement spends as much time talking about the symbolism of the office's new insignia—which includes an eagle superimposed on an image meant to represent a neural network and which garnered a lot of comments on Twitter—as it does about anything concrete the new office will do. You can read the announcement and see the new office's badge here.
- FAA approves first fully autonomous drone flights. The aviation regulator has granted approval for Massachusetts-based American Robotics Inc. to fly drones without a pilot overseeing their actions on the ground and beyond the visual line-of-sight of an operator in a first that is could pave the way for the use of such aircraft in farming, mining and the energy sector, The Wall Street Journal reported. The drones will have to operate in rural areas and below 400 feet.
- Social media app Parler, popular with right-wing groups, plans a comeback with help from A.I. After being banned by app stores and dropped by its web hosting service for allowing its users to incite violence and promote hate speech, Parler CEO John Matze told Fox News that he plans to relaunch the messaging service with help from A.I. He said the company would use the technology for content moderation. But, as other social media sites have discovered, A.I. is no easy fix for difficult problems such as hate speech and online bullying and an army of human content moderators is almost always still necessary to review the A.I.-decisions and handle tricky edge cases and adapt rapidly to users' ability to game the A.I.-based filters.
- Biden names Lander to the Office of Science and Technology Policy. U.S. President-elect Joe Biden named Eric Lander, a geneticist and mathematician who served as co-chair President Barack Obama's Council of Advisors on Science and Technology, to serve as director of the White House Office of Science and Technology, which Biden also said he is elevating to a Cabinet-level position. You can read more about the announcement here.
- Signal AI, an A.I.-enabled media intelligence company, has appointed Clancy Childs as chief product officer. Childs previously held a number of senior roles at both Google and Dow Jones, most recently serving as general manager of Dow Jones' innovation business unit.
- British telecom company BT has hired Harmeen Mehta to be its chief digital and innovation officer. Mehta will lead a new BT Digital division and join BT's executive committee. She had previously been chief information officer at Indian telecom group Bharti Airtel.
- Google trains a 1.6 trillion parameter language model. Google has unveiled a new ultra-large language model architecture, which it calls a Switch Transformer, that it says has achieved state-of-the-art performance on a number of natural language processing tasks while also being more efficient in its use of computing power. It says the algorithm is able to take in much more data--in this case, 1.6 trillion parameters, far larger than any model ever trained before--but only needs to reference parts of its massive neural network at a time, making the training more efficient. This is important because a major criticism of these ultra-large language models is how expensive, in terms of data center time, as well as power consumption, they are to train. But it's also interesting that even though the model is almost ten times larger than even OpenAI's GPT-3 language model, its performance doesn't seem to be 10 times better—which mean that this approach to natural language processing has significant diminishing returns.
- A.I. in the beauty industry: How the pandemic finally made consumers care about it—by Gaby Shacknai
- Business could be on the precipice of an automation explosion—by Chris Morris
- A Facebook case in Belgium could open the floodgates for GDPR privacy suits—by David Meyer
- How patents help us invent the future—by Dario Gil
- Axios, the fast-growing news service known for its bullet-point style reports, announced this week that it is expanding into local news. As part of that push, it published what it called an "audience Bill of Rights" that included, as its first principle, a pledge that "every item will be written or produced by a real person with a real identity. There will be NO AI-written stories. NO bots. NO fake accounts." (The all-caps emphasis is Axios'.)
- I think a "no fake accounts and no fake bylines" policy is fair enough. News organizations should never mislead readers about who or what is producing the information they are consuming. If A.I. is being used to write a news story, then readers should know it. I also think that local communities deserve dedicated, locally-based reporters who can ask tough questions of officials and dig into important issues. If Axios is planning to hire a whole bunch of such folks, that's great. All that said, I can't help but wonder whether Axios' pledge goes too far, and is, by implication, too disparaging of the value A.I.-produced news can actually provide to local communities.
- In a previous job, I worked on a podcast episode about something called Project Radar (it stood for Reporters and Data and Robots) that was a joint effort of the British news wire the Press Association and a startup called Urbs Media. The idea was to take large government data sets and use them to generate dozens or even hundreds of tailored stories for local newspapers. The way it worked, a human journalist still had to find the data set and write a template which would then be automatically populated with data for each local area, with natural language processing software adjusting the language used depending on whether the trends was up or down, etc.
- I went to interview journalists at The Bournemouth Echo, one of the local papers that was using the system. The Echo's editor at the time, Andy Martin, told me he saw the system as a godsend. It wasn't replacing human reporters per se. The Echo already had too few of those any way, thanks to endless rounds of job cuts that had nothing to do with automation and everything to do with the way the Internet destroyed local news. The deployment of Radar didn't affect Martin's staff headcount one way or another. But what it did do is let Martin allow the few reporters he did have to spend more of their time doing legwork and digging—the stuff local reporters should be doing to hold public officials to account, but which these days, thanks to staff cuts, is often not possible. Similarly, the Associated Press, uses A.I. software to write simple local sports news summaries. Using A.I. in this way doesn't hurt local readers. If anything, it helps them by freeing media organizations to use the few reporters they do have to do the investigative work that, for now, only humans can do.
- I think rather than making a big deal about all of its news being written by humans, Axios would be better off pledging that no matter how it produces the news, its stories will serve the public interest.
- © 2023 Fortune Media IP Limited. All Rights Reserved. Use of this site constitutes acceptance of our Terms of Use and Privacy Policy | CA Notice at Collection and Privacy Notice | Do Not Sell/Share My Personal Information | Ad Choices 
FORTUNE is a trademark of Fortune Media IP Limited, registered in the U.S. and other countries. FORTUNE may receive compensation for some links to products and services on this website. Offers may be subject to change without notice.
S&P Index data is the property of Chicago Mercantile Exchange Inc. and its licensors. All rights reserved. Terms & Conditions. Powered and implemented by Interactive Data Managed Solutions.

URL: https://www.inc.com/minda-zetlin/ai-is-now-analyzing-candidates-facial-expressions-during-video-job-interviews.html
- Please enable JS and disable any ad blocker

URL: https://www.ft.com/content/c0b03d1d-f72f-48a8-b342-b4a926109452
- Keep abreast of significant corporate, financial and political developments around the world.
				Stay informed and spot emerging risks and opportunities with independent global reporting, expert
				commentary and analysis you can trust.
- Then 65 € per month  New customers only  Cancel anytime during your trial
- During your trial you will have complete digital access to FT.com with everything in both of our Standard Digital and Premium Digital packages.
- Standard Digital includes access to a wealth of global news, analysis and expert opinion.  Premium Digital includes access to our premier business column, Lex, as well as 15 curated newsletters covering key business themes with original, in-depth reporting.  For a full comparison of Standard and Premium Digital, click here.
- Change the plan you will roll onto at any time during your trial by visiting the “Settings & Account” section.
- If you do nothing, you will be auto-enrolled in our premium digital monthly subscription plan and retain complete access for 65 € per month.
- For cost savings, you can change your plan at any time online in the “Settings & Account” section. If you’d like to retain your premium access and save 20%, you can opt to pay annually at the end of the trial.
- You may also opt to downgrade to Standard Digital, a robust journalistic offering that fulfils many user’s needs. Compare Standard and Premium Digital here.
- Any changes made can be done at any time and will become effective at the end of the trial period, allowing you to retain full access for 4 weeks, even if you downgrade or cancel.
- You may change or cancel your subscription or trial at any time online. Simply log into Settings & Account and select "Cancel" on the right-hand side.
- You can still enjoy your subscription until the end of your current billing period.
- We support credit card, debit card and PayPal payments.
- Find the plan that suits you best.
- Premium access for businesses and educational institutions.
- Check if your
							
university
 or
							
organisation
 offers FT membership to read for free.
- We use
								cookies
								and other data for a number of reasons, such as keeping FT Sites reliable and secure,
								personalising content and ads, providing social media features and to
								analyse how our Sites are used.
- International Edition

URL: https://www.technologyreview.com/2020/02/14/844765/ai-emotion-recognition-affective-computing-hirevue-regulation-ethics/
- A lack of government regulation isn’t just bad for consumers. It’s bad for the field, too.
- Perhaps you’ve heard of AI conducting interviews. Or maybe you’ve been interviewed by one yourself. Companies like HireVue claim their software can analyze video interviews to figure out a candidate’s “employability score.” The algorithms don’t just evaluate face and body posture for appearance; they also tell employers whether the interviewee is tenacious, or good at working on a team. These assessments could have a big effect on a candidate’s future. In the US and South Korea, where AI-assisted hiring has grown increasingly popular, career consultants now train new grads and job seekers on how to interview with an algorithm. This technology is also being deployed on kids in classrooms and has been used in studies to detect deception in courtroom videos.
- But many of these promises are unsupported by scientific consensus. There are no strong, peer-reviewed studies proving that analyzing body posture or facial expressions can help pick the best workers or students (in part because companies are secretive about their methods). As a result, the hype around emotion recognition, which is projected to be a $25 billion market by 2023, has created a backlash from tech ethicists and activists who fear that the technology could raise the same kinds of discrimination problems as predictive sentencing or housing algorithms for landlords deciding whom to rent to.
- The hype worries the researchers too. Many agree that their work—which uses various methods (like analyzing micro-expressions or voice) to discern and interpret human expressions—is being co-opted and used in commercial applications that have a shaky basis in science. They say that a lack of government regulation isn’t just bad for consumers. It’s bad for them, too.
- 
- 
- Emotion recognition, a subset of affective computing, is still a nascent technology. As AI researchers have tested the boundaries of what we can and can’t quantify about human behavior, the underlying science of emotions has continued to develop. There are still multiple theories, for example, about whether emotions can be distinguished discretely or fall on a continuum. Meanwhile, the same expressions can mean different things in different cultures. In July, a meta-study concluded that it isn’t possible to judge emotion by just looking at a person’s face. The study was widely covered (including in this publication), often with headlines suggesting that “emotion recognition can’t be trusted.”
- Emotion recognition researchers are already aware of this limitation. The ones we spoke to were careful about making claims of what their work can and cannot do. Many emphasized that emotion recognition cannot actually assess an individual’s internal emotions and experience. It can only estimate how that individual’s emotions might be perceived by others, or suggest broad, population-based trends (such as one film eliciting, on average, a more positive reaction than another). “No serious researcher would claim that you can analyze action units in the face and then you actually know what people are thinking,” says Elisabeth André, an affective computing expert at the University of Augsburg.
- Researchers also note that emotion recognition involves far more than just looking at someone’s face. It can also involve observing body posture, gait, and other characteristics, as well as using biometric sensors and audio to gather more holistic data.
- Such distinctions are fine but important: they disqualify applications like HireVue, which claim to assess an individual’s inherent competence, but support others, such as technologies aiming to make machines into more intelligent collaborators and companions for humans. (HireVue did not respond to a request for comment.) A humanoid robot could smile when you smiled—a mirroring action humans often use to make interactions feel more natural. A wearable device could remind you to rest if it detected higher than baseline levels of cortisol, the body’s stress hormone. None of these applications require an algorithm to assess your private thoughts and feelings; they only require an estimation of an appropriate response to cortisol levels or body language. They also do not make high-stakes decisions about an individual’s life—unlike unproven hiring algorithms. “If we want computers and computing systems to help us, it would be positive if they had a sense of how we are feeling,” says Nuria Oliver, the chief data scientist at the nonprofit DataPop Alliance.
- But much of this nuance gets lost when emotion recognition research is used to make lucrative commercial applications. The same stress-monitoring algorithms in a wearable could be used by a company trying to make sure you’re working hard enough. Even for companies like Affectiva, founded by researchers who speak about the importance of privacy and ethics, the boundaries are tough to define. It has sold its technology to HireVue. (Affectiva declined to comment on specific companies.)
- 
- 
- In December, the AI Now research institute called for a ban on emotion recognition technologies “in important decisions that impact people’s lives.” It’s one of the first calls to ban a technology that has received less regulatory attention than other forms of artificial intelligence, even though its use in job screening and classrooms could have serious effects.
- In contrast, Congress just held its third hearing on facial recognition in less than a year, and it has become an issue in the 2020 election. Activists are working to boycott facial recognition technologies, and several representatives are acknowledging the need for regulation in both the private and public sectors. For affective computing, there haven’t been as many dedicated campaigns and working groups, and attempts at regulation have been limited. An Illinois law regulating AI analysis of job interview videos went into effect in January, and the Federal Trade Commission has been asked to investigate HireVue (though there’s no word on whether it intends to do so).
- Though many researchers believe a ban is too broad, they agree that a regulatory vacuum is also harmful. “We have clearly defined processes to certify that certain products that we consume—be it food that we eat, be it medications that we take—they are safe for us to take them, and they actually do whatever they claim that they do,” says Oliver. “We don't have the same processes for technology.” She thinks companies whose technologies can significantly affect people’s lives should have to prove that they meet a certain standard of safety.
- Rosalind Picard, a professor at the MIT Media Lab who cofounded Affectiva and another affective computing startup, Empatica, echoes this sentiment. For an existing model of regulation, she points to the Employee Polygraph Protection Act limiting the use of lie detectors, which she says are essentially an affective computing technology. For example, the law prohibits most private employers from using polygraphs and doesn’t let employers ask about the results of lie detector tests.
- She suggests that all use of such technologies should be opt-in and that companies should be required to disclose how their technologies were tested and what their limitations are. “What we have today is that [companies] can make these outrageous claims which are just false, because right now the buyer is not that well educated,” she says. “And we shouldn’t require the buyers to be well educated.” (Picard, who says she left Affectiva in 2013, doesn’t support the claims that HireVue is making.)
- For her part, Meredith Whittaker, a research scientist at NYU and co-director of AI Now, emphasizes the difference between research and commercialization. “We are not impugning the entire field of affective computing,” she says. “We are particularly calling out the unregulated, unvalidated, scientifically unfounded deployment of commercial affect recognition technologies. Commercialization is hurting people right now, potentially, because it’s making claims that are determining people’s access to resources.”
- A ban on using emotion recognition in applications such as job screening would help stop commercialization from outpacing science. Halt the deployment of the technologies first, she says, and then invest in research. If the research confirms that the technologies work as companies claim, then consider loosening the ban.
- Other regulations would still be needed, however, to keep people safe: there’s ultimately more to consider, Whittaker argues, than just scientific credibility. “We need to ensure, when these systems are used in sensitive contexts, that they are contestable, that they are used fairly,” she says, “and that they are not leading to increased power asymmetries between the people who use them and the people on whom they're used.”
- “I have suddenly switched my views on whether these things are going to be more intelligent than us.”
- The narrative around cheating students doesn’t tell the whole story. Meet the teachers who think generative AI could actually make learning better.
- Hinton will be speaking at EmTech Digital on Wednesday.
- Large language models are full of security vulnerabilities, yet they’re being embedded into tech products on a vast scale.
- Discover special offers, top stories,
            upcoming events, and more.
- Thank you for submitting your email!
- It looks like something went wrong.
- We’re having trouble saving your preferences.
                Try refreshing this page and updating them one
                more time. If you continue to get this message,
                reach out to us at
                customer-service@technologyreview.com with a list of newsletters you’d like to receive.
- 
- © 2023 MIT Technology Review

URL: https://www.fastcompany.com/90597594/ai-algorithm-auditing-hirevue
- Please enable JS and disable any ad blocker

URL: https://www.brookings.edu/research/auditing-employment-algorithms-for-discrimination/
- Algorithmic hiring systems are proliferating, and while some present opportunities to reduce systemic biases, others create new modes of discrimination. Broadly, the use of algorithms can enable fairer employment processes, but this is not guaranteed to be the case without meaningful standards. Algorithmic audits have been proposed as one such way to ensure those standards, and early examples of audits of algorithmic hiring systems have been released to the public. An examination of these audits and the relevant incentives demonstrates how algorithmic auditing will not produce accountability on its own. This paper presents steps toward specific standards of what constitutes an algorithmic audit and a path to enforce those with regulatory oversight.
- The role of artificial intelligence (AI) in the job market and in hiring has been expanding rapidly—an industry survey found that 55% of human resources leaders in the United States use predictive algorithms in hiring. There are now algorithmic tools available for almost every stage of the employment process. Candidates can find job openings and recruiters can find candidates through sourcing platforms such as LinkedIn, Monster, and Indeed. Many companies employ algorithmic systems to analyze resumes, while another set of companies, such as JobScan and VMock, use algorithms to improve how resumes appear to other algorithms. Other companies, such as Pymetrics and PredictiveHire, create specialized questionnaires and assessments as inputs to AI to predict job performance. Firms may use AI to transcribe recorded statements to text, then analyze those textual responses with natural language processing. Some vendors, such as HireVue and TalView, have used facial analysis in these interviews—a task that goes far beyond the limits of what AI can do. Beyond these vendors, larger companies also build their own internal algorithmic tools to aid in hiring. Even on-the-job employee surveillance, which often uses data to partially determine retention, salaries, and promotion, can be considered part of this algorithmic funnel.
- The common characteristic of these algorithmic tools is that they use data collected about a candidate to infer how well they might perform in a job, often using a subfield of AI called supervised machine learning. Companies build or procure these tools in order to reduce the duration and cost of hiring, as well as potentially improve workplace diversity and new hire performance, according to a thorough review of discrimination concerns in algorithmic employment by Upturn. For individual job candidates, the effects are not as obvious, and those who don’t fit neatly into algorithmic expectations could be left out. Further, the proliferation of algorithms across the hiring process is unprecedented, engendering concerns about how these systems are affecting the labor economy—especially around systemic discrimination.
- Research has suggested there are many ways for bias to enter into algorithmic hiring processes. Machine learning models that predict metrics of workplace success, such as performance reviews or salary, may attribute good marks to competency, when they are in part the result of result of environmental factors unrelated to skill, such as historical workplaces biases.
- “[T]he proliferation of algorithms across the hiring process is unprecedented, engendering concerns about how these systems are affecting the labor economy—especially around systemic discrimination.”
- Models used to analyze natural language, such as in resumes or transcribed from interviews, have demonstrated biases against women and people with disabilities. Speech recognition models have demonstrated clear biases against African Americans and potential problems across dialectical and regional variations of speech. Commercial AI facial analysis, aside from being largely pseudoscientific, has shown clear disparities across skin color and is highly concerning for people with disabilities. Algorithms that disseminate job postings can unintentionally result in biased outcomes against young women for STEM jobs and similarly ageism against older candidates. The many documented instances of algorithmic biases are especially problematic in light of the long series of algorithmic evaluations detailed above—small biases in individual algorithms could easily accumulate to larger structural issues.
- As demonstrated by an impactful paper that switched white sounding names and African American sounding names on otherwise identical resumes, humans also exhibit significant biases in hiring. In 2017, a meta-analysis of 28 studies observed “no change in the level of hiring discrimination against African Americans over the past 25 years” and a modest decline in discrimination against Latinos. Unfortunately, a thorough meta-analysis of psychological research shows that it is very difficult to mitigate implicit biases in human decision-making. Research also shows how diversity and inclusion interventions have not improved outcomes, such as for women in the technology industry. Other candidate evaluation tools currently in use, such as cognitive ability tests, can also exhibit dramatic racial biases resulting in disparate impact. Cognitive ability tests, including IQ tests, have played an extensive role in hiring processes for decades, despite significant evidence of their bias and relatively weak evidence of their efficacy. It is therefore critical to consider that hiring processes before algorithmic tools also exhibit systemic biases and should not necessarily be considered preferable.
- Broadly speaking, using algorithms can, under certain circumstances and with careful implementation, reduce the prevalence of bias in a decision-making process. Bias can be reduced at technical levels within language models or by changing the outcome variable that is being predicted. Alternatively, changing how algorithmically-generated rankings are used can enable fair outcomes from an otherwise biased model. Specifically in hiring, algorithms have also been used to aid in debiasing specific aspects of the employment process, such helping to mask race, gender, and demographic information from applications and rewording job descriptions to attract more diverse candidates.
- The 2019 paper “Discrimination in the Age of Algorithms” makes the argument for algorithms most holistically, concluding correctly that algorithms can be more transparent than human decision-making, and thus “have the potential to make important strides in combating discrimination.” The prevailing evidence supports this conclusion, although it needs more evaluation in applied settings. For instance, there have not yet been systemic analyses of the effects of employment algorithms in practice, due to their proprietary nature. Still, the deeply flawed status quo and potential capacity of algorithmic approaches to reduce bias warrant consideration of what sociotechnical systems and incentives would lead to the best outcomes.
- It is important to consider how market incentives and governmental oversight affect algorithm development, because it is not obvious that best practices will otherwise prevail. In the absence of oversight or threat of litigation, there are good reasons to be skeptical that these employment models are typically rigorous in their approach to fairness. Most prominent of these reasons is that it is more expensive to take a thorough approach to fairness. It is time-consuming to task highly skilled data scientists and engineers with making robust and fair algorithmic processes, rather than building new features or delivering a model to a client. Further, it adds expense to collect more diverse and representative data to use in developing these models before deploying them. The scale at which these systems can operate is also a concern—these employment algorithms cumulatively affect tens of millions of people. This means that discriminatory outcomes can easily harm many thousands or even millions of people. Client companies, who procure these algorithmic systems to hire employees, may often be genuinely interested in fostering diversity through algorithmic hiring systems. However, it can be difficult to differentiate between the vendors of these systems in terms of non-discrimination, because all of them make strong claims about “unbiased models.” Algorithmic audits are one promising way to verify these claims about non-discrimination.
- There is a growing interest in the role of algorithmic audits in response to concerns about bias in algorithmic systems. In an algorithmic audit, an independent party may evaluate an algorithmic system for bias, but also accuracy, robustness, interpretability, privacy characteristics, and other unintended consequences. The audit would identify problems and suggest improvements or alternatives to the developers. Beyond improving systems, algorithmic auditing can also help to build consumer and client trust if its results are made public. A cottage industry has arisen around this idea, including firms that specialize in algorithmic auditing, such as O’Neil Risk Consulting & Algorithmic Auditing (ORCAA) and Parity AI, and other companies focused on algorithmic monitoring, such as Fiddler, Arthur, and Weights & Biases.
- While there is a long history of audits in computer security and database privacy, the idea of an algorithmic audit is relatively new and remains nebulously defined,1 though research is ongoing. The lack of a formal definition means there are myriad decisions that can be made within the bounds of an algorithmic audit. A vendor of algorithmic systems may have many different models2 in use, and thus an auditor may examine one model, many models of one type, or a random sample of some or all model types. The auditor might directly access the company’s data in order to run its own statistical tests, or it might request and take the statistics as provided by the audited company. The auditor might also directly examine the algorithmic models themselves to examine its features and test new use cases. Further the auditor might make its conclusions fully public, partially public, or only provide the results to the client company.
- These choices decide whether the effect of an audit is private introspection, meaningful public accountability, or theater for public relations. Recent examples from the field demonstrate some of the possible outcomes.
- The difference in both depth of analysis and level of resulting transparency is noteworthy, both of which were substantially better in the case of Pymetrics and the Northeastern researchers. Still, there are some benefits to both of these audits—the HireVue audit created marginally more transparency than entirely private introspection and could have fostered other changes in the algorithmic processes. As an alternative, consider when Amazon discovered their resume analysis tool was biased against women job candidates and eventually shut down the system. Amazon did not make this public and it was only discovered a few years later by an investigative reporter at Reuters. Aside from transparency, these early algorithmic audits demonstrate how smaller vendors might evaluate and improve their algorithmic services if they do not have in-house expertise on these topics.
- An algorithmic audit will not automatically prevent the use of biased algorithms in employment. While the idea of auditing is associated with accountability, auditing does not automatically produce accountability. When an algorithmic hiring developer contracts with an algorithmic auditor, that auditor is providing a service for a client—a service on which the auditor is often financially dependent. That financial dependency can fundamentally undermine the public value of the audit. While an auditing company may be concerned about its public reputation or holding high professional standards, the need to sell their services will be the most important factor and will ultimately decide which auditors survive and which fail. Although both ORCAA and the Northeastern University researchers were paid for their auditing work, it is relevant that the more thorough audit was done by academic researchers with other means of financial support, and the less thorough audit was done by a company with a direct financial dependency. This encourages consideration of the incentives that would lead companies to choose and enable comprehensive audits, as well as the incentives for auditors to execute robust and critical audits.
- “While the idea of auditing is associated with accountability, auditing does not automatically produce accountability.”
- On this topic, important lessons can be taken from financial accounting, where three potential mechanisms stand out for their relevance to algorithmic auditing. These mechanisms are government enforcement that verifies those audits, market incentives that demand rigorous and thorough audits, and professional norms that hold individual auditors to a high standard. Unfortunately, these are all largely absent from algorithmic auditing, but could be enabled in ways analogous to financial accounting.
- First, consider market incentives. If the clients of a financial auditing company keep releasing clean balance sheets until the moment they go bankrupt, the auditor will lose crucial trust. For example, consider the reputational damage that Ernst & Young has suffered from missing $2 billion of alleged fraud from its client, Wirecard. That Ernst & Young is taking such criticism is a good thing—it means that other financial auditing companies will take heed and avoid similar mistakes. Auditing firms work not only to verify public-facing financial statements, but also the internal controls and processes that prevent fraud. This means that shareholders and scrupulous executives have good reason to hire effective auditors, and financial auditors actively compete to excel in evaluations of auditor quality. Yet there is no parallel in algorithmic auditing, as algorithmic harms tend to be individualized and are hard to identify. Companies will not fail if their algorithms are discriminatory, and even when evidence of discrimination arises publicly, it is not clear that they face proportionate consequences.
- Professional standards and codes of practice are also used to ensure rigor in professional auditing services. In financial accounting, a private sector organization, the Financial Accounting Standards Boards, enforces a common set of principles, the Generally Accepted Auditing Standards (GAAS), which detail how financial accounting should be audited by an independent organization. Critically, the GAAS constrain the number of choices that can be made within an audit. While there is still some flexibility and room for potential manipulation, the standards enable a more consistent evaluation of whether audits are performed thoroughly. Again, there are no equivalent guidelines in algorithmic accounting, which explains in part the huge difference in depth between the HireVue and Pymetrics audits. In fact, if a clear set of standards were established, the HireVue analysis may not even qualify as an audit, but instead be defined as a case study or review.
- The government plays a role in financial accounting oversight in two critical ways. First, accountants are liable if they knowingly participate in, or enable through negligence, financial fraud, just like the perpetrating company. Auditors who deviate from the best practices in the GAAS standards have a more difficult time defending their actions as diligent and in good faith if they are sued by a client, or its investors and creditors, for allowing fraud. Once again, there is no civil liability established in algorithmic accounting—if a company was cleared of discriminatory effects by an auditor but was then successfully sued for discrimination in its algorithms, the auditor would not share that liability unless it was explicitly written into a contract. This scenario is also unlikely because proving algorithmic discrimination in a lawsuit may be highly difficult, as it would often require plaintiffs to get broad access to hiring data for many job candidates, not just their own case.
- “Beyond civil liability, direct government oversight can be a mechanism for keeping audits honest.”
- Beyond civil liability, direct government oversight can be a mechanism for keeping audits honest. In finance, a non-profit called the Public Company Accounting Oversight Board (PCAOB) performs this role. In practice however, the PCAOB has not made extensive use of its oversight powers, and its effect on the marketplace is uncertain. Tax accounting illustrates the role of government oversight more clearly—if clients of a certain tax accountant are routinely fined by the Internal Revenue Service, potential customers will know to go elsewhere. Yet there is also no clear parallel to IRS audits or the PCAOB oversight in algorithmic auditing.3
- Unfortunately, because none of these checks exist for algorithmic hiring systems, putting illegal or unethical activity into an algorithm can effectively shield it from scrutiny. However, there are clear changes that can be made to improve this situation. Creating a specific and robust definition of what constitutes an algorithmic audit, and then enabling government oversight and civil liability as enforcement mechanisms, would all greatly improve the status quo.
- Defining what should be included in algorithmic audits for biases in employment systems can encourage more rigorous future audits and provide a benchmark of comparison for completed audits. Of course, this is not a task that can be done in a single analytical paper. This task will require a community effort with a range of expert contributions and buy-in from companies in this field, for instance through a professional standards organization. Further, many considerations will be specific to the specific modeling applications and how they are deployed. However, there are some important criteria that will often apply. These can act as a starting point for these standards and can help evaluate the quality of voluntary audits performed before such standards exist.
- These criteria are meant as a starting point for a broader discussion. They likely neither apply in all situations, nor are fully sufficient, and additional discussion and investment is warranted. As other analyses have noted, going beyond technical implementations to a wider evaluation of stakeholder needs may also be an important role for algorithmic audits, though that is less likely to be enforced by law. Further, bias is not the only concern of employment algorithms, and audits may also consider how the models can be made more transparent to candidates or secure in their protection of sensitive data.
- Creating formal and robust standards for algorithmic audits is an important step, but it must still be backed up by governmental oversight. In the United States, the Equal Employment Opportunity Commission (EEOC) should consider what specific steps are needed to execute algorithmic audits of this kind. It appears this work was started in late 2016, but it is not clear that it continued under the Trump administration (though other relevant work has). The Department of Labor’s Office of Federal Contract Compliance could also enforce new rules, ensuring that federal government only enabled contracts with algorithmic hiring companies that produced compelling evidence of unbiased outcomes.
- “Creating formal and robust standards for algorithmic audits is an important step, but it must still be backed up by governmental oversight.”
- As I have argued before, the Biden administration should tackle these challenges in earnest. For the EEOC, this includes exploring the limitations in agency authority to access corporate data when responding to charges of employment discrimination. In fact, 10 Senators recently wrote to the EEOC, specifically asking about its ability to investigate companies that build and deploy employment algorithms. This is a critical question, as little rigorous analysis can take place without the government gaining access to the data used for these algorithmic systems. The relevant agencies might also need new technical expertise, and it was encouraging to see the EEOC open job postings for two data scientists as part of an important effort to improve federal hiring processes.
- In addition to building the capacity to do algorithmic audits, the EEOC needs to revisit existing policies and draft new guidance to adapt to the proliferation of algorithmic systems. This includes revisiting the idea of predictive validity in the EEOC’s Uniform Guidelines on Employee Selection. As Manish Raghaven and Solon Barocas have argued, the Uniform Guidelines currently enable algorithmic developers to defend against charges of discriminatory effects by demonstrating that their tool is predictive, even if it is also discriminatory. This is an unreasonable standard, because these systems typically employ supervised machine learning, which entails they are definitionally predictive. This provision needs to change, or little to no oversight can occur. Further, the EEOC should clarify that, when it performs an audit of algorithmic hiring systems, it will look at a broader range of metrics than just differences in rates of selection across subgroup, which has long been used as a rule-of-thumb for discriminatory impact. It is also necessary to consider requirements for the storage of data, code, and models by the regulated companies, so that they can be audited. The EEOC should also consider other ways to better enable civil liability as a mechanism for enforcing the fair use of algorithms.
- The goal of government intervention in this field is not to make the labor market perform perfectly, but to meaningfully raise the floor of quality in algorithmic employee assessment. By holding companies to a consistent and higher standard, responsible companies that invest time and money into deployment of fair algorithms will be better able to compete, while unscrupulous companies face fines and lawsuits. Algorithmic audits alone are likely insufficient to prevent abuses, and other interventions warrant consideration. For instance, a total ban on using facial affect analysis for employment services is worth consideration. Further, these audits may not clearly apply to the online job sourcing platforms, such as Monster, LinkedIn, and Indeed. Given the importance of those companies to the labor marker, working to open up their data to independent researcher access may be a valuable step.
- If the government is able to create meaningful regulatory oversight, algorithmic audits would become much more impactful. This would lead to stronger incentives for investing in fairer algorithmic systems, and the world of algorithmic hiring might improve on the pervasive biases present in candidate review processes done by humans or outdated cognitive assessments. If this were to become the case, the EEOC could even highlight best practices in the field and urge employers to move toward algorithmically sound and responsible candidate selection processes. With these changes, the proliferation of algorithms in employment systems could realize its potential and significantly reduce systemic biases that have disadvantaged many individuals and undermined the broader labor market.
- The Brookings Institution is a nonprofit organization devoted to independent research and policy solutions. Its mission is to conduct high-quality, independent research and, based on that research, to provide innovative, practical recommendations for policymakers and the public. The conclusions and recommendations of any Brookings publication are solely those of its author(s), and do not reflect the views of the Institution, its management, or its other scholars.
- Microsoft provides support to The Brookings Institution’s Artificial Intelligence and Emerging Technology (AIET) Initiative, and Amazon provides general, unrestricted support to the Institution. The findings, interpretations, and conclusions in this report are not influenced by any donation. Brookings recognizes that the value it provides is in its absolute commitment to quality, independence, and impact. Activities supported by its donors reflect this commitment.
- Report Produced by Center for Technology Innovation

URL: https://www.brookings.edu/blog/techtank/2019/10/31/for-some-employment-algorithms-disability-discrimination-by-default/
- Last week, Washington Post’s Drew Harwell reported that HireVue’s artificial intelligence (AI) software has assessed over a million video job interviews. Its autonomous interview system asks questions of candidates, films their responses, and then uses the resulting video and audio to assess candidates for jobs, such as in investment banking and accounting. The AI attempts to predict how a candidate will perform on the job based on how they act in an interview—their gestures, pose, lean, as well as their tone and cadence—and the content of their responses. This process produces an employability score, which employers use to decide who advances in the application process.
- Yet a number of ethical AI observers have been very critical of HireVue. In the Washington Post story, AI Now Institute Co-Founder Meredith Whittaker calls this development “profoundly disturbing” and the underlying methodology “pseudoscience.” Princeton Computer Science Professor Arvind Narayanan says this is “AI whose only conceivable purpose is to perpetuate societal biases.” Scientific evidence suggests that accurately inferring emotions from facial expressions is very difficult and it stands to reason that inferring personality traits is even harder, if it’s possible at all.
- What has been not been noted, however, is the way in which these systems likely discriminate against people with disabilities. The problem that people with disabilities face through this kind of AI is, even if they have a strong set of positive qualities for certain jobs, the AI is unlikely to highlight those features and could generate low scores for those individuals.
- Characteristics such as typical enunciation and speaking at a specific pace are qualities that might correlate with effective salespersons. Further, perhaps leaning forward with one arm on the table signals an interpersonal comfort that prior high-performing salespersons often display. The AI system would have identified this relationship from the “training data”—the video interviews and the sales outcomes collected from current employees. However, people with disabilities will not benefit if their qualities manifest physically in a way the algorithm has not seen in that training data. If their facial attributes or mannerisms are different than the norm, they get no credit, even if their traits would be as beneficial to the job.
- Advocates suggest that this is a big problem. Sheri Byrne-Haber, Head of Accessibility at VMware, has argued that “the range of characteristics of disability is very, very broad,” contributing to this algorithmic discrimination problem. Shari Trewin, an Accessibility Researcher at IBM, agrees, arguing: “The way that AI judges people is with who it thinks they’re similar to—even when it may never have seen anybody similar to them—is a fundamental limitation in terms of fair treatment for people with disabilities.”
- To account for this problem, AI training data would have to include many people with diverse disabilities. Since each job type has a distinct model, this would have to be true for many different models (for context, HireVue has over 200 models). While it is possible AI software could include such a range of different individuals, it would take a tremendous effort. Without diverse training, an AI system would not be able to learn any characteristics demonstrated by people with disabilities who were later successful. With some of their qualities ignored, these candidates would be pushed towards the middle of the distribution. And since most applicants for any specific job do not get hired, applicants with no similar, high-performing past employees do not stand a chance.
- On their ethical AI page, HireVue says they actively promote equality opportunity “regardless of gender, ethnicity, age, or disability status.” Further, HireVue does allow people with disabilities to request more time for questions and otherwise implement accommodations as requested by the employer. However, the core problem of inferring from videos of people with disabilities remains. In the Post’s reporting, Nathan Mondragon, the chief industrial-organizational psychologist at HireVue, says that facial actions can make up 29% of a person’s employability score.
- Broadly, this issue of coverage (meaning the training data containing enough relevant examples) is a genuine concern when applying AI systems to people with disabilities. Potentially relevant to this software, there is research showing that speech recognition works poorly for people with atypical speech patterns, such as a deaf accent. Google researchers demonstrated that some AI considers language about having a disability as inherently negative. As another problematic example, imagine how driverless cars might learn human movements to avoid the path of pedestrians. This is a type of situation in which humans still dramatically outperform AI: choosing not to narrowly interpret a situation based only on what we have seen before.
- About 13% of Americans have a disability of some kind and they already suffer from worse employment outcomes. Their unemployment rate stands at 6.1%, twice that of people without disabilities. Americans without disabilities also out-earn their peers with disabilities: $35,000 to $23,000 in median earnings over the past year, according to the Census Bureau. Specifically relevant to facial analysis, estimates suggest that around 500,000 to 600,000 people in the United States have been diagnosed with a craniofacial condition, meaning an abnormality in the face of head. Additionally, millions of Americans have autism spectrum disorder, one of many conditions which can manifest itself in unusual facial or speech expressions.
- While systems like these may embolden recent calls for facial recognition bans, there are other policy implications as well. For algorithms that are crucial in hiring, companies should publicly release bias audit reports—summaries of the predictions made across subgroups, especially protected classes—rather than simply claiming their models have been evaluated and are bias-free. Further, the Equal Employment Opportunity Commission (EEOC) should review these systems and issue guidance on whether these systems violate the Americans with Disabilities Act. While there are many positive applications of AI for people with disabilities, we need to be especially careful that AI for video and audio analysis treats all people fairly.

URL: https://www.reuters.com/article/global-tech-ai-hiring-idUSL5N2NF5ZC
- Discover Thomson Reuters
- By Avi Asher-Schapiro, Thomson Reuters Foundation
- 8 Min Read
- * Companies increasingly using AI software in hiring
- * Tools seen locking in racial discrimination, other biases
- * Policymakers grapple with how to regulate the industry
- LOS ANGELES, June 7 (Thomson Reuters Foundation) - Since graduating from a U.S. university four years ago, Kevin Carballo has lost count of the number of times he has applied for a job only to receive a swift, automated rejection email - sometimes just hours after applying.
- Like many job seekers around the world, Carballo’s applications are increasingly being screened by algorithms built to automatically flag attractive applicants to hiring managers.
- “There’s no way to apply for a job these days without being analyzed by some sort of automated system,” said Carballo, 27, who is Latino and the first member of his family to go to university.
- “It feels like shooting in the dark while being blindfolded - there’s just no way for me to tell my full story when a machine is assessing me,” Carballo, who hoped to get work experience at a law firm before applying to law school, told the Thomson Reuters Foundation by phone.
- From Artificial Intelligence (AI) programs that assess an applicant’s facial expressions during a video interview, to resume screening platforms predicting job performance, the AI recruitment industry is valued at more than $500 million.
- “They are proliferating, they are fast, they are relatively cheap - they are everywhere,” said Alex Enger, a fellow at the Brookings Institute, who studies AI in hiring.
- “But at this point there’s very little incentive to build these tools in a way that’s not biased,” he added, saying the cost and time involved in thoroughly testing a system for bias was likely to be prohibitive without regulations requiring it.
- For Carballo, racial bias is his topmost concern.
- “I worry these algorithms aren’t designed by people like me, and they aren’t designed to pick people like me,” he said, adding that he has undergone a plethora of different AI assessments - from video analytics to custom logic games.
- The risk of discrimination is also a central issue for lawmakers around the world as they weigh how to regulate the use of AI technology, particularly in the labor market.
- While the EU is set to impose rules on the use of AI in hiring, U.S. lawmakers are considering federal laws to address algorithmic bias. Last year, legislators in New York City proposed a law specifically to regulate AI in hiring.
- “We’re approaching an inflection point,” Enger said.
- ‘IT’S A MINEFIELD’
- According to the most recent survey by human resource (HR) industry group Mercer, more than 55% of HR managers in the United States use predictive algorithms to help them make hiring choices.
- AI is being introduced at every stage of the hiring pipeline, from the job adverts that potential applicants see to the analysis and assessment of their applications and resumes.
- The COVID-19 pandemic has sped up the adoption of such tools. HireVue, a AI hiring firm that builds tools to analyze and score the answers job applicants give in video interviews, reported a 46% surge in usage this year compared to last.
- The rise in AI could represent a real opportunity to root out prejudice in the hiring process, said Manish Raghavan, a computer scientist at Cornell University who studies bias in hiring algorithms.
- “No one is going to tell you that traditional hiring was equitable,” he said. “And with AI systems we can test them in ways we could never test or audit people’s own biases.”
- Subjecting all candidates to the same interview, judged by the same algorithm, eliminates the subjectivity and bias of people in hiring, said Kevin Parker, chief executive of HireVue.
- “We can measure how men and women score, and compare how people of color score against white candidates,” he said. “We really try to fine-tune the algorithm to eliminate anything that can cause adverse impact, and come to very close parity.”
- But the problem, Raghavan said, is that when you build a machine learning algorithm, bias can creep into it in many ways that are difficult to detect.
- Enger echoed that view.
- “Natural language processing systems have been shown to associate white names as being more qualified. Resume screening systems have been shown to weed out all applicants who went to a women’s college,” he said.
- “It’s a minefield,” he added.
- For job seekers like Carballo - who belong to ethnic minorities and have disadvantaged backgrounds, automated tools can easily reinforce patterns of discrimination, Raghavan said.
- In 2017, Amazon stopped using an AI resume screener after discovering it penalized resumes that included the word “women”, automatically downgrading graduates of all-women’s colleges.
- Because applicants often have no way of understanding how they were scored, they are left wondering if bias crept in, Carballo said.
- “I’m a first generation college student, I’m Latino, and I didn’t go to a top university - and every time I get a rejection, I wonder if the system was designed to weed someone like me out.”
- Industry is eager to be perceived as fighting bias, Raghavan said, citing his own research showing that 12 of the 15 largest firms have announced some efforts to tackle discrimination.
- But Enger said there was currently little incentive for companies to invest significant resources in detecting and rooting out bias, as regulators are not yet cracking down.
- That could start to change, however, as policymakers begin to take a look at the industry.
- Regulatory proposals being considered by the European parliament would designate AI used in hiring as “high-risk”, meaning any companies selling such systems would have to be included in a public data-base.
- It would also impose requirements on firms selling such tools in the EU, such as ensuring datasets are “relevant, representative, free of errors and complete”, according to Daniel Leufer, an analyst at digital rights group Access Now.
- Leufer said the draft regulations do not go far enough, calling for a blanket ban on certain AI tools in hiring, including any that use biometric information such as facial movements or voice tone.
- “The length of my nose; how I speak, the way I move my mouth; we should not allow people to make inferences about someone’s job performance from these kinds of inputs,” he said.
- In New York City, the city council is considering a law that would regulate the AI hiring industry, and compel companies to do their own audits for bias, but critics fear it will not be sufficient to rein in discrimination.
- “One flawed algorithm can impact hundreds of millions of people,” said Albert Fox Cahn, executive director of the Surveillance Technology Oversight Project (STOP), who wants a freeze on AI in hiring pending further bias investigations.
- STOP and 11 other digital and civil rights groups sent a letter to New York City Council late last year, asking for stronger protections, including allowing applicants who were discriminated against to file lawsuits.
- “We need to press pause until we are able to come up with effective regulatory structures to block AI bias and discrimination,” Cahn said.
- In April, after working a string of short-term temporary jobs over the past year, Carballo finally got a full-time job at a law firm. The hiring manager interviewed him without the use of an AI screener.
- "I think that made a difference - I wasn't just a guy from a rough neighborhood, with a Spanish last name," he said. "I was able to make an impression." (Reporting by Avi Asher-Schapiro @AASchapiro; Editing by Helen Popper. Please credit the Thomson Reuters Foundation, the charitable arm of Thomson Reuters, that covers the lives of people around the world who struggle to live freely or fairly. Visit news.trust.org)
- All quotes delayed a minimum of 15 minutes. See here for a complete list of exchanges and delays.

URL: https://cdt.org/insights/hirevue-ai-explainability-statement-mostly-fails-to-explain-what-it-does/
- Privacy & Data
- September 8, 2022
/

Matt Scherer
- The increasing prevalence and prominence of automated employment decision tools in recruitment and hiring has led regulators and advocacy organizations to demand greater transparency and accountability from the vendors of such tools. CDT supports efforts to make AI systems more explainable; in the context of hiring assessment technologies, explainability means ensuring that workers are “meaningfully notified about how they will be assessed so they can seek redress under existing civil rights protections or request a reasonable accommodation.”
- This spring, one of the most prominent vendors of AI-based assessment tools, HireVue, released what it billed as a “first of its kind” explainability statement “intended to provide information on how [HireVue’s] Artificial Intelligence (AI)-based assessments” work and how it makes employment decisions and recommendations. The statement provides a general overview of the types of AI assessments that HireVue uses, including some details about how HireVue developed, trained, and now monitors its assessments.
- HireVue’s explainability statement offers a concrete opportunity to assess what works well and what doesn’t in providing transparency about the use of AI in hiring tools. While its statement sheds some useful light on how HireVue’s technology works, it is also incomplete in important respects. Moreover, the information it does provide suggests crucial deficiencies in the fairness and job-relatedness of HireVue’s approach to assessments.
- Overview
- HireVue sells AI-based assessment tools that employers can use when making hiring/employment decisions. HireVue claims that its AI assessment technologies can be used to assess more than 20 different “competencies” relevant to different jobs, such as adaptability, teamwork, problem-solving skills, communication, and “drive for results.” HireVue uses two basic types of AI assessments to assess these competencies:
- To evaluate a candidate for a particular job using HireVue’s technology, an employer chooses from the competencies HireVue claims to measure to build a “competency model” for that job. HireVue evaluates the candidate against that competency model using a series of video interviews and game-based questions. Per the explainability statement, “[a] typical assessment will consist of 3-6 video interview questions … and 2-3 game questions.”
- The statement provides information – albeit in sometimes paltry levels of detail – regarding how these AI assessments were constructed, how they are tested for job-relatedness and discriminatory impact, and how they are monitored and updated over time. Below are some of the notable takeaways (both positive and negative) from the explainability statement.
- The Good
- Informative description of the development and structure of HireVue’s video interview assessments
- The explainability statement provided a reasonable level of detail on how HireVue developed its video interview assessments. The document:
- This overview strikes a good balance between concision and detail – it provides enough information to give a fair understanding of how HireVue developed and built the key components of its video interview assessments, but not so much that regulators, employers, and other educated laypeople would be overwhelmed in trying to understand the assessments’ development process or basic architecture.
- That, in turn, allows for a well-informed critique of both the positive and negative aspects of how the development process may have affected the fairness and validity of the assessment technology. Enabling such analysis and criticism is one key goal of explainability in the hiring technology space. The document does well in that regard with respect to the video interview (as opposed to its game-based) assessments.
- Video interview assessments were tested on a sample of workers that was representative in terms of race and gender
- The explainability statement also provided demographic information regarding the population of workers whose data HireVue used to train its video interview assessments. [1] According to the tables HireVue provided, the sample included a representative sample of women (52%) compared to the labor force at large, and actually slightly oversampled black (17%) and Hispanic (33%) workers. Such representative sampling helps reduce one avenue through which systemic biases are reinforced by ensuring that the training data includes sufficient examples of historically underrepresented groups.
- Reasonable overall approach to ongoing monitoring and review
- Finally, HireVue’s overall approach to monitoring and reviewing its assessments after deployment makes sense. The Civil Rights Principles for Hiring Assessment Technologies, of which CDT is a signatory, calls for organizations to “engage in rigorous self-testing of their own hiring assessment technologies before and after deployment” and “continually audit[]” the technologies for disparities once deployed. HireVue’s explainability statement states that after employers begin using its assessments, HireVue (1) monitors them continuously and is alerted “if a particular metric goes ‘out of bounds’” (although the statement does not provide examples of such metrics or alerts) and (2) does deeper checks for performance and adverse impacts on a regular (typically annual) basis. At a high level, this combination of dynamic/real-time monitoring plus more thorough reviews at standardized intervals is a good approach that generally tracks with the Civil Rights Principles.
- The Bad: Lack of Explanation
- Lack of explanation regarding game-based assessments
- The explainability statement provides virtually no explanation of how HireVue devised, operated, or tested its game-based assessments. What little information the document does provide on the games is confusing – in the first mention of game-based assessments, the document states that the games measure both “cognitive” and “non-cognitive abilities,” but the description of the games’ design states only that it measures “cognitive ability.” It is not at all clear from the document how the games were validated or whether and how they were tested for issues relating to accessibility or bias.
- HireVue kicks disclosure responsibility to employers under EU/UK data regulations
- HireVue claims that because employers make the ultimate hiring decision, they are responsible for providing explanations to candidates regarding hiring decisions involving the use of HireVue’s tools under prevailing UK and EU data regulations. The problem is that, as described further below, HireVue does not tailor its products to individual employers or provide employers with enough information to provide an adequate explanation as to how HireVue’s assessments work. The various explanatory products that HireVue offers (samples of which are included in the explainability statement) are decidedly generic, with no explanation of how different competencies were measured. By disclaiming disclosure responsibilities, HireVue essentially ensures that candidates in Europe remain in the dark when its assessments influence employment decisions.
- HireVue’s approach to “competency” assessment is unclear
- HireVue provides a similarly incomplete explanation of the competencies that its interview and game-based assessments test – and the information it does provide is troubling. While the explainability statement mentions or alludes to several of the competencies its assessments supposedly measure, HireVue does not actually provide a list of all such competencies. Failing to list the competencies deprives workers and their advocates of the information needed to determine what sorts of candidates, particularly those with disabilities, might be disadvantaged by HireVue’s assessment and/or require accommodation to demonstrate the competencies that it measures.
- The Bad: Concerning Disclosures
- Some disclosed “competencies” that HireVue assesses pose a risk of discrimination to disabled workers
- The few competencies the statement mentions may actually lead to discrimination. Among the broad competencies the statement does mention are “interpersonal skills,” “empathy,” “influence,” and “personality traits.” As discussed in CDT’s December 2020 report on disability discrimination in automated hiring tools, assessments that measure personality traits may screen out candidates with depression or anxiety, and game-based assessments may disadvantage workers with a wide variety of disabilities, including ADHD, autism, and visual impairments.
- While the explainability document describes efforts to detect and minimize adverse impacts based on race and sex, it does not indicate that HireVue paid comparable attention to how its assessments could unfairly disadvantage disabled workers. The “accessibility” section of the statement does not suggest that it took any steps to actually design its assessments in a manner that ensures they can measure the competencies of disabled workers. Instead, HireVue merely provides a general sense of what the tests involve and leaves it to applicants and employers to determine whether a particular candidate requires accommodations.
- The only HireVue accommodation mentioned in the explainability statement is that candidates can request additional time to complete assessments. Given that HireVue delivers its assessments through its integrated “end-to-end hiring platform,” it is not clear if or how employers could offer any other accommodations to disabled workers, aside from having them assessed by completely different means.
- HireVue assesses only generic candidate qualities, and makes no effort to alter or supplement its assessments to match the essential functions of any actual jobs
- The “competencies” that HireVue claims to measure through its assessments are not moored to the actual responsibilities and functions of specific jobs, and HireVue does not allow employers to incorporate more job-specific content into its assessments. All the examples of competencies that HireVue purports to assess – such as “empathy,” “influence,” “personality,” “attention,” “communication,” and “problem-solving” – are highly abstract qualities, not specific knowledge, skills, abilities, or other characteristics that are tailored to particular jobs. HireVue links these competencies to somewhat more specific “behaviors” – for example, an appendix showing the rating skill for the “Communication” competency includes “Shares Information” and “Engages Others” as key behaviors associated with that competency. But even these behaviors are highly generic, not accounting for the ways different behaviors and skills manifest themselves in different jobs, settings, or sectors.
- HireVue’s claim is that it can build – in its own words – a “single comprehensive assessment of each candidate” or a test of “overall job aptitude” simply by asking a small number of questions that supposedly shed light on a comparably small number of abstract candidate qualities. In reality, no job’s essential functions can be reduced to a few items from an assortment of generic competencies.
- Moreover, competencies themselves manifest differently in different jobs – and even different workers. Problem-solving in engineering is quite different from problem-solving in social work; some people may convey empathy through the substance of their words, while others rely more on body language and tone of voice. While HireVue allows employers to choose competencies and adjust how they are weighted relative to each other, it is not clear whether and how it takes such variation into account. HireVue’s explainability statement emphasizes that its competency models are trained solely on HireVue’s own data drawn from a series of “Rater Studies” it conducted using interview transcripts.
- The explainability statement provides no information regarding what sorts of industries and positions the interview transcripts came from, but the description of the Rater Studies states that more than 30,000 video interviews were aggregated together, suggesting that the resulting models are not tailored to specific employers, occupations, or industries. This generic training data, combined with the similarly generic competencies, means that HireVue’s assessments measure candidates against an abstraction of an abstraction. That approach cannot, contrary to HireVue’s claims, provide employers with even a glimpse of a candidate’s “overall job aptitude” for any actual, specific job.
- Conclusion
- HireVue’s explainability statement, while showing some promise, is ultimately disappointing, particularly because transparency is such a basic and fundamental aspect of developing artificial intelligence systems. It’s clear that policy action is needed so that workers, enforcement agencies, and other stakeholders can access the information needed to understand the tools that make or influence employment decisions and hold vendors and employers alike accountable for ineffective, inaccessible, or discriminatory tools.
- [1] It is not clear from the explainability statement whether the same worker population was used to develop/test its game-based assessments.
- Share
- CDT works to strengthen individual rights and freedoms by defining, promoting, and influencing technology policy and the architecture of the internet that impacts our daily lives.
- The content throughout this website that originates with CDT can be freely copied and used as long as you make no substantive changes and clearly give us credit. More on CDT's content reuse policy is available here.
- Copyright © 2023 by Center for Democracy and Technology. Created by nclud.

- Estee Lauder employee performance assessments
- Retorio talent personality assessments
- Page info Type: SystemPublished: January 2023
