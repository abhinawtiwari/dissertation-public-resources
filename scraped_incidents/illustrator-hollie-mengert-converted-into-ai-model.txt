- Occurred: November 2022
- Can you improve this page?Share your insights with us
- Illustrator Hollie Mengert has discovered that her online art portfolio has been used to train Illustration Diffusion, a text-to-image model created by Canada-based Nigerian engineering student Ogbogu Kalu, without her permission.
- Per Andy Biao at Waxy, Kalu used 32 of her illustrations to fine-tune Stable Diffusion to recreate Hollie Mengert's style using Google's DreamBooth, a technique for introducing new subjects to a pretrained text-to-image diffusion model. Kalu then released the model on Hugging Face under an open license for anyone to use.
- The act triggered a heated debate about the ethics and legality of using artwork developed and owned by other people or organisations without their consent. Dreambooth was also criticised for the ease with which it can be used to generate offensive or malicious images, and that it can be re-purposed given its open source nature.
- Mengert pointed out to Andy Baio that she was in no position to grant Kalu permission to train his model on her work even if she wanted to as her work involves characters owned by corporations like Disney or Penguin Random House.
- On the other hand, Kalu says he thinks his act is legal and 'likely to be determined fair use in court'. He reckons it is also inevitable. 'The technology is here, like we've seen countless times throughout history,' he argued.
- According to Kalu, 'there is no argument based on morality. That's just an arbitrary line drawn on the sand. I don't really care if you think this is right or wrong.'
- Operator: Ogbogu Kalu Developer: Ogbogu Kalu; Alphabet/Google
- Country: USA
- Sector: Media/entertainment/sports/arts
- Purpose: Fine-tune text-to-image models
- Technology: Text-to-image; Machine learning Issue: Copyright; Ethics; Mis/disinformation; Safety
- Transparency: Governance
- Google Research Dreambooth website
- Ruiz N., Li Y., Jampani V, Pritch Y., Rubinstein M., Aberman K. (2022). DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation
- DreamBooth Wikipedia profile
- Ogbogu Kalu Illustration Diffusion model
- Hollie Mangert training images
URL: https://waxy.org/2022/11/invasive-diffusion-how-one-unwilling-illustrator-found-herself-turned-into-an-ai-model/
- Last weekend, Hollie Mengert woke up to an email pointing her to a Reddit thread, the first of several messages from friends and fans, informing the Los Angeles-based illustrator and character designer that she was now an AI model.
- The day before, a Redditor named MysteryInc152 posted on the Stable Diffusion subreddit, “2D illustration Styles are scarce on Stable Diffusion, so I created a DreamBooth model inspired by Hollie Mengert’s work.”
- Using 32 of her illustrations, MysteryInc152 fine-tuned Stable Diffusion to recreate Hollie Mengert’s style. He then released the checkpoint under an open license for anyone to use. The model uses her name as the identifier for prompts: “illustration of a princess in the forest, holliemengert artstyle,” for example.
- The post sparked a debate in the comments about the ethics of fine-tuning an AI on the work of a specific living artist, even as new fine-tuned models are posted daily. The most-upvoted comment asked, “Whether it’s legal or not, how do you think this artist feels now that thousands of people can now copy her style of works almost exactly?”
- Great question! How did Hollie Mengert feel about her art being used in this way, and what did MysteryInc152 think about the explosive reaction to it? I spoke to both of them to find out — but first, I wanted to understand more about how DreamBooth is changing generative image AI.
- Since its release in late August, I’ve written about the explosive creativity and complex ethical and legal debates unleashed by the open-source release of Stable Diffusion, explored the billions of images it was trained on, and talked about the data laundering that shields corporations like Stability AI from accountability.
- By now, we’ve all heard stories of artists who have unwillingly found their work used to train generative AI models, the frustration of being turned into a popular prompt for people to mimic you, or how Stable Diffusion was being used to generate pornographic images of celebrities.
- But since its release, Stable Diffusion could really only depict the artists, celebrities, and other notable people who were popular enough to be well-represented in the model training data. Simply put, a diffusion model can’t generate images with subjects and styles that it hasn’t seen very much.
- When Stable Diffusion was first released, I tried to generate images of myself, but even though there are a bunch of photos of me online, there weren’t enough for the model to understand what I looked like.
- That’s true of even some famous actors and characters: while it can make a spot-on Mickey Mouse or Charlize Theron, it really struggles with Garfield and Danny DeVito. It knows that Garfield’s an orange cartoon cat and Danny DeVito’s general features and body shape, but not well enough to recognizably render either of them.
- On August 26, Google AI announced DreamBooth, a technique for introducing new subjects to a pretrained text-to-image diffusion model, training it with as little 3-5 images of a person, object, or style.
- Today, along with my collaborators at @GoogleAI, we announce DreamBooth! It allows a user to generate a subject of choice (pet, object, etc.) in myriad contexts and with text-guided semantic variations! The options are endless. (Thread 👇)webpage: https://t.co/EDpIyalqiK1/N pic.twitter.com/FhHFAMtLwS
- Google’s researchers didn’t release any code, citing the potential “societal impact” risk that “malicious parties might try to use such images to mislead viewers.”
- Nonetheless, 11 days later, an AWS AI engineer released the first public implementation of DreamBooth using Stable Diffusion, open-source and available to everyone. Since then, there have been several dramatic optimizations in speed, usability, and memory requirements, making it extremely accessible to fine-tune it on multiple subjects quickly and easily.
- Yesterday, I used a simple YouTube tutorial and a popular Google Colab notebook to fine-tune Stable Diffusion on 30 cropped 512×512 photos of me. The entire process, start to finish, took about 20 minutes and cost me about $0.40. (You can do it for free but it takes 2-3 times as long, so I paid for a faster Colab Pro GPU.)
- The result felt like I opened a door to the multiverse, like remaking that scene from Everything Everywhere All at Once, but with me instead of Michelle Yeoh.
- Frankly, it was shocking how little effort it took, how cheap it was, and how immediately fun the results were to play with. Unsurprisingly, a bunch of startups have popped up to make it even easier to DreamBooth yourself, including Astria, Avatar AI, and ProfilePicture.ai.
- But, of course, there’s nothing stopping you from using DreamBooth on someone, or something, else.
- I talked to Hollie Mengert about her experience last week. “My initial reaction was that it felt invasive that my name was on this tool, I didn’t know anything about it and wasn’t asked about it,” she said. “If I had been asked if they could do this, I wouldn’t have said yes.”
- She couldn’t have granted permission to use all the images, even if she wanted to. “I noticed a lot of images that were fed to the AI were things that I did for clients like Disney and Penguin Random House. They paid me to make those images for them and they now own those images. I never post those images without their permission, and nobody else should be able to use them without their permission either. So even if he had asked me and said, can I use these? I couldn’t have told him yes to those.”
- She had concerns that the fine-tuned model was associated with her name, in part because it didn’t really represent what makes her work unique.
- “What I pride myself on as an artist are authentic expressions, appealing design, and relatable characters. And I feel like that is something that I see AI, in general, struggle with most of all,” Hollie said.
- “I feel like AI can kind of mimic brush textures and rendering, and pick up on some colors and shapes, but that’s not necessarily what makes you really hireable as an illustrator or designer. If you think about it, the rendering, brushstrokes, and colors are the most surface-level area of art. I think what people will ultimately connect to in art is a lovable, relatable character. And I’m seeing AI struggling with that.”
- “As far as the characters, I didn’t see myself in it. I didn’t personally see the AI making decisions that that I would make, so I did feel distance from the results. Some of that frustrated me because it feels like it isn’t actually mimicking my style, and yet my name is still part of the tool.”
- She wondered if the model’s creator simply didn’t think of her as a person. “I kind of feel like when they created the tool, they were thinking of me as more of a brand or something, rather than a person who worked on their art and tried to hone things, and that certain things that I illustrate are a reflection of my life and experiences that I’ve had. Because I don’t think if a person was thinking about it that way that they would have done it. I think it’s much easier to just convince yourself that you’re training it to be like an art style, but there’s like a person behind that art style.”
- “For me, personally, it feels like someone’s taking work that I’ve done, you know, things that I’ve learned — I’ve been a working artist since I graduated art school in 2011 — and is using it to create art that that I didn’t consent to and didn’t give permission for,” she said. “I think the biggest thing for me is just that my name is attached to it. Because it’s one thing to be like, this is a stylized image creator. Then if people make something weird with it, something that doesn’t look like me, then I have some distance from it. But to have my name on it is ultimately very uncomfortable and invasive for me.”
- I reached out to MysteryInc152 on Reddit to see if they’d be willing to talk about their work, and we set up a call.
- MysteryInc152 is Ogbogu Kalu, a Nigerian mechanical engineering student in New Brunswick, Canada. Ogbogu is a fan of fantasy novels and football, comics and animation, and now, generative AI.
- His initial hope was to make a series of comic books, but knew that doing it on his own would take years, even if he had the writing and drawing skills. When he first discovered Midjourney, he got excited and realized that it could work well for his project, and then Stable Diffusion dropped.
- Unlike Midjourney, Stable Diffusion was entirely free, open-source, and supported powerful creative tools like img2img, inpainting, and outpainting. It was nearly perfect, but achieving a consistent 2D comic book style was still a struggle. He first tried hypernetwork style training, without much success, but DreamBooth finally gave him the results he was looking for.
- Before publishing his model, Ogbogu wasn’t familiar with Hollie Mengert’s work at all. He was helping another Stable Diffusion user on Reddit who was struggling to fine-tune a model on Hollie’s work and getting lackluster results. He refined the image training set, got to work, and published the results the following day. He told me the training process took about 2.5 hours on a GPU at Vast.ai, and cost less than $2.
- Reading the Reddit thread, his stance on the ethics seemed to border on fatalism: the technology is inevitable, everyone using it is equally culpable, and any moral line is completely arbitrary. In the Reddit thread, he debated with those pointing out a difference between using Stable Diffusion as-is and fine-tuning an AI on a single living artist:
- There is no argument based on morality. That’s just an arbitrary line drawn on the sand. I don’t really care if you think this is right or wrong. You either use Stable Diffusion and contribute to the destruction of the current industry or you don’t. People who think they can use [Stable Diffusion] but are the ‘good guys’ because of some funny imaginary line they’ve drawn are deceiving themselves. There is no functional difference.
- On our call, I asked him what he thought about the debate. His take was very practical: he thinks it’s legal to train and use, likely to be determined fair use in court, and you can’t copyright a style. Even though you can recreate subjects and styles with high fidelity, the original images themselves aren’t stored in the Stable Diffusion model, with over 100 terabytes of images used to create a tiny 4 GB model. He also thinks it’s inevitable: Adobe is adding generative AI tools to Photoshop, Microsoft is adding an image generator to their design suite. “The technology is here, like we’ve seen countless times throughout history.”
- Toward the end of our conversation, I asked, “If it’s fair use, it doesn’t really matter in the eye of the law what the artist thinks. But do you think, having done this yourself and released a model, if they don’t find flattering, should the artist have any say in how their work is used?”
- He paused for a few seconds. “Yeah, that’s… that’s a different… I guess it all depends. This case is rather different in the sense that it directly uses the work of the artists themselves to replace them.” Ultimately, he thinks many of the objections to it are a misunderstanding of how it works: it’s not a form of collage, it’s creating new images and clearly transformative, more like “trying to recall a vivid memory from your past.”
- “I personally think it’s transformative,” he concluded. “If it is, then I guess artists won’t really have a say in how these models get written or not.”
- As I was playing around with the model trained on myself, I started thinking about how cheap and easy it was to make. In the short term, we’re going to see fine-tuned for anything you can imagine: there are over 700 models in the Concepts Library on HuggingFace so far, and trending in the last week alone on Reddit, models based on classic Disney animated films, modern Disney animated films, Tron: Legacy, Cyberpunk: Edgerunners, K-pop singers, and Kurzgesagt videos.
- Aside from the IP issues, it’s absolutely going to be used by bad actors: models fine-tuned on images of exes, co-workers, and, of course, popular targets of online harassment campaigns. Combining those with any of the emerging NSFW models trained on large corpuses of porn is a disturbing inevitability.
- DreamBooth, like most generative AI, has incredible creative potential, as well as incredible potential for harm. Missing in most of these conversations is any discussion of consent.
- The day after we spoke, Ogbogu Kalu reached out to me through Reddit to see how things went with Hollie. I said she wasn’t happy about it, that it felt invasive and she had concerns about it being associated with her name. If asked for permission, she would have said no, but she also didn’t own the rights to several of the images and couldn’t have given permission even if she wanted to.
- “I figured. That’s fair enough,” he responded. “I did think about using her name as a token or not, but I figured since it was a single artist, that would be best. Didn’t want it to seem like I was training on an artist and obscuring their influence, if that makes sense. Can’t change that now unfortunately but I can make it clear she’s not involved.”
- Two minutes later, he renamed the Huggingface model from hollie-mengert-artstyle to the more generic Illustration-Diffusion, and added a line to the README, “Hollie is not affiliated with this.”
- Two days later, he released a new model trained on 40 images by concept and comic book artist James Daly III.
- The “It’s not a collage” argument:
- “”[…]He thinks many of the objections to it are a misunderstanding of how it works: it’s not a form of collage, it’s creating new images and clearly transformative, more like “trying to recall a vivid memory from your past.”“
- I am so, so very tired of these cheap and lazy “arguments” that try to build their alleged “check-mate” moment on the idea of “people just don’t get AI. It’s not a collage. The data is not stored. It’s parameters.”, misleading people who are less tech-savvy into believing them whereas this point of theirs does nothing to actually support their position.
- The point is: It doesn’t matter.
- It does not matter that AI does not patch actual pixels from actual images together into a collage to determine whether using data by other people as training and validation data to build an AI model is legal, at least from a commercial perspective.
- It’s true, AI does not store the images. It analyzes them, derives patterns, rules, relationships etc., and stores its analysis results as abstract mathematical parameters.
- It does not change the fact that you would not be able to build your tool without the data.
- It is completely irrelevant that the data is discarded.
- It is still a crucial component in your process of creating your software. Without it, it would literally not be possible.
- This means, you are dependent on the data to build your tool.
- But if you depend on the property of other people then you need to get their permission to use it for commercial purposes and reimburse them if they demand it.
- It’s not “transformative”. It is not “reliving a vivid memory from the past”. This has to be the most misleading and incorrect “assessments” I have read on this issue.
- Data is literally the heart of AI. Good data is one of the biggest treasures for AI creation. It is immensely valuable. That is why big tech companies are obsessed with collecting it and pay millions to acquire it. Sometimes they even buy an entire vacuum company (Amazon buying Roomba) just to get access to its treasure trove of data.
- It bears absolutely zero logic, then, that in the case of art AI the very people who create these valuable components should neither have any rights to their property, any say in how their property is used and any rights to compensation.
- (“If it is [transformative], then I guess artists won’t really have a say in how these models get written or not.”)
- (Based on his comments I do not take it as him only wanting to use AI art programs, that use copyrighted data without permission and compensation, for non-commercial purposes. He very explicitly talks about tools like Stable Diffusion destroying the industry, i.e., a commercial entity. He also said he wanted to make a comic series, so unless it’s a complete non-profit comic without any intention to ever generate any commercial revenue, he sounds like being in favor of people creating content that can be commercialized with such art AI tools.
- Additionally, his emphasis on the “transformative” nature of these tools makes limited sense if it is exclusively applied to non-commercial content, as the debate around transformative work is generally tied to commercial copyright and whether something is infringing it or transformative enough to be legally in the clear.
Also, some programs like Midjourney offer payment plans, directly benefitting commercially from the data they used to train their tool with, and people defend these tools with the very same arguments.)
- Fair Use
- The fair use argument is also unconvincing in my eyes. At least where I live fair use has little to do with whether actual pictures are used or not. You can already use actual images – pixels – noncommercially under fair use in certain situations. So, saying, it’s fair use because the images are not actually stored makes no sense because it completely misses the nature of what fair use constitutes.
- Images not being stored and something being fair use are not actually related.
- The deciding factor should be again whether the outcome of whatever you do with that data is commercial or not.
- “You can’t copy style.”
- Lastly, the “you can’t copy style” argument is another appeal to a completely different topic that does not actually describe the problem at hand. It’s a distraction.
- First, what he did is not just “copying a style” or “being inspired” in the way a human artist is. He literally took the property (actual image data) of other people to build a tool, a software, that needs said actual data as a component to be created.
- He can’t use mere abstract concepts and ideas represented as thoughts in his brain to train and validate – to build – his model.
- Again, he needs actual, “tangible” property. He is dependent on other people’s output, work results, in his own model building process.
- (Also, if he wants to create a depiction in the style of Mickey Mouse (not Mickey Mouse himself) he will have to feed his AI model actual images of Mickey Mouse, image material created and licensed by Disney. He is still using “tangible” property (actual data) that is copyright protected in the creation of his model to imitate the Mickey Mouse style.)
- I know, uncritical art AI supporters love to say “it’s inspiration!” but unfortunately humans and AI are still different entities. Humans are not AI models that are built by another agent and AI models are not sentient, self-contained minds.
- I am aware that the argument of sentience is often dismissed with a scoff. Perhaps because it’s so not technical, rather philosophical and so difficult to define neatly unlike technology and software. Despite naming certain data processing approaches “neural networks” in machine learning, the processes in such neural networks and in the sentient mind(!) of a person are not the same.
- (See how I say “mind” instead of “brain”. Although I would also argue that currently the actual brain and artificial neural networks are not (yet) to be regarded as interchangeable either.)
- Sentient beings “process” data first and foremost for the self-contained purpose of existing. Unfortunately, we need the input of our eyes and ears (and touch and smell etc.) to navigate this world, to survive.
- We cannot shut down our perceptive (“collecting data” with our eyes, ears etc.) and higher-order cognitive processes (“analyzing that data” to classify it as usable information and to derive meaningful conclusions from it) because we are accidentally perceiving (“collecting the data” of) copyrighted material. It is literally not possible for us.
- It IS possible for an AI tool because it is just that, a tool. It is used in specific situations and only fulfills its function when employed by a subject.
- It is therefore not dependent on constant data collection and processing just to exist as a self-contained sentient being without any other purpose.
- We are subjects with no purpose. And as soon as we use our perception and cognition (“data collection and data processing”) for a purpose that could infringe on the right (e.g., property) of other people we DO get problems. If my inspiration comes too close to another person’s original it is very well possible for me to face repercussions and reprimand. People who cite inspiration act as if we could just use “inspiration” as a constant free-out-of-jail card prior to AI when that’s not the case.
- Second, “You can’t copy style” itself is already questionable. I am pretty sure if I were to create commercial art that is based on one very distinct IPs I’m not sure if I wouldn’t get legal problems. E.g., if I created something that looks like it could be 1:1 from Kim Possible or Spongebob Squarepants, or which copies Mickey Mouse’s style 1:1 or if I were to draw creatures whose style is indistinguishable from Pokemon – all franchises that don’t just have a “common” style like Superhero Comic style, Disney Princess style etc. – but whose styles are perceived as an integral part of a franchise itself. I can’t speak with certainty of course but I would not be surprised if a big corporation could actually argue successfully that a very unique style was part of a distinct IP and therefore copyright protected.
- (I’m not saying I’d agree with this particular decision but it doesn’t seem inconceivable to me.)
- In the end, artists are told that they are “unreasonably afraid” or “stuck in the past” because they don’t want to be robbed of their own data for others to build and profit off of tools that are advancing so fast, that it is not unreasonable to assume they will soon be able to replace the very artists whose works they are trained with in certain domains.
- Demanding fairness is not being stuck in the past or being anti-tech. I’m sure most artists would feel very differently about art AI if it had been created under different circumstances: without living in a society that cares very little about art and artists and that is hell-bent on optimizing and automating everything economically with no regard for its impact on human.
- This is a very long comment but at its core your argument is that because the original property is used as training input the artist must grant permission and be paid.
- If you, as a human, look at artwork and then create art inspired by it, is that not the same?
- Information flows from the artists brain onto their canvas into your eyes into your brain onto your canvas.
- One can argue that machine learning models are fundamentally different from humans and therefore different rules apply, but the information flow is comparable.
- Rational arguments can be made for your position and against it, it is not as clear cut as you argue.
- @Sebastian
- “If you, as a human, look at artwork and then create art inspired by it, is that not the same? One can argue that machine learning models are fundamentally different from humans and therefore different rules apply, but the information flow is comparable.”
- I have already laid out my argument why it is not the same:
- “First, what he did is not just “copying a style” or “being inspired” in the way a human artist is. He literally took the !property! (actual image data) of other people to !build! a tool, a software, that needs said !actual data! as a !component! to be created.
- He can’t use mere !abstract! concepts and ideas represented as thoughts in his brain to train and validate – to !build! – his model.
- Again, he needs actual, “tangible” property. He is dependent on other people’s output, work results, in his own model building process.
- (If he wants to create a depiction in the !style! of Mickey Mouse (not Mickey Mouse himself) he will have to feed his AI model actual images of Mickey Mouse, image material created and licensed by Disney. He is still using “tangible” property (actual data) that is copyright protected in the creation of his model to imitate the Mickey Mouse style.)”
- And:
- “Sentient beings “process” data first and foremost for the !self-contained! purpose of existing. Unfortunately, we need the input of our eyes and ears (and touch and smell etc.) to navigate this world, to !survive!.
- We !cannot! shut down our perceptive (“collecting data” with our eyes, ears etc.) and higher-order cognitive processes (“analyzing that data” to classify it as usable information and to derive meaningful conclusions from it) because we are accidentally perceiving (“collecting the data” of) copyrighted material. It is literally not possible for us.
- It !IS! possible for an AI tool because it is just that, a !tool!. It is !used! in specific situations and only fulfills its function when employed by a subject.
- It is therefore not dependent on constant data collection and processing just to !exist! as a self-contained sentient being !without any other purpose!.
- We are subjects with no purpose. But !as soon as we use our perception and cognition! (“data collection and data processing”) !for a purpose! that could infringe on the right (e.g., property) of other people we DO get problems. If my inspiration comes too close to another person’s original it is very well possible for me to face repercussions and reprimand. People who cite inspiration act as if we could just use “inspiration” as a constant free-out-of-jail card prior to AI when that’s not the case.”
- Sebastian’s reply is a classic one, but the Adpah already pointed out that human minds and AI are not the same. Training data is as fundamental to these programs as the code which reads the training data. People are willing to respect copyright (and pay for) that code — why aren’t they willing to do the same for the training data itself?
- @Sebastian
- “This is a very long comment but at its core your argument is that because the original property is used as training input the artist must grant permission and be paid.”
- Yes, just as you would need to in any other scenario where an image is used. Image data is not an abstract concept. It is a concrete entity or output that somebody has ownership over.
- And in case you want to argue “the image data is discarded” point again as to why this situation should be judged differently, I have already addressed that in the original comment, so I’m not going to repeat it again.
- Data is one of the most important components of AI. AI stands and falls with it. AI is a tool, and in a commercial context a product. That is why data used to build it should be treated as components of product development (commercially speaking) and not as a misplaced analogy of sentient human inspiration (which as mentioned above also has its boundaries and is not a free get-out-of-jail card for any “inspired” human creation).
- To argue that the creators of this crucial component of AI development should not have any say on it, any claim to their own work and output, in the context of commercially applied (art) AI model building – that everybody in the development chain can profit and decide – except those who provide the actual data – is not rational, it’s just vile.
- People will like to draw “like Disney” and if the original data is “behind a paywall”, then they pay someone to draw 32 or 64 Disney “like” pictures. They can pool their resources to do so; there will be a cottage industry of cheap global artists that are able to clone any style. Then the AI is fed the alternative images. Those will never be published, but the AI data is. Since style isn’t copyrightable, this ends exactly here.
- There are already music generation AIs that are fed the whole Mozart catalog and they create wild music that sometimes sounds like Mozart. You can correctly license the note sheet for dollars since you will never recreate the songs. But the result might sound like Michael Jackson wrote the song. This is just the beginning. Logically nothing prevents us creating whole movies this way, with actors that don’t exist. Just a matter of computer power.
- @Sebastian
- Another point I forgot to mention is that even on a purely functional level human inspiration/”data processing” and AI data processing are not the same.
- Our brain, our “neural network”, works differently from AI. Hence, I’ll argue that your point that they are essentially the same (“artist sees image, image is processed in brain, processed information flows into the hand”) is incorrect.
- Human perception and information processing is fundamentally holistic. We are not able to process something “information unit by information unit”: pixel by pixel. Bit by bit. Etc. Unless you are one of the very very few exceptional savants on this planet you cannot process that much information and consciously retrieve it.
- We can only ever soak up a holistic impression at best. AI on the other hand is able to process images pixel by pixel. Hence it is capable of deriving patterns, rules and relationships on a pixel-by-pixel basis. Something (most) humans would never be able to do.
AI models are reality “scanners”. We do not have the ability to “scan” data like that.
- Another aspect that is often ignored is the influence of emotion and attention on human perception and memory formation. As these factors thoroughly moderate said cognitive processes our “data collection” (perception), “data storing/representation” (memories) and “data processing” work very differently compared to AI.
- Attention I probably don’t have to explain much but depending on the level of attention you dedicate to a given piece of information the better or worse – the more complete or incomplete – you will process and remember it. This fundamentally determines how well you are able to form mental representations of external information, incl. the works of other artists. We are a lot more prone to faulty, incomplete and incorrect representation of other artists’ work than AI. This will influence how we process their data (what information we have to work with) and the quality of processing (how good our transformations and derivatives will be).
- AI does not have that problem. AI is not stifled by lacking attention. Its performance does not fluctuate based on varying attention levels, a factor that in humans can lead to performance differences within the same hour.
- Emotions are lot more interesting though. First, emotional states can color your interpretation of the data you perceive and memorize, giving you a unique version of reality that differs a) from the version of reality of another person AND b) your own version of reality if you were in a different mood.
- AI’s perception and processing of data is not altered by emotional responses. As long as the labels that are assigned to the data are stable its “perception” of reality (input data) will always remain the same (regardless of whether this “reality” – the labels – are truthful or accurate). And for the AI to have a different perception of the same data, external agents have to change the labels. AI doesn’t change its perception of the data itself based on any personal feelings.
- Emotions can also influence memory. If you form a memory in a certain emotional state you are more likely to remember it (“recall the data”) if you are in the same state again. On the flip side, if you are not in that state your memory performance might decrease.
- However, memory is very important to process data and create transformative content: You need to activate memories first in order to analyze and process them to derive any transformed and evolved creative and logical conclusions from it.
- AI’s “memory” is not influenced in this way. Its recall performance is not moderated by emotions. At this point in time, people are not even sure whether an AI model can forget at all (they might have to be retrained from scratch if you don’t want certain data to be included).
- The most extreme (but also the most illustrative example) of emotions influencing your memory are traumatic experiences, often leading to very fragmented, anachronistic, highly detailed but completely disordered memories. As a result, people can often not actually remember what happened but can at the same time relive very intense experiences of isolated details.
At the same time, there is emerging evidence that victims of trauma may actually form less trauma-typical memories if they are immediately distracted with a cognitive task afterwards –> the emotional response is altered, hence the memory formation process is altered. (I remember a study where people were asked to play Tetris immediately after a traumatic experience and apparently it provided first evidence for better regulated formation of memories.)
- There is also very thorough psychological research on information representation in our active working memory. Depending on the input format (numbers, sounds, images, words) you can only keep a limited number of pieces of information in your active memory for short-term recall. E.g., most people can remember 3 – 5 short number strings, but considerably more words. The number of recallable images is different yet again. AI does not function like this at all.
- You will most likely forget everything beyond that. Also, your active recall is severely modulated by different factors. For example, the position of the information. Information that is given in the beginning and the end of an information string is generally remembered more easily than information in the middle of that string.
- And interestingly enough, once the information you have kept in your active working memory served its purpose you will often not be able to recall it (with the same precision) as you did before. For example, a waitress or waiter that was able to recite your order perfectly while still serving you will most likely not be able to do so an hour later after your order has been completed and is not relevant anymore.
- I probably don’t need to mention yet again that AI does not have this limitation. AI does not immediately discard its memories once they are no longer useful. If activated again (e.g., by another prompt) it will be able to recall it.
- And that is just our active working memory. Our long-term memory has its very own set of complexities and forgetting plays an incredibly important part in this.
- The thing is: software (in general) does not have this problem. If software records information it does not automatically forget due to memory erosion as it happens in humans. It has to receive the order to delete the information. I am not saying forgetting is inherently bad. In fact, psychologists argue that it may fulfil a necessary function for our sentient minds. But when it comes to pure data handling, it’s a weakness software in general, and thus AI in particular, does not have.
- Another important aspect is “processing power”. You do not only need perception and memory to analyze and transform information into creative derivatives. Unlike AI we cannot rely on powerful GPUs that will grow even more potent in the future.
- Our ability to concentrate but also our ability how much information we are able to hold in our active mind at the same time, how we can represent the data (some people are better at visualizing than others, some people purely think in words etc.) and how fast we can analyze the data (derive conclusions, make connections) determines the quality of our output.
If we are under stress, if we have attention deficit disorders, if we are intoxicated, upset, if we are confronted with multiple stimuli at the same time: all of this can severely impact our processing performance. It can lead to us forgetting needed pieces of activated information more quickly, drawing false (or sub-par/uncreative) conclusions, being slower and less efficient or biased.
- Sometimes we have literal art block or need to wait till inspiration strikes.
- AI’s performance does not suffer in a comparable and unpredictable way. AI does not have art block. AI does not need to wait for inspiration. AI is not stressed. AI is not biased due to internal factors. (As mentioned before it’s biased by external faulty inputs in the form of labels.)
- AI’s output performance is consistent. If more people access it or it has to handle bigger amounts of data, then it might have longer processing times. But it will not be as prone as humans are to incomplete and “faulty” data analysis, nor do any flaws in the final output of an AI stem from the same processes as for humans.
- In short: AI’s whole analysis process is completely different.
- Lastly, you are also ignoring the idea-output-gap. AI can create a perfect output of what it internally computed.
- Humans will often not be able to create a perfect replication of the visual representation in their mind. I might be able to think up the sickest character design or composition in my head but as soon as I start putting it on paper it will become inaccurate. An approximation at best. And sometimes, the image on the canvas strays away more and more compared to the (initial) image in the artist’s mind. It is a very flexible, dynamic process that is often unpredictable or at least hard to control. This effect is even bigger if you are not as skilled yet.
- The same applies for copying (e.g., still life, portrait): You might be able to perfectly represent the information you see in front of your eyes in your head but as soon as you try to convert this information into actual output executed by your hand inaccuracies will most likely occur.
- The eye-brain-hand relationship is not as simple as you described it in your initial comment. There are a lot of disruptive factors in the human process that AI does not have to grapple with.
- Adaph you need to learn to pare down your arguments! However, I agree with you. One thing I’d say, a rephrasing and expansion of your argument:
- AI doesn’t patch pixels from actual images together, it’s true. But I argue that storing the data of the images it learns from as sets of parameters instead of pixel data isn’t fundamentally different. Vector graphics don’t store images the same way as raster images but it’s still the image. The can basically recreate the images it can trained on if you prompted it correctly.
- Anyone saying it’s no different from people learning from other artists and imitating their styles is wrong. Humans learn by understanding the subject. AI don’t understand anything. When I am thinking about how to do something in another artist’s style, I am thinking about that artist’s style, how they do lines, or shading, or colours, or form. AI isn’t doing that, it’s just recreating parameters that it has lifted — not learned — by processing terabytes of data of other people’s work. I emphasize “not learned” because the AI doesn’t learn like humans do at all. There’s no understanding going on. The AI does not understand anything.
- Your argument is based on the idea that Intellectually Property is a real thing but it is not. There is no *property* involved no matter how often you say it. That doesn’t mean I disagree with the existence of copyright or patents, but terminology matters in these arguments.
- All of this argumentation is ultimately futile. The technology exists, and it will be used. The bell can’t be unrung.
- There’s a big gap in your understanding of how stable diffusion works. It is absolutely transformative in at least the literal sense, as what sd learns is the relationship between vectors and noise reduction patterns, and it’s not a 1:1 relationship even on a fine-tune (which is already heavily influenced by the base stable diffusion model).
Every additional image trained does not”add” a new feature to the existing checkpoint, it “only modifies the existing weights” of those vectors. That’s why distributive training isn’t practical, you can’t synchronize the whole model for every single image outside of a data center environment.
- Technically, there’s no law against accessing pictures posted publicly on clearnet. They’re posted for the explicit purpose of being accessible. If you want to add terms and conditions to accessing your works, that’s your choice and I respect that. If you don’t want your name used in a training set, I think that’s perfectly reasonable although I have no idea if you have any actual legal right to that. Right now, the law isn’t built to handle this situation, and that’s a problem.
- This is a legal gray zone, but it also seems to be an emotionally charged issue that a lot of artists are missing the point on. You’ve just been given an incredible new tool for your own work. Be it canvas and inks, tablet and illustrator or simple diffusion – your art isn’t worse because other people can make imitations.
- On a philosophical level, you’re right that if intellectual property law were a singular body of law that extended to all the limits of creative output, it wouldn’t be right to add copies of 30-plus works by an artist, and output more pictures in the style of that same artist.
- The legal reality, though, at least in the U.S., is that there is ironclad separation of copyright and trademark law. Copyright is only what you do with copies of the individual pictures, and if your output picture isn’t recognizable as a copy of any one input picture, fair use probably is going to cover, at least for non-commercial use, copying those original pictures from the Internet onto your hard drive.
- Your argument blurs the distinction between copyright and trademark law in a way that those bodies of law can’t currently accommodate. Trademark doesn’t protect artistic style. That’s why designer clothes that aren’t bespoke one-offs are emblazoned prominently with designers’ trademarks. And they have been since the late 70’s and early 80’s, when the courts ruled that trademarks didn’t protect the artistic style of clothes (or anything) the way that artists now want to protect pictures.
- At least in the U.S. Twenty years ago, Congress bridged the gap between patent and copyright law with the Digital Millennium Copyright Act. And big players in the visual arts probably will lobby Congress for a similar law to bridge this gap between copyright and trademark. But that sort of law, because of its corporate backers, will probably make it illegal to create an image in the style of a Disney or Pixar cartoon using copied frame grabs from works for hire. Or a J.J. Abrams or Spielberg movie. Or a Pokémon, like you suggest. But that law will probably still leave out artists like Hollie who work across different publishers almost like a freelancer.
- The problem with your logic is that if we apply it equally to all areas it would completely cripple the machine learning industry and all the products it produces. All modern machine learning methods rely on massive datasets collected from human generated content. Take spam detectors for example. They are trained on e-mail data created by people. Should spam detectors pay royalties to all the people whose e-mail they rely on?
- All seems very arbitrary. Going back to the original purpose of copyright, “To promote the Progress of Science and useful Arts, by securing for limited Times to Authors and Inventors the exclusive Right to their respective Writings and Discoveries” I tend to lean towards a morality that supports the freedom to reproduce, to mimic and copy.
- But surely, others will want to stop one another from such mimicry when there’s a protectionist racket to put keep dollars in one pocket and out of another.
- Seems somewhat arbitrary to say your tool can’t duplicate aspects of a drawing but a human is fine to do so.
- Why is it arbitrary to say a human can, but a software program can’t, and on an industrial scale? Why are you attributing human rights to a company’s software program, and where is there a precedent to do that? In fact, it’s the opposite of arbitrary if you think about it. It’s a clear and apparent dividing line.
- Exactly. Artists have imitated the styles of other artists for generations. Artists have no moral or ethical or legal right to prevent others from doing that. They have the right to protect their TRADEMARKS, they do not have the right to sue others for looking like them. To analogize to the realm of audio production, imagine if any artists could sue (successfully) any other artist for merely sounding like them.
- As an artist and a habitual pessimist it’s hard not to see the bleakness of these AIs. I know I’m being dramatic with the following opinion.
- All the creative jobs and hobbies of artist, musician, writer, etc, are going to be outsourced and given to these programs in the near future. What little space there is for making a living doing these things will shrink. Technology should be freeing us from tedium work, but instead it’s entrapping us from the freedom to create,
- …from YOUR perspective. For me, Stable Diffusion has given me the ability to transform my ideas into actual images without being prohibited by not having drawing talent, money, and time, and like me, the vast majority of humanity will feel that way.
- > Technology should be freeing us from tedium work, but instead it’s entrapping us from the freedom to create
- You’ll still have the freedom to create. If you enjoy the process of painting, writing, or creating music in its own right, then nothing can take that away from you. What technology will do, and I think this is now inevitable, is make commercial art a much smaller industry. Some kind of artist will still be required, whether it’s to create original art that gets fed into models, or to pick which of the generated images should be used.
- The relationship between art and commerce has been evolving since time immemorial, from roving bards to samples and streaming. Yes, anyone with any empathy will feel bad for artists who lose out, just as we feel bad for coal miners or factory workers.
- I’m a writer and programmer, and I’m not feeling massively secure about either industry. But I’ll probably continue to be active in both, even if I’m forced to change profession, because I enjoy those endeavours at a fundamental level. I’m pessimistic in the sense that I think this is inevitable, but I’m optimistic because I think it will create opportunities that we haven’t even thought about yet.
- The only way your work would be outsourced is if it’s so mechanical and reproducible that it hardly should qualify as art in the first place. Just because you work with images doesn’t make you an artist, just like merely working with sound doesn’t make you a musician. And how do you justify the fact that every single time technology as moved forward, artists who do not adapt are left in the dust studying their classic, outdated method. How many people are actually upset that hand-drawn animation is a niche thing now? Do we blame computer animation for it? Some do. Others recognize that art lives on, artists live on, those who work will keep working, and the sky will stay in the sky.
- This is likely to be ruled fair use, at least in the United States. It’s textbook transformativity—as the article points out, the images themselves aren’t being used; the data model is built off of them. In linguistics, copyrighted works are compiled en masse for use in corpora already and this is recognized as fair use.
- AI art is the logical end of “information wants to be free”. You can’t have your copyright and end it, too.
- I think we should be able to agree that by the laws on the book, AI creations based on style transfer are not copyright violations. The pixels do not have to be the same (i.e. Andy’s experience with Kind of Bloop) for it to be a copyright violation, but our laws are not set up for lifting the _idea_ of how to draw and drawing something else. Our laws on data storage also don’t call this a violation, right now — we’re not storing the raw image, and we can’t possible get the raw image back out of the system, and arguments based on ephemeral storage are kind of weak these days (remember when people used to think that just loading an MP3 into RAM to play it was a copyvio? yeah, we’re not really doing that anymore)
- So the argument here seems to be much more about morality/ethics, with a hint of a wish for legal protection. Let’s say this wish is granted and style transfer becomes illegal in approximately the same way that copyright makes straight-up copying illegal. What then? How does one enforce this styleright? Let’s say that some cases are clear — the artist in this example can clearly claim that their style was lifted wholesale and that the model is thus an illegal copying device. Note that even with copyright we do not attack the method but rather the infringing output (remember the Betamax case? is the model in this case like a VCR, or is it an actual tape? is the model _generation_ code like Betamax? these would all have to be litigated).
- But in this future, all you need is to train a model with _two_ artists who are similar in style. So if you go after output then a dual-artist model has plausible deniability: any particular output is not provable to be your artist’s style, or is provable to be a remix of styles. So really the only thing to do is to go after the model.
- And the code for making models is open source, widely available and used, and is a genie that we are quite unlikely to stuff back in the bottle. Remember DECSS? Besides, all it requires is for someone to train a model and not disclose the origin of content, and now you can’t even prove where it came from, even if it looks mighty familiar.
- All this is to say, I sympathize with artists whose creations are being disrupted by models that take their work and can create convincingly similar output in seconds. I think that trying to fight a battle of ethics and morals against something that is literally thousands of times faster and cheaper, armed with no plausible legal defenses, is a terrible way to go out. Getting society to pass laws against style transfer is a decades-long quest with no guarantee of success. Relying on existing art communities to ban AI art will just encourage people to migrate to more permissive spaces. I think that the only choices are to stop doing this kind of creative work, or live to learn with it (perhaps by attaching more value to authentication of your original work and selling that authentication to clients who care about it). I am very sorry if people have to exit this profession after being replaced by machines, but this is hardly the first profession in which this happens, and — actually — hardly the first time this has happened even in this profession.
- One more thing to add. One creative way to live with models like this is to actually own your own model. Imagine if an artist doesn’t just create a new drawing or illustration, but instead releases a model for generating new creations in a particular style. If you don’t even release the original work, it’s hard to train a model off of it. You can create an example or two, and I guess people could train a downstream model off of it, but yours will likely be better. Just like you can’t stop _people_ from copying your idea once they see your output, you probably can’t stop people from creating downstream models, but those who want the original will still come to you cause you control the source and can make it better.
- Generative AI products based upon training data which is trademarked, copyrighted or patented need their rights defined by the highest court in each applicable jurisdiction.
- In the USA, this issue should not be partisan, it is commercial without precedence in religion or culture.
- One can expect the winning side to offer the most net benefit to society.
- On one hand unrestrained generative AI is much less expensive, more flexible, and more available than custom human creations.
- On the other hand, with AI competition and no compensation for creating the required training data, the supply of new human creations will greatly diminish, hurting generative AI in the long run. Generative AI studios would have to hire teams of human creators of training data which is not made public – for ripping off.
- In the USA, let’s get this case before our Supreme Court for a decision.
- What Adpah said. The issue isn’t output images infringing on input images. It’s about making a software product out of stolen data. You stole specific images to produce a specific result, that you couldn’t get any other way. Or you could, but you chose not to.
- Most people get that just because you can find an image on google doesn’t mean you can stick it on a t-shirt and sell it. You will probably get away with it on small scale, but that does not legitimize it on any scale. It’s still profiting off of someone else’s work without compensation. And releasing it publicly means enabling it at all scales.
- The finer points of the technology don’t matter. You stole. In order to make something that robs the livelihood of those you stole from.
- What articles on AI generated art always seem to ignore are the huge limitations. At least currently, it takes a great many iterations to produce full images of living subjects that are mostly anatomically acceptable. Almost all of the images of people I have generated using the Stability AI model have gross defects, like 3 arms, seven fingers, hands that are melded together, or even limbs that float in the air unattached to anything. Try to make an image with more than one person close together, and the problems multiply. The AI it cannot adequately distinguish between one object and another. Humans riding horses become like mutilated centaurs; two people hugging become conjoined twins.
- You have to rely on luck to get usable images, and they may not actually be what you want. At least at present, your ability to describe in detail exactly what you need seems to be limited to about six or seven discrete details, with no real way to alter an image that is “almost right,” and that’s assuming you’re willing to generate A LOT of useless pics.
- These are not the kind of circumstances that will allow you to create commercial images for clients, who are usually very specific about their desires, and even more precise about the changes they want to any given result.
- If billions of images have been used to create these models, then I wonder just how much training would be required to build a model that could *consistently* generate an anatomically correct group of humans from head to foot in swim wear.
- Why bother, when *any* illustrator of reasonable skill can produce such an image in minutes?
- There are severe limitations to machine learning that make it a wholly inappropriate technology for commission based commercial work.
- Maybe that will change if we’re willing to fry the planet by using massive amounts of energy to continue training machines on quadrillions of images. Or if someone invents a better learning model than the current “throw the kitchen sink” in data at the machine and then see what comes out.
- But it’s fun to play with, and acceptable for casual art, or for art that an artist intends to repaint manually.
- As a disabled, self-taught artist there is so much opportunity for unique self-expression with AI art. I can train AI using three decades of my own images in different styles and media (realism, cartoon, pen, charcoal, digital, etc.). I can also generate reference “photographs” of subjects such as animals and buildings for free when getting out to take my own reference photographs or paying for copyrighted image is out of reach. It can help overcome art block with inspiring design detail or composition and generate color tests and variations in moments reducing trial and error. For a savvy artist, these tools can streamline workflow and help them spend more time on the expression, composition, and rendering final details. Rather than creating a single brush to generate a single flower in their personal style, they can generate a field of flowers approximately in their style and rework the composition and details to their personal taste.
- However, it is disturbing to see any artistic work used without permission. Ideally no artist’s work would be used without permission, even as a reference. This includes being data mining by AI without permission or compensation or statement of usage. Photographers and artists on Flickr, DeviantArt, and other sites allow and disallow reference, alteration, etc. based on artist preference. Some artists and photographers make their work freely available for fair use, commercial free, or commercial licensing. Some don’t. I ask photographers whether I can reference photographs unless an artist had expressly given permission. Some artists don’t. This is rude at the least, even when the work is transformative, to a breech of copyright permissions at the worst when the art heavily references the original image.
Unless laws change (which they may with giants like Disney and Pixar being heavily represented in AI prompts) I believe it will become more a matter of etiquette. Companies utilizing data to train AI bear the most responsibility, individuals training AI without permission would be looked on unfavorably. But the average AI user’s choice to use styles is more akin deciding to repost the original artist’s post instead of reposting the image with credit (one directs more traffic flow to the artist directly). Since AI is currently lacking in many ways, it can actually increase traffic, interest, visibility for various artists. Hopefully that translates into actual increase in hiring, commission, artblock sales, wtc. because artists can’t live on exposure alone. But that remains to be seen.
Collectors, art enthusiasts, and big corporate media conglomerates don’t want a randomized approximations of a similar product. They would rather have a signed original, support artists, or a hire for precise crafted images that fit their vision. Logo makers haven’t put Designers out of work. Quality, precision, and artist’s unique expression and touches are still valued. AI mimics that, but lacks a critical eye for understanding composition, emotion, or responding directly to fine-tuned requests.
- AI generated coloring and images can be incredibly helpful tools for disabled artists or artists as they age or otherwise facing decline in output. It can inspire creativity and overcome artblock, act reference materials, generate explorative variations, and materials for rework, “photo-bashing”, and redraw/repaint/paint overs saving artists time and struggle.
- Artists and AI both struggle with hands, but so far, only an artist can go in to find tune a hand and rework it to exact specifications with minimal randomization and maximum character, intent, and style.
- Much like using mirror projections to trace during the Renaissance or Ctrl-Z for digital art, it is a useful tool that helps save time and may improve quality, but it cannot replace basic understanding, specialized training, or years of practice and hard work. It won’t replace artists, but those who don’t utilize it may be at a disadvantage.
- Obviously plagiarism and copyright infringement should be avoided. However, within this new technology and gray areas, I hope artists and public that utilize AI will figure out best etiquette practice. Examples may be including multiple artists within AI prompts to not copy one style exclusively and respecting the wishes of living artists who do not wish to be included. Within AI art communities users are already complaining of Clone Images (exact regeneration of an AI result) which can be avoided by altering prompts, generation steps, collage composition, etc.
- It is a scary time for many, but all new technologies present possibilities for the best and worst of human nature. With plenty of doomer editorials already, I’ll point out that AI art may lead to a resurgence in interest in the arts the way electricity led to renewed public interest in science. Increased requests for tailored images, more visits to museums and exhibitions, even more interest in original works and artists with unique styles. It may help the public to understand how much work goes actually goes into original art. For example, while AI can quickly generate dozens of random images, it takes hard work, practice, and understanding of human anatomy to repaint or edit human hands into something less Cronenberg.
- Ultimately the core reason to become an artist, to practice and acquire skills, remains unchanged: to create the images we and others imagine and express them in a satisfactory way that makes us fulfilled. AI still hasn’t managed that level of intentionality. Yet.
- Wonderful post, and a very interesting comment section.
- I am building a program named Illuminarty that tries to detect AI-generated artworks, and the comments here show exactly why I decided to create this program.
- I am siding with Adpah here.
- The AI models, and thus the generated artworks, are dependent on the source images that are drawn by human artists. This dependency is a very valuable piece of information for potential consumers and many people will be willing to know this information, if available. To take an example from the classic argument of “being inspired”, I believe that many people will be interested in knowing if there was a source image that “inspired” the artwork.
- Currently, as the others have mentioned, we do not have the regulations necessary to force the AI artists to show this dependency. This is a market mismatch. I believe there could be two ways to solve this, penalizing those that do not respect the dependency, and providing incentives to those that disclose this info. An example of the former would be the recent lawsuit against GitHub Copilot. Sharing and respecting the dependency between the AI artworks themselves (e.g. copied prompts) could be an example of the latter.
- I think both methods are equally feasible and will try to support both movements through Illuminarty. Please let me know if anyone else is interested.
- Your email address will not be published. Required fields are marked *
- Comment *
- Name *
- Email *
- Website
- 
- 
- Δ

URL: https://boingboing.net/2022/11/03/illustrator-discovers-her-art-was-used-to-train-an-ai-art-generator.html
- Los Angeles artist and character designer Hollie Mengert found out that someone used her online portfolio to train an AI text-to-art model. The AI art looks a lot like Mengert's work, at least at a first glance. Andy Baio has a great article about it.
- Using 32 of her illustrations, MysteryInc152 fine-tuned Stable Diffusion to recreate Hollie Mengert's style. He then released the checkpoint under an open license for anyone to use. The model uses her name as the identifier for prompts: "illustration of a princess in the forest, holliemengert artstyle," for example.
- Andy asked Mengert how she felt about having her art used this way without her permission:
- "For me, personally, it feels like someone's taking work that I've done, you know, things that I've learned — I've been a working artist since I graduated art school in 2011 — and is using it to create art that that I didn't consent to and didn't give permission for," she said. "I think the biggest thing for me is just that my name is attached to it. Because it's one thing to be like, this is a stylized image creator. Then if people make something weird with it, something that doesn't look like me, then I have some distance from it. But to have my name on it is ultimately very uncomfortable and invasive for me."
- Andy also spoke to the person who made the model, Ogbogu Kalu, ("a young Nigerian engineer living and working in Halifax, Canada"):
- His take was very practical: he thinks it's legal to train and use, likely to be determined fair use in court, and you can't copyright a style. Even though you can recreate subjects and styles with high fidelity, the original images themselves aren't stored in the Stable Diffusion model, with over 100 terabytes of images used to create a tiny 4 GB model. He also thinks it's inevitable: Adobe is adding generative AI tools to Photoshop, Microsoft is adding an image generator to their design suite. "The technology is here, like we've seen countless times throughout history."
- Boy, AI really caught us with our pants down, huh? Despite the conversation surrounding the very real advent and proliferation of AI technology existing since the early 2010s, millions of…        READ THE REST
- In 1971, artist Harold Cohen (1928 – 2016) became a visiting scholar at Stanford's Artificial Intelligence Laboratory. There, he created a computer program called Aaron to answer the question, "What…        READ THE REST
- I wonder what kind of storylines The Office would have in a cyberpunk world?** Milan Jaram is the human pushing the Midjourney buttons behind the AI-generated art in this video.…        READ THE REST
- We thank our sponsor for making this content possible; it is not written by the editorial staff nor does it necessarily reflect its views. TL;DR: If you're looking to take your…        READ THE REST
- We thank our sponsor for making this content possible; it is not written by the editorial staff nor does it necessarily reflect its views. TL;DR: Draw, write, and sketch with ease…        READ THE REST
- We thank our sponsor for making this content possible; it is not written by the editorial staff nor does it necessarily reflect its views. TL;DR: Microsoft Office is the most essential software…        READ THE REST
- Read the rules you agree to by using this website in our Terms
                            of
                            Service.
- We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising
                        program
                        designed to provide a means for us to earn fees by linking to Amazon.com and affiliated
                        sites.
- Boing Boing uses cookies and analytics trackers, and is supported by advertising, merchandise
                        sales
                        and affiliate links. Read about what we do with the data we gather in our Privacy Policy.
- Who will be eaten first? Our forum rules are detailed in the Community Guidelines.
- Boing Boing is published under a Creative Commons
                            license except where otherwise noted.

URL: https://www.fastcompany.com/90848228/why-generative-ai-scares-artists-but-not-writers
- Please enable JS and disable any ad blocker

URL: https://www.axios.com/2022/11/05/artificial-intelligence-ai-art-author-ownership-rights

URL: https://www.ft.com/content/24f07261-f95d-4bb3-8aa4-3799f1f75e52
- Keep abreast of significant corporate, financial and political developments around the world.
				Stay informed and spot emerging risks and opportunities with independent global reporting, expert
				commentary and analysis you can trust.
- Then 65 € per month  New customers only  Cancel anytime during your trial
- During your trial you will have complete digital access to FT.com with everything in both of our Standard Digital and Premium Digital packages.
- Standard Digital includes access to a wealth of global news, analysis and expert opinion.  Premium Digital includes access to our premier business column, Lex, as well as 15 curated newsletters covering key business themes with original, in-depth reporting.  For a full comparison of Standard and Premium Digital, click here.
- Change the plan you will roll onto at any time during your trial by visiting the “Settings & Account” section.
- If you do nothing, you will be auto-enrolled in our premium digital monthly subscription plan and retain complete access for 65 € per month.
- For cost savings, you can change your plan at any time online in the “Settings & Account” section. If you’d like to retain your premium access and save 20%, you can opt to pay annually at the end of the trial.
- You may also opt to downgrade to Standard Digital, a robust journalistic offering that fulfils many user’s needs. Compare Standard and Premium Digital here.
- Any changes made can be done at any time and will become effective at the end of the trial period, allowing you to retain full access for 4 weeks, even if you downgrade or cancel.
- You may change or cancel your subscription or trial at any time online. Simply log into Settings & Account and select "Cancel" on the right-hand side.
- You can still enjoy your subscription until the end of your current billing period.
- We support credit card, debit card and PayPal payments.
- Find the plan that suits you best.
- Premium access for businesses and educational institutions.
- Check if your
							
university
 or
							
organisation
 offers FT membership to read for free.
- We use
								cookies
								and other data for a number of reasons, such as keeping FT Sites reliable and secure,
								personalising content and ads, providing social media features and to
								analyse how our Sites are used.
- International Edition

URL: https://t3n.de/news/ki-zeichnet-menschen-als-comicfigur-ethische-implikationen-1511452/
- Dutzende KI-Tools verwandeln euch für Geld in einen Comic-Superhelden. Während die Preise stark variieren, steckt dahinter immer die gleiche Technik. Doch nicht die Kosten sollten uns zu denken geben, sondern die ethischen Implikationen.
- 
- KI-gestützte Bildgeneratoren wie Stable Diffusion oder Dall-E können auf Zuruf zu fast allem ein Bild erzeugen. Nur ein Bild von euch oder eurem Nachbarn Karl-Heinz werden sie nicht erzeugen. Denn selbst wenn Fotos von euch – was gar nicht mal ausgeschlossen ist – Teil der Trainingsdaten der zugrunde liegenden KI-Modelle waren, reichen die schlicht nicht aus, damit die KI weiß, wer ihr seid. Daher schaffen es diese Systeme auch nur, Bilder von Berühmtheiten auf Vorgabe zu erzeugen.
- Ende August hat Googles KI-Team unter dem Namen Dreambooth eine Lösung für genau dieses Problem vorgestellt. Dreambooth kann mit Fotos einer Person gefüttert werden und anschließend anhand von Textvorgaben Bilder von genau diesem Menschen erzeugen.
- In Googles Forschungsabteilung war man sich jedoch bewusst, dass Dreambooth eben auch ein Missbrauchspotenzial birgt. Daher hat der Konzern auch nur ein Paper über die Funktionsweise von Dreambooth veröffentlicht, nicht aber den eigentlichen Code.
- „Böswillige Parteien könnten versuchen, solche Bilder zu verwenden, um die Betrachter in die Irre zu führen“, warnt Google auf der Dreambooth-Website. Wirklich geholfen hat das freilich nicht.
- Nur elf Tage später hat ein KI-Entwickler von Amazon eine erste freie Implementation der Dreambooth-Idee auf Basis des quelloffenen Bildgenerators Stable Diffusion veröffentlicht. Seitdem sind nicht nur allerlei Open-Source-Projekte auf Basis des Codes entstanden, auch eine Vielzahl von kommerziellen Anbietern nutzen die Technik für sich.
- Avatar AI, Profile Picture AI oder AI Paintr sind nur ein paar der kostenpflichtigen Dreambooth-Implementationen. Die Preise reichen je nach gewünschter Komplexität und Auflösung der fertigen Bilder und je nach Anbieter von 3 bis 80 US-Dollar. Das Angebot ist in aller Regel aber nahezu identisch: Ihr ladet eine Reihe von Fotos von euch hoch und könnt dann aus Textvorgaben Comiczeichnungen oder andere kreative Variationen eures Konterfeis erzeugen.
- Die Tools sind in aller Regel relativ leicht zu bedienen, und die Ergebnisse können sich durchaus sehen lassen. Zumindest wenn ihr euch etwas Mühe bei der Auswahl der Fotos gebt. Unterschiedliche Frisuren beispielsweise verwirren die KI eher. Auch Sonnenbrillen oder Kapuzen führen in der Regel zu suboptimalen Ergebnissen.
- Die Anbieter kontrollieren nicht, wessen Fotos ihr da eigentlich hochladet. So könntet ihr also ohne Probleme auch Fotos von anderen Menschen als Grundlage für eure KI-generierten Bilder nehmen. Spätestens da sollte das Missbrauchspotenzial offensichtlich werden. Fotos von der Instagram-Seite einer anderen Person nehmen und die KI damit füttern? Im Grunde kein Problem – sofern ethische Aspekte ignoriert werden.
- Die Diskussion über die Nutzung von Fotos und Bildern zum Training einer KI erreicht damit auch eine private Dimension. Die meisten Menschen dürften sich zu Recht unwohl damit fühlen, wenn ein:e Fremde:r ihr Gesicht verwendet, um sie auf Bildern tun zu lassen, wonach auch immer ihm oder ihr ist.
- Aber es gibt auch ein zweites Problem dabei: Mit der Dreambooth-Technik kann eine Bild-KI nicht nur gezielt an Fotos einer Person trainiert werden, sondern es können beispielsweise auch gezielt Zeichenstile von Künstler:innen kopiert werden. Das unterscheidet sich auf den ersten Blick zwar nicht direkt davon, dass Stable Diffusion von Haus aus Bilder im Stil von Picasso oder anderen berühmten Künstler:innen generieren kann, tatsächlich ist aber eben doch etwas anderes, wenn gezielt das Werk lebender Künstler:innen ausgeschlachtet wird.
- Gleich mehrere Beispiele dafür finden sich in dem KI-Modell Comic-Diffusion. Das von dem Studenten Ogbogu Kalu erstellte Modell wurde anhand von Grafiken mehrerer lebender Comic-Zeichner trainiert. Dementsprechend kann das KI-Modell auch Bilder erzeugen, die genau jene charakteristischen Eigenschaften aufweisen, auf denen die Karriere der Künstler:innen fußt.
- Gefragt wurden die Künstler:innen natürlich nicht. Nach Vorstellung des ersten Comic-Diffusion-Modells gab es im Stable-Diffusion-Subreddit durchaus Kritik an Kalu. Von dem ehemaligen Kickstarter-CTO und Tech-Blogger Andy Baio darauf angesprochen, zeigte sich auch die von Kalus KI-Modell kopierte Künstlerin Hollie Mengert nicht begeistert. „Hätte man mich gefragt, ob sie das tun können, hätte ich nicht ja gesagt“, so die Künstlerin. Damit dürfte sie nicht alleine dastehen.
- Bitte gib eine gültige E-Mail-Adresse ein.
- Es gab leider ein Problem beim Absenden des Formulars. Bitte versuche es erneut.
- Bitte gib eine gültige E-Mail-Adresse ein.
- Hinweis zum Newsletter & Datenschutz
- Bitte klicke auf den Link in der Bestätigungsmail, um deine Anmeldung abzuschließen.
- Du willst noch weitere Infos zum Newsletter?
                Jetzt mehr erfahren
- Wir freuen uns über kontroverse Diskussionen, die gerne auch mal hitzig geführt werden dürfen. Beleidigende, grob anstößige, rassistische und strafrechtlich relevante Äußerungen und Beiträge tolerieren wir nicht.
                            Bitte achte darauf, dass du keine Texte veröffentlichst, für die du keine ausdrückliche Erlaubnis des Urhebers hast. Ebenfalls nicht erlaubt ist der Missbrauch der Webangebote unter t3n.de als Werbeplattform. Die Nennung
                            von
                            Produktnamen, Herstellern, Dienstleistern und Websites ist nur dann zulässig, wenn damit nicht vorrangig der Zweck der Werbung verfolgt wird.
                            Wir behalten uns vor, Beiträge, die diese Regeln verletzen, zu löschen und Accounts zeitweilig oder auf Dauer zu sperren.
- Trotz all dieser notwendigen Regeln: Diskutiere kontrovers, sage anderen deine Meinung, trage mit weiterführenden Informationen zum Wissensaustausch bei, aber bleibe dabei fair und respektiere die Meinung anderer.
                            Wir wünschen Dir viel Spaß mit den Webangeboten von t3n und freuen uns auf spannende Beiträge.
- Dein t3n-Team
- Nach den Vorschußlorbeeren auf heise.de und anderen Plattformen bin ich nach Durchsicht der Galerie etwas ernüchtert – etwas mehr als dieses kompositionelle Gekrakel, dem dann oft noch die Visualierung eines zentralen Begriffs der textuellen Anfrage fehlt, hätte ich schon erwartet.
- Vielleicht bin ich hier überkritisch, aber das ist nicht mehr als ein Spielzeug.
- Die Auswahl aus der Galerie muss Monate alt sein.
Da liegen inzwischen mehrere WELTEN dazwischen.
- Ich empfehle mal nach „Midjourney Community Showcase“ zu googlen.
Dort dann rechts oben auf „Top“ zu klicken.
Viel Spaß.
- Der völlig abstrakte Gedanke eines Stils ist zum Glück noch nicht so leicht zu schützen. Menschen dürfen ja genauso einfach den „Stil“ eines anderen Menschen kopieren, sehe also keinen Grund warum eine Maschine das nicht dürfen sollte. Genauso dürfen Menschen andere Menschen bei beliebigen Handlungen zeichnen/malen/photoshoppen (erst beim veröffentlichen wird es rechtlich problematisch, bei AI-Bildern aber nicht anders), ist also auch nicht wirklich problematisch wenn eine Maschine das gleiche macht.
- Während der Stil urheberrechtlich nicht geschützt ist, ist es das veröffentliche Bild/Foto aber schon. Abgesehen vom Urheberrecht gibt es noch das Recht am eigenen Bild, daher Vorsicht mit Photoshop und co.
Nur weil ein Bild öffentlich zugänglich ist, heißt nicht, dass es in irgendeiner Weise ohne die Zustimmung des Urhebers verwendet/verwertet werden darf. Aus diesem Grund ist es problematisch, wenn eine Maschine das gleiche macht. Es wäre nur in Ordnung, wenn für die KI-Trainings nur lizenzfreie Bilder verwendet würden.
- Melde dich mit deinem t3n Account an oder fülle die unteren Felder aus.
- Dein Kommentar
- Kommentar absenden
- Kommentar absenden
- Bitte schalte deinen Adblocker für t3n.de aus, um diesen Artikel zu lesen.
- Wir sind ein unabhängiger Publisher mit einem Team von mehr als 75 fantastischen Menschen,
                            aber ohne riesigen Konzern im Rücken. Banner und ähnliche Werbemittel sind für unsere
                            Finanzierung sehr wichtig.
- Schon jetzt und im Namen der gesamten t3n-Crew: vielen Dank für deine Unterstützung! 🙌
- Deine t3n-Crew
- Bitte melde dich an, um diesen Artikel in deiner persönlichen Merkliste auf t3n zu speichern.
- Du hast schon einen t3n-Account? Hier anmelden
- Spreading knowledge & future optimism.
- agof- und IVW-geprüft
- Ausgezeichnet von kununu

URL: https://www.thestar.com/business/technology/2022/12/01/these-ai-images-look-just-like-me-what-does-that-mean-for-the-future-of-deepfakes.html
- Sign In
- Sign In
- The Star Edition
- CHANGE LOCATION
- This copy is for your personal non-commercial use only. To order
    presentation-ready copies of Toronto Star content for distribution
    to colleagues, clients or customers, or inquire about
    permissions/licensing, please go to: www.TorontoStarReprints.com
- At a glance, the above three images look just like me. Look closer and you might notice that my skin is too smooth, my clothes distorted in places — details that might be dismissed as an overdone Photoshop edit.
- Thing is, I never posed for these pictures. Nor have I ever sported shoulder-length hair or a cowboy hat, for that matter. These images are entirely the product of artificial intelligence, utilizing a cutting-edge technology developed by Google scientists called DreamBooth.
- Since its release in late August, DreamBooth has already advanced the field of AI art by leaps and bounds. In a nutshell, it gives AI the ability to study what individuals or objects look like, then synthesizes a “photorealistic” image of the subject in a completely new context.
- The development has some AI researchers and ethicists concerned about its use in so-called “deepfakes” — media manufactured via AI deep-learning to depict fake events. People are already mapping the faces of unconsenting women onto the bodies of pornstars, making political figures deliver statements that never occurred in reality and creating impossible ads featuring dead celebrities.
- Currently, most deepfakes can be distinguished on close inspection. However, as multiple experts told me, DreamBooth may soon facilitate the creation of deepfakes indistinguishable from real photos or videos, at scales never before seen.
- What is DreamBooth and how can I use it?
- DreamBooth is an AI model that fine-tunes existing text-to-image generators like Midjourney or DALL-E to produce “personalized” images. It’s also open source, meaning the code is freely available to anyone to modify and redistribute. According to the original research paper, the model only needs to train on three to five photos of a subject before it can reproduce that subject’s likeness in a variety of styles and contexts.
- Using DreamBooth by itself can be both daunting for coding novices and VRAM-intensive — it requires a great deal of graphics resources. In response, a number of apps harnessing DreamBooth technology have emerged, offering an accessible entry into image creation.
- To produce the fake photos of myself, I enlisted the help of Alfred Wahlforss, a 26-year-old graduate student studying data science at Harvard University. Wahlforss is one of the developers behind BeFake, a new app utilizing DreamBooth to generate AI art of its users in a variety of styles and settings.
- BeFake uses DreamBooth in conjunction with another popular AI image generator called Stable Diffusion, Wahlforss said: “To get it perfectly right, you have to add algorithms on top or fine tune it in a certain way.”
- To create my images, I sent Wahlforss 20 close-up photos of myself in varying backgrounds and clothing. The higher quality, the better, he said: “Garbage in, garbage out.”
- After the AI was trained on these pictures, Wahlforss asked it to create an image of me using different prompts. He could have it reproduce me in the style of famous painters or photographers, dress me up in different outfits and change my hair or even my gender.
- The same goes when using his app — users are asked to submit 14 to 20 pictures of themselves, the more the better. People can then select different prompts to recreate themselves. The first training set is free; users will have to pay for more.
- “The user-submitted images are used to train the model and then deleted after we have trained it,” Wahlforss said. “We don’t do anything other than train the model with the user images.”
- At the moment, BeFake is broken — a viral Reddit post last week sent thousands of people to the app, crashing the server of its cloud provider, Wahlforss said. He believes the app will be back in service later this week.
- “There are clear ways to misuse this technology,” Wahlforss said. “On our end, we have a lot of checks and balances, and that’s one of the algorithms that I was talking about … obviously we’re very cautious around generating anything that is not safe for work, either violence or nudity.”
- “But I don’t want to go into too much detail because then, obviously, you’re opening yourself up to attacks.”
- The team behind DreamBooth did not respond to the Star’s requests for comment before publication.
- According to Tom Mason, the chief technology officer of Stability. AI, which created AI image generator Stable Diffusion, DreamBooth is “just the beginning of a new wave of generative models in image, language, video, audio and 3D.”
- The technology is not limited to 2D images, he said — image generation models are already being used in animation and “techniques for fine-tuning such as Dreambooth will absolutely be possible with other modalities, further extending the options.”
- In response to the many people making use of the open source technology, Mason said: “We encourage ethical use of the models and a sensible open discussion with transparency on the techniques and applications that the community is developing.”
- The future of deepfakes?
- Abhishek Gupta, founder, principal researcher and director of the Montreal AI Ethics Institute, said DreamBooth represents a “step forward in a bad direction.”
- “It’s maybe a leap forward in terms of the potential negative impacts that are going to arise, because it’s now allowed outputs that are a lot more convincing. They’re a lot easier to produce. They can be produced at scale,” he said.
- For example, DreamBooth could be used to copy signatures or official signage to fake documents, create misleading photos or videos of politicians, manufacture revenge porn of individuals and more, Gupta continued.
- At the moment, DreamBooth can be relatively resource intensive to run, he said, limiting the scale at which deepfakes can proliferate. At the rapid rate AI is advancing, however, Gupta believes the technology will soon become faster and easier to operate and access.
- “Because it’s an automated process, you can have multiple targets and attack them in not just a targeted manner, but in a scaled up manner where you can generate many, many outputs that are convincing,” Gupta said.
- The technology’s ease of use for malicious actors also creates an “asymmetry of power,” Gupta said. At the moment, it’s easy to target people with DreamBooth and victims have little ability to do anything about it.
- “There aren’t, at the moment, any strong legal protections because this is such an evolving space,” Gupta said. “We’re seeing efforts in this space in terms of being able to combat the negative impacts, but those are nascent. Everybody’s still trying to figure out what does it even mean to have generative outputs and how to combat them.”
- A specific issue with DreamBooth and Stable Diffusion is that they’re open source, Gupta continued. Unlike centralized AI-generation models that can impose regulations and barriers to image creation, the decentralized models like DreamBooth mean anyone can access and improve on the technology.
- “When you release a model open source first, a lot of those protections are bypassed,” Gupta said. Even if it had protections originally, “they can be bypassed by people who are savvy — and then you can have a re-release downstream that people can do whatever they want with.”
- On the flip side, the decentralized nature of DreamBooth has sparked an “explosion” in innovation and progress in AI, according to Lewis Hackett, a UK-based artist that incorporates AI image generation into his work. Hackett helps run the main Discord server for AI researchers and hobbyists exploring and expanding on DreamBooth.
- “In the (Discord server), we have constantly, all day, people developing new things, testing new things with DreamBooth and Stable Diffusion,” Hackett said. His server currently has over 8,000 members. “That kind of opened the doors to this explosion in access to image tools.”
- What’s the next step for AI art?
- “Rather than it being a big centralized group that’s controlling the research of these different tools, it’s now the entire community and they all work together,” Hackett said. “Back then, this (level of) democratization was pretty unheard of in the software community … I think it’s why the field is growing at a phenomenal rate.”
- Originally, DreamBooth took about an hour and a half and 40 gigabytes of VRAM to train a model, Hackett said. Once released to the public, people in his Discord began developing their own versions based on and building off the paper.
- “It’s at the point now where it takes about 12 gigabytes, so any consumer graphics card that has 12 gigabytes would be able to do our training,” he said, adding that they’re rapidly improving on the image generation time too. “ … It really shows what’s possible with open source and how beneficial it is for people to be able to distribute and work on these things.”
- Hackett admits the technology’s open source nature leaves it vulnerable to malicious actors: “Obviously, anyone can get the software and anyone can alter the software in any way they want to make it do whatever they want.
- “But I think it’s a net positive to have open source, because there’s always going to be people that are misusing tools,” he continued. “If more researchers have access to these tools, they can actually research this stuff outside and actually try to develop harm reduction techniques.”
- There’s a great potential for good in DreamBooth and AI art — For example, it can help disadvantaged people create art and serve as a valuable tool for artists everywhere, Hackett said.
- “There’s always going to be bad actors out there who are trying to use these technologies,” Hackett said. “That doesn’t necessarily mean that the entire community is bad and that the tool is bad itself.”
- “I just think there are so many positives, it outweighs the negative aspects of it,” Hackett said. “But that doesn’t mean that it should go unchecked.”
- How can we stop deepfakes?
- While some experts recommended regulations to reign in bad actors in the AI community, Hackett is skeptical this approach would work: “The cat’s already out of the bag,” he said.
- “I think the larger companies can (issue regulations). But, because these tools are already in the hands of the community, they’re going to do what they want regardless,” he continued.
- “I genuinely think education is the only solution.”
- At the moment, AI-generated deepfakes can still be told apart from real photographs, Hackett said. Therefore, it’s critical to spread awareness of AI art and its differences from real photographs.
- In the future, however, AI generated art may become indistinguishable from reality to the human eye, Hackett and other experts say. But the difference could be clear to other AI.
- Just as AI has the capacity to create deepfakes, it also has the ability to detect them, according to Wael Abd-Almageed, an associate research professor and founding director of the Visual Intelligence and Multimedia Analytics Laboratory at the University of Southern California.
- For now, “the technology is not bullet proof. Like any technology, it can make mistakes, false alarms and false negatives … but even assuming that the technology works between 60 per cent to 70 per cent accuracy, it’s certainly better than absolutely nothing,” he said.
- That said, AI-detection software has been rapidly improving, aided by incentives from the industry. For example, Stability. AI is about to launch a deepfake detection contest with a $200,000 prize to “encourage the development of tools in the community that can be used to identify deep fakes,” its CTO said.
- Because deepfakes have the potential to spread viral misinformation on social platforms, Abd-Almageed has been advocating for social media companies to adopt AI countermeasures. At the moment, little is being done, he said.
- If we keep doing nothing, the proliferation of realistic deepfakes may soon “erode the line between what’s real and what’s not,” he continued.
- “At some point, everything will suddenly become suspicious. We will not be able to believe any information,” Abd-Almageed said. “ … And the biggest problem with these deepfakes is that once people believe them, it becomes extremely difficult to reverse that effect.”
- “Can you imagine, for example, in the next election, a few hours before polls close, a viral video of Biden comes out saying ‘I am sick and dying?’” he continued.
- “ … With all the advances (in AI), it can harm not just individual people now. It can harm society and the stock market, and it can harm democracies as well.”
- 
- Anyone can read Conversations, but to contribute, you should be a registered Torstar account holder. If you do not yet have a Torstar account, you can create one now (it is free)
- Sign In
- Register
- Copyright owned or licensed by Toronto Star Newspapers Limited. All
    rights reserved. Republication or distribution of this content is
    expressly prohibited without the prior written consent of Toronto
    Star Newspapers Limited and/or its licensors. To order copies of
    Toronto Star articles, please go to: www.TorontoStarReprints.com

URL: https://www.reddit.com/r/StableDiffusion/comments/yaquby/2d_illustration_styles_are_scarce_on_stable/
- Reddit and its partners use cookies and similar technologies to provide you with a better experience.
- By accepting all cookies, you agree to our use of cookies to deliver and maintain our services and site, improve the quality of Reddit, personalize Reddit content and advertising, and measure the effectiveness of advertising.
- By rejecting non-essential cookies, Reddit may still use certain cookies to ensure the proper functionality of our platform.
- For more information, please see our
              Cookie Notice
              and our
              Privacy Policy.
- Welcome to the unofficial Stable Diffusion subreddit! We encourage you to share your awesome generations, discuss the various repos, news about releases, and more! Be sure to check out the pinned post for our rules and tips on how to get started! Prompt sharing is highly encouraged, but not required.

URL: https://www.resetera.com/threads/reddit-user-trains-ai-to-imitate-an-illustrator-without-asking-her-permission-i-dont-really-care-if-you-think-this-is-right-or-wrong.650148/

- DeviantArt DreamUp art generator
- Novel AI storytelling generator
- Page infoType: IncidentPublished: February 2023
