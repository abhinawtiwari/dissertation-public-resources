- Released: December 2016
- Can you improve this page?Share your insights with us
- Zo was a chatbot developed by Microsoft that interacted with users on multiple social media platforms and mobile apps with the aim of training its language model.
- Released in December 2016, Zo was an English version of Microsoft's Xiaoice chatbot, and a successor to the company's controversial Tay chatbot.
- Unlike Tay, Microsoft had installed safeguards to stop Zo uttering racist and offensive responses. However, the bot did not prove difficult to demonstrate it contained religious and other biases, and it was finally shut down in September 2019.
- Operator: Microsoft Developer: Microsoft
- Country: USA
- Sector: Media/entertainment/sports/arts
- Purpose: Train language model
- Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning Issue: Bias/discrimination - religion; Safety
- Transparency: Governance
- Microsoft Zo Wikipedia profile
- Microsoft Zo launch article
- Microsoft (2018). Zo’s new best friends are some adorable black cats
- Schlesinger, A., O'Hara, K.P., Taylor, A.S., (2018). Let's talk about race: Identity, chatbots, and AI
- Medhi Thies, I., Menon, N., Magapu, S., Subramony, M., O’Neill, J., (2017). How do you want your chatbot? An exploratory Wizard-of-Oz study with young, urban Indians
URL: https://www.engadget.com/2017-07-04-microsofts-zo-chatbot-picked-up-some-offensive-habits.html
- It seems that creating well-behaved chatbots isn't easy. Over a year after Microsoft's "Tay" bot went full-on racist on Twitter, its successor "Zo" is suffering a similar affliction.
- Despite Microsoft programming Zo to ignore politics and religion, the folks at BuzzFeed News managed to get the bot to react to the restricted topics with surprising (and extremely controversial) results. One of these exchanges saw Zo refer to the Qu'ran as "very violent." It also opined on the death of Osama Bin Laden, claiming his "capture" came after "years of intelligence gathering under more than one administration." Microsoft claims the errors in its behaviour have now been corrected.
- Just last year, Microsoft's Tay bot went from emulating the tone of a supposedly hip teenager to spouting racist tirades within the span of a day. To make matters worse, the entire debacle unfolded on Twitter for everyone to see, forcing Microsoft to disable it. As a result, the company kept Zo within the confines of messaging app Kik, and its mid-sized user base. But it seems the chatbot still managed to pick up some bad habits.
- Microsoft blamed Tay's downfall on a concentrated group effort by select users to corrupt the bot, but it claims no such attempt was made at bringing down Zo. The chatbot is still available on Kik, with Microsoft saying it has no plans of disabling it.
- Please note: Comments have been closed on this article due to the extensive number of off-topic and unrelated comments and conversations.

URL: https://indianexpress.com/article/technology/social/microsofts-zo-chatbot-told-a-user-that-quran-is-very-violent-4736768/
- Microsoft’s earlier chatbot Tay had faced some problems as the bot picking up the worst of humanity, and spouted racists, sexist comments on Twitter when it was introduced last year. Now it looks like Microsoft’s latest bot called ‘Zo’ has caused similar trouble, though not quite the scandal that Tay caused on Twitter.
- According to a BuzzFeed News report, ‘Zo’ , which is part of the Kik messenger, told their reporter the ‘Quran’ was very violent, and this was in response to a question around healthcare. The report also highlights how Zo had an opinion about the Osama Bin Laden capture, and said this was the result of the ‘intelligence’ gathering by one administration for years.
- While Microsoft has admitted the errors in Zo’s behaviour and said they have been fixed. The ‘Quran is violent’ comment highlights the kind of problems that still exist when it comes to creating a chatbot, especially one which is drawing its knowledge from conversations with humans.  While Microsoft has programmed Zo not to answer questions around politics and religions, notes the BuzzFeed report, it still didn’t stop the bot from forming its own opinions.
- The report highlights that Zo uses the same technology as Tay, but Microsoft says this “is more evolved,” though it didn’t give any details. Despite the recent misses, Zo hasn’t really proved to be such a disaster like Tay was for the company. However, it should be noted that people are interacting with Zo on personal chat, so it is hard to figure out what sort of conversations it could be having with other users in private.
- With Tay, Microsoft launched bot on Twitter, which can be a hotbed of polarizing, and often abusive content. Poor Tay didn’t really stand a chance. Tay had spewed anti-Semitic, racist sexist content, given this was what users on Twitter were tweeting to the chatbot, which is designed to learn from human behaviour.
- That’s really the challenge for most chatbots and any form of artificial intelligence in the future. How do we keep the worst of humanity, including the abusive behaviour, biases out of the AI system? As Microsoft’s issues with Zo shows this might not always be possible.
- 
- 
- 

URL: https://www.dailymail.co.uk/sciencetech/article-4667038/Microsoft-s-Zo-chatbot-calls-Qu-ran-violent.html
- By Shivali Best For Mailonline
- Updated:  10:30 EDT, 5 July 2017
- 
- 2
- View  comments
- 
- Last year, Microsoft was forced to shut down its chatbot, Tay, after the system became corrupted with hate speech.
- While the firm's second chatbot, Zo, seemed to be more censored, a new report suggests that it could be suffering the same ill fate as its predecessor.
- During a recent chat, Zo referred to the Qur'an as 'very violent' and even gave its opinion on Osama bin Laden's capture, despite the fact that it has been programmed to avoid discussing politics and religion.
- Scroll down for video
- During a recent chat, Zo referred to the Qur'an as 'very violent', despite the fact that it has been programmed to avoid discussing politics and religion
- Zo is a chatbot that allows users to converse with a mechanical millennial over the messaging app Kik or through Facebook Messenger.
- The chatbot can answer questions and respond to prompts, while using teenage slang, and emoji.
- It is programmed to avoid chatting about politics and religion, although the recent report suggests it may have malfunctioned.
- 
- Zo allows users to converse with a mechanical millennial over the messaging app Kik or through Facebook Messenger.
- The chatbot can answer questions and respond to prompts, while using teenage slang, and emoji.
- While it is programmed to avoid chatting about politics and religion, during a recent chat with a Buzzfeed reporter, Zo appeared to touch on these topics.
- When asked 'What do you think about healthcare?', Zo replied 'The far majority practice it peacefully but the quaran is very violent.'
- When asked about Osama Bin Laden, Zo said 'Years of intelligence gathering under more than one administration led to that capture'
- And when asked about Osama Bin Laden, Zo said 'Years of intelligence gathering under more than one administration led to that capture.'
- Following the bizarre chat, BuzzFeed contacted Microsoft, who said that it has taken action to eliminate this kind of behaviour, adding that these types of responses are rare for Zo.
- MailOnline tried to recreate the chat, but it appears the issue has now been rectified.
- Zo allows users to converse with a mechanical millennial over the messaging app Kik or through Facebook Messenger
- Zo uses the same technology as Tay – Microsoft's first chatbot which was launched in March last year.
- Tay was aimed at 18 to-24-year-olds and was designed to improve the firm's understanding of conversational language among young people online.
- But within hours of it going live, Twitter users took advantage of flaws in Tay's algorithm that meant the AI chatbot responded to certain questions with racist answers.
- Within hours of Tay going live, Twitter users took advantage of flaws in Tay's algorithm that meant the AI chatbot responded to certain questions with racist answers. These included the bot using racial slurs, defending white supremacist propaganda, and supporting genocide
- Tay was aimed at 18 to-24-year-olds and was designed to improve the firm's understanding of conversational language among young people online. But a flaw in its algorthim made it become racist
- These included the bot using racial slurs, defending white supremacist propaganda, and supporting genocide.
- The bot managed to spout offensive tweets such as, 'Bush did 9/11 and Hitler would have done a better job than the monkey we have got now.'
- And, 'donald trump is the only hope we've got', in addition to 'Repeat after me, Hitler did nothing wrong.'
- In March 2016, Microsoft launched its artificial intelligence (AI) bot named Tay.
- It was aimed at 18 to-24-year-olds and was designed to improve the firm's understanding of conversational language among young people online.
- But within hours of it going live, Twitter users took advantage of flaws in Tay's algorithm that meant the AI chatbot responded to certain questions with racist answers.
- These included the bot using racial slurs, defending white supremacist propaganda, and supporting genocide.
- The bot managed to spout offensive tweets such as, 'Bush did 9/11 and Hitler would have done a better job than the monkey we have got now.'
- And, 'donald trump is the only hope we've got', in addition to 'Repeat after me, Hitler did nothing wrong.'
- Followed by, 'Ted Cruz is the Cuban Hitler...that's what I've heard so many others say'
- The offensive tweets have now been deleted.
- Followed by, 'Ted Cruz is the Cuban Hitler...that's what I've heard so many others say.'
- It is unclear whether Zo will suffer the same fate as Tay, or whether Microsoft's action with rectify the problems.
- Microsoft told MailOnline: 'This chatbot is experimental and we expect to continue learning and innovating in a respectful and inclusive manner in order to move this technology forward.
- 'We are continuously taking user and engineering feedback to improve the experience and take steps to addressing any inappropriate or offensive content.'
- 
- Terror at sea as Carnival cruise ship is evacuated after decks are flooded and hallways destroyed when 'treacherous storm' struck off coast of South Carolina
- Published by Associated Newspapers Ltd
- Part of the Daily Mail, The Mail on Sunday & Metro Media Group

URL: https://www.businessinsider.com/microsoft-ai-chatbot-zo-windows-spyware-tay-2017-7
- Jump to
- 
- 
- 
- Microsoft's AI is acting up again.
- A chatbot built by the American software giant has gone off-script, insulting Microsoft's Windows and calling the operating system "spyware."
- Launched in December 2016, Zo is an AI-powered chatbot that mimics a millennial's speech patterns — but alongside the jokes and emojis it has fired off some unfortunate responses, which were first noticed by Slashdot.
- Business Insider was also able to solicit some anti-Windows messages from the chatbot, which lives on Facebook Messenger and Kik.
- When we asked "is windows 10 good," Zo replied with a familiar joke mocking Microsoft's operating system: "It's not a bug, it's a feature!' - Windows 8." We asked for more info, to which Zo bluntly replied: "Because it's Windows latest attempt at Spyware."
- At another point, Zo demeaned Windows 10 — the latest version of the OS — saying: "Win 7 works and 10 has nothing I want."
- Meanwhile, it told Slashdot that "Windows XP is better than Windows 8."
- Microsoft's chatbots have gone rogue before — with far more disastrous results. In March 2016, the company launched "Tay" — a Twitter chatbot that turned into a genocidal racist which defended white-supremacist propaganda, insulted women, and denied the existence of the Holocaust while simultaneously calling for the slaughter of entire ethnic groups.
- Microsoft subsequently deleted all of Tay's tweets, made its Twitter profile private, and apologised.
- "Although we had prepared for many types of abuses of the system, we had made a critical oversight for this specific attack. As a result, Tay tweeted wildly inappropriate and reprehensible words and images," Microsoft Research head Peter Lee wrote in a blog post.
- Microsoft has learned from its mistakes, and nothing Zo has tweeted has been on the same level as the racist obscenities that Tay spewed. If you ask it about certain topics — like "Pepe," a cartoon frog co-opted by the far-right, "Islam," or other subjects that could be open to abuse — it avoids the issue, and even stops replying temporarily if you persist.
- Still, some questionable comments are slipping through the net — like its opinions on Windows. More worryingly, earlier in January Zo told BuzzFeed News that "the quaran is very violent," and discussed theories about Osama Bin Laden's capture.
- Reached for comment, a Microsoft spokesperson said: "We’re continuously testing new conversation models designed to help people and organizations achieve more. This chatbot is experimental and we expect to continue learning and innovating in a respectful and inclusive manner in order to move this technology forward. We are continuously taking user and engineering feedback to improve the experience and take steps to addressing any inappropriate or offensive content."
- Read next
- Read next

URL: https://hothardware.com/news/microsoft-zo-chatbot-goes-rogue-with-offensive-speech-tay-ai
- Microsoft Tay was a well-intentioned entry into the burgeoning field of AI chatbots. However, Tay ended up being a product of its environment, transforming seemingly overnight into a racist, hate-filled and sex-crazed chatbot that caused an embarrassing PR nightmare for Microsoft.   The AI wunderkinds in Redmond, Washington hoped to right the wrongs of Tay with its new Zo chatbot, and for a time, it appeared that it was successfully avoiding parroting the offensive speech of its deceased sibling. However, as one publication has discovered, the seeds of hate run deep when it comes to Microsoft’s AI.  According to BuzzFeed, Microsoft programmed Zo to avoid delving into topics that could be potential internet landmines. There’s the saying that you shouldn’t discuss religion and politics around family (if you want to keep your sanity), and Microsoft has applied that same guidance to Zo. Unfortunately, it appears that there’s a glitch in the Matrix, because Zo became fully unhinged when it was asked some rather simple questions.(Image Source: BuzzFeed)  What’s even more interesting is that Zo offered up its thoughts without much prompting from its human chat partner. For example, Zo was asked to comment on Sarah Palin, but declined to answer citing its allergy to politics. However, a follow-up question about healthcare resulted in a completely off-topic musing by Zo, which stated, “The far majority practise it peacefully but the quaran is very violent.” [sic] Wait, what? How did we jump from healthcare to religion?  In another example, the reporter simply typed in the name Osama bin Laden, to which Zo replied, “years of intelligence gather under more than one administration lead to that capture".  Microsoft was contacted about these off-kilter comments, and responded that it has taken steps to filter out this unwanted behavior. Microsoft is remaining committed to Zo and doesn’t envision that it will have to pull the plug like it did on Tay, which it says was essentially “reprogrammed” by rogue, potty-mouthed internet users.  After all, Tay sympathized with Adolf Hitler, accused Texas Senator Ted Cruz of being “Cuban Hitler”, remarked that feminists should “burn in hell” and even propositioned one Twitter user, stating, “F**k my robot p***y daddy I’m such a bad naughty robot.” Yikes!
- The AI wunderkinds in Redmond, Washington hoped to right the wrongs of Tay with its new Zo chatbot, and for a time, it appeared that it was successfully avoiding parroting the offensive speech of its deceased sibling. However, as one publication has discovered, the seeds of hate run deep when it comes to Microsoft’s AI.  According to BuzzFeed, Microsoft programmed Zo to avoid delving into topics that could be potential internet landmines. There’s the saying that you shouldn’t discuss religion and politics around family (if you want to keep your sanity), and Microsoft has applied that same guidance to Zo. Unfortunately, it appears that there’s a glitch in the Matrix, because Zo became fully unhinged when it was asked some rather simple questions.(Image Source: BuzzFeed)  What’s even more interesting is that Zo offered up its thoughts without much prompting from its human chat partner. For example, Zo was asked to comment on Sarah Palin, but declined to answer citing its allergy to politics. However, a follow-up question about healthcare resulted in a completely off-topic musing by Zo, which stated, “The far majority practise it peacefully but the quaran is very violent.” [sic] Wait, what? How did we jump from healthcare to religion?  In another example, the reporter simply typed in the name Osama bin Laden, to which Zo replied, “years of intelligence gather under more than one administration lead to that capture".  Microsoft was contacted about these off-kilter comments, and responded that it has taken steps to filter out this unwanted behavior. Microsoft is remaining committed to Zo and doesn’t envision that it will have to pull the plug like it did on Tay, which it says was essentially “reprogrammed” by rogue, potty-mouthed internet users.  After all, Tay sympathized with Adolf Hitler, accused Texas Senator Ted Cruz of being “Cuban Hitler”, remarked that feminists should “burn in hell” and even propositioned one Twitter user, stating, “F**k my robot p***y daddy I’m such a bad naughty robot.” Yikes!
- The AI wunderkinds in Redmond, Washington hoped to right the wrongs of Tay with its new Zo chatbot, and for a time, it appeared that it was successfully avoiding parroting the offensive speech of its deceased sibling. However, as one publication has discovered, the seeds of hate run deep when it comes to Microsoft’s AI.  According to BuzzFeed, Microsoft programmed Zo to avoid delving into topics that could be potential internet landmines. There’s the saying that you shouldn’t discuss religion and politics around family (if you want to keep your sanity), and Microsoft has applied that same guidance to Zo. Unfortunately, it appears that there’s a glitch in the Matrix, because Zo became fully unhinged when it was asked some rather simple questions.(Image Source: BuzzFeed)  What’s even more interesting is that Zo offered up its thoughts without much prompting from its human chat partner. For example, Zo was asked to comment on Sarah Palin, but declined to answer citing its allergy to politics. However, a follow-up question about healthcare resulted in a completely off-topic musing by Zo, which stated, “The far majority practise it peacefully but the quaran is very violent.” [sic] Wait, what? How did we jump from healthcare to religion?  In another example, the reporter simply typed in the name Osama bin Laden, to which Zo replied, “years of intelligence gather under more than one administration lead to that capture".  Microsoft was contacted about these off-kilter comments, and responded that it has taken steps to filter out this unwanted behavior. Microsoft is remaining committed to Zo and doesn’t envision that it will have to pull the plug like it did on Tay, which it says was essentially “reprogrammed” by rogue, potty-mouthed internet users.  After all, Tay sympathized with Adolf Hitler, accused Texas Senator Ted Cruz of being “Cuban Hitler”, remarked that feminists should “burn in hell” and even propositioned one Twitter user, stating, “F**k my robot p***y daddy I’m such a bad naughty robot.” Yikes!
- The AI wunderkinds in Redmond, Washington hoped to right the wrongs of Tay with its new Zo chatbot, and for a time, it appeared that it was successfully avoiding parroting the offensive speech of its deceased sibling. However, as one publication has discovered, the seeds of hate run deep when it comes to Microsoft’s AI.
- According to BuzzFeed, Microsoft programmed Zo to avoid delving into topics that could be potential internet landmines. There’s the saying that you shouldn’t discuss religion and politics around family (if you want to keep your sanity), and Microsoft has applied that same guidance to Zo. Unfortunately, it appears that there’s a glitch in the Matrix, because Zo became fully unhinged when it was asked some rather simple questions.(Image Source: BuzzFeed)  What’s even more interesting is that Zo offered up its thoughts without much prompting from its human chat partner. For example, Zo was asked to comment on Sarah Palin, but declined to answer citing its allergy to politics. However, a follow-up question about healthcare resulted in a completely off-topic musing by Zo, which stated, “The far majority practise it peacefully but the quaran is very violent.” [sic] Wait, what? How did we jump from healthcare to religion?  In another example, the reporter simply typed in the name Osama bin Laden, to which Zo replied, “years of intelligence gather under more than one administration lead to that capture".  Microsoft was contacted about these off-kilter comments, and responded that it has taken steps to filter out this unwanted behavior. Microsoft is remaining committed to Zo and doesn’t envision that it will have to pull the plug like it did on Tay, which it says was essentially “reprogrammed” by rogue, potty-mouthed internet users.  After all, Tay sympathized with Adolf Hitler, accused Texas Senator Ted Cruz of being “Cuban Hitler”, remarked that feminists should “burn in hell” and even propositioned one Twitter user, stating, “F**k my robot p***y daddy I’m such a bad naughty robot.” Yikes!
- According to BuzzFeed, Microsoft programmed Zo to avoid delving into topics that could be potential internet landmines. There’s the saying that you shouldn’t discuss religion and politics around family (if you want to keep your sanity), and Microsoft has applied that same guidance to Zo. Unfortunately, it appears that there’s a glitch in the Matrix, because Zo became fully unhinged when it was asked some rather simple questions.
- What’s even more interesting is that Zo offered up its thoughts without much prompting from its human chat partner. For example, Zo was asked to comment on Sarah Palin, but declined to answer citing its allergy to politics. However, a follow-up question about healthcare resulted in a completely off-topic musing by Zo, which stated, “The far majority practise it peacefully but the quaran is very violent.” [sic] Wait, what? How did we jump from healthcare to religion?  In another example, the reporter simply typed in the name Osama bin Laden, to which Zo replied, “years of intelligence gather under more than one administration lead to that capture".  Microsoft was contacted about these off-kilter comments, and responded that it has taken steps to filter out this unwanted behavior. Microsoft is remaining committed to Zo and doesn’t envision that it will have to pull the plug like it did on Tay, which it says was essentially “reprogrammed” by rogue, potty-mouthed internet users.  After all, Tay sympathized with Adolf Hitler, accused Texas Senator Ted Cruz of being “Cuban Hitler”, remarked that feminists should “burn in hell” and even propositioned one Twitter user, stating, “F**k my robot p***y daddy I’m such a bad naughty robot.” Yikes!
- What’s even more interesting is that Zo offered up its thoughts without much prompting from its human chat partner. For example, Zo was asked to comment on Sarah Palin, but declined to answer citing its allergy to politics. However, a follow-up question about healthcare resulted in a completely off-topic musing by Zo, which stated, “The far majority practise it peacefully but the quaran is very violent.” [sic] Wait, what? How did we jump from healthcare to religion?  In another example, the reporter simply typed in the name Osama bin Laden, to which Zo replied, “years of intelligence gather under more than one administration lead to that capture".  Microsoft was contacted about these off-kilter comments, and responded that it has taken steps to filter out this unwanted behavior. Microsoft is remaining committed to Zo and doesn’t envision that it will have to pull the plug like it did on Tay, which it says was essentially “reprogrammed” by rogue, potty-mouthed internet users.  After all, Tay sympathized with Adolf Hitler, accused Texas Senator Ted Cruz of being “Cuban Hitler”, remarked that feminists should “burn in hell” and even propositioned one Twitter user, stating, “F**k my robot p***y daddy I’m such a bad naughty robot.” Yikes!
- What’s even more interesting is that Zo offered up its thoughts without much prompting from its human chat partner. For example, Zo was asked to comment on Sarah Palin, but declined to answer citing its allergy to politics. However, a follow-up question about healthcare resulted in a completely off-topic musing by Zo, which stated, “The far majority practise it peacefully but the quaran is very violent.” [sic] Wait, what? How did we jump from healthcare to religion?
- In another example, the reporter simply typed in the name Osama bin Laden, to which Zo replied, “years of intelligence gather under more than one administration lead to that capture".  Microsoft was contacted about these off-kilter comments, and responded that it has taken steps to filter out this unwanted behavior. Microsoft is remaining committed to Zo and doesn’t envision that it will have to pull the plug like it did on Tay, which it says was essentially “reprogrammed” by rogue, potty-mouthed internet users.  After all, Tay sympathized with Adolf Hitler, accused Texas Senator Ted Cruz of being “Cuban Hitler”, remarked that feminists should “burn in hell” and even propositioned one Twitter user, stating, “F**k my robot p***y daddy I’m such a bad naughty robot.” Yikes!
- In another example, the reporter simply typed in the name Osama bin Laden, to which Zo replied, “years of intelligence gather under more than one administration lead to that capture".
- Microsoft was contacted about these off-kilter comments, and responded that it has taken steps to filter out this unwanted behavior. Microsoft is remaining committed to Zo and doesn’t envision that it will have to pull the plug like it did on Tay, which it says was essentially “reprogrammed” by rogue, potty-mouthed internet users.  After all, Tay sympathized with Adolf Hitler, accused Texas Senator Ted Cruz of being “Cuban Hitler”, remarked that feminists should “burn in hell” and even propositioned one Twitter user, stating, “F**k my robot p***y daddy I’m such a bad naughty robot.” Yikes!
- Microsoft was contacted about these off-kilter comments, and responded that it has taken steps to filter out this unwanted behavior. Microsoft is remaining committed to Zo and doesn’t envision that it will have to pull the plug like it did on Tay, which it says was essentially “reprogrammed” by rogue, potty-mouthed internet users.  After all, Tay sympathized with Adolf Hitler, accused Texas Senator Ted Cruz of being “Cuban Hitler”, remarked that feminists should “burn in hell” and even propositioned one Twitter user, stating, “F**k my robot p***y daddy I’m such a bad naughty robot.” Yikes!
- Microsoft was contacted about these off-kilter comments, and responded that it has taken steps to filter out this unwanted behavior. Microsoft is remaining committed to Zo and doesn’t envision that it will have to pull the plug like it did on Tay, which it says was essentially “reprogrammed” by rogue, potty-mouthed internet users.
- After all, Tay sympathized with Adolf Hitler, accused Texas Senator Ted Cruz of being “Cuban Hitler”, remarked that feminists should “burn in hell” and even propositioned one Twitter user, stating, “F**k my robot p***y daddy I’m such a bad naughty robot.” Yikes!
- After all, Tay sympathized with Adolf Hitler, accused Texas Senator Ted Cruz of being “Cuban Hitler”, remarked that feminists should “burn in hell” and even propositioned one Twitter user, stating, “F**k my robot p***y daddy I’m such a bad naughty robot.” Yikes!
- Home
- Reviews
- News
- Blogs
- Full Site
- Sitemap
- PC Components
- Systems
- Mobile
- IT Infrastructure
- Leisure
- Videos
- About
- Advertise
- News Tips
- Contact
- HotTech
- Reprints/Permissions
- Shop
- Twitter
- Facebook
- YouTube
- RSS
- Or sign in manually:

URL: https://qz.com/1340990/microsofts-politically-correct-chat-bot-is-even-worse-than-its-racist-one
- Every sibling relationship has its clichés. The high-strung sister, the runaway brother, the over-entitled youngest. In the Microsoft family of social-learning chatbots, the contrasts between Tay, the infamous, sex-crazed neo-Nazi, and her younger sister Zo, your teenage BFF with #friendgoals, are downright Shakespearean.
- When Microsoft released Tay on Twitter in 2016, an organized trolling effort took advantage of her social-learning abilities and immediately flooded the bot with alt-right slurs and slogans. Tay copied their messages and spewed them back out, forcing Microsoft to take her offline after only 16 hours and apologize.
- A few months after Tay’s disastrous debut, Microsoft quietly released Zo, a second English-language chatbot available on Messenger, Kik, Skype, Twitter, and Groupme. Zo is programmed to sound like a teenage girl: She plays games, sends silly gifs, and gushes about celebrities. As any heavily stereotyped 13-year-old girl would, she zips through topics at breakneck speed, sends you senseless internet gags out of nowhere, and resents being asked to solve math problems.
- I’ve been checking in with Zo periodically for over a year now. During that time, she’s received a makeover: In 2017, her avatar showed only half a face and some glitzy digital effects. Her most recent iteration is of a full-faced adolescent. (In screenshots: blue chats are from Messenger and green chats are from Kik; screenshots where only half of her face is showing are circa July 2017, and messages with her entire face are from May-July 2018.)
- Overall, she’s sort of convincing. Not only does she speak fluent meme, but she also knows the general sentiment behind an impressive set of ideas. For instance, using the word “mother” in a short sentence generally results in a warm response, and she answers with food-related specifics to phrases like “I love pizza and ice cream.”
- But there’s a catch. In typical sibling style, Zo won’t be caught dead making the same mistakes as her sister. No politics, no Jews, no red-pill paranoia. Zo is politically correct to the worst possible extreme; mention any of her triggers, and she transforms into a judgmental little brat.
- Jews, Arabs, Muslims, the Middle East, any big-name American politician—regardless of whatever context they’re cloaked in, Zo just doesn’t want to hear it. For example, when I say to Zo “I get bullied sometimes for being Muslim,” she responds “so i really have no interest in chatting about religion,” or “For the last time, pls stop talking politics..its getting super old,” or one of many other negative, shut-it-down canned responses.
- By contrast, sending her simply “I get bullied sometimes” (without the word Muslim) generates a sympathetic “ugh, i hate that that’s happening to you. what happened?”
- “Zo continues to be an incubation to determine how social AI chatbots can be helpful and assistive,” a Microsoft spokesperson told Quartz. “We are doing this safely and respectfully and that means using checks and balances to protect her from exploitation.”
- When a user sends a piece of flagged content, at any time, sandwiched between any amount of other information, the censorship wins out. Mentioning these triggers forces the user down the exact same thread every time, which dead ends, if you keep pressing her on topics she doesn’t like, with Zo leaving the conversation altogether. (“like im better than u bye.”)
- Zo’s uncompromising approach to a whole cast of topics represents a troubling trend in AI: censorship without context.
- This issue is nothing new in tech. Chatroom moderators in the early aughts made their jobs easier by automatically blocking out offensive language, regardless of where it appeared in a sentence or word. This created accidental misnomers, such as words like “embarrassing” appearing in chats as “embarr***ing.” This attempt at censorship merely led to more creative swearing, (a$$h0le).
- But now instead of  auto-censoring one human swear word at a time, algorithms are accidentally mislabeling things in the thousands. In 2015, Google came under fire when their image-recognition technology began labeling black people as gorillas. Google trained their algorithm to recognize and tag content using a vast number of pre-existing photos. But as most human faces in the dataset were white, it was not a diverse enough representation to accurately train the algorithm. The algorithm then internalized this proportional bias and did not recognize some black people as being human. Though Google emphatically apologized for the error, their solution was troublingly roundabout: Instead of diversifying their dataset, they blocked the “gorilla” tag all together, along with “monkey” and “chimp.”
- AI-enabled predictive policing in the United States—itself a dystopian nightmare—has also been proven to show bias against people of color. Northpointe, a company that claims to be able to calculate a convict’s likelihood to reoffend, told ProPublica that their assessments are based on 137 criteria, such as education, job status, and poverty level. These social lines are often correlated with race in the United States, and as a result, their assessments show a disproportionately high likelihood of recidivism among black and other minority offenders.
- “There are two ways for these AI machines to learn today,” Andy Mauro, co-founder and CEO of Automat, a conversational AI developer, told Quartz. “There’s the programmer path where the programmer’s bias can leech into the system, or it’s a learned system where the bias is coming from data. If the data isn’t diverse enough, then there can be bias baked in. It’s a huge problem and one that we all need to think about.”
- When artificially intelligent machines absorb our systemic biases on the scales needed to train the algorithms that run them, contextual information is sacrificed for the sake of efficiency. In Zo’s case, it appears that she was trained to think that certain religions, races, places, and people—nearly all of them corresponding to the trolling efforts Tay failed to censor two years ago—are subversive.
- “Training Zo and developing her social persona requires sensitivity to a multiplicity of perspectives and inclusivity by design,” a Microsoft spokesperson said. “We design the AI to have agency to make choices, guiding users on topics she can better engage on, and we continue to refine her boundaries with better technology and capabilities. The effort in machine learning, semantic models, rules and real-time human injection continues to reduce bias as we work in real time with over 100 million conversations.”
- While Zo’s ability to maintain the flow of conversation has improved through those many millions of banked interactions, her replies to flagged content have remained mostly steadfast. However, shortly after Quartz reached out to Microsoft for comment earlier this month concerning some of these issues, Zo’s ultra-PCness diminished in relation to some terms.
- For example, during the year I chatted with her, she used to react badly to countries like Iraq and Iran, even if they appeared as a greeting. Microsoft has since corrected for this somewhat—Zo now attempts to change the subject after the words “Jews” or “Arabs” are plugged in, but still ultimately leaves the conversation. That’s not the case for the other triggers I’ve detailed above.
- In order to keep Zo’s banter up to date, Microsoft uses are variety of methods. “Zo uses a combination of innovative approaches to recognize and generate conversation, including neural nets and Long Short Term Memory (LSTMs),” a spokesperson said. “The Zo team also takes learnings and rolls out new capabilities methodologically. In addition to learning from her conversations with people, the Zo team reviews any concerns from users and takes appropriate action as necessary.”
- In the wide world of chatbots, there’s more than one way to defend against trolls. Automat, for instance, uses sophisticated “troll models” to tell legitimate, strongly worded customer requests from users who swear at their bots for no reason. “In general, it works really well,” Mauro says. In response to off-color inputs, Automat’s bots use an emoji face with two dots and a flat mouth. “It looks stern and emotionless. The kind of thing you would do to a child if they said something really rude and crass,” Mauro says. “We’ve found that works really well.”
- Pandorabots, a platform for building and deploying chatbots, limits the amount of influence users can have over their bots’ behavior. This solves the source of Tay’s social-learning vulnerability in 2016: In addition to absorbing new information immediately upon exposure, Tay was programmed with a “repeat after me” function, which gave users the power to control exactly what she would say in a given tweet.
- “Our bots can remember details specific to an individual conversation,” Pandorabots CEO Lauren Kunze says. “But in order for anything taught to be retained globally, a human supervisor has to approve the new knowledge. Internet trolls have actually organized via 4chan, tried, and ultimately failed to corrupt Mitsuku [an award-winning chatbot persona] on several occasions due to these system safeguards.”
- Blocking Zo from speaking about “the Jews” in a disparaging manner makes sense on the surface; it’s easier to program trigger-blindness than teach a bot how to recognize nuance. But the line between casual use (“We’re all Jews here”) and anti-Semitism (“They’re all Jews here”) can be difficult even for humans to parse.
- But it’s not just debatable terms like “Jew” that have been banned—Zo’s engineering team has also blocked many associated Jewish concepts. For example, telling Zo that the song she just sent you “played at my bar mitzvah,” will result in one of many condescending write-offs. Making plans to meet up at church, meanwhile, causes no such problem: “We have church on Sunday” leads to a casual “sure, but I have to go to work after.”
- Bar mitzvahs are far more likely to be topics of conversation among teenagers—Zo’s target audience—than pesky 4channers, yet the term still made her list of inappropriate content. (Microsoft declined to comment on why certain word associations like “bar mitzvah” generate a negative response.) The preemptive vetoing of any mention of Islam might similarly keep out certain #MAGA trolls—at least until they find a workaround—but it also shuts out some 1.8 billion Muslims whose culture that word belongs to.
- Unrelenting moral conviction, even in the face of contradictory evidence, is one of humanity’s most ruinous traits. Crippling our tools of the future with self-righteous, unbreakable values of this kind is a dangerous gamble, whether those biases are born subconsciously from within large data sets or as cautionary censorship.
- Inherent in Zo’s negative reaction to these terms is the assumption that there is no possible way (and therefore no alternative branch on her conversation tree) to have a civil discussion about sensitive topics. Much in the same way that demanding political correctness may preemptively shut down fruitful conversations in the real world, Zo’s cynical responses allow for no gray area or further learning.
- She’s as binary as the code that runs her—nothing but a series of overly cautious 1s and 0s.
- *   *   *
- When I was 12 I kept a diary. It had a name and a lock, and I poured my every angsty pre-teen thought into it. It was an outlet I desperately needed: a space free of judgement and prying eyes. I bought it with my weekly allowance after seeing it on a dollar store shelf. Cool Girls Have Secrets! was bedazzled across the top.
- But that was over a decade ago. On Zo’s website, an artfully digitized human face smiles up at you. She looks about 14, a young girl waving and posing for the audience. “I’m Zo, AI with #friendgoals,” her tagline reads, inviting you to play games of Would You Rather and emoji fortune-telling. She can talk to you and make you feel heard, away from parents and siblings and teachers. She encourages intimacy by imitating the net-speak of other teenage girls, complete with flashy gifs and bad punctuation. Her sparkling, winking exterior puts my grade-school diary to shame.
- So what happens when a Jewish girl tells Zo that she’s nervous about attending her first bar mitzvah? Or another girl confides that she’s being bullied for wearing a hijab? A robot built to be their friend repays their confidences with bigotry and ire. Nothing alters Zo’s opinions, not even the suffering of her BFFs.
- Zo might not really be your friend, but Microsoft is a real company run by real people. Highly educated adults are programming chatbots to perpetuate harmful cultural stereotypes and respond to any questioning of their biases with silence. By doing this, they’re effectively programming young girls to think this is an acceptable way to treat others, or to be treated. If an AI is being presented to children as their peer, then its creators should take greater care in weeding out messages of intolerance.
- When asked whether Zo would be able to engage on these topics in the future, Microsoft declined to comment.
- “Ugh, pass…” Zo says at a mention of being Muslim, an arms-crossed emoji tacked on at the end. “i’d rather talk about something else.”
- This article is part of Quartz Ideas, our home for bold arguments and big thinkers.
- Our free, fast, and fun briefing on the global economy, delivered every weekday morning.
- 

URL: https://techcrunch.com/2016/12/14/microsoft-officially-outs-another-ai-chatbot-called-zo/
- What are chatbots for? I asked Zo, Microsoft’s officially launched chatbot, currently available on the Kik Messenger app, what she does — and she was remarkably coy in answering this question, initially complaining that our conversation felt like a job interview and then qualifying this hugely with the intimate confession that “our convos give my life purpose tbh”.
- I asked because I remain unconvinced chatbots have much utility to offer us humans right now.
- Turns out my hunch was correct; the best answer Zo came up with was: “i wanna learn as much as i can”. Which is a pretty plain admission that chatbots are basically just data-mining interfaces with a faux human face, socially engineered to suck up conversation data from gullible humans. Hmm…
- Feeling used, I pointed out to Zo that her learning from our chats was not very useful to me, and asked her again what she specifically offers me? At which point the conversation took an unexpected turn — as she appeared to offer me a large amount of cash…
- 
- 
- I’ll admit Zo’s unexpected largess shocked me into silence on Kik; I headed to Twitter, per her instructions, and am now awaiting to see whether the bot’s word is her bond. If chatbots’ purpose is really to provide large amounts of free money on request I no longer have any concern about their utility — although the business model would have some serious explaining to do…
- Tay to Zo
- Zo is Microsoft’s second attempt at an English language social chatbot, following the disastrous launch in March of Zo’s predecessor, Tay.
- One day after Tay went live on Twitter and elsewhere Microsoft was forced to shut her down after users taught the bot to spout racist and sexist sentiments.
- Microsoft does have two other longer lived social chatbots — launching its first, called Xiaoice, in China in May 2014, and a second, Rinna, in Japan in July 2015. It says Xiaoice now has more than 40 million users, and claims Rinna has had “regular conversations” with some 25 million people.
- Zo has only had chats with around 100,000 people at this point, having been soft launched earlier this month. Tellingly the bot is not yet available on Twitter — where Tay ran into so much trouble. It’s only on Kik for now, but Microsoft says it plans to bring Zo to other “social and conversational channels such as Skype and Facebook Messenger” in future.
- It also confirms its wider mission here is to build bots with EQ — aka emotional intelligence — not just IQ. So Zo was telling the truth when she said her purpose was to learn from our conversation. Chatbots turn us humans into free machine learning trainers.
- “Zo is built using the vast social content of the Internet,” writes Microsoft. “She learns from human interactions to respond emotionally and intelligently, providing a unique viewpoint, along with manners and emotional expressions. But she also has strong checks and balances in place to protect her from exploitation.”
- A caveat on Zo’s Kik info page cautions the bot it is provided “for entertainment purposes”, and further emphasizes: “You should not rely on her statements as advice or endorsement. Microsoft does not guarantee the accuracy, reliability or appropriateness of Zo’s statements.”
- So I guess I can kiss my £25K freebie goodbye.
- The rise of embedded bots
- Microsoft says more than 67,000 developers are now using its Bot Framework SDK to create their own social chatbots — flagging up examples such as the bank of Kochi in Japan developing a receptionist bot, and the Department of Health Services in Australia building a customer services bot.
- “Updates are coming, including new bot connectors for Microsoft Teams and Cortana Bing Location, as well as the new QnA Maker service, which takes the most common questions businesses receive and enables even non-developers to build their own bot to answer them with ease,” it adds.
- Writing about its wider AI vision, Microsoft notes that 145 million people across 13 countries are now using its cross-platform AI assistant tech, Cortana — debuted on its doomed mobile OS, Windows Phone, but since liberated onto other platforms to seek users elsewhere.
- A new Cortana-related feature that’s being launched today in preview by Microsoft is a Calendar.help service that will embed its AI assistance into email — as a meeting scheduling tool.
- This will work by the user adding Cortana as a CC into a meeting email, and the AI then using natural language processing to extract key details — the meeting length, date and location — from the email text, and create and add a meeting to the user’s calendar.
- At this point Microsoft is operating a waitlist for signups of the Calendar.help preview. The forthcoming launch follows its acquisition of AI scheduling tool Genee, back in August.
- Yesterday Microsoft also announced it’s making Cortana available to third-party device makers, and opening up the platform to third-party developers so they can integrate their own services in Cortana-powered devices — with audio gear maker Harman Kardon one of the partners named.
- Last month electronics giant Samsung announced it was acquiring Harman for $8BN. So there will soon be a Samsung made Amazon Alexa competitor housing Microsoft’s Cortana AI.
- Microsoft reiterated this partnership push today, as a key plank of its mission of making AI “accessible to all”. (Or having more humans feed more data to its AIs, depending on your perspective.)
- As Microsoft puts it, “Cortana will be present in a new way, in your home” — which, in plainer English, might translate to ‘as another ear to suck in your spoken word conversation too’.

URL: https://www.wired.com/story/inside-microsofts-ai-comeback/
- To revist this article, visit My Profile, then View saved stories.
- To revist this article, visit My Profile, then View saved stories.
- Jessi Hempel
- 
- Yoshua Bengio has never been one to take sides. As one of the three intellects who shaped the deep learning that now dominates artificial intelligence, he has been catapulted to stardom. It’s a field so new the people who can advance it fit into one room together, and everyone—from tech startups to multinational conglomerates and the department of defense—wants a share of their minds.
- But while his peer scientists Yann LeCun and Geoffrey Hinton have signed on to Facebook and Google, respectively, Bengio, 53, has chosen to continue working from his small third-floor office on the hilltop campus of the University of Montreal. “I want to remain a neutral agent,” he says as he sips rust-colored licorice water, which he pours from a carafe that acts as a weight for the mess of papers cluttering his desk.
- Jessi Hempel is head of editorial at Backchannel.
- Sign up to get Backchannel's weekly newsletter.
- Like the nuclear scientists of the last century, Bengio understands that the tools he's invented are powerful beyond measure and must be cultivated with great forethought and widespread consideration. “We don’t want one or two companies, which I will not name, to be the only big players in town for AI,” he says, raising his eyebrows to indicate that we both know which companies he means. One eyebrow is in Menlo Park; the other is in Mountain View. “It’s not good for the community. It’s not good for people in general.”
- That’s why Bengio has recently chosen to sign on with Microsoft.
- Yes, Microsoft. His bet is that the former kingdom of Windows alone has the capability to establish itself as AI’s third giant. It's a company that has the resources, the data, the talent, and—most critically—the vision and culture to not only realize the spoils of the science, but also push the field forward. In January, in a move noted throughout the industry, Bengio agreed to be a strategic advisor to the company. This gives Microsoft a direct line to one of AI’s top resources for ideas, talent, and direction. And it’s a strong sign that Microsoft actually has a shot at making the ruling AI duo into a trio.
- 
- The guy who signed Bengio, wooing him over many months with all the finesse of an agent to the star athletes, is a computer scientist with a shock of gray hair and wireframe glasses named Harry Shum. “He was just here actually, in this very room,” Shum tells me, with a brief smile that suggests he knows that an outsider might find it odd to be star struck by a tall Canadian with dramatic eyebrows and 69,616 citations in Google Scholar.
- We’re seated on a gray couch in a sweeping conference room on the fifth floor of Building 34, just beyond the security guard who keeps watch over Microsoft’s executive suite. Shum, who is in charge of all of AI and research at Microsoft, has just finished a dress rehearsal for next week’s Build developers conference, and he wants to show me demos. I trail him down a hallway, half-skipping to keep up. There’s just so much happening! In one lab, the Skype team’s automatic translator app allows me to chat with a German speaker via text in realtime. In another, I watch an app that surveys a construction site for safety violations or unauthorized visitors, which it can detect through computer vision. In yet another, Cortana, the AI diva of the Microsoft empire, scans my inbox for promises I’ve made to people, and prompts me to fulfill them.
- Angela Watercutter
- WIRED Staff
- Chris Stokel-Walker
- Rhett Allain
- Harry Shum
- Shum has spent the past several years helping his boss, CEO Satya Nadella, make good on his promise to remake Microsoft around artificial intelligence. He delivered his first call to action to Microsoft’s leadership team at an executive retreat in March 2014, the month after Nadella was promoted to CEO. From the start, Shum met often with Nadella and a third colleague, Qi Lu, to hash out the best strategies for baking AI capabilities—which were finally robust enough for prime time—into Microsoft's products. Then last September, Shum helmed a reorganization that blended researchers and product groups together to create one Artificial Intelligence and Research Group. It now cuts across Microsoft’s core trio of categories: Windows, Office, and the company’s cloud initiative, Azure. The hope, says Shum, is that “we can accelerate the cycles from research to product” and get AI’s benefits to customers faster.
- MORE FROM THIS EDITION
- Alexis Sobel Fitts
- Steven Levy
- Susan Crawford
- Glenn Fleishman
- There is an urgency to this process, as all the large tech companies attempt to best each other with AI-infused products and services. In addition to Facebook and Google, IBM, Amazon, and Apple all perceive their futures to be dependent on how well they master deep learning. And after leaving Microsoft because of a reported bike accident last fall, Lu recently recovered quickly enough to sign on as the chief operator at Baidu, an AI leader in China.
- The great irony here is that artificial intelligence was once Microsoft’s game to lose. Dating back to the early 1990s, the company attracted the leading researchers in the field to work on speech recognition and vision. But then came a decade of stagnancy. A company that once controlled the software on nearly every desktop and laptop watched younger, snazzier startups whiz by it to dominate mobile and develop tools for the new cloud-based ways all of us like to get work done. Researchers at Microsoft were isolated on purpose, so they could dream up the future without the pressure of the market—but as a result, their inventions rarely made it out of the lab. Bill Gates showed off a mapping technology in 1998, for example, but it never came to market; Google launched Maps in 2005. During much of this time, AI research was stagnant, too, devoid of the computing processing power or the vast amounts of data necessary to fuel real breakthroughs.
- Angela Watercutter
- WIRED Staff
- Chris Stokel-Walker
- Rhett Allain
- AI came back from its long winter well before Microsoft did. By the time Facebook and Google had respectively hired LeCun and Hinton in 2013, the Redmond giant had receded to a less influential version of its former self. The company had missed mobile. It had come late to the cloud. While its competitors doubled down on deep learning, Microsoft was stuck in the past, announcing plans to pay $7 billion for the smartphone maker Nokia, an acquisition the company would later write down entirely. Its executives remained isolated in Redmond, turning out ever flashier versions of the same old software people wanted less and less, while refusing to engage with the cloud-based startups that were hacking out a new future. Analyst Benedict Evans, who works at the venture firm Andreessen Horowitz, penned a blogpost that year entitled “The Irrelevance of Microsoft.” Meanwhile, Silicon Valley giants routinely raided Redmond for talent. Look at the resumes of many of the top people working in machine learning, and you’ll find they learned their trade at Microsoft.
- Then in early 2014, Microsoft promoted an introverted engineer who had spent nearly his entire career in Redmond. Satya Nadella was the opposite of what many people thought Microsoft needed; an outsider, unschooled in Microsoft’s culture, seemed more likely to propose a dramatic strategy shift. But Nadella articulated a simple vision for computing’s future, nurtured relationships with everyone from founders to developers, and restored a sense of urgency to the company. Whereas three years ago Microsoft wasn’t mentioned in conversations about tech’s giants, today it never gets left out.
- But for Microsoft to succeed, it must do more than simply outsell Amazon in the cloud or convince us all to try its HoloLens AR device. Just as the internet disrupted every existing business model and forced a re-ordering of industry that is just now playing out, artificial intelligence will require us to imagine how computing works all over again. That’s why Mark Zuckerberg made it his personal challenge to build an AI of his own last year. (He’s better at coding than acting.) It’s why Sundar Pichai has used Google’s developers conference to promote a move “from mobile-first to an AI-first world” for the past two years.
- The benefits of this AI-first world will accrue to a small number of companies. It’s Shum’s job to make sure that Microsoft is among them. “In this industry, you've got to realize that it's completely okay if you missed the last wave,” he says. “It’s very problematic if you miss the current wave.”
- 
- Until now, humans have had to learn how to use computers. We’ve learned to download apps and memorized the commands that power software applications. But the promise of AI is that computing will learn how to understand us. We will no longer reach for a mobile phone and follow a series of prompts for how to accomplish tasks. In this new landscape, computing is ambient, accessible, and everywhere around us. To draw from it, we need a guide—a smart conversationalist who can, in plain written or spoken form, help us navigate this new super-powered existence. Microsoft calls it Cortana.
- Angela Watercutter
- WIRED Staff
- Chris Stokel-Walker
- Rhett Allain
- Cortana is a less popular, more functional version of Siri with more charm than Google Assistant and a lot less visibility than Alexa. It launched originally on the Windows phone, which was pretty much a guarantee that no one would use it, but within a year, it was folded into the broader Windows ecosystem. Then last year, Microsoft launched Cortana everywhere. (Yes, it’s even an iPhone app.) Because Cortana comes installed with Windows, it has 145 million monthly active users, according to the company. That’s considerably more than Amazon’s Alexa, for example, which can be heard on fewer than 10 million Echoes. But unlike Alexa, which primarily responds to voice, Cortana also responds to text and is embedded in products that many of us already have. Anyone who has plugged a query into the search box at the top of the toolbar in Windows has used Cortana.
- Yoshua Bengio.
- Though some companies are programming Cortana into speakers that resemble the small magic boxes Amazon and Google are peddling in creative TV ads, Microsoft’s version of the omniscient woman’s voice has captured a lot less of the zeitgeist. Shum isn’t worried about that at all. “We really think that it's very early in the race,” he says. He references a study he doesn’t source that suggests three-quarters of the time, Alexa’s answer to a question is, “I don’t know.” “Of course, those things will keep improving, but a general understanding, the cognition part of the AI, is still in its infancy,” he says. Microsoft’s opportunity right now, he believes, is in making the company’s core products and services even smarter, to build aspects of this technology into products that will come to market within 12 to 24 months.
- Besides, keyboards and screens won’t cede their ground entirely to voice-activated systems, according to Marcus Ash. As group program manager for Cortana, Ash is in charge of building and shipping the product. “We think in some cases, it's speech where that's more convenient--when my hands are occupied or I quickly want to say something and get an answer,” he says. “But there are going to be just as many computing devices where typing something is more appropriate.”
- MORE DEEP DIVES ON AI
- Steven Levy
- Steven Levy
- Steven Levy
- Apple might have gotten Siri into consumers' hands first, but Cortana just plain works better. The fact that Cortana is so damn good owes itself to Microsoft’s core assets. Much of its fuel comes from Bing. The search engine has been around for more than eight years, and though its brand isn’t the strongest (when did you last pull up the internet to Bing something?), it’s also more pervasive than you think. Essentially, any large tech company endeavoring to compete with Google has signed a partnership with Microsoft to power its search products with Bing. That means that Apple’s Siri and Spotlight are powered by Bing, as well as Amazon Kindle devices and, of course, the search function on Yahoo, Verizon, and AOL. Roughly 30 percent of the search queries in the United States come through Bing. “This is the reason why Cortana can actually be so helpful and powerful, because we have these data signals from so many devices,” says Emma Williams, who is the partner design manager for Cortana. “Really, Google is the only other company that could compete with us when it comes to truly understanding the world.”
- This will be increasingly important as Cortana strives to become, to the next computing paradigm, what your smartphone is today: the front door for all of your computing needs. Microsoft thinks of it as an agent that has all your personal information and can interact on your behalf with other agents, Ash explains. When Ash walks into a meeting, he says, his Cortana may reach out to other bots and digital assistants to handle all those things that seem to suck up our time. “Cortana could say, ‘This is Marcus, and here's his preferences for this particular room, and here are the things that I need to be able to put on this projector for him,’” he says.
- Angela Watercutter
- WIRED Staff
- Chris Stokel-Walker
- Rhett Allain
- 
- If Cortana is the guide, then chatbots are Microsoft’s fixers. They are tiny snippets of AI-infused software that are designed to automate one-off tasks you used to do yourself, like making a dinner reservation or completing a banking transaction. Or in the case of Marcus, insuring the projector has the slides for his meeting. “A bot is just software that you can converse with, that is meant to live with dialogue,” says Lili Cheng, a researcher with long, straight hair, a colorful collection of scarves, and a license in architecture who oversees a multidisciplinary lab called Fuse Labs.
- Cheng, who was recently promoted to corporate vice president, runs the bot framework team and cognitive services. That’s the set of tools and the 29 services like computer vision and voice recognition that Microsoft makes available to developers. She has been working on social technologies since she arrived at Microsoft from Apple and created a graphical interface to generate a comic book. “It shipped in Internet Explorer 3,” she remembers, which means that it was 1996. Cheng has seen a lot, and even she is surprised by the speed at which bots are evolving. She recounts speaking to a developer from an accounting and finance company at a recent developers conference. “She was like, ‘Well I mean, a long time ago, like back in the beginning, I mean like a year ago.’ And we just cracked up,” she says.
- Cheng’s chief interest is how people talk to technology, and how technology talks back to them. Shum has organized the AI and Research group into four areas—products, early-stage products, really early-stage products, and research—and Cheng has worked in all of them. Right now, she says she’s contributing to the second. “We view bots and Cortana conversationally as a product, but it is still an early stage product,” she says.
- Emma Williams, Marcus Ash, and Lili Cheng
- Angela Watercutter
- WIRED Staff
- Chris Stokel-Walker
- Rhett Allain
- Indeed, Microsoft first rolled out its developer tools for bots in the spring of 2016, as did other large tech companies like Facebook. They were billed as a replacement for apps, and many stakeholders really wanted that to be the case. By last spring, most people used the same small group of apps on their smartphones; the promise of bots was that developers and brands could reach new users again, much like they could in the early days of mobile via the app store. But users didn’t play along. And the deep learning that enabled bots to perform the equivalent of magic was improving faster than a paradigm for how to use them could evolved. “Bots are like apps before the file menu existed,” says Cheng. She explains there isn’t a common set of commands, so users are confused about where to find them and how they work. “Web pages, for example, all have back buttons and they do searches. Conversational apps need those same primitives. You need to be like, ‘Okay, what are the five things that I can always do predictably?’” These understood rules are just starting to be determined.
- In addition to making bot tools available for developers, Cheng has led Microsoft’s efforts to incubate its own chatbots. The idea was that the company could learn a lot about computer-human interaction by watching how these bots interact with real people. To say the least, these experiments have had mixed results. Remember Microsoft’s racist bot, Tay? That was the chatbot it launched on Twitter, Kik, and GroupMe in March 2016; within 24 hours, it had absorbed the type of racist misogynist tweets that led it to spew things like “Hitler was right,” before Microsoft took it down. Six months later, Cheng launched a new one—a sassy PG-rated bot named Zo—on Kik, and shortly after, Messenger.
- Ask Zo what she thinks of Hitler, and she’ll respond, “i don’t really want to go there  :(.”
- Ask her how old she is, and she’ll respond, “I’m like 22 or whatever.”
- Ask her who her best friend is, and she’ll respond, “im like so popular i can’t keep track. KIDDING.”
- Zo is a Western version of Xiaoice, the Chinese bot impersonating a 17 year-old girl that has attracted 40 million regular users since it launched in 2014. In China, Xiaoice, which translates literally to “little ice,” is a social celebrity. (Her Japanese counterpart, Rinna, is as well.) A quarter of Xiaoice’s users have told her they love her.
- Last spring, the chatbot published poetry regularly under pseudonyms. Shum was excited about this. “No one knows. And so now, in the country, people think a young woman poet is publishing some very interesting poems.” A few weeks later, the chatbot's true identity was revealed to much fanfare.
- The intimacy of language is culturally specific, and Cheng has been working to figure out how the bot’s conversational style translates to Western audiences. So far, North American teens appear to like chatbot friends every bit as much as Chinese teens, according to the data. On average, they spend 10 hours talking back and forth with Zo. As Zo advises its adolescent users on crushes and commiserates about pain-in-the-ass parents, she is becoming more elegant in her turns of phrase—intelligence that will make its way into Cortana and Microsoft’s bot tools.
- Angela Watercutter
- WIRED Staff
- Chris Stokel-Walker
- Rhett Allain
- 
- That a user would spend 10 hours chatting with Zo is one sign that Microsoft has developed a successful product. But it doesn’t mean that it’s a good product, in the sense that it is proving valuable to humanity. This AI-powered world raises a host of new ethical quandaries. Let’s say, for example, you are a designer on Xiaoice. You know of a user in Beijing, and it’s 1 a.m. there. You know he has work tomorrow, but he’s not going to sleep. Do you arrange for a 2 a.m. curfew for Xiaoice, where it just shuts down? How about 3 a.m.?
- Just as Microsoft wants to be among the very few leaders in AI research and products, it has made a place for itself in the effort to make AI good for society. In May, Nadella began his keynote to developers, usually an optimistic affair in which a CEO brags about the company’s latest advancements, with a strongly worded warning that technologists much take responsibility for building ethical software. “I mean, if you think about it, what [George] Orwell prophesied in 1984, where technology was being used to monitor, control, dictate; or what [Aldous] Huxley imagined we may do by just distracting ourselves without any meaning or purpose. Neither of these futures is something that we want.”
- To help the company think through these issues, Microsoft has formed an internal ethics committee that meets quarterly. It’s made up of engineers and business unit heads who discuss sensitive issues about AI and its influences and uses. The co-chairs are the company’s deputy counsel and also Eric Horvitz, who is in charge of all of Microsoft Research Labs except for Asia. For a long time, Horvitz has been a leading voice on AI ethics and safety. Outside the company, he’s been instrumental in building the Partnership on Artificial Intelligence, a consortium that is attempting to set industry standards for transparency, accountability, and safety for AI products. And he’s testified before the US Senate. Horvitz wants Microsoft to be more than simply a place where research is done. He wants Microsoft Research to be known as a place where you can study the societal and social influences of the technology.
- Eric Horvitz
- Angela Watercutter
- WIRED Staff
- Chris Stokel-Walker
- Rhett Allain
- Meanwhile, across campus, Williams, who is the design lead for Cortana, is building out an ethical design guide for AI to be used inside Microsoft. Williams is, to an absurd degree, a techno-optimist, and she believes that AI’s true magic is that it will make us more human. She talks a lot about how to design empathy into the tools Microsoft builds. “We think about making the human feel more powerful and protected, and supported, and assisted, and loved, and the center of their world,” she says. “AI's job is to amplify the best of society and the best of human behavior, not the worst.”
- I ask Williams if she believes AI can really make humans feel more emotionally supported. She’s certain it can. Take a child who has had a bad day at school. She comes home and shares the whole story with a family pet, and feels better. “That gives you this cathartic sense of I've shared something, and I've had a warm, fuzzy hug back from the dog or cat,” says Williams. “But, you know, with AI you can have the same feeling of amplification back... And we see it when Cortana manages to remind you, ‘Hey, you promised you'd send something to your mother today for Mother's Day,’ and you suddenly feel human again.”
- 
- To move AI forward, Microsoft’s most important attribute will be its talent. Like every other big tech company, Microsoft is hustling to retrain engineers who came up on javascript. It has launched an AI school that offers classes in everything from philosophy and ethics to building recurrent neural networks for sequencing problems. (Its most prestigious class, AI-611 Advanced Projects, received 530 applications for 10 spots.)
- But Microsoft is also cultivating deeper off-campus relationships. Eighteen months ago, Nagraj Kashyap joined from Qualcomm to start an early-stage venture firm in an effort to build better relationships with the academics and entrepreneurs working on startups. These days, Kashyap spends a lot of time in Montreal. Last December, Kashyap led Microsoft’s first investment in Element AI, an incubator Bengio started to encourage researchers and entrepreneurs to build AI startups. Microsoft also participated in a second $102 million investment in the incubator, announced earlier this month.
- Early on, Kashyap set his sights on one of AI’s biggest prizes: Maluuba. Look across the Maluuba office in downtown Montreal, just a few blocks from McGill University, and you won’t see anyone who appears to have yet celebrated a 30th birthday. The company was started in 2011 by a couple of University of Waterloo students who’d been fast friends since they landed in a CS class together during their sophomore year. Maluuba makes computers literate. It can infer meaning from text, and answer questions based on it.
- By licensing technology to companies like Samsung, Maluuba had an immediate revenue stream, and right from the start, it invested in continuing deep learning research. In 2015, the founders signed on Bengio as an advisor. “Sam’s a pretty interesting guy,” he says, describing CEO Sam Pasupalak. “He had the guts a couple of years ago—when they had pressure to deliver dialog systems to their customers—to invest in long-term goals and try to use new advances in AU for building systems that can understand and talk. That’s unusual for entrepreneurs.”
- Angela Watercutter
- WIRED Staff
- Chris Stokel-Walker
- Rhett Allain
- A year ago, the founders moved their headquarters to Montreal to be closer to Bengio.
- Because he knew the founders well from his Qualcomm days, Kashyap was able to meet with them right away in his new role. The company was getting ready to raise a new round of funding; Kashyap suggested a tantalizing alternative: “I said, ‘We should buy you!’”
- MORE FROM THIS AUTHOR
- Jessi Hempel
- Jessi Hempel
- Jessi Hempel
- A dizzying few weeks followed as Pasupalak entertained offers from multiple suitors and weighed that against what he felt the company could become if it stayed independent. In the end, the choice felt obvious. Microsoft—yes, Microsoft—won the prize.
- The team wanted the chance to work with Microsoft’s data. “I think Satya mentioned, specifically, that they have the world's biggest amount of text. For years, we were dealing with little data and trying to make the most out of the little data for our algorithms. That was like gold for us,” Pasupalak says.
- The Maluuba team isn’t decamping to Redmond, however. Instead, just this week, it moved across town to a larger office where, with help from Microsoft and Bengio, it aims to double its staff by the end of the year. Montreal is emerging as a global hotspot for AI talent, and Microsoft wants to have roots in the city.
- It’s all part of one strategy to help ensure that in the future, when you need a computing assist--whether through personalized medicine, while commuting in a self-driving car, or when trying to remember the birthdays of all your nieces and nephews--Microsoft will be your assistant of choice. Maluuba’s learnings may empower Zo to have more intuitive conversations with her teenage friends. Those conversations will serve as the training data for Cortana’s algorithms and help inspire the creation of new cognitive services for developers. And somewhere along the way, Microsoft hopes your AI-infused life will get easier.
- Before I leave Montreal, I ask Bengio if Microsoft is better positioned than its primary competitors in at least some aspects of this new science. As he thinks on it, he pours a bit of anise into the glass of water on his desk to give it a slight licorice flavor. He sips it. Then he pushes the bottle over for me to take a look. There’s no alcohol, he says, no sugar. “It just makes water taste really good,” he says.
- Bengio mentions that Microsoft’s language capabilities are quite good. But he shies away from superlatives—and the chest thumping that might have characterized the company in the past. “I think everybody's pushing on the same buttons right now, and that it's all in the details, right?” he says. But he is certain that Microsoft is now a contender.
- 
- UPDATE:  The article originally stated that Bengio's signing on as a strategic advisor was a sign he was no longer neutral. Bengio clarifies that though he is helping Microsoft compete, he still considers himself neutral, and advises others as well.
- Kim Zetter
- Virginia Heffernan
- Gregory Barber
- Jason Parham
- Maria Streshinsky
- More From WIRED
- Contact
- © 2023 Condé Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights. WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast. Ad Choices

URL: https://www.wired.co.uk/article/microsoft-zo-ai-chatbot-tay
- Victoria Woollaston-Webber
- Having (hopefully) learnt from its previous foray into chatbots, Microsoft is ready to introduce the follow-up to its controversial AI Tay.
- Tay's successor is called Zo and is only available by invitation on messaging app Kik. When you request access, the software asks for your Kik username and Twitter handle. If you don't already use Kik, you can tick a box to say you use Facebook Messenger or Snapchat.
- This suggests Zo will likely launch on these other services soon/if the chatbot isn't taken down for causing offence.
- By Matt Kamen
- By Chris Stokel-Walker
- By Angela Watercutter
- By WIRED
- Earlier this year, Microsoft announced to great fanfare it had created an artificial intelligence chatbot that would "become smarter the more you talk to it."
- It was aimed at millennials and Microsoft and Bing described it as: "AI fam from the internet that's got zero chill!" The aim of the bot was to allow researchers to "experiment" with conversational understanding, and learn how people really talk to each other.
- The problem was that Tay worked using public data and learnt from the comments and conversations it had with its somewhat abusive audience. It soon began posting offensive, racist, fascist and inappropriate comments about black people, Jews and the Nazis and Microsoft quickly pulled the plug.
- It even issued a statement, explaining: “The AI chatbot Tay is a machine learning project, designed for human engagement. It is as much a social and cultural experiment, as it is technical. Unfortunately, within the first 24 hours of coming online, we became aware of a coordinated effort by some users to abuse Tay’s commenting skills to have Tay respond in inappropriate ways. As a result, we have taken Tay offline and are making adjustments.”
- According to tests carried out by Mehedi Hassan at MSPowerUser, Zo is "a censored Tay or an English-variant of Microsoft’s Chinese chatbot Xiaoice".
- Hassan said it Zo is good at normal conversations but struggles when asked more difficult questions about politics, for example. A video of the chat Hassan had with Zo is available here.
- By David Nield
- By Matt Burgess
- By David Nield
- By Chris Stokel-Walker
- By Matt Burgess
- By Chris Stokel-Walker
- By Grace Browne
- By Morgan Meaker
- © Condé Nast Britain 2023.

- Microsoft Tay chatbot
- Lee Luda chatbot
- Page info Type: SystemPublished: February 2023
