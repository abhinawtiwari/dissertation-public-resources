- Occurred: July 2019
- Can you improve this page?Share your insights with us
- NotJordanPeterson.com, a website for generating AI audio clips of Jordan Peterson saying whatever you want has shut down after the controversial psychologist threatened legal action.
- Vistors to the site were asked to type fewer than 280 characters of text into a box that would be fed into a neural network trained on Peterson's actual voice. The result sounded uncannily genuine.
- Needless to say, journalists and others prompted the site to make offensive and outrageous remarks, including reading some passages from Valerie Solanas, a feminist author who wrote the anti-capitalist, anti-male SCUM Manifesto.
- The real Peterson responded by slamming deepfakes and voicing his concern that they 'need to be stopped, using whatever legal means are necessary.'
- 'In light of Dr. Peterson's response to the technology demonstrated by this site…and out of respect for Dr. Peterson, the functionality of the site will be disabled for the time being,' the site creator wrote.
- Operator: Chris Vigorito Developer: Chris Vigorito Country: Canada
- Sector: Education
- Purpose: Damage reputation
- Technology: Deepfake - audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning Issue: Privacy; Ethics; Mis/disinformation
- Transparency: Governance; Marketing
- NotJordanPeterson website
- Chris Vigorito (2019). The Strange Future of Digital Media
- Gregory S., Cizek K. (2021). Just Joking. Deepfakes, Satire, and the Politics of Synthetic Media
- Bateman J. (2020). Deepfakes and Synthetic Media in the Financial System: Assessing Threat Scenarios
- Kietzmann J., Lee L.W., McCarthy I.P., Kietzmann T.C., (2020). Deepfakes: Trick or treat?
- Reclaim the Net (2019). Jordan Peterson deepfake voice simulator taken offline after Peterson suggests legal action
URL: https://www.jordanbpeterson.com/blog-posts/i-didnt-say-that/
- Can't find what you need? Take a moment and do a search below!
- Contact Us
- Jordan Peterson Mailing List Signup:   Leave this field empty if you're human:
- Jordan Peterson Mailing List Signup:
- 

URL: https://nationalpost.com/opinion/jordan-peterson-deep-fake
- I can tell you from personal experience how disturbing it is to discover a website devoted to making fake audio clips of you — for comic or malevolent purposes
- Something very strange and disturbing happened to me recently. If it was just relevant to me, it wouldn’t be that important (except perhaps to me), and I wouldn’t be writing this column. But it’s something that is likely more important and more ominous than we can even imagine.
- Enjoy the latest local, national and international news.
- Enjoy the latest local, national and international news.
- Create an account or sign in to continue with your reading experience.
- Don't have an account?  Create Account
- There are already common fraudulent schemes being perpetrated by both telephone and internet. One known as the “Grandparent Scam” is particularly reprehensible, first because it is perpetrated on elderly people who are, in general, more susceptible to tech-savvy criminals and second because it is based on the manipulation of familial love, trust and compassion. The criminal running the Grandparent Scam calls, or emails the victim, pretending to represent a grandchild who is now in trouble with the law or who needs money for a hospital bill for an injury that can’t be discussed, say, with parents, because of the moral trouble that might ensue. They generally call late at night — say at four in the morning — because that adds to the confusion. The preferred mechanism of money movement is wire transfer — and that’s a warning: don’t transfer money by wire without knowing for certain who is receiving it, because once it’s gone, it’s not coming back.
- Now what if it was possible to conduct such a scam using the actual voice of the hypothetical victim? Worse, what if it was possible to do so with voice and video image, indistinguishable from the real thing? If we’re not at that point now (and we probably are) we will be within months.
- In April of this year, a company called Coding Elite exposed an artificial intelligence (AI) program that took a substantial sample of my voice, which is easily accessible on the YouTube lectures and podcasts that I have posted over the last years. In consequence, they were able to duplicate my manner of speaking with exceptional precision, starting out by producing versions of me rapping Eminem songs such as Lose Yourself (which has now garnered 250,000 views) and Rap God (which has only garnered 17,000) as well as Rock Lobster (1,400 views). They have done something similar with Bernie Sanders (singing Dancing Queen), Donald Trump (Sweet Dreams) and Ben Shapiro, who also delivered Rap God. The company has a model, the address of which you can find on their YouTube channel, which allows the user to make Trump, Obama, Clinton or Sanders say anything whatsoever.
- It’s hard to imagine a technology with more power to disrupt
- I happen to think Rap God is an amazing piece of work, and when I first encountered my verbal avatar belting out the lyrics I thought that it was cool, in a teenage tech-geek sort of way. And I suppose it was. This caused quite a stir on the net in April, with media companies such as Forbes and Motherboard (a division of Vice) noting that the machine learning technology only required six hours of original audio (that is, actually generated by me) to produce its credible fakes, matching rhythm, stress, sound and prose intonation.
- Recently, however, a company called notjordanpeterson.com put an AI engine online that allows anyone to type anything and have it reproduced in my voice. It’s hard to get access to or use the site, at the moment, presumably because it is currently attracting more traffic than its servers can handle. A variety of sites that pass themselves off as news portals — and sometimes are — have either reported this story straight (Sputnik News) or had a field day (Gizmodo) having me read, for example, the SCUM manifesto (hypothetically an acronym for Society for Cutting Up Men), a radical feminist rant by Valerie Solanas published in 1967. Solanas, by the way, later shot the artist Andy Warhol, an act, driven by her developing paranoia. He was seriously wounded, requiring a surgical corset to hold his organs in place for the rest of his life. TNW takes a middle path, reporting the facts of the situation with little bias but using the system to have me voice very vulgar phrases.
- Some of you might know — and those of you who don’t should — that similar technology has also been developed for video. This was reported, for example, by the BBC, as far back as July 2017, when it broadcast a speech delivered by an AI Obama, that was essentially indistinguishable from the real thing. Similar technology has been used, equally notoriously, to superimpose the faces of famous actresses on porn stars, while they perform their various sexual exploits. Movies have also been reshot so that the main actor is transformed from someone unknown to someone with real box office draw. This has happened, for example, to Nicolas Cage, primarily on a YouTube site known as Derpfakes, a play on “deepfakes,” which is what the video recordings created fraudulently by AI have come to be known. More recently Ctrl Shift Face, a YouTube channel, posted a video showing Bill Hader transforming very subtly into Tom Cruise as he performs an impression of the latter on Dave Letterman’s show. It’s picked up four million views in a week. It’s important to note that this ability is available to amateurs. I don’t mean people with no tech knowledge whatsoever, obviously — more that the electronic machinery that makes such things possible will soon be within the reach of everyone.
- It’s hard to imagine a technology with more power to disrupt. I’m already in the position (as many of you soon will be as well) where anyone can produce a believable audio and perhaps video of me saying absolutely anything they want me to say. How can that possibly be fought? More to the point: how are we going to trust anything electronically mediated in the very near future (say, during the next presidential election)? We’re already concerned, rightly or wrongly, with “fake news” — and that’s only news that has been slanted, arguably, by the bias of the reporter or editor or news organization. What do we do when “fake news” is just as real as “real news”? What do we do when anyone can imitate anyone else, for any reason that suits them?
- And what of the legality of this process? It seems to me that active and aware lawmakers would take immediate steps to make the unauthorized production of AI deepfakes a felony offence, at least in the case where the fake is being used to defame, damage or deceive. And it seems to be that we should perhaps throw caution to the wind, and make this an exceptionally wide-ranging law. We need to seriously consider the idea that someone’s voice is an integral part of their identity, of their reality, of their person — and that stealing that voice is a genuinely criminal act, regardless (perhaps) of intent. What’s the alternative? Are we entering a future where the only credible source of information will be direct personal contact? What’s that going to do to mass media, of all types? Why should we not assume that the noise to signal ratio will creep so high that all political and economic information disseminated broadly will be rendered completely untrustworthy?
- I can tell you from personal experience, for what that’s worth, that it is far from comforting to discover an entire website devoted to allowing whoever is inspired to do so to produce audio clips imitating my voice delivering whatever content the user chooses — for serious, comic or malevolent purposes. I can’t imagine what the world will be like when we will truly be unable to distinguish the real from the unreal, or exercise any control whatsoever on what videos reveal about behaviours we never engaged in, or audio avatars broadcasting any opinion at all about anything at all. I see no defense, and a tremendously expanded opportunity for unscrupulous troublemakers to warp our personal and collective reality in any manner they see fit.
- Wake up. The sanctity of your voice, and your image, is at serious risk. It’s hard to imagine a more serious challenge to the sense of shared, reliable reality that keeps us linked together in relative peace. The deepfake artists need to be stopped, using whatever legal means are necessary, as soon as possible.
- (NOTE: As of August 23, the website notjordanpeterson.com posted the following announcement: “In light of Dr. Peterson’s response to the technology demonstrated by this site, which you can read here, and out of respect for Dr. Peterson, the functionality of the site will be disabled for the time being.”)
- Jordan Peterson is a professor of psychology at the University of Toronto, a clinical psychologist and the author of the multi-million copy bestseller 12 Rules for Life: An Antidote to Chaos. His blog and podcasts can be found at jordanbpeterson.com.
- Postmedia is committed to maintaining a lively but civil forum for discussion and encourage all readers to share their views on our articles. Comments may take up to an hour for moderation before appearing on the site. We ask you to keep your comments relevant and respectful. We have enabled email notifications—you will now receive an email if you receive a reply to your comment, there is an update to a comment thread you follow or if a user you follow comments. Visit our Community Guidelines for more information and details on how to adjust your email settings.
- Gifts for new dads, grandfathers and everything in between
- Don't miss this deal on Microsoft Office's full suite of applications and tools
- A discussion of the causes of dandruff and some of the products that can help control it
- Five swimsuits to help kickstart your search.
- When it comes to one-and-done outfits, it doesn't get much better than a dress.
- 365 Bloor Street East, Toronto, Ontario, M4W 3L4
- © 2023 National Post, a division of Postmedia Network Inc. All rights reserved. Unauthorized distribution, transmission or republication strictly prohibited.
- This website uses cookies to personalize your content (including ads), and allows us to analyze our traffic. Read more about cookies here. By continuing to use our site, you agree to our Terms of Service and Privacy Policy.

URL: https://www.youtube.com/watch?v=ZYiWdhNQi2g

URL: https://www.vice.com/en_us/article/43kwgb/not-jordan-peterson-voice-generator-shut-down-deepfakes
- The owner of NotJordanPeterson.com, a website for generating convincing clips of Jordan Peterson saying whatever you want using AI, shut down their creation this week after the real Peterson announced his displeasure and raised the possibility of legal action.
- While the site was up, a 21-second recording greeted visitors to the site, saying in Peterson's voice, "This is not Jordan Peterson. In fact, I'm a neural network designed to sound like Dr. Peterson." The clip implored the visitor to type some text into a box, that would be fed into a neural network trained on hours of Peterson's actual voice, and generated into audio that sounded a lot like the real thing.
- "The Deep Fake artists need to be stopped, using whatever legal means are necessary, as soon as possible."
- Several media outlets tested the program and published the results, making him pantomime feminist texts and vulgarities. Aside from the outrageous content, the results sounded a lot like the real thing.
- It turns out that Peterson—a controversial Canadian professor known for his lectures defending the patriarchy and denying the existence of white privilege while decrying "postmodern neo-Marxists,"—did not find NotJordanPeterson.com flattering.
- "Something very strange and disturbing happened to me this week," Peterson wrote on his website. "If it was just relevant to me, it wouldn’t be that important (except perhaps to me), and I wouldn’t be writing this column about it. But it’s something that is likely more important and more ominous than we can even imagine."
- He then goes on to spend over 1,300 words decrying deepfakes—algorithmically-generated face-swapped videos, not fake audio but sometimes combined with fake voices—as a threat to politics, personal privacy, and veracity of evidence, and ends with a vague allusion toward making fake audio and video illegal. Or, possibly, suing creators.
- "Wake up. The sanctity of your voice, and your image, is at serious risk," he wrote. "It’s hard to imagine a more serious challenge to the sense of shared, reliable reality that keeps us linked together in relative peace. The Deep Fake artists need to be stopped, using whatever legal means are necessary, as soon as possible."
- After Peterson published this blog post, the NotJordanPeterson website shut down operations. "In light of Dr. Peterson's response to the technology demonstrated by this site…and out of respect for Dr. Peterson, the functionality of the site will be disabled for the time being," the site owner wrote.
- The site owner told Motherboard that despite Peterson's hinting at legal action in his blog, Peterson isn't suing him, and he took NotJordanPeterson down after he saw his negative reaction. At the time of publication, Peterson has not responded to Motherboard's request for comment.
- It's interesting to see a public figure like Peterson address deepfakes so directly. Plenty of other celebrities have been subject to the algorithmic face-swap and fake-audio treatment, including podcast host Joe Rogan, Nicholas Cage, and Elon Musk.
- The AI models that generate fake video or audio rely on a huge amount of existing data to analyze and "learn" from. As it happens, refusing to shut the fuck up—as so many powerful men are wont to—is great training material for an AI algorithm to train a realistic model of someone on.
- Before Peterson, the closest any powerful men have come to commenting on deepfakes as a phenomenon is Mark Zuckerberg, after an artist created a deepfake of him saying some insidious things. The media coverage of that satirical art project forced his platform to enact policies around handling fake video content.
- But what Peterson is implying in this screed—that deepfakes, even as art, should be stopped, banned, and otherwise made illegal—is something legislators and AI ethicists have grappled with since the dawn of deepfakes two years ago. Many experts say that regulating deepfakes is a bad idea, because trying to do so could chill First Amendment rights and free speech online.
- Peterson mentions Rep. Yvette Clark's proposed DEEPFAKES Accountability Act as a potential solution to his embarrassment, and what he sees as the dangers of deepfakes as a whole. The Electronic Frontier Foundation notes that in that bill, "while there is an exception for parodies, satires, and entertainment—so long as a reasonable person would not mistake the 'falsified material activity' as authentic—the bill fails to specify who has the burden of proof, which could lead to a chilling effect for creators."
- As a big fan of free speech, Peterson of all people should be wary of suggesting we sue the pants off anyone who makes an unflattering mimicry of us online. If he really wants to do something to combat the real dangers of deepfakes, he could start with advocating for improving the legislation that does exist to get help for victims of revenge porn and non-consensual nudes. Those are the people who are really impacted by harassment and intimidation online.

URL: https://www.gizmodo.co.uk/2019/08/make-jordan-peterson-say-anything-you-want-with-this-spooky-audio-generator/
- People are really freaking out about deepfake videos, the technology that can make people say things they never…
- Advertisement

URL: https://thenextweb.com/shareables/2019/08/16/jordan-peterson-voice-ai/
- You have been blacklisted, KTHXBAI
- XID: 13230199
- Varnish cache server

URL: https://www.theverge.com/2019/5/17/18629024/joe-rogan-ai-fake-voice-clone-deepfake-dessa
- By  James Vincent, a senior reporter who has covered AI, robotics, and more for eight years at The Verge.
- Artificial intelligence isn’t just creating fake photos and videos — it can do fake voices, too.
- Up until now, these voices have been noticeably stilted and robotic, but researchers from AI  startup Dessa have created what is by far the most convincing voice clone we’ve ever heard — perfectly mimicking the sound of MMA-commentator-turned-podcaster Joe Rogan.
- Listen to clips of Dessa’s AI Rogan below, or take a quiz on the company’s site to see if you can spot the difference between real Rogan and faux Rogan. (It’s surprisingly hard!	)
- In terms of making a convincing fake, Dessa chose its target well. Rogan is probably the world’s most popular podcaster, and has recorded nearly 1,300 episodes of The Joe Rogan Experience to date. That provides ample training data for any AI system.
- It doesn’t hurt that the company’s engineers are obviously familiar with Rogan’s favorite talking points. Speculating about whether or not we’re living in a computer simulation, or admiring the upper body strength of chimps — that’s all prime Rogan material.
- But of course, being able to convincingly fake someone’s voice has disturbing implications, too. As Dessa’s engineers note in a blog post, malicious uses cases for fake voices include spam calls that impersonate your loved ones; using fake voices to bully or harass people; and creating misinformation through faked recordings of politicians.
- “Clearly, the societal implications for technologies like speech synthesis are massive,” Dessa writes. “And the implications will affect everyone. Poor consumers and rich consumers. Enterprises and governments.”
- Fake AI voices could be used for misinformation, but they could also improve technology
- The company notes there are benefits as well. These include the creation of more realistic AI assistants; quicker and more accurate dubbing for TV and film; and designing realistic, personalized synthetic voices for individuals with speech impairments.
- We’ve reached out to Dessa for more information about their work, but the company says because of the possibility of malicious uses it won’t be releasing its research in full or making its AI models publicly accessible. (A stance we’ve seen from larger AI labs like OpenAI, which controversially withheld the final version of its text-generating AI system.)
- Although there’s a good argument to be made that fears about deepfakes are overblown (the technology has been available for years but a fake has yet to impact mainstream politics), it’s also clear that the technology is only going to improve and become more accessible in the future.
- “Right now, technical expertise, ingenuity, computing power and data are required to make models like RealTalk perform well,” says the company. “But in the next few years (or even sooner), we’ll see the technology advance to the point where only a few seconds of audio are needed to create a life-like replica of anyone’s voice on the planet.”
- Listening to AI Joe Rogan talk about chimps ripping your balls off is, strangely, only the beginning.
- Update 2.40PM ET: In an Instagram post, Rogan responded to the Dessa voice clone, saying: “At this point I’ve long ago left enough content out there that they could basically have me saying anything they want, so my position is to shrug my shoulders and shake my head in awe, and just accept it. The future is gonna be really fucking weird, kids.”
- / Sign up for Verge Deals to get deals on products we've tested sent to your inbox daily.
- The Verge is a vox media network
- © 2023 Vox Media, LLC. All Rights Reserved

URL: https://thenextweb.com/news/jordan-peterson-voice-ai
- You have been blacklisted, KTHXBAI
- XID: 13230204
- Varnish cache server

URL: https://thenextweb.com/neural/2020/07/28/celebrity-voices-deepfake-ai-app/
- You have been blacklisted, KTHXBAI
- XID: 5865272
- Varnish cache server

- Dubai USD 35m voice cloning fraud
- ChatGPT accuses Jonathan Turley of sexual harrassment
- Page infoType: Incident Published: February 2023
