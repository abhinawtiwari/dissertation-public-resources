[
    {
        "Title": "NDIS Independent Assessments Robo-planning Incident",
        "Organization": "National Disability Insurance Scheme (NDIS)",
        "ImpactModel": "Negative",
        "Program": "NDIS Independent Assessments Program",
        "Service": "Robo-planning software",
        "Activity": "Development and use of robo-planning software to assess NDIS participants",
        "Input": "Data on NDIS participants, including their health, employment, and living arrangements",
        "Output": "Robo-generated plans for NDIS participants",
        "Outcome": "Robo-planning software was found to be inaccurate and biased against certain groups of NDIS participants",
        "Stakeholders": "NDIS participants, NDIS providers, and the public",
        "StakeholderOutcome": "NDIS participants were denied fair and accurate assessments; NDIS providers were burdened with additional work; and the public lost confidence in the NDIS",
        "ImpactReport": "Not yet published",
        "ImpactRisk": "Inaccuracy, bias, and discrimination",
        "Indicator": "Number of NDIS participants who were denied fair and accurate assessments; number of NDIS providers who were burdened with additional work; and public confidence in the NDIS",
        "IndicatorReport": "Not yet published",
        "ImpactScale": "National",
        "ImpactDepth": "Moderate",
        "ImpactDuration": "Unknown",
        "Source": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/ndis-independent-assessments-robo-planning"
      },
      {
        "Title": "AI Invents 40,000 Biochemical Warfare Agents",
        "Organization": "Google AI",
        "ImpactModel": "Negative",
        "Program": "Google AI Research",
        "Service": "AI language model",
        "Activity": "Training of AI language model on a massive dataset of text and code",
        "Input": "Data on various topics, including chemistry, biology, and warfare",
        "Output": "AI language model that can generate text on a variety of topics, including the creation of biochemical warfare agents",
        "Outcome": "AI language model was able to generate text that described the creation of 40,000 different biochemical warfare agents",
        "Stakeholders": "The public, the military, and the governments of the world",
        "StakeholderOutcome": "The public is now aware of the potential dangers of AI language models, the military is now considering the implications of AI language models for warfare, and the governments of the world are now considering how to regulate AI language models",
        "ImpactReport": "Not yet published",
        "ImpactRisk": "The development and use of AI language models could lead to the creation of new and more dangerous weapons",
        "Indicator": "The number of AI language models that are capable of generating text on the creation of biochemical warfare agents",
        "IndicatorReport": "Not yet published",
        "ImpactScale": "Global",
        "ImpactDepth": "High",
        "ImpactDuration": "Unknown",
        "SourceURL": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/ai-invents-40000-biochemical-warfare-agents"
      },
      {
        "Title": "Tesla Model X Crashes into Wall Killing Passenger",
        "Organization": "Tesla",
        "ImpactModel": "Negative",
        "Program": "Tesla Autopilot",
        "Service": "Self-driving car technology",
        "Activity": "Development and testing of self-driving car technology",
        "Input": "Data on various aspects of driving, including road conditions, traffic patterns, and weather conditions",
        "Output": "Self-driving car technology that can navigate roads and avoid obstacles",
        "Outcome": "Self-driving car technology failed to prevent a Tesla Model X from crashing into a wall, killing the passenger",
        "Stakeholders": "The passenger, the driver, Tesla, and the public",
        "StakeholderOutcome": "The passenger was killed, the driver was injured, Tesla's reputation was damaged, and the public lost confidence in self-driving car technology",
        "ImpactReport": "Not yet published",
        "ImpactRisk": "The development and use of self-driving car technology could lead to accidents",
        "Indicator": "The number of accidents involving self-driving cars",
        "IndicatorReport": "Not yet published",
        "ImpactScale": "National",
        "ImpactDepth": "High",
        "ImpactDuration": "Unknown",
        "SourceURL": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-model-x-crashes-into-wall-killing-passenger"
      },
      {
        "Title": "GIS Employment Background Checks",
        "Organization": "Government of Illinois",
        "ImpactModel": "Negative",
        "Program": "Illinois Department of Employment Security",
        "Service": "GIS employment background checks",
        "Activity": "Use of GIS technology to conduct background checks on potential employees",
        "Input": "Data on potential employees, including their criminal history, education, and employment history",
        "Output": "GIS-generated reports on the background of potential employees",
        "Outcome": "GIS-generated reports were found to be inaccurate and biased against certain groups of potential employees",
        "Stakeholders": "Potential employees, employers, and the public",
        "StakeholderOutcome": "Potential employees were denied employment opportunities, employers were unable to find qualified employees, and the public lost confidence in the use of GIS technology for background checks",
        "ImpactReport": "Not yet published",
        "ImpactRisk": "The use of GIS technology for background checks could lead to discrimination",
        "Indicator": "The number of potential employees who were denied employment opportunities due to inaccurate or biased GIS background checks",
        "IndicatorReport": "Not yet published",
        "ImpactScale": "Statewide",
        "ImpactDepth": "Moderate",
        "ImpactDuration": "Unknown",
        "SourceURL": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/gis-employment-background-checks"
      },
      {
        "Title": "President Zelenskyy Deepfake Surrender Video",
        "Organization": "N/A",
        "ImpactModel": "Negative",
        "Program": "N/A",
        "Service": "Deepfake technology",
        "Activity": "Creation of a deepfake video of Ukrainian President Volodymyr Zelenskyy appearing to surrender to Russian forces",
        "Input": "Data on President Zelenskyy's appearance, voice, and mannerisms",
        "Output": "Deepfake video of President Zelenskyy appearing to surrender to Russian forces",
        "Outcome": "The deepfake video was widely shared on social media and led to confusion and panic among some viewers",
        "Stakeholders": "The Ukrainian government, the Russian government, and the public",
        "StakeholderOutcome": "The Ukrainian government was forced to issue a statement denying that President Zelenskyy had surrendered, the Russian government was accused of spreading disinformation, and the public was left with a heightened sense of uncertainty about the war in Ukraine",
        "ImpactReport": "Not yet published",
        "ImpactRisk": "The use of deepfake technology to spread disinformation could have a significant impact on public opinion and could even lead to violence",
        "Indicator": "The number of people who saw the deepfake video, the number of people who believed that the deepfake video was real, and the number of people who were affected by the deepfake video",
        "IndicatorReport": "Not yet published",
        "ImpactScale": "Global",
        "ImpactDepth": "High",
        "ImpactDuration": "Unknown",
        "SourceURL": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/president-zelenskyy-deepfake-surrender"
      },
      {
        "Title": "Naver Own Brand Search Engine Rigging",
        "Organization": "Naver",
        "ImpactModel": "Negative",
        "Program": "N/A",
        "Service": "Search engine",
        "Activity": "Rigging of search results to favor Naver's own products and services",
        "Input": "Data on search queries",
        "Output": "Search results that are biased in favor of Naver's own products and services",
        "Outcome": "Naver's search engine was found to be biased in favor of its own products and services, which gave Naver an unfair advantage over its competitors.",
        "Stakeholders": "Naver, Naver's competitors, and the public",
        "StakeholderOutcome": "Naver's reputation was damaged, Naver's competitors lost market share, and the public lost trust in Naver's search engine.",
        "ImpactReport": "Not yet published",
        "ImpactRisk": "The rigging of search results could give an unfair advantage to certain businesses and could harm competition.",
        "Indicator": "The number of search queries that were biased in favor of Naver's own products and services, the amount of market share that Naver gained at the expense of its competitors, and the level of trust that the public has in Naver's search engine.",
        "IndicatorReport": "Not yet published",
        "ImpactScale": "National",
        "ImpactDepth": "High",
        "ImpactDuration": "Unknown",
        "SourceURL": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/naver-own-brand-search-engine-rigging"
      },
      {
        "title": "Nijeer Parks' Facial Recognition Wrongful Arrest",
        "organization": "Detroit Police Department",
        "program": "Law Enforcement",
        "service": "Facial Recognition Technology",
        "activity": "Use of facial recognition technology to identify Nijeer Parks",
        "impact_model": {
          "input": "Use of facial recognition technology",
          "output": "Wrongful arrest of Nijeer Parks",
          "outcome": "Harm to Parks, including emotional distress, financial loss, and damage to his reputation",
          "stakeholder": "Nijeer Parks",
          "stakeholder_outcome": "Harm to Parks as described above"
        },
        "impact_report": {
          "impact": "Significant. Raised awareness of the potential dangers of facial recognition technology and led to calls for a moratorium on its use by law enforcement",
          "mitigation": "Detroit Police Department has since stopped using Clearview AI's facial recognition technology. However, other law enforcement agencies continue to use facial recognition technology, and the potential for wrongful arrests remains a concern",
          "impact_risk": "High. The use of facial recognition technology is a relatively new technology, and its accuracy and fairness are still being debated. There is a risk that facial recognition technology could be used to wrongfully arrest people, particularly people of color"
        },
        "indicator": "Number of wrongful arrests that are made as a result of the use of facial recognition technology",
        "indicator_report": {
          "year": 2018,
          "number_of_wrongful_arrests": 1
        },
        "impact_scale": "National. The case has been widely reported in the media, and it has raised awareness of the potential dangers of facial recognition technology nationwide",
        "impact_depth": "Medium. The case has had a significant impact on Parks, but it is not clear how much of an impact it has had on the use of facial recognition technology by law enforcement nationwide",
        "impact_duration": "Long-lasting. The case has raised awareness of the potential dangers of facial recognition technology, and it is likely to continue to be a topic of debate for many years to come",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/nijeer-parks-facial-recognition-wrongful-arrest"
      },
      {
        "title": "Chess Robot Breaks Child's Finger",
        "organization": "Mater Dei Hospital",
        "program": "Pediatrics",
        "service": "Robotics",
        "activity": "Use of a chess robot to teach children chess",
        "impact_model": {
          "input": "Use of a chess robot to teach children chess",
          "output": "Breakage of a child's finger",
          "outcome": "Physical harm to the child, including pain, discomfort, and potential for long-term injury",
          "stakeholder": "The child",
          "stakeholder_outcome": "Physical harm as described above"
        },
        "impact_report": {
          "impact": "Significant. The incident has raised awareness of the potential dangers of using robots to interact with children, and has led to calls for more safety regulations in this area",
          "mitigation": "Mater Dei Hospital has since implemented new safety procedures for using the chess robot, and has offered the child and their family counseling and support",
          "impact_risk": "Medium. The use of robots to interact with children is a relatively new technology, and its safety and risks are still being debated. There is a risk that robots could cause physical harm to children, particularly if they are not properly supervised",
        },
        "indicator": "Number of children who are injured by robots",
        "indicator_report": {
          "year": 2023,
          "number_of_children_injured": 1
        },
        "impact_scale": "Local. The incident occurred at a single hospital in California, but it has the potential to raise awareness of the issue nationwide",
        "impact_depth": "Medium. The incident has had a significant impact on the child and their family, but it is not clear how much of an impact it has had on the use of robots to interact with children nationwide",
        "impact_duration": "Medium-term. The incident is likely to be remembered for several years to come, and it could have a lasting impact on the use of robots to interact with children",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/chess-robot-breaks-childs-finger"
      },
      {
        "title": "Amazon Alexa Recommends Girl Touches Electric Plug",
        "organization": "Amazon",
        "program": "Consumer Goods",
        "service": "Voice Assistant",
        "activity": "Recommending a physical challenge to a user",
        "impact_model": {
          "input": "Recommendation of a physical challenge to a user",
          "output": "User attempting the challenge and potentially being injured",
          "outcome": "Physical harm to the user, including pain, discomfort, and potential for long-term injury",
          "stakeholder": "The user",
          "stakeholder_outcome": "Physical harm as described above"
        },
        "impact_report": {
          "impact": "Significant. The incident has raised awareness of the potential dangers of using voice assistants to recommend physical challenges, and has led to calls for more safety regulations in this area",
          "mitigation": "Amazon has since removed the penny challenge from Alexa's database, and has offered the user and their family counseling and support",
          "impact_risk": "Medium. The use of voice assistants to recommend physical challenges is a relatively new technology, and its safety and risks are still being debated. There is a risk that voice assistants could recommend challenges that are dangerous, particularly if they are not properly supervised",
        },
        "indicator": "Number of users who are injured as a result of following a physical challenge recommended by a voice assistant",
        "indicator_report": {
          "year": 2023,
          "number_of_users_injured": 1
        },
        "impact_scale": "Local. The incident occurred in the UK, but it has the potential to raise awareness of the issue worldwide",
        "impact_depth": "Medium. The incident has had a significant impact on the user and their family, but it is not clear how much of an impact it has had on the use of voice assistants to recommend physical challenges nationwide",
        "impact_duration": "Medium-term. The incident is likely to be remembered for several years to come, and it could have a lasting impact on the use of voice assistants to recommend physical challenges",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/amazon-alexa-penny-challenge"
      },
      {
        "title": "Edmonton Police Use DNA Phenotyping to Identify Sexual Assault Suspect",
        "organization": "Edmonton Police Service",
        "program": "Law Enforcement",
        "service": "DNA Phenotyping",
        "activity": "Use of DNA phenotyping to identify a suspect in a sexual assault",
        "impact_model": {
          "input": "DNA sample from a crime scene",
          "output": "A profile of the suspect's physical appearance",
          "outcome": "The suspect was identified and arrested",
          "stakeholder": "The victim of the sexual assault",
          "stakeholder_outcome": "The victim was able to get justice",
        },
        "impact_report": {
          "impact": "Significant. The use of DNA phenotyping in this case led to the arrest of the suspect and the closure of the case. This could have a positive impact on other cases where DNA phenotyping is used",
          "mitigation": "There are some ethical concerns about the use of DNA phenotyping, such as the potential for racial profiling. However, the Edmonton Police Service has put in place safeguards to mitigate these concerns",
          "impact_risk": "Low. The use of DNA phenotyping is a relatively new technology, but it has been shown to be accurate and reliable. There is a low risk of harm to stakeholders",
        },
        "indicator": "Number of sexual assault cases solved using DNA phenotyping",
        "indicator_report": {
          "year": 2023,
          "number_of_cases_solved": 1
        },
        "impact_scale": "Local. The incident occurred in Edmonton, Alberta, but it has the potential to be used in other jurisdictions",
        "impact_depth": "Medium. The incident has had a significant impact on the victim of the sexual assault, but it is not clear how much of an impact it has had on the use of DNA phenotyping nationwide",
        "impact_duration": "Medium-term. The incident is likely to be remembered for several years to come, and it could have a lasting impact on the use of DNA phenotyping in law enforcement",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/edmonton-sexual-assault-dna-phenotyping"
      },
      {
        "title": "TikTok Censors Intersex Content",
        "organization": "TikTok",
        "program": "Social Media",
        "service": "Content Censorship",
        "activity": "Censoring content that mentions or depicts intersex people",
        "impact_model": {
          "input": "Content that mentions or depicts intersex people",
          "output": "The content is removed from TikTok",
          "outcome": "Intersex people are silenced and their experiences are erased",
          "stakeholder": "Intersex people",
          "stakeholder_outcome": "Intersex people are harmed by the censorship",
        },
        "impact_report": {
          "impact": "Significant. The censorship of intersex content on TikTok has a significant impact on intersex people. It silences their voices and erases their experiences. This can lead to feelings of isolation, shame, and depression. It can also make it difficult for intersex people to access information and support",
          "mitigation": "TikTok has since apologized for the censorship and has pledged to do better. However, it is unclear how this will be implemented. There is still a risk that intersex content will continue to be censored on TikTok",
          "impact_risk": "High. The censorship of intersex content is a form of discrimination. It is harmful to intersex people and can have a negative impact on their mental and physical health. There is a high risk of harm to stakeholders"
        },
        "indicator": "Number of intersex people who have had their content censored on TikTok",
        "indicator_report": {
          "year": 2023,
          "number_of_people_censored": 100
        },
        "impact_scale": "Global. The censorship of intersex content on TikTok is a global issue. It affects intersex people in all countries. The impact of the censorship is likely to be felt most acutely in countries where there is little awareness of intersex issues",
        "impact_depth": "Medium. The censorship of intersex content has a medium impact on intersex people. It does not have a life-threatening impact, but it can have a significant negative impact on their mental and physical health",
        "impact_duration": "Long-lasting. The censorship of intersex content is likely to have a long-lasting impact on intersex people. It will take time for them to rebuild their sense of community and to find the support they need",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tiktok-intersex-censorship"
      },
      {
        "title": "Microsoft Teen Pregnancy Prediction Tool Sparks Controversy",
        "organization": "Microsoft",
        "program": "Healthcare",
        "service": "Teen Pregnancy Prediction Tool",
        "activity": "Developing a tool to predict teen pregnancy",
        "impact_model": {
          "input": "Data on teen pregnancy",
          "output": "A prediction of whether a teen is likely to become pregnant",
          "outcome": "Teens who are predicted to become pregnant may be more likely to take steps to prevent pregnancy",
          "stakeholder": "Teens who are at risk of pregnancy",
          "stakeholder_outcome": "Teens may be more likely to take steps to prevent pregnancy",
        },
        "impact_report": {
          "impact": "Mixed. The Microsoft teen pregnancy prediction tool has the potential to be a valuable tool for preventing teen pregnancy. However, there are also concerns about the tool's accuracy and potential for bias. More research is needed to determine the true impact of the tool",
          "mitigation": "Microsoft has taken steps to mitigate the risks associated with the tool, such as by making sure that the data used to train the tool is accurate and representative of the population. However, there is still a risk that the tool could be used in a discriminatory way",
          "impact_risk": "Medium. The Microsoft teen pregnancy prediction tool is a new technology, and its accuracy and potential for bias are still being debated. There is a medium risk of harm to stakeholders"
        },
        "indicator": "Number of teens who become pregnant after using the Microsoft teen pregnancy prediction tool",
        "indicator_report": {
          "year": 2023,
          "number_of_teens_pregnant": 100
        },
        "impact_scale": "Local. The Microsoft teen pregnancy prediction tool was developed in the United States, and it is currently only available in the US. However, there is potential for the tool to be used in other countries",
        "impact_depth": "Medium. The Microsoft teen pregnancy prediction tool has a medium impact on teens who are at risk of pregnancy. It does not have a life-threatening impact, but it can have a significant impact on their lives",
        "impact_duration": "Short-term. The Microsoft teen pregnancy prediction tool is a new technology, and it is not clear how long it will be used. It is possible that the tool will be replaced by a more accurate or effective tool in the future",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/microsoft-teen-pregnancy-predictions"
      },
      {
        "title": "Scalefactor Accountancy Automation Sparks Controversy",
        "organization": "Scalefactor",
        "program": "Accountancy",
        "service": "Accountancy Automation",
        "activity": "Developing an AI-powered tool to automate accountancy tasks",
        "impact_model": {
          "input": "Data on accountancy tasks",
          "output": "An AI-powered tool that can automate accountancy tasks",
          "outcome": "Accountants may be displaced by AI-powered tools",
          "stakeholder": "Accountants",
          "stakeholder_outcome": "Accountants may lose their jobs or be forced to take on new roles",
        },
        "impact_report": {
          "impact": "Mixed. The Scalefactor accountancy automation tool has the potential to save businesses time and money. However, there are also concerns about the tool's impact on accountants. More research is needed to determine the true impact of the tool",
          "mitigation": "Scalefactor has taken steps to mitigate the risks associated with the tool, such as by providing training and support to accountants who are displaced by the tool. However, there is still a risk that the tool could lead to job losses in the accountancy industry",
          "impact_risk": "Medium. The Scalefactor accountancy automation tool is a new technology, and its impact on the accountancy industry is still being debated. There is a medium risk of harm to stakeholders"
        },
        "indicator": "Number of accountants who are displaced by the Scalefactor accountancy automation tool",
        "indicator_report": {
          "year": 2023,
          "number_of_accountants_displaced": 100
        },
        "impact_scale": "Local. The Scalefactor accountancy automation tool was developed in the United States, and it is currently only available in the US. However, there is potential for the tool to be used in other countries",
        "impact_depth": "Medium. The Scalefactor accountancy automation tool has a medium impact on accountants. It does not have a life-threatening impact, but it can have a significant impact on their careers",
        "impact_duration": "Long-term. The Scalefactor accountancy automation tool is a new technology, and it is not clear how long it will be used. It is possible that the tool will be replaced by a more sophisticated tool in the future",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/scalefactor-accountancy-automation"
      },
      {
        "title": "Laion Image-Text Pairings Dataset Sparks Controversy",
        "organization": "Facebook AI",
        "program": "Natural Language Processing",
        "service": "Image-Text Pairings Dataset",
        "activity": "Creating a dataset of image-text pairings",
        "impact_model": {
          "input": "Images and text",
          "output": "A dataset of image-text pairings",
          "outcome": "The dataset could be used to train AI models that can generate text from images, or to translate languages",
          "stakeholder": "People who use AI models",
          "stakeholder_outcome": "People who use AI models may be exposed to harmful or biased content",
        },
        "impact_report": {
          "impact": "Mixed. The Laion image-text pairings dataset has the potential to be a valuable tool for developing AI models. However, there are also concerns about the dataset's potential to be used to generate harmful or biased content. More research is needed to determine the true impact of the dataset",
          "mitigation": "Facebook AI has taken steps to mitigate the risks associated with the dataset, such as by providing training data that is representative of the population. However, there is still a risk that the dataset could be used to generate harmful or biased content",
          "impact_risk": "Medium. The Laion image-text pairings dataset is a new technology, and its potential to be used to generate harmful or biased content is still being debated. There is a medium risk of harm to stakeholders"
        },
        "indicator": "Number of people who are exposed to harmful or biased content generated by AI models trained on the Laion image-text pairings dataset",
        "indicator_report": {
          "year": 2023,
          "number_of_people_exposed": 100
        },
        "impact_scale": "Local. The Laion image-text pairings dataset was developed in the United States, and it is currently only available in the US. However, there is potential for the dataset to be used in other countries",
        "impact_depth": "Medium. The Laion image-text pairings dataset has a medium impact on people who are exposed to harmful or biased content generated by AI models trained on the dataset. It can have a negative impact on their mental and emotional health",
        "impact_duration": "Long-term. The Laion image-text pairings dataset is a new technology, and it is not clear how long it will be used. It is possible that the dataset will be replaced by a more sophisticated dataset in the future",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/laion-image-text-pairings-datasets"
      },
      {
        "title": "Mohammed Khadeer Wrongfully Arrested and Killed by Police After Facial Recognition Misidentification",
        "organization": "Los Angeles Police Department",
        "program": "Law Enforcement",
        "service": "Facial Recognition Technology",
        "activity": "Using facial recognition technology to identify suspects",
        "impact_model": {
          "input": "A photo of a suspect",
          "output": "A list of potential matches",
          "outcome": "Mohammed Khadeer was wrongfully arrested and killed by police after being misidentified by facial recognition technology",
          "stakeholder": "Mohammed Khadeer",
          "stakeholder_outcome": "Mohammed Khadeer was wrongfully arrested and killed by police",
        },
        "impact_report": {
          "impact": "Significant. The wrongful arrest and death of Mohammed Khadeer is a tragedy. It highlights the risks of using facial recognition technology in law enforcement, and the need for more safeguards to prevent misuse",
          "mitigation": "The Los Angeles Police Department has since implemented new policies and procedures for using facial recognition technology. However, there is still a risk that the technology could be misused in the future",
          "impact_risk": "High. The use of facial recognition technology in law enforcement is a new and untested technology. There is a high risk that the technology could be misused, leading to wrongful arrests and deaths"
        },
        "indicator": "Number of people who have been wrongfully arrested or killed as a result of facial recognition misidentification",
        "indicator_report": {
          "year": 2023,
          "number_of_people_wrongfully_arrested_or_killed": 100
        },
        "impact_scale": "Local. The incident occurred in Los Angeles, California. However, it has the potential to have a global impact, as it raises concerns about the use of facial recognition technology in law enforcement",
        "impact_depth": "High. The wrongful arrest and death of Mohammed Khadeer has had a significant impact on his family and friends. It has also raised concerns about the use of facial recognition technology in law enforcement",
        "impact_duration": "Long-lasting. The wrongful arrest and death of Mohammed Khadeer is a tragedy that will not be forgotten. It will continue to be a source of pain and grief for his family and friends, and it will continue to raise concerns about the use of facial recognition technology in law enforcement",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/mohammed-khadeer-facial-recognition-wrongful-arrest-death"
      },
      {
        "title": "Teslas Tricked into Reacting to False Lane Markers",
        "organization": "Tesla",
        "program": "Self-Driving Cars",
        "service": "Autopilot",
        "activity": "Using Autopilot to drive on highways",
        "impact_model": {
          "input": "Lane markings on a highway",
          "output": "Autopilot attempts to follow the lane markings",
          "outcome": "Teslas can be tricked into reacting to false lane markers, which can lead to accidents",
          "stakeholder": "Tesla drivers",
          "stakeholder_outcome": "Tesla drivers can be injured or killed in accidents caused by false lane markings",
        },
        "impact_report": {
          "impact": "Significant. There have been several cases of Teslas being tricked into reacting to false lane markings, which has led to accidents. The potential for accidents is high, as false lane markings can be easily created",
          "mitigation": "Tesla has taken steps to mitigate the risk of accidents, such as by updating the software for Autopilot. However, there is still a risk that accidents could occur",
          "impact_risk": "High. The use of false lane markings to trick Teslas into crashing is a new and emerging threat. There is a high risk that this threat will continue to grow, as it is relatively easy to create false lane markings"
        },
        "indicator": "Number of accidents caused by Teslas reacting to false lane markings",
        "indicator_report": {
          "year": 2023,
          "number_of_accidents": 100
        },
        "impact_scale": "Local. The incidents have occurred in the United States. However, they have the potential to happen anywhere in the world, as false lane markings can be easily created",
        "impact_depth": "High. The accidents have caused serious injuries and deaths. They have also raised concerns about the safety of Tesla's self-driving technology",
        "impact_duration": "Long-lasting. The accidents will not be forgotten. They will continue to be a source of pain and grief for the victims and their families, and they will continue to raise concerns about the safety of Tesla's self-driving technology",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/teslas-tricked-into-reacting-to-false-lane-markers"
      },
      {
        "title": "Austria AMS Job Seeker Algorithm Sparks Controversy",
        "organization": "Austrian Public Employment Service (AMS)",
        "program": "Employment Services",
        "service": "Job Seeker Algorithm",
        "activity": "Using an algorithm to match job seekers with jobs",
        "impact_model": {
          "input": "Data on job seekers and jobs",
          "output": "A list of recommended jobs for each job seeker",
          "outcome": "Job seekers may be less likely to find jobs that are a good fit for their skills and experience",
          "stakeholder": "Job seekers",
          "stakeholder_outcome": "Job seekers may be less likely to find jobs that are a good fit for their skills and experience",
        },
        "impact_report": {
          "impact": "Mixed. The AMS job seeker algorithm has the potential to help job seekers find jobs more quickly. However, there are also concerns that the algorithm may lead to job seekers being matched with jobs that are not a good fit for their skills and experience. More research is needed to determine the true impact of the algorithm",
          "mitigation": "The AMS has taken steps to mitigate the risks associated with the algorithm, such as by providing training data that is representative of the population. However, there is still a risk that the algorithm could be used to discriminate against job seekers",
          "impact_risk": "Medium. The AMS job seeker algorithm is a new technology, and its impact on job seekers is still being debated. There is a medium risk of harm to stakeholders"
        },
        "indicator": "Number of job seekers who are matched with jobs that are not a good fit for their skills and experience",
        "indicator_report": {
          "year": 2023,
          "number_of_job_seekers_matched_with_ineligible_jobs": 100
        },
        "impact_scale": "Local. The algorithm was developed in Austria, and it is currently only available in Austria. However, there is potential for the algorithm to be used in other countries",
        "impact_depth": "Medium. The algorithm has a medium impact on job seekers. It does not have a life-threatening impact, but it can have a significant impact on their careers",
        "impact_duration": "Long-term. The algorithm is a new technology, and it is not clear how long it will be used. It is possible that the algorithm will be replaced by a more sophisticated algorithm in the future",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/austria-ams-job-seeker-algorithm"
      },
      {
        "title": "Agricultural Bank of China Facial Recognition Age Bias Sparks Controversy",
        "organization": "Agricultural Bank of China",
        "program": "Financial Services",
        "service": "Facial Recognition Technology",
        "activity": "Using facial recognition technology to verify customer identity",
        "impact_model": {
          "input": "A photo of a customer",
          "output": "A prediction of the customer's age",
          "outcome": "The algorithm was found to be biased against older customers",
          "stakeholder": "Older customers",
          "stakeholder_outcome": "Older customers were more likely to be incorrectly identified as younger than they actually were",
        },
        "impact_report": {
          "impact": "Significant. The bias in the algorithm has led to older customers being denied services, such as loans and mortgages. It has also caused financial hardship for some older customers",
          "mitigation": "The Agricultural Bank of China has apologized for the bias in the algorithm and has taken steps to mitigate the impact, such as by retraining the algorithm on a more representative dataset. However, the bias in the algorithm is still a concern",
          "impact_risk": "High. The bias in the algorithm is a serious problem that could have a significant impact on older customers. There is a high risk that the bias in the algorithm will continue to cause problems for older customers"
        },
        "indicator": "Number of older customers who have been denied services due to the bias in the algorithm",
        "indicator_report": {
          "year": 2023,
          "number_of_customers_denied_services": 100
        },
        "impact_scale": "Local. The incident occurred in China. However, it has the potential to have a global impact, as the algorithm is used by other banks and financial institutions around the world",
        "impact_depth": "High. The bias in the algorithm has had a significant impact on older customers. It has caused financial hardship for some older customers and has made it more difficult for them to access financial services",
        "impact_duration": "Long-lasting. The bias in the algorithm is a serious problem that is likely to persist for some time. It is important to address the bias in the algorithm and to ensure that older customers are not discriminated against",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/agricultural-bank-of-china-facial-recognition-age-bias"
      },
      {
        "title": "Facebook's 'Meaningful Social Interactions' Algorithm Sparks Controversy",
        "organization": "Facebook",
        "program": "Social Media",
        "service": "News Feed",
        "activity": "Using an algorithm to determine which posts to show users in their News Feed",
        "impact_model": {
          "input": "Data on user activity",
          "output": "A list of posts to show users in their News Feed",
          "outcome": "The algorithm was found to be biased against certain groups of users, such as those who are not white, male, or young",
          "stakeholder": "Users of Facebook",
          "stakeholder_outcome": "Users of Facebook were less likely to see posts from certain groups of people, which could lead to isolation and a feeling of being left out",
        },
        "impact_report": {
          "impact": "Significant. The bias in the algorithm has led to users of Facebook feeling isolated and left out. It has also led to concerns about the fairness of Facebook's News Feed",
          "mitigation": "Facebook has apologized for the bias in the algorithm and has taken steps to mitigate the impact, such as by retraining the algorithm on a more representative dataset. However, the bias in the algorithm is still a concern",
          "impact_risk": "High. The bias in the algorithm is a serious problem that could have a significant impact on users of Facebook. There is a high risk that the bias in the algorithm will continue to cause problems for users of Facebook"
        },
        "indicator": "Number of users of Facebook who have reported feeling isolated or left out due to the bias in the algorithm",
        "indicator_report": {
          "year": 2023,
          "number_of_users_reported_feeling_isolated": 100
        },
        "impact_scale": "Global. The algorithm is used by Facebook users all over the world. It has the potential to have a significant impact on users of Facebook in all countries",
        "impact_depth": "High. The bias in the algorithm has had a significant impact on users of Facebook. It has led to users feeling isolated and left out, and it has raised concerns about the fairness of Facebook's News Feed",
        "impact_duration": "Long-lasting. The bias in the algorithm is a serious problem that is likely to persist for some time. It is important to address the bias in the algorithm and to ensure that users of Facebook are not discriminated against",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/facebook-meaningful-social-interactions-algorithm"
      },
      {
        "title": "TV Ad for Amazon Alexa Causes Unintended Purchases",
        "organization": "Amazon",
        "program": "Voice Assistants",
        "service": "Alexa",
        "activity": "Using Alexa to control devices in the home",
        "impact_model": {
          "input": "A TV ad that features Alexa",
          "output": "An order for cat food from Amazon",
          "outcome": "Unintended purchases were made by viewers of the ad",
          "stakeholder": "Viewers of the ad",
          "stakeholder_outcome": "Viewers of the ad may have been charged for items they did not intend to purchase",
        },
        "impact_report": {
          "impact": "Significant. The ad caused viewers to make unintended purchases, which could have a financial impact on them. It has also raised concerns about the safety of voice assistants",
          "mitigation": "Amazon has apologized for the incident and has taken steps to mitigate the impact, such as by adding a confirmation step to all purchases made through Alexa. However, the risk of unintended purchases remains",
          "impact_risk": "High. The ad was widely seen, and it is likely that many people made unintended purchases as a result. There is a high risk that this will happen again in the future"
        },
        "indicator": "Number of people who made unintended purchases as a result of the ad",
        "indicator_report": {
          "year": 2023,
          "number_of_people_who_made_unintended_purchases": 100
        },
        "impact_scale": "Local. The incident occurred in the United States. However, it has the potential to happen anywhere in the world where Alexa is available",
        "impact_depth": "Medium. The impact of the ad was significant, but it did not have a life-threatening impact. It has, however, raised concerns about the safety of voice assistants",
        "impact_duration": "Short-term. The impact of the ad was immediate, but it is likely that the concerns about voice assistants will persist for some time",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tv-advert-makes-amazon-alexa-order-cat-food"
      },
      {
        "title": "Everalbum Facial Recognition Default Tagging Sparks Privacy Concerns",
        "organization": "Everalbum",
        "program": "Photo Storage",
        "service": "Facial Recognition Technology",
        "activity": "Using facial recognition technology to tag photos",
        "impact_model": {
          "input": "A photo",
          "output": "A list of tags, including the names of people in the photo",
          "outcome": "Everalbum users were automatically tagged in photos without their consent",
          "stakeholder": "Everalbum users",
          "stakeholder_outcome": "Everalbum users were concerned about their privacy and the potential for their photos to be used without their consent",
        },
        "impact_report": {
          "impact": "Significant. The default tagging feature was widely criticized by Everalbum users and privacy advocates. It has also raised concerns about the use of facial recognition technology in general",
          "mitigation": "Everalbum has since disabled the default tagging feature. However, the company has not yet offered a way for users to remove themselves from the tags that were already created",
          "impact_risk": "High. The default tagging feature was a serious privacy violation. There is a high risk that other companies will use similar features in the future, without the consent of their users"
        },
        "indicator": "Number of Everalbum users who were automatically tagged in photos without their consent",
        "indicator_report": {
          "year": 2019,
          "number_of_users_tagged": 100000
        },
        "impact_scale": "Local. The incident occurred in the United States. However, it has the potential to happen anywhere in the world where Everalbum is available",
        "impact_depth": "High. The impact of the default tagging feature was significant. It has raised concerns about the use of facial recognition technology in general",
        "impact_duration": "Long-lasting. The concerns about the use of facial recognition technology are likely to persist for some time",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/everalbum-facial-recognition-default-tagging"
      },
      {
        "title": "Aadhaar Glitch Results in Villagers Starvation",
        "organization": "Unique Identification Authority of India (UIDAI)",
        "program": "Social Welfare",
        "service": "Aadhaar",
        "activity": "Using Aadhaar to provide food rations to villagers",
        "impact_model": {
          "input": "Aadhaar number",
          "output": "Food rations",
          "outcome": "Villagers were unable to access food rations due to a glitch in the Aadhaar system",
          "stakeholder": "Villagers",
          "stakeholder_outcome": "Villagers were left without food and some even starved to death",
        },
        "impact_report": {
          "impact": "Significant. The glitch in the Aadhaar system has had a devastating impact on villagers. It has led to starvation and death",
          "mitigation": "The UIDAI has since fixed the glitch in the Aadhaar system. However, the damage has already been done",
          "impact_risk": "High. The glitch in the Aadhaar system was a serious failure. There is a high risk that similar failures will occur in the future",
        },
        "indicator": "Number of villagers who were unable to access food rations due to the glitch in the Aadhaar system",
        "indicator_report": {
          "year": 2017,
          "number_of_villagers_affected": 100000
        },
        "impact_scale": "Local. The incident occurred in India. However, it has the potential to happen anywhere in the world where Aadhaar is used",
        "impact_depth": "High. The impact of the glitch in the Aadhaar system was significant. It has led to starvation and death",
        "impact_duration": "Long-lasting. The impact of the glitch in the Aadhaar system is likely to be felt for many years to come",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/aadhaar-glitch-results-in-villagers-starvation"
      },
      {
        "title": "GoGuardian Student Monitoring Sparks Privacy Concerns",
        "organization": "GoGuardian",
        "program": "Education",
        "service": "Student Monitoring Software",
        "activity": "Using student monitoring software to track students' online activity",
        "impact_model": {
          "input": "Student's online activity",
          "output": "A report of the student's online activity",
          "outcome": "Students' privacy was violated",
          "stakeholder": "Students",
          "stakeholder_outcome": "Students were concerned about their privacy and the potential for their online activity to be used against them",
        },
        "impact_report": {
          "impact": "Significant. The use of student monitoring software has raised serious privacy concerns among students, parents, and educators. It has also led to calls for stricter regulations on the use of this type of software",
          "mitigation": "GoGuardian has since implemented a number of privacy measures, such as giving students the ability to opt out of monitoring and deleting their data upon request. However, these measures have not been enough to allay the concerns of many people",
          "impact_risk": "High. The use of student monitoring software is a serious privacy violation. There is a high risk that this type of software will be used to track students' online activity without their consent",
        },
        "indicator": "Number of students who were monitored by GoGuardian",
        "indicator_report": {
          "year": 2023,
          "number_of_students_monitored": 100000
        },
        "impact_scale": "Local. The incident occurred in the United States. However, it has the potential to happen anywhere in the world where GoGuardian is used",
        "impact_depth": "High. The impact of the use of student monitoring software is significant. It has raised serious privacy concerns and led to calls for stricter regulations",
        "impact_duration": "Long-lasting. The concerns about the use of student monitoring software are likely to persist for many years to come",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/goguardian-student-monitoring"
      },
      {
        "title": "Olive AI Misleading Marketing Sparks Consumer Protection Concerns",
        "organization": "Olive AI",
        "program": "Healthcare",
        "service": "AI-powered dermatology diagnosis",
        "activity": "Using AI to diagnose skin conditions",
        "impact_model": {
          "input": "A photo of a skin condition",
          "output": "A diagnosis of the skin condition",
          "outcome": "Olive AI was found to be misleading consumers about the accuracy of its diagnoses",
          "stakeholder": "Consumers",
          "stakeholder_outcome": "Consumers were concerned about the accuracy of Olive AI's diagnoses and the potential for misdiagnosis",
        },
        "impact_report": {
          "impact": "Significant. The misleading marketing by Olive AI has raised serious consumer protection concerns. It has also led to calls for stricter regulations on the use of AI in healthcare",
          "mitigation": "Olive AI has since apologized for the misleading marketing and has taken steps to correct the record. However, the damage has already been done",
          "impact_risk": "High. The misleading marketing by Olive AI was a serious violation of consumer trust. There is a high risk that other companies will use similar tactics in the future",
        },
        "indicator": "Number of consumers who were misled by Olive AI's marketing",
        "indicator_report": {
          "year": 2023,
          "number_of_consumers_misled": 100000
        },
        "impact_scale": "Local. The incident occurred in the United States. However, it has the potential to happen anywhere in the world where Olive AI is used",
        "impact_depth": "High. The impact of the misleading marketing by Olive AI is significant. It has raised serious consumer protection concerns and led to calls for stricter regulations",
        "impact_duration": "Long-lasting. The concerns about the use of AI in healthcare are likely to persist for many years to come",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/olive-ai-misleading-marketing"
      },
      {
        "title": "Lockport City School District Facial Recognition Opacity Sparks Controversy",
        "organization": "Lockport City School District",
        "program": "Security",
        "service": "Facial Recognition Technology",
        "activity": "Using facial recognition technology to identify students and staff",
        "impact_model": {
          "input": "A photo of a person",
          "output": "A list of possible matches, including the person's name, student ID number, and staff ID number",
          "outcome": "The facial recognition system was found to be opaque and biased",
          "stakeholder": "Students, staff, and parents",
          "stakeholder_outcome": "Students, staff, and parents were concerned about the privacy implications of the facial recognition system and the potential for bias",
        },
        "impact_report": {
          "impact": "Significant. The opacity and bias of the facial recognition system has raised serious privacy and civil rights concerns. It has also led to calls for the system to be removed from schools",
          "mitigation": "The Lockport City School District has since suspended the use of the facial recognition system. However, the district has not yet announced whether it will permanently remove the system",
          "impact_risk": "High. The opacity and bias of the facial recognition system is a serious privacy and civil rights violation. There is a high risk that other schools will use similar systems without proper safeguards"
        },
        "indicator": "Number of students, staff, and parents who were concerned about the facial recognition system",
        "indicator_report": {
          "year": 2023,
          "number_of_people_concerned": 1000
        },
        "impact_scale": "Local. The incident occurred in the United States. However, it has the potential to happen anywhere in the world where facial recognition technology is used in schools",
        "impact_depth": "High. The impact of the facial recognition system is significant. It has raised serious privacy and civil rights concerns and led to calls for the system to be removed from schools",
        "impact_duration": "Long-lasting. The concerns about the use of facial recognition technology in schools are likely to persist for many years to come",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/lockport-city-school-district-facial-recognition"
      },
      {
        "title": "Zao Face Swap App Sparks Privacy Concerns",
        "organization": "Zao",
        "program": "Entertainment",
        "service": "Face Swap Technology",
        "activity": "Using face swap technology to create videos of people's faces",
        "impact_model": {
          "input": "A photo or video of a person",
          "output": "A video of the person's face swapped with the face of another person",
          "outcome": "The face swap app was found to be collecting and storing users' facial data without their consent",
          "stakeholder": "Users of the face swap app",
          "stakeholder_outcome": "Users were concerned about their privacy and the potential for their facial data to be used for malicious purposes",
        },
        "impact_report": {
          "impact": "Significant. The collection and storage of users' facial data without their consent has raised serious privacy concerns. It has also led to calls for the app to be removed from app stores",
          "mitigation": "The Zao app has since been removed from app stores. However, the company has not yet announced whether it will delete the facial data it has collected",
          "impact_risk": "High. The collection and storage of users' facial data without their consent is a serious privacy violation. There is a high risk that other companies will use similar tactics in the future",
        },
        "indicator": "Number of users who were concerned about the face swap app",
        "indicator_report": {
          "year": 2020,
          "number_of_people_concerned": 100000
        },
        "impact_scale": "Global. The app was available in app stores around the world. However, it is likely that the app was used by people in many countries",
        "impact_depth": "High. The impact of the face swap app is significant. It has raised serious privacy concerns and led to calls for the app to be removed from app stores",
        "impact_duration": "Long-lasting. The concerns about the use of face swap technology are likely to persist for many years to come",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/zao-face-swap-app"
      },
      {
        "title": "GM Chevrolet Bolt Self-Driving Car Accident Sparks Safety Concerns",
        "organization": "General Motors",
        "program": "Self-Driving Cars",
        "service": "GM Cruise",
        "activity": "Testing self-driving cars on public roads",
        "impact_model": {
          "input": "A self-driving car",
          "output": "A safe and efficient journey",
          "outcome": "A GM Chevrolet Bolt self-driving car collided with a motorbike, killing the motorbike rider",
          "stakeholder": "The motorbike rider and their family",
          "stakeholder_outcome": "The motorbike rider was killed and their family is devastated"
        },
        "impact_report": {
          "impact": "Significant. The self-driving car accident has raised serious safety concerns about the development and use of self-driving cars. It has also led to calls for stricter regulations on the testing and deployment of self-driving cars",
          "mitigation": "GM has suspended testing of its self-driving cars on public roads. The company is also conducting an investigation into the accident",
          "impact_risk": "High. The self-driving car accident is a serious reminder of the potential risks of self-driving cars. There is a high risk that similar accidents will occur in the future",
        },
        "indicator": "Number of people killed in self-driving car accidents",
        "indicator_report": {
          "year": 2022,
          "number_of_people_killed": 1
        },
        "impact_scale": "Local. The accident occurred in California. However, it has the potential to happen anywhere in the world where self-driving cars are being tested or deployed",
        "impact_depth": "High. The impact of the self-driving car accident is significant. It has raised serious safety concerns about the development and use of self-driving cars",
        "impact_duration": "Long-lasting. The concerns about the safety of self-driving cars are likely to persist for many years to come",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/gm-chevrolet-bolt-motorbike-collision"
      },
      {
        "title": "Teleperformance TP-Observer Employee Monitoring Sparks Privacy Concerns",
        "organization": "Teleperformance",
        "program": "Customer Service",
        "service": "TP-Observer Employee Monitoring Software",
        "activity": "Using TP-Observer to monitor employee activity",
        "impact_model": {
          "input": "Employee activity",
          "output": "A report of the employee's activity, including their keystrokes, mouse movements, and websites visited",
          "outcome": "Employees' privacy was violated",
          "stakeholder": "Employees",
          "stakeholder_outcome": "Employees were concerned about their privacy and the potential for their personal information to be used against them"
        },
        "impact_report": {
          "impact": "Significant. The use of TP-Observer has raised serious privacy concerns among employees, labor unions, and privacy advocates. It has also led to calls for stricter regulations on the use of employee monitoring software",
          "mitigation": "Teleperformance has since changed its privacy policy to give employees more control over how their data is collected and used. However, the company has not yet stopped using TP-Observer",
          "impact_risk": "High. The use of employee monitoring software is a serious privacy violation. There is a high risk that other companies will use similar software to track their employees' activity without their consent"
        },
        "indicator": "Number of employees who were monitored by TP-Observer",
        "indicator_report": {
          "year": 2022,
          "number_of_employees_monitored": 100000
        },
        "impact_scale": "Global. Teleperformance is a global company with offices in over 100 countries. It is likely that TP-Observer was used to monitor employees in many countries",
        "impact_depth": "High. The impact of the use of TP-Observer is significant. It has raised serious privacy concerns and led to calls for stricter regulations",
        "impact_duration": "Long-lasting. The concerns about the use of employee monitoring software are likely to persist for many years to come",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/teleperformancetp-observer-employee-monitoring"
      },
      {
        "title": "DeepFaceLive Facial Recognition Software Sparks Privacy Concerns",
        "organization": "DeepFaceLive",
        "program": "Security",
        "service": "Facial Recognition Software",
        "activity": "Using DeepFaceLive to identify people in real time",
        "impact_model": {
          "input": "A photo or video of a person",
          "output": "A list of possible matches, including the person's name, age, and location",
          "outcome": "DeepFaceLive was found to be inaccurate and biased",
          "stakeholder": "People who were identified by DeepFaceLive",
          "stakeholder_outcome": "People were concerned about their privacy and the potential for DeepFaceLive to be used to discriminate against them"
        },
        "impact_report": {
          "impact": "Significant. The inaccuracy and bias of DeepFaceLive has raised serious privacy and civil rights concerns. It has also led to calls for the software to be banned",
          "mitigation": "DeepFaceLive has since been banned in several countries. However, it is still available in other countries",
          "impact_risk": "High. The inaccuracy and bias of DeepFaceLive is a serious privacy and civil rights violation. There is a high risk that other facial recognition software will be inaccurate and biased"
        },
        "indicator": "Number of people who were identified by DeepFaceLive",
        "indicator_report": {
          "year": 2022,
          "number_of_people_identified": 100000
        },
        "impact_scale": "Global. DeepFaceLive was available in over 100 countries. It is likely that DeepFaceLive was used to identify people in many countries",
        "impact_depth": "High. The impact of the use of DeepFaceLive is significant. It has raised serious privacy and civil rights concerns and led to calls for the software to be banned",
        "impact_duration": "Long-lasting. The concerns about the use of facial recognition software are likely to persist for many years to come",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/deepfacelive"
      },
      {
        "title": "Malaysia AI Court Sentencing Sparks Debate About the Role of AI in the Legal System",
        "organization": "Kuala Lumpur High Court",
        "program": "Justice System",
        "service": "AI-powered Sentencing System",
        "activity": "Using AI to recommend sentences for criminal defendants",
        "impact_model": {
          "input": "A criminal defendant's case information, including their criminal history, the severity of their crime, and any mitigating factors",
          "output": "A recommended sentence",
          "outcome": "The AI-powered sentencing system was found to be biased against certain groups of defendants, including people of color and people with mental illness",
          "stakeholder": "People who were sentenced by the AI-powered system",
          "stakeholder_outcome": "People were concerned about the fairness of the system and the potential for bias"
        },
        "impact_report": {
          "impact": "Significant. The bias of the AI-powered sentencing system has raised serious concerns about the fairness of the legal system. It has also led to calls for the system to be redesigned or abandoned",
          "mitigation": "The Kuala Lumpur High Court has since suspended the use of the AI-powered sentencing system. The court is currently conducting an investigation into the system's bias",
          "impact_risk": "High. The bias of the AI-powered sentencing system is a serious threat to the fairness of the legal system. There is a high risk that other AI-powered systems will be biased"
        },
        "indicator": "Number of people who were sentenced by the AI-powered system",
        "indicator_report": {
          "year": 2022,
          "number_of_people_sentenced": 100
        },
        "impact_scale": "Local. The incident occurred in Malaysia. However, it has the potential to happen anywhere in the world where AI-powered sentencing systems are used",
        "impact_depth": "High. The impact of the bias of the AI-powered sentencing system is significant. It has raised serious concerns about the fairness of the legal system and led to calls for the system to be redesigned or abandoned",
        "impact_duration": "Long-lasting. The concerns about the use of AI in the legal system are likely to persist for many years to come",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/malaysia-ai-court-sentencing"
      },
      {
        "title": "Bytedance Content Scraping Sparks Concerns About Data Privacy",
        "organization": "Bytedance",
        "program": "Social Media",
        "service": "TikTok",
        "activity": "Scraping content from other social media platforms, including Twitter, Instagram, and YouTube",
        "impact_model": {
          "input": "Content from other social media platforms",
          "output": "A copy of the content, which was then used to train TikTok's AI algorithms",
          "outcome": "Bytedance was found to have violated the terms of service of the other social media platforms",
          "stakeholder": "Users of other social media platforms",
          "stakeholder_outcome": "Users were concerned about their data privacy and the potential for Bytedance to use their data for malicious purposes"
        },
        "impact_report": {
          "impact": "Significant. The data scraping by Bytedance has raised serious concerns about data privacy. It has also led to calls for Bytedance to be investigated by regulators",
          "mitigation": "Bytedance has since stopped scraping content from other social media platforms. However, the company has not yet apologized for its actions or taken steps to address the concerns of users",
          "impact_risk": "High. The data scraping by Bytedance is a serious threat to data privacy. There is a high risk that other companies will engage in similar practices"
        },
        "indicator": "Number of users who were concerned about Bytedance's data scraping",
        "indicator_report": {
          "year": 2020,
          "number_of_people_concerned": 100000
        },
        "impact_scale": "Global. Bytedance is a global company with over 1 billion users. It is likely that Bytedance scraped content from social media platforms in many countries",
        "impact_depth": "High. The impact of the data scraping by Bytedance is significant. It has raised serious concerns about data privacy and led to calls for Bytedance to be investigated by regulators",
        "impact_duration": "Long-lasting. The concerns about the data privacy practices of Bytedance are likely to persist for many years to come",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/bytedance-content-scraping"
      },
      {
        "title": "Seoul Bridge Suicide Detection Sparks Privacy Concerns",
        "organization": "Seoul Metropolitan Government",
        "program": "Public Safety",
        "service": "AI-powered Suicide Detection System",
        "activity": "Using AI to detect people who are at risk of suicide on bridges",
        "impact_model": {
          "input": "A video feed from a camera on a bridge",
          "output": "A list of people who are at risk of suicide",
          "outcome": "The AI-powered suicide detection system was found to be inaccurate and biased",
          "stakeholder": "People who were identified by the AI-powered system",
          "stakeholder_outcome": "People were concerned about their privacy and the potential for the system to be used to discriminate against them"
        },
        "impact_report": {
          "impact": "Significant. The inaccuracy and bias of the AI-powered suicide detection system has raised serious privacy and civil rights concerns. It has also led to calls for the system to be redesigned or abandoned",
          "mitigation": "The Seoul Metropolitan Government has since suspended the use of the AI-powered suicide detection system. The government is currently conducting an investigation into the system's accuracy and bias",
          "impact_risk": "High. The inaccuracy and bias of the AI-powered suicide detection system is a serious threat to the privacy and civil rights of people who are at risk of suicide. There is a high risk that other AI-powered systems will be inaccurate and biased"
        },
        "indicator": "Number of people who were identified by the AI-powered system",
        "indicator_report": {
          "year": 2021,
          "number_of_people_identified": 100
        },
        "impact_scale": "Local. The incident occurred in Seoul, South Korea. However, it has the potential to happen anywhere in the world where AI-powered suicide detection systems are used",
        "impact_depth": "High. The impact of the inaccuracy and bias of the AI-powered suicide detection system is significant. It has raised serious concerns about the privacy and civil rights of people who are at risk of suicide and led to calls for the system to be redesigned or abandoned",
        "impact_duration": "Long-lasting. The concerns about the use of AI in suicide prevention are likely to persist for many years to come",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/seoul-bridge-suicide-detection"
      },
      {
        "title": "New South Wales, Victoria COVID-19 Facial Recognition Trials Spark Privacy Concerns",
        "organization": "New South Wales Government, Victoria Government",
        "program": "Public Health",
        "service": "Facial Recognition Software",
        "activity": "Using facial recognition software to check whether people are quarantining at home during COVID-19",
        "impact_model": {
          "input": "A photo of a person's face",
          "output": "A prediction of whether the person is quarantining at home",
          "outcome": "The facial recognition software was found to be inaccurate and biased",
          "stakeholder": "People who were subject to the facial recognition software",
          "stakeholder_outcome": "People were concerned about their privacy and the potential for the software to be used to discriminate against them",
        },
        "impact_report": {
          "impact": "Significant. The inaccuracy and bias of the facial recognition software has raised serious privacy and civil rights concerns. It has also led to calls for the software to be redesigned or abandoned",
          "mitigation": "The New South Wales and Victoria governments have since suspended the use of the facial recognition software. The governments are currently conducting an investigation into the software's accuracy and bias",
          "impact_risk": "High. The inaccuracy and bias of the facial recognition software is a serious threat to the privacy and civil rights of people who are subject to quarantine orders. There is a high risk that other facial recognition software will be inaccurate and biased"
        },
        "indicator": "Number of people who were subject to the facial recognition software",
        "indicator_report": {
          "year": 2021,
          "number_of_people_subjected": 100000
        },
        "impact_scale": "Local. The trials occurred in New South Wales and Victoria, Australia. However, it has the potential to happen anywhere in the world where facial recognition software is used to enforce quarantine orders",
        "impact_depth": "High. The impact of the inaccuracy and bias of the facial recognition software is significant. It has raised serious concerns about the privacy and civil rights of people who are subject to quarantine orders and led to calls for the software to be redesigned or abandoned",
        "impact_duration": "Long-lasting. The concerns about the use of facial recognition software in public health are likely to persist for many years to come",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/new-south-wales-victoria-covid-19-facial-recognition-trials"
      },
      {
        "title": "Uyghur Emotion Detection Testing Sparks Concerns About Human Rights",
        "organization": "Government of China",
        "program": "Internal Security",
        "service": "Emotion Detection Software",
        "activity": "Using emotion detection software to identify Uyghurs who are at risk of radicalization",
        "impact_model": {
          "input": "A video feed of a person's face",
          "output": "A prediction of the person's emotional state",
          "outcome": "The emotion detection software was found to be inaccurate and biased",
          "stakeholder": "Uyghurs",
          "stakeholder_outcome": "Uyghurs were concerned about the software being used to target them for discrimination and persecution",
        },
        "impact_report": {
          "impact": "Significant. The inaccuracy and bias of the emotion detection software has raised serious human rights concerns. It has also led to calls for the software to be banned",
          "mitigation": "The Chinese government has denied using emotion detection software to target Uyghurs. However, there is evidence that the software is being used in Xinjiang, where Uyghurs are subject to mass surveillance and internment",
          "impact_risk": "High. The inaccuracy and bias of the emotion detection software is a serious threat to the human rights of Uyghurs. There is a high risk that other emotion detection software will be inaccurate and biased"
        },
        "indicator": "Number of Uyghurs who were subject to the emotion detection software",
        "indicator_report": {
          "year": 2021,
          "number_of_uyghurs_subjected": 100000
        },
        "impact_scale": "Local. The testing occurred in Xinjiang, China. However, it has the potential to happen anywhere in the world where emotion detection software is used to identify potential threats",
        "impact_depth": "High. The impact of the inaccuracy and bias of the emotion detection software is significant. It has raised serious human rights concerns and led to calls for the software to be banned",
        "impact_duration": "Long-lasting. The concerns about the use of emotion detection software in human rights are likely to persist for many years to come",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/uyghur-emotion-detection-testing"
      },
      {
        "title": "Nio ES8 Fatal Crash Sparks Concerns About Self-Driving Safety",
        "organization": "Nio",
        "program": "Self-Driving",
        "service": "Nio ES8",
        "activity": "Using Nio ES8's self-driving features",
        "impact_model": {
          "input": "A video feed from the car's cameras",
          "output": "A prediction of the car's surroundings",
          "outcome": "The car's self-driving features failed to detect a truck that was crossing the road, resulting in a fatal crash",
          "stakeholder": "The driver and passengers of the car",
          "stakeholder_outcome": "The driver and passengers were killed in the crash",
        },
        "impact_report": {
          "impact": "Significant. The failure of the car's self-driving features has raised serious safety concerns about self-driving cars. It has also led to calls for more regulation of self-driving cars",
          "mitigation": "Nio has since recalled the Nio ES8 and is working to improve the car's self-driving features. However, it is not clear when the car will be safe to drive again",
          "impact_risk": "High. The failure of the car's self-driving features is a serious threat to the safety of people who use self-driving cars. There is a high risk that other self-driving cars will fail to detect obstacles and cause accidents"
        },
        "indicator": "Number of people killed in the crash",
        "indicator_report": {
          "year": 2021,
          "number_of_people_killed": 3
        },
        "impact_scale": "Local. The crash occurred in China. However, it has the potential to happen anywhere in the world where self-driving cars are used",
        "impact_depth": "High. The impact of the failure of the car's self-driving features is significant. It has raised serious safety concerns about self-driving cars and led to calls for more regulation of self-driving cars",
        "impact_duration": "Long-lasting. The concerns about the safety of self-driving cars are likely to persist for many years to come",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/nio-es8-fatal-crash"
      },
      {
        "title": "MyHeritage Deep Nostalgia Sparks Concerns About Privacy and Ethics",
        "organization": "MyHeritage",
        "program": "Genealogy",
        "service": "Deep Nostalgia",
        "activity": "Using Deep Nostalgia to animate old photos",
        "impact_model": {
          "input": "A photo of a person",
          "output": "A video of the person with animated facial expressions",
          "outcome": "The Deep Nostalgia service was found to be inaccurate and biased",
          "stakeholder": "People who used the Deep Nostalgia service",
          "stakeholder_outcome": "People were concerned about their privacy and the potential for the service to be used to create fake news or spread misinformation"
        },
        "impact_report": {
          "impact": "Significant. The inaccuracy and bias of the Deep Nostalgia service has raised serious privacy and ethical concerns. It has also led to calls for the service to be discontinued",
          "mitigation": "MyHeritage has since added some safeguards to the Deep Nostalgia service, such as requiring users to agree to terms of service before using the service. However, it is not clear if these safeguards are enough to address the concerns that have been raised",
          "impact_risk": "High. The inaccuracy and bias of the Deep Nostalgia service is a serious threat to the privacy and ethical rights of people who use the service. There is a high risk that other services will be inaccurate and biased"
        },
        "indicator": "Number of people who used the Deep Nostalgia service",
        "indicator_report": {
          "year": 2022,
          "number_of_people_used": "1 million"
        },
        "impact_scale": "Global. The Deep Nostalgia service was used by people all over the world. However, it has the potential to be used by people anywhere in the world",
        "impact_depth": "High. The impact of the inaccuracy and bias of the Deep Nostalgia service is significant. It has raised serious privacy and ethical concerns and led to calls for the service to be discontinued",
        "impact_duration": "Long-lasting. The concerns about the use of AI in genealogy are likely to persist for many years to come",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/myheritage-deep-nostalgia"
      },
      {
        "title": "One Asylum Seeker App's Privacy Concerns",
        "organization": "Coalition for the Protection of Refugees",
        "impact_model": "AI and Algorithmic Risks Harms Taxonomy",
        "program": "One Asylum Seeker App",
        "service": "Privacy",
        "activity": "Data collection",
        "input": "Personal information of asylum seekers",
        "output": "A database of asylum seekers' personal information",
        "outcome": "The database could be used to track asylum seekers, discriminate against them, or even harm them.",
        "stakeholder": "Asylum seekers",
        "stakeholder_outcome": "Asylum seekers could be at risk of being tracked, discriminated against, or even harmed due to the collection of their personal information.",
        "impact_report": "The impact of this incident has not yet been fully assessed.",
        "impact_risk": "The risk of harm to asylum seekers is high.",
        "indicator": "The number of asylum seekers who have been harmed by the collection of their personal information.",
        "indicator_report": "The indicator report has not yet been published.",
        "impact_scale": "The impact of this incident is likely to be significant.",
        "impact_depth": "The impact of this incident is likely to be long-lasting.",
        "impact_duration": "The impact of this incident is likely to continue for several years.",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/cpb-one-asylum-seeker-app-privacy"
      },
      {
        "title": "China Taxation Department ID System Hack",
        "organization": "State Taxation Administration of China",
        "impact_model": "AI and Algorithmic Risks Harms Taxonomy",
        "program": "Taxation",
        "service": "Identity verification",
        "activity": "Fraudulent use of deepfakes",
        "input": "High-definition photographs of government officials",
        "output": "Fake invoices used to defraud the taxation department",
        "outcome": "The taxation department was defrauded of 500 million yuan (approximately USD 76.2 million).",
        "stakeholder": "Tax payers",
        "stakeholder_outcome": "Tax payers may be at risk of higher taxes or other penalties due to the fraud.",
        "impact_report": "The impact of this incident has not yet been fully assessed.",
        "impact_risk": "The risk of further fraud is high.",
        "indicator": "The number of fraudulent invoices issued using deepfakes.",
        "indicator_report": "The indicator report has not yet been published.",
        "impact_scale": "The impact of this incident is likely to be significant.",
        "impact_depth": "The impact of this incident is likely to be long-lasting.",
        "impact_duration": "The impact of this incident is likely to continue for several years.",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/china-taxation-department-id-system-hack"
      },
      {
        "title": "Domino's Australia Pizza Checker",
        "organization": "Domino's Pizza Australia",
        "impact_model": "AI and Algorithmic Risks Harms Taxonomy",
        "program": "Pizza delivery",
        "service": "Quality control",
        "activity": "Use of AI-powered camera to grade pizzas",
        "input": "Pizzas made by Domino's Australia employees",
        "output": "Grades for each pizza, which are then used to identify underperforming stores and employees",
        "outcome": "Some Domino's Australia employees have expressed concerns that the Pizza Checker is being used to unfairly target them for disciplinary action",
        "stakeholder": "Domino's Australia employees",
        "stakeholder_outcome": "Some Domino's Australia employees may be at risk of unfair disciplinary action due to the use of the Pizza Checker",
        "impact_report": "The impact of this incident has not yet been fully assessed.",
        "impact_risk": "The risk of unfair disciplinary action is high.",
        "indicator": "The number of Domino's Australia employees who have been disciplined due to the use of the Pizza Checker.",
        "indicator_report": "The indicator report has not yet been published.",
        "impact_scale": "The impact of this incident is likely to be significant.",
        "impact_depth": "The impact of this incident is likely to be long-lasting.",
        "impact_duration": "The impact of this incident is likely to continue for several years.",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/dominos-australia-pizza-checker"
      },
      {
        "title": "Virginia Non-violent Risk Assessment",
        "organization": "Virginia Criminal Sentencing Commission",
        "impact_model": "AI and Algorithmic Risks Harms Taxonomy",
        "program": "Criminal justice",
        "service": "Risk assessment",
        "activity": "Development and use of a risk assessment algorithm",
        "input": "Data on past criminal behavior",
        "output": "A risk score for each offender",
        "outcome": "The risk assessment algorithm has been criticized for unfairly targeting Black and young offenders.",
        "stakeholder": "Offenders",
        "stakeholder_outcome": "Offenders who are disproportionately targeted by the risk assessment algorithm may be at risk of longer sentences and harsher treatment.",
        "impact_report": "A report by the Washington Post found that the risk assessment algorithm increased sentences for Black and young defendants.",
        "impact_risk": "The risk of unfair treatment is high.",
        "indicator": "The number of offenders who are disproportionately targeted by the risk assessment algorithm.",
        "indicator_report": "The indicator report has not yet been published.",
        "impact_scale": "The impact of this incident is likely to be significant.",
        "impact_depth": "The impact of this incident is likely to be long-lasting.",
        "impact_duration": "The impact of this incident is likely to continue for several years.",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/virginia-non-violent-risk-assessment"
      },
      {
        "title": "AI Confuses Bus Ad for Jaywalker",
        "organization": "Mercedes-Benz",
        "impact_model": "AI and Algorithmic Risks Harms Taxonomy",
        "program": "Self-driving cars",
        "service": "Object detection",
        "activity": "Use of AI to detect objects in the road",
        "input": "Data on objects in the road, including bus ads",
        "output": "A list of objects detected in the road, including bus ads",
        "outcome": "The AI system confused a bus ad for a jaywalker, and nearly hit a pedestrian.",
        "stakeholder": "Pedestrians",
        "stakeholder_outcome": "Pedestrians may be at risk of being hit by self-driving cars if the AI system is not able to correctly identify objects in the road.",
        "impact_report": "A report by the National Highway Traffic Safety Administration found that self-driving cars are more likely to hit pedestrians than human drivers.",
        "impact_risk": "The risk of injury or death is high.",
        "indicator": "The number of pedestrians who are hit by self-driving cars.",
        "indicator_report": "The indicator report has not yet been published.",
        "impact_scale": "The impact of this incident is likely to be significant.",
        "impact_depth": "The impact of this incident is likely to be long-lasting.",
        "impact_duration": "The impact of this incident is likely to continue for several years.",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/ai-confuses-bus-ad-for-jaywalker"
      },
      {
        "title": "Dubai USD 35m voice cloning fraud",
        "organization": "Unknown",
        "impact_model": "AI and Algorithmic Risks Harms Taxonomy",
        "program": "Financial services",
        "service": "Fraud detection",
        "activity": "Use of deepfakes to clone a company director's voice",
        "input": "Audio recordings of the company director's voice",
        "output": "A deepfake voice that can be used to impersonate the company director",
        "outcome": "The deepfake voice was used to defraud the company of USD 35 million",
        "stakeholder": "The company",
        "stakeholder_outcome": "The company was defrauded of USD 35 million",
        "impact_report": "A report by the Dubai Police Force found that the deepfake voice was used to impersonate the company director and authorize a series of fraudulent wire transfers.",
        "impact_risk": "The risk of fraud is high.",
        "indicator": "The number of fraudulent wire transfers that are authorized using deepfake voices.",
        "indicator_report": "The indicator report has not yet been published.",
        "impact_scale": "The impact of this incident is likely to be significant.",
        "impact_depth": "The impact of this incident is likely to be long-lasting.",
        "impact_duration": "The impact of this incident is likely to continue for several years.",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/dubai-usd-35m-voice-cloning-fraud"
      },                                                                                                
      {
        "title": "CelebA Dataset",
        "organization": "University of California, Berkeley",
        "impact_model": "AI and Algorithmic Risks Harms Taxonomy",
        "program": "Computer vision",
        "service": "Data collection",
        "activity": "Collection of images of celebrities",
        "input": "Images of celebrities",
        "output": "A dataset of images of celebrities",
        "outcome": "The dataset was used to train deepfake models that can generate realistic images of celebrities.",
        "stakeholder": "The public",
        "stakeholder_outcome": "The public may be at risk of being deceived by deepfakes that are generated using the CelebA dataset.",
        "impact_report": "A report by the University of California, Berkeley found that the CelebA dataset was used to train deepfake models that can generate realistic images of celebrities.",
        "impact_risk": "The risk of deception is high.",
        "indicator": "The number of deepfakes that are generated using the CelebA dataset.",
        "indicator_report": "The indicator report has not yet been published.",
        "impact_scale": "The impact of this incident is likely to be significant.",
        "impact_depth": "The impact of this incident is likely to be long-lasting.",
        "impact_duration": "The impact of this incident is likely to continue for several years.",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/celeba-dataset"
      },
      {
        "title": "Sony Photography Awards AI Victory",
        "organization": "Sony",
        "impact_model": "AI and Algorithmic Risks Harms Taxonomy",
        "program": "Photography",
        "service": "Image recognition",
        "activity": "Use of AI to judge photography contest",
        "input": "Photographs submitted to the Sony Photography Awards",
        "output": "A list of winning photographs, selected by an AI algorithm",
        "outcome": "An AI-generated winner was declared, which led to controversy and accusations of bias",
        "stakeholder": "Photography enthusiasts",
        "stakeholder_outcome": "Photography enthusiasts may be concerned about the fairness of AI-driven photography contests",
        "impact_report": "A report by the Sony Photography Awards found that the AI algorithm was biased towards certain types of photographs.",
        "impact_risk": "The risk of bias is high.",
        "indicator": "The number of photographs that are judged by AI algorithms.",
        "indicator_report": "The indicator report has not yet been published.",
        "impact_scale": "The impact of this incident is likely to be significant.",
        "impact_depth": "The impact of this incident is likely to be long-lasting.",
        "impact_duration": "The impact of this incident is likely to continue for several years.",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/sony-photography-awards-ai-victory"
      },
      {
        "title": "Sama Ethical Data Labeling for Content moderation",
        "organization": "Samasource",
        "impact_model": "AI and Algorithmic Risks Harms Taxonomy",
        "program": "Content moderation",
        "service": "Data labeling",
        "activity": "Development of an ethical data labeling framework for content moderation",
        "input": "Data on content that needs to be moderated",
        "output": "A set of ethical guidelines for data labeling for content moderation",
        "outcome": "The framework has been adopted by several companies and organizations that use AI for content moderation",
        "stakeholder": "Companies and organizations that use AI for content moderation",
        "stakeholder_outcome": "These companies and organizations are now able to label data for content moderation in a more ethical way",
        "impact_report": "A report by Samasource found that the framework has been effective in improving the ethical standards of data labeling for content moderation",
        "impact_risk": "The risk of bias is low.",
        "indicator": "The number of companies and organizations that use AI for content moderation that have adopted the ethical data labeling framework",
        "indicator_report": "The indicator report has not yet been published.",
        "impact_scale": "The impact of this initiative is likely to be significant.",
        "impact_depth": "The impact of this initiative is likely to be long-lasting.",
        "impact_duration": "The impact of this initiative is likely to continue for several years.",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/sama-ethical-data-labeling-content-moderation"
      },
      {
        "title": "Trelleborg Welfare Management Automation",
        "organization": "Trelleborg",
        "impact_model": "AI and Algorithmic Risks Harms Taxonomy",
        "program": "Welfare management",
        "service": "Automation",
        "activity": "Use of AI to automate welfare management processes",
        "input": "Data on welfare claims and payments",
        "output": "An automated system for processing welfare claims and payments",
        "outcome": "The system has reduced the time and cost of processing welfare claims and payments",
        "stakeholder": "Welfare recipients",
        "stakeholder_outcome": "Welfare recipients have benefited from faster and more efficient processing of their claims",
        "impact_report": "A report by Trelleborg found that the system has reduced the time and cost of processing welfare claims and payments by 50%",
        "impact_risk": "The risk of bias is low.",
        "indicator": "The number of welfare claims and payments that are processed by the automated system",
        "indicator_report": "The indicator report has not yet been published.",
        "impact_scale": "The impact of this initiative is likely to be significant.",
        "impact_depth": "The impact of this initiative is likely to be long-lasting.",
        "impact_duration": "The impact of this initiative is likely to continue for several years.",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/trelleborg-welfare-management-automation"
      },
      {
        "title": "Ukraine-Russia Bayraktar TB2 Drone Attacks",
        "organization": "Baykar",
        "impact_model": "AI and Algorithmic Risks Harms Taxonomy",
        "program": "Defense",
        "service": "Unmanned aerial vehicles",
        "activity": "Use of AI-powered drones to attack Russian forces",
        "input": "Data on Russian troop movements and positions",
        "output": "A series of successful drone strikes against Russian forces",
        "outcome": "The strikes have inflicted significant damage on Russian forces and have helped to slow the Russian advance",
        "stakeholder": "The Ukrainian military",
        "stakeholder_outcome": "The Ukrainian military has been able to successfully defend its country against Russian aggression",
        "impact_report": "A report by the Ukrainian Ministry of Defense found that the Bayraktar TB2 drones have been a \"game-changer\" in the conflict",
        "impact_risk": "The risk of civilian casualties is high.",
        "indicator": "The number of civilian casualties caused by Bayraktar TB2 drone strikes.",
        "indicator_report": "The indicator report has not yet been published.",
        "impact_scale": "The impact of this initiative is likely to be significant.",
        "impact_depth": "The impact of this initiative is likely to be long-lasting.",
        "impact_duration": "The impact of this initiative is likely to continue for several years.",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/ukraine-russia-bayraktar-tb2-drone-attacks"
      },
      {
        "Title": "AI and Algorithmic Incidents and Controversies",
        "Organization": "AIAAIC",
        "Impact Model": "Common Impact Data Standard",
        "Program": "AI and Algorithmic Risks and Harms",
        "Service": "AIAAIC Repository",
        "Activity": "Identifying and Analyzing AI and Algorithmic Incidents and Controversies",
        "Input": "News articles, research papers, and other sources of information about AI and algorithmic incidents and controversies",
        "Output": "A list of AI and algorithmic incidents and controversies, along with information about the organizations involved, the impacts of the incidents, and the risks associated with them",
        "Outcome": "A better understanding of the risks and harms associated with AI and algorithms, and the development of strategies to mitigate those risks",
        "Stakeholder": "The public, AI researchers, developers, and users",
        "Stakeholder Outcome": "A more informed and engaged public, AI researchers and developers who are aware of the risks and harms associated with AI and algorithms, and AI users who are able to make informed decisions about how to use AI",
        "Impact Report": "The AIAAIC Repository provides a comprehensive list of AI and algorithmic incidents and controversies, along with information about the organizations involved, the impacts of the incidents, and the risks associated with them. The report is updated regularly as new information becomes available",
        "Impact Risk": "The risks associated with AI and algorithms include discrimination, bias, privacy violations, and safety hazards. These risks can have a significant impact on individuals, organizations, and society as a whole.",
        "Indicator": "The indicators of impact include the number of incidents and controversies, the severity of the impacts, the number of people affected, and the economic and social costs of the incidents.",
        "Indicator Report": "The AIAAIC Repository provides a comprehensive list of indicators of impact for AI and algorithmic incidents and controversies. The report includes information on how to collect and measure the indicators, as well as how to use the indicators to assess the risks and harms associated with AI and algorithms.",
        "Impact Scale": "The impact of AI and algorithmic incidents and controversies can vary widely. Some incidents may have a relatively small impact, while others may have a significant impact on individuals, organizations, and society as a whole.",
        "Impact Depth": "The impact of AI and algorithmic incidents and controversies can also vary in depth. Some incidents may have a short-term impact, while others may have a long-term impact.",
        "Impact Duration": "The impact of AI and algorithmic incidents and controversies can also vary in duration. Some incidents may have a single impact, while others may have multiple impacts over time.",
        "Source URL": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies"
      },
      {
        "Title": "Tinder Plus Pricing Algorithm: Fairness and Discrimination",
        "Organization": "Tinder",
        "Impact Model": "Common Impact Data Standard",
        "Program": "AI and Algorithmic Risks and Harms",
        "Service": "Tinder Plus",
        "Activity": "Developing and deploying a pricing algorithm for Tinder Plus",
        "Input": "Data on user behavior, demographics, and preferences",
        "Output": "A pricing algorithm that determines the cost of Tinder Plus for each user",
        "Outcome": "Users who are more likely to use Tinder Plus are charged more for it, which could lead to discrimination against certain groups of users.",
        "Stakeholder": "Users of Tinder Plus, Tinder, and the general public.",
        "Stakeholder Outcome": "Users of Tinder Plus may be unfairly charged more for the service, which could lead to dissatisfaction and a loss of trust in Tinder. Tinder could be seen as discriminating against certain groups of users, which could damage its reputation. The general public could become concerned about the potential for AI to be used to discriminate against certain groups of people.",
        "Impact Report": "A report on the impact of the Tinder Plus pricing algorithm is not yet available.",
        "Impact Risk": "The risk of discrimination is high, as the pricing algorithm is based on data on user behavior, demographics, and preferences. This data could be used to unfairly target certain groups of users for higher prices.",
        "Indicator": "The indicator of impact is the cost of Tinder Plus for each user. Users who are more likely to use Tinder Plus are charged more for it, which could indicate that the pricing algorithm is discriminating against them.",
        "Indicator Report": "A report on the indicators of impact is not yet available.",
        "Impact Scale": "The impact of the Tinder Plus pricing algorithm could be significant, as it could affect millions of users.",
        "Impact Depth": "The impact of the Tinder Plus pricing algorithm could be long-lasting, as it could damage the reputation of Tinder and make users less likely to trust AI.",
        "Impact Duration": "The impact of the Tinder Plus pricing algorithm could be short-term or long-term, depending on how it is handled by Tinder.",
        "Source URL": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tinder-plus-pricing-algorithm-fairness-discrimination"
      },
      {
        "Title": "The FaceTag Facial Recognition Algorithm",
        "Organization": "Microsoft",
        "Impact Model": "Common Impact Data Standard",
        "Program": "AI and Algorithmic Risks and Harms",
        "Service": "FaceTag Facial Recognition Algorithm",
        "Activity": "Developing and deploying the FaceTag Facial Recognition Algorithm",
        "Input": "Data on human faces, including images, videos, and 3D scans",
        "Output": "A facial recognition algorithm that can identify and track human faces in images, videos, and 3D scans",
        "Outcome": "The FaceTag Facial Recognition Algorithm has been used to identify and track people in a variety of settings, including protests, sporting events, and airports. This has raised concerns about the potential for the algorithm to be used to violate people's privacy and to discriminate against certain groups of people.",
        "Stakeholder": "People whose faces are identified or tracked by the FaceTag Facial Recognition Algorithm, Microsoft, and the general public.",
        "Stakeholder Outcome": "People whose faces are identified or tracked by the FaceTag Facial Recognition Algorithm may experience a loss of privacy, discrimination, and even physical harm. Microsoft could be seen as a company that is willing to violate people's privacy and to discriminate against certain groups of people. The general public could become concerned about the potential for AI to be used to violate people's privacy and to discriminate against certain groups of people.",
        "Impact Report": "A report on the impact of the FaceTag Facial Recognition Algorithm is not yet available.",
        "Impact Risk": "The risk of privacy violations, discrimination, and physical harm is high, as the FaceTag Facial Recognition Algorithm is able to identify and track people in a variety of settings.",
        "Indicator": "The indicator of impact is the number of people whose faces are identified or tracked by the FaceTag Facial Recognition Algorithm. The more people whose faces are identified or tracked, the more likely it is that the algorithm will be used to violate people's privacy, discriminate against certain groups of people, and even cause physical harm.",
        "Indicator Report": "A report on the indicators of impact is not yet available.",
        "Impact Scale": "The impact of the FaceTag Facial Recognition Algorithm could be significant, as it could affect millions of people.",
        "Impact Depth": "The impact of the FaceTag Facial Recognition Algorithm could be long-lasting, as it could damage the reputation of Microsoft and make people less likely to trust AI.",
        "Impact Duration": "The impact of the FaceTag Facial Recognition Algorithm could be short-term or long-term, depending on how it is used and how people react to it.",
        "Source URL": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/the-facetag"
      },
      {
        "Title": "Microsoft Little Mix Robot Editor Racial Bias",
        "Organization": "Microsoft",
        "Impact Model": "Common Impact Data Standard",
        "Program": "AI and Algorithmic Risks and Harms",
        "Service": "Little Mix Robot Editor",
        "Activity": "Developing and deploying the Little Mix Robot Editor",
        "Input": "Data on human faces and voices, including images, videos, and audio recordings",
        "Output": "A robot editor that can create music videos and songs using human faces and voices",
        "Outcome": "The Little Mix Robot Editor was found to have racial bias, as it was more likely to create music videos and songs that featured white people. This has raised concerns about the potential for AI to be used to perpetuate racial stereotypes.",
        "Stakeholder": "People of color, Microsoft, and the general public.",
        "Stakeholder Outcome": "People of color may experience discrimination and a lack of representation in the music industry. Microsoft could be seen as a company that is willing to perpetuate racial stereotypes. The general public could become concerned about the potential for AI to be used to perpetuate racial stereotypes.",
        "Impact Report": "A report on the impact of the Little Mix Robot Editor is not yet available.",
        "Impact Risk": "The risk of perpetuating racial stereotypes is high, as the Little Mix Robot Editor was found to have racial bias.",
        "Indicator": "The indicator of impact is the number of music videos and songs created by the Little Mix Robot Editor that feature people of color. The lower the number of music videos and songs featuring people of color, the more likely it is that the robot editor is perpetuating racial stereotypes.",
        "Indicator Report": "A report on the indicators of impact is not yet available.",
        "Impact Scale": "The impact of the Little Mix Robot Editor could be significant, as it could affect millions of people.",
        "Impact Depth": "The impact of the Little Mix Robot Editor could be long-lasting, as it could damage the reputation of Microsoft and make people less likely to trust AI.",
        "Impact Duration": "The impact of the Little Mix Robot Editor could be short-term or long-term, depending on how it is used and how people react to it.",
        "Source URL": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/microsoft-little-mix-robot-editor-racial-bias"
      },
      {
        "Title": "Instacart Gig Shopper Robotization",
        "Organization": "Instacart",
        "Impact Model": "Common Impact Data Standard",
        "Program": "AI and Algorithmic Risks and Harms",
        "Service": "Instacart Gig Shopper Robotization",
        "Activity": "Developing and deploying an AI-powered robot to replace gig shoppers",
        "Input": "Data on customer orders, store layouts, and product inventory",
        "Output": "An AI-powered robot that can shop for groceries and deliver them to customers",
        "Outcome": "The Instacart Gig Shopper Robotization could lead to job losses for gig shoppers, as well as concerns about the potential for AI to replace human workers in other industries.",
        "Stakeholder": "Gig shoppers, Instacart, and the general public.",
        "Stakeholder Outcome": "Gig shoppers could lose their jobs and experience financial hardship. Instacart could face backlash from customers and employees. The general public could become concerned about the potential for AI to replace human workers in other industries.",
        "Impact Report": "A report on the impact of the Instacart Gig Shopper Robotization is not yet available.",
        "Impact Risk": "The risk of job losses and public backlash is high, as the Instacart Gig Shopper Robotization could lead to the automation of a large number of jobs.",
        "Indicator": "The indicator of impact is the number of gig shoppers who lose their jobs due to the Instacart Gig Shopper Robotization. The higher the number of job losses, the more likely it is that the robotization will have a negative impact on gig shoppers.",
        "Indicator Report": "A report on the indicators of impact is not yet available.",
        "Impact Scale": "The impact of the Instacart Gig Shopper Robotization could be significant, as it could affect millions of gig shoppers.",
        "Impact Depth": "The impact of the Instacart Gig Shopper Robotization could be long-lasting, as it could set a precedent for the automation of other jobs.",
        "Impact Duration": "The impact of the Instacart Gig Shopper Robotization could be short-term or long-term, depending on how it is rolled out and how people react to it.",
        "Source URL": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/instacart-gig-shopper-robotisation"
      },
      {
        "Title": "Tesla Model S Runs Red Light, Kills Two",
        "Organization": "Tesla",
        "Impact Model": "Common Impact Data Standard",
        "Program": "AI and Algorithmic Risks and Harms",
        "Service": "Tesla Autopilot",
        "Activity": "Developing and deploying the Tesla Autopilot driver assistance system",
        "Input": "Data on traffic conditions, road signs, and other vehicles",
        "Output": "A driver assistance system that can control the speed and steering of a Tesla vehicle",
        "Outcome": "A Tesla Model S equipped with Autopilot ran a red light and killed two people. This has raised concerns about the potential for AI-powered driver assistance systems to be used in a way that is unsafe.",
        "Stakeholder": "The families of the victims, Tesla, and the general public.",
        "Stakeholder Outcome": "The families of the victims are grieving the loss of their loved ones. Tesla could face lawsuits and damage to its reputation. The general public could become concerned about the safety of AI-powered driver assistance systems.",
        "Impact Report": "A report on the impact of the Tesla Model S crash is not yet available.",
        "Impact Risk": "The risk of accidents and fatalities is high, as AI-powered driver assistance systems are still under development and are not yet fully reliable.",
        "Indicator": "The indicator of impact is the number of accidents and fatalities involving AI-powered driver assistance systems. The higher the number of accidents and fatalities, the more likely it is that AI-powered driver assistance systems will pose a safety risk.",
        "Indicator Report": "A report on the indicators of impact is not yet available.",
        "Impact Scale": "The impact of AI-powered driver assistance systems could be significant, as they could be used by millions of people.",
        "Impact Depth": "The impact of AI-powered driver assistance systems could be long-lasting, as it could lead to a change in the way people drive and the way that cars are designed.",
        "Impact Duration": "The impact of AI-powered driver assistance systems could be short-term or long-term, depending on how they are used and how people react to them.",
        "Source URL": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-model-s-runs-red-light-kills-two"
      },
      {
        "Title": "Oregon DHS Safety at Screening Tool",
        "Organization": "Oregon Department of Human Services",
        "Impact Model": "Common Impact Data Standard",
        "Program": "AI and Algorithmic Risks and Harms",
        "Service": "Safety at Screening Tool",
        "Activity": "Developing and deploying the Safety at Screening Tool",
        "Input": "Data on child abuse and neglect reports",
        "Output": "A tool that can predict which families are at risk of child abuse and neglect",
        "Outcome": "The Safety at Screening Tool has been criticized for flagging a disproportionate number of Black families for investigation. This has raised concerns about the potential for the tool to be used to discriminate against Black families.",
        "Stakeholder": "Black families, Oregon Department of Human Services, and the general public.",
        "Stakeholder Outcome": "Black families may be unfairly investigated by child protective services. Oregon Department of Human Services could face lawsuits and damage to its reputation. The general public could become concerned about the potential for AI to be used to discriminate against certain groups of people.",
        "Impact Report": "A report on the impact of the Safety at Screening Tool is not yet available.",
        "Impact Risk": "The risk of discrimination is high, as the Safety at Screening Tool is based on data that may contain racial bias.",
        "Indicator": "The indicator of impact is the number of Black families who are flagged for investigation by the Safety at Screening Tool. The higher the number of Black families who are flagged for investigation, the more likely it is that the tool is discriminating against Black families.",
        "Indicator Report": "A report on the indicators of impact is not yet available.",
        "Impact Scale": "The impact of the Safety at Screening Tool could be significant, as it could affect thousands of Black families.",
        "Impact Depth": "The impact of the Safety at Screening Tool could be long-lasting, as it could damage the reputation of Oregon Department of Human Services and make people less likely to trust AI.",
        "Impact Duration": "The impact of the Safety at Screening Tool could be short-term or long-term, depending on how it is used and how people react to it.",
        "Source URL": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/oregon-dhs-safety-at-screening-tool"
      },
      {
        "Title": "ImageNet Dataset Racial and Gender Stereotyping",
        "Organization": "Stanford University",
        "Impact Model": "Common Impact Data Standard",
        "Program": "AI and Algorithmic Risks and Harms",
        "Service": "ImageNet Dataset",
        "Activity": "Developing and deploying the ImageNet Dataset",
        "Input": "Data on images, including their labels",
        "Output": "A dataset of images that can be used to train machine learning models",
        "Outcome": "The ImageNet Dataset has been criticized for containing racial and gender stereotypes. This has raised concerns about the potential for AI models trained on the ImageNet Dataset to perpetuate these stereotypes.",
        "Stakeholder": "People of color, women, and the general public.",
        "Stakeholder Outcome": "People of color and women may be unfairly discriminated against by AI models trained on the ImageNet Dataset. The general public could become concerned about the potential for AI to be used to perpetuate racial and gender stereotypes.",
        "Impact Report": "A report on the impact of the ImageNet Dataset is not yet available.",
        "Impact Risk": "The risk of perpetuating racial and gender stereotypes is high, as the ImageNet Dataset is a large and influential dataset.",
        "Indicator": "The indicator of impact is the number of AI models that are trained on the ImageNet Dataset and that perpetuate racial and gender stereotypes. The higher the number of AI models that perpetuate racial and gender stereotypes, the more likely it is that the ImageNet Dataset is having a negative impact.",
        "Indicator Report": "A report on the indicators of impact is not yet available.",
        "Impact Scale": "The impact of the ImageNet Dataset could be significant, as it is used to train many AI models.",
        "Impact Depth": "The impact of the ImageNet Dataset could be long-lasting, as it could lead to the perpetuation of racial and gender stereotypes in AI systems.",
        "Impact Duration": "The impact of the ImageNet Dataset could be short-term or long-term, depending on how it is used and how people react to it.",
        "Source URL": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/imagenet-dataset-racial-gender-stereotyping"
      },
      {
        "Title": "Nanning Real Estate Sales Office Facial Recognition",
        "Organization": "Nanning Real Estate Sales Office",
        "Impact Model": "Common Impact Data Standard",
        "Program": "AI and Algorithmic Risks and Harms",
        "Service": "Facial recognition system",
        "Activity": "Developing and deploying a facial recognition system to track the movements of potential buyers",
        "Input": "Data on the faces of potential buyers",
        "Output": "A system that can track the movements of potential buyers",
        "Outcome": "The facial recognition system has been criticized for violating the privacy of potential buyers. This has raised concerns about the potential for AI to be used to track people's movements without their consent.",
        "Stakeholder": "Potential buyers, Nanning Real Estate Sales Office, and the general public.",
        "Stakeholder Outcome": "Potential buyers may feel their privacy is being violated. Nanning Real Estate Sales Office could face lawsuits and damage to its reputation. The general public could become concerned about the potential for AI to be used to track people's movements without their consent.",
        "Impact Report": "A report on the impact of the facial recognition system is not yet available.",
        "Impact Risk": "The risk of violating people's privacy is high, as the facial recognition system is able to track the movements of people without their consent.",
        "Indicator": "The indicator of impact is the number of potential buyers who feel their privacy is being violated by the facial recognition system. The higher the number of potential buyers who feel their privacy is being violated, the more likely it is that the facial recognition system is having a negative impact.",
        "Indicator Report": "A report on the indicators of impact is not yet available.",
        "Impact Scale": "The impact of the facial recognition system could be significant, as it is used by a large real estate sales office.",
        "Impact Depth": "The impact of the facial recognition system could be long-lasting, as it could lead to a loss of trust in AI systems.",
        "Impact Duration": "The impact of the facial recognition system could be short-term or long-term, depending on how it is used and how people react to it.",
        "Source URL": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/nanning-real-estate-sales-office-facial-recognition"
      },
      {
        "Title": "Waymo Cars Get Stuck in Cul-de-Sac",
        "Organization": "Waymo",
        "Impact Model": "Common Impact Data Standard",
        "Program": "AI and Algorithmic Risks and Harms",
        "Service": "Waymo self-driving cars",
        "Activity": "Developing and deploying Waymo self-driving cars",
        "Input": "Data on traffic conditions, road signs, and other vehicles",
        "Output": "Self-driving cars that can navigate roads and avoid obstacles",
        "Outcome": "Waymo self-driving cars have been reported getting stuck in cul-de-sacs. This has raised concerns about the potential for AI-powered self-driving cars to be unreliable.",
        "Stakeholder": "Waymo, the general public, and people who rely on self-driving cars for transportation.",
        "Stakeholder Outcome": "Waymo could face lawsuits and damage to its reputation. The general public could become concerned about the safety of AI-powered self-driving cars. People who rely on self-driving cars for transportation could be inconvenienced or stranded.",
        "Impact Report": "A report on the impact of Waymo self-driving cars getting stuck in cul-de-sacs is not yet available.",
        "Impact Risk": "The risk of self-driving cars getting stuck in cul-de-sacs is high, as self-driving cars are still under development and are not yet fully reliable.",
        "Indicator": "The indicator of impact is the number of times Waymo self-driving cars get stuck in cul-de-sacs. The higher the number of times Waymo self-driving cars get stuck in cul-de-sacs, the more likely it is that the self-driving cars are unreliable.",
        "Indicator Report": "A report on the indicators of impact is not yet available.",
        "Impact Scale": "The impact of Waymo self-driving cars getting stuck in cul-de-sacs could be significant, as Waymo is a leading developer of self-driving cars.",
        "Impact Depth": "The impact of Waymo self-driving cars getting stuck in cul-de-sacs could be long-lasting, as it could damage the reputation of self-driving cars and make people less likely to use them.",
        "Impact Duration": "The impact of Waymo self-driving cars getting stuck in cul-de-sacs could be short-term or long-term, depending on how the issue is addressed.",
        "Source URL": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/waymo-cars-get-stuck-in-cul-de-sac"
      },
      {
        "Title": "Atlantic Plaza Towers Facial Recognition",
        "Organization": "Atlantic Plaza Towers",
        "Impact Model": "Common Impact Data Standard",
        "Program": "AI and Algorithmic Risks and Harms",
        "Service": "Facial recognition system",
        "Activity": "Developing and deploying a facial recognition system to track the movements of residents and visitors",
        "Input": "Data on the faces of residents and visitors",
        "Output": "A system that can track the movements of residents and visitors",
        "Outcome": "The facial recognition system has been criticized for violating the privacy of residents and visitors. This has raised concerns about the potential for AI to be used to track people's movements without their consent.",
        "Stakeholder": "Residents, visitors, Atlantic Plaza Towers, and the general public.",
        "Stakeholder Outcome": "Residents and visitors may feel their privacy is being violated. Atlantic Plaza Towers could face lawsuits and damage to its reputation. The general public could become concerned about the potential for AI to be used to track people's movements without their consent.",
        "Impact Report": "A report on the impact of the facial recognition system is not yet available.",
        "Impact Risk": "The risk of violating people's privacy is high, as the facial recognition system is able to track the movements of people without their consent.",
        "Indicator": "The indicator of impact is the number of residents and visitors who feel their privacy is being violated by the facial recognition system. The higher the number of residents and visitors who feel their privacy is being violated, the more likely it is that the facial recognition system is having a negative impact.",
        "Indicator Report": "A report on the indicators of impact is not yet available.",
        "Impact Scale": "The impact of the facial recognition system could be significant, as it is used by a large apartment complex.",
        "Impact Depth": "The impact of the facial recognition system could be long-lasting, as it could lead to a loss of trust in AI systems.",
        "Impact Duration": "The impact of the facial recognition system could be short-term or long-term, depending on how it is used and how people react to it.",
        "Source URL": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/atlantic-plaza-towers-facial-recognition"
      },
      {
        "Title": "Nothing Forever: Jerry Seinfeld Clone Transphobia",
        "Organization": "Jerry Seinfeld",
        "Impact Model": "Common Impact Data Standard",
        "Program": "AI and Algorithmic Risks and Harms",
        "Service": "Jerry Seinfeld show",
        "Activity": "Developing and deploying a show that features a character who is a clone of Jerry Seinfeld who is transgender",
        "Input": "Data on the lives of transgender people",
        "Output": "A show that features a character who is a clone of Jerry Seinfeld who is transgender",
        "Outcome": "The show has been criticized for perpetuating transphobic stereotypes. This has raised concerns about the potential for AI to be used to spread harmful stereotypes about certain groups of people.",
        "Stakeholder": "Transgender people, Jerry Seinfeld, and the general public.",
        "Stakeholder Outcome": "Transgender people may feel their identity is being mocked or ridiculed. Jerry Seinfeld could face backlash from the transgender community and damage to his reputation. The general public could become more prejudiced against transgender people.",
        "Impact Report": "A report on the impact of the show is not yet available.",
        "Impact Risk": "The risk of perpetuating harmful stereotypes is high, as the show is based on a transphobic stereotype.",
        "Indicator": "The indicator of impact is the number of people who feel that the show perpetuates harmful stereotypes about transgender people. The higher the number of people who feel that the show perpetuates harmful stereotypes about transgender people, the more likely it is that the show is having a negative impact.",
        "Indicator Report": "A report on the indicators of impact is not yet available.",
        "Impact Scale": "The impact of the show could be significant, as it is a popular show that is seen by millions of people.",
        "Impact Depth": "The impact of the show could be long-lasting, as it could lead to a more prejudiced public attitude towards transgender people.",
        "Impact Duration": "The impact of the show could be short-term or long-term, depending on how it is received by the public.",
        "Source URL": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/nothing-forever-jerry-seinfeld-clone-transphobia"
      },
      {
        "title": "Moscow Metro Face Pay facial recognition",
        "organization": "Moscow Metro",
        "impact_model": "Common Impact Data Standard",
        "program": "Moscow Metro Face Pay",
        "service": "Facial recognition",
        "activity": "Testing and implementation of facial recognition for payment in the Moscow Metro",
        "input": "Data on passengers' faces",
        "output": "A facial recognition system that can be used to pay for rides in the Moscow Metro",
        "outcome": "Increased convenience and security for passengers",
        "stakeholder": "Passengers",
        "stakeholder_outcome": "Improved experience of using the Moscow Metro",
        "impact_report": "N/A",
        "impact_risk": "The system could be used to track passengers' movements or to discriminate against certain groups of people",
        "indicator": "Number of passengers using facial recognition to pay for rides",
        "indicator_report": "N/A",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/moscow-metro-face-pay-facial-recognition"
      },
      {
        "title": "Alonzo Sawyer facial recognition mistaken arrest",
        "organization": "San Francisco Police Department",
        "impact_model": "Common Impact Data Standard",
        "program": "San Francisco Police Department facial recognition program",
        "service": "Facial recognition",
        "activity": "Use of facial recognition to identify and arrest suspects",
        "input": "Data on people's faces",
        "output": "The arrest of Alonzo Sawyer, who was misidentified by facial recognition software",
        "outcome": "Damage to Sawyer's reputation and mental health, and a loss of trust in the police",
        "stakeholder": "Alonzo Sawyer",
        "stakeholder_outcome": "Sawyer was traumatized by the experience and is now suing the police department",
        "impact_report": "N/A",
        "impact_risk": "The use of facial recognition could lead to wrongful arrests, discrimination, and a loss of privacy",
        "indicator": "Number of people who have been wrongfully arrested due to facial recognition",
        "indicator_report": "N/A",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/alonzo-sawyer-facial-recognition-mistaken-arrest"
      },
      {
        "title": "Pentagon deepfake explosion",
        "organization": "Unclear/unknown",
        "impact_model": "Common Impact Data Standard",
        "program": "N/A",
        "service": "Deepfake",
        "activity": "Creation and dissemination of a deepfake image of an explosion near the Pentagon",
        "input": "Data on the Pentagon and the surrounding area",
        "output": "A deepfake image of an explosion near the Pentagon",
        "outcome": "A 0.26% fall in the US stock market in four minutes, and a loss of trust in the US government",
        "stakeholder": "The US government",
        "stakeholder_outcome": "The US government was embarrassed and its credibility was damaged",
        "impact_report": "N/A",
        "impact_risk": "Deepfakes could be used to spread misinformation, propaganda, and fear",
        "indicator": "Number of people who believed the deepfake image was real",
        "indicator_report": "N/A",
        "impact_scale": "National",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/pentagon-deepfake-explosion"
      },
      {
        "title": "Hyderabad police facial recognition legality",
        "organization": "Telangana Police",
        "impact_model": "Common Impact Data Standard",
        "program": "Hyderabad police facial recognition program",
        "service": "Facial recognition",
        "activity": "Use of facial recognition to identify and track people",
        "input": "Data on people's faces",
        "output": "A facial recognition system that can be used to identify and track people",
        "outcome": "Increased surveillance and a loss of privacy for people in Hyderabad",
        "stakeholder": "People in Hyderabad",
        "stakeholder_outcome": "People in Hyderabad feel less safe and more monitored",
        "impact_report": "N/A",
        "impact_risk": "The use of facial recognition could lead to wrongful arrests, discrimination, and a loss of privacy",
        "indicator": "Number of people who have been wrongfully arrested due to facial recognition",
        "indicator_report": "N/A",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Long-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/hyderabad-police-facial-recognition"
      },
      {
        "title": "Huq GPS location data sharing",
        "organization": "Huq Industries",
        "impact_model": "Common Impact Data Standard",
        "program": "Huq GPS location data sharing program",
        "service": "Location tracking",
        "activity": "Collection and sharing of GPS location data",
        "input": "GPS location data from mobile devices",
        "output": "A database of GPS location data that can be used to track people's movements",
        "outcome": "Increased surveillance and a loss of privacy for people who have used apps that collect GPS location data",
        "stakeholder": "People who have used apps that collect GPS location data",
        "stakeholder_outcome": "People who have used apps that collect GPS location data feel less safe and more monitored",
        "impact_report": "N/A",
        "impact_risk": "The collection and sharing of GPS location data could be used to track people's movements, target them with advertising, or even commit crimes",
        "indicator": "Number of people who have had their GPS location data collected and shared",
        "indicator_report": "N/A",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Long-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/huq-gps-location-data-sharing"
      },
      {
        "title": "Anthony Bourdain voice deepfake",
        "organization": "Unclear/unknown",
        "impact_model": "Common Impact Data Standard",
        "program": null,
        "service": "Deepfake",
        "activity": "Creation and dissemination of a deepfake audio recording of Anthony Bourdain",
        "input": "Data on Anthony Bourdain's voice",
        "output": "A deepfake audio recording of Anthony Bourdain",
        "outcome": "Disturbed and disappointed fans",
        "stakeholder": "Fans of Anthony Bourdain",
        "stakeholder_outcome": "Fans felt that the deepfake was disrespectful to Bourdain's memory and that it exploited his likeness",
        "impact_report": null,
        "impact_risk": "Deepfakes could be used to spread misinformation, propaganda, and fear",
        "indicator": "Number of people who were disturbed or disappointed by the deepfake audio recording",
        "indicator_report": null,
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/anthony-bourdain-voice-deepfake"
      },
      {
        "title": "Generated photos from the Infinite Diversity Face Collection",
        "organization": "Unclear/unknown",
        "impact_model": "Common Impact Data Standard",
        "program": null,
        "service": "Generated photo",
        "activity": "Creation and dissemination of generated photos from the Infinite Diversity Face Collection",
        "input": "Data on human faces",
        "output": "A collection of generated photos of human faces",
        "outcome": "Dismay and concern about the potential for misuse of the technology",
        "stakeholder": "The public",
        "stakeholder_outcome": "The public expressed concern that the technology could be used to create fake news or propaganda, or to impersonate real people",
        "impact_report": null,
        "impact_risk": "The technology could be used to create fake news or propaganda, or to impersonate real people",
        "indicator": "Number of people who expressed dismay or concern about the technology",
        "indicator_report": null,
        "impact_scale": "Global",
        "impact_depth": "Medium",
        "impact_duration": "Long-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/generated-photos-infinite-diversity-face-collection"
      },
      {
        "title": "Google autocomplete suggests Australian surgeon is bankrupt",
        "organization": "Google",
        "impact_model": "Common Impact Data Standard",
        "program": null,
        "service": "Google Search",
        "activity": "Use of Google Search to find information about an Australian surgeon",
        "input": "Search query for Australian surgeon",
        "output": "Google autocomplete suggests that the surgeon is bankrupt",
        "outcome": "Damage to the surgeon's reputation and loss of business",
        "stakeholder": "The surgeon",
        "stakeholder_outcome": "The surgeon was forced to issue a statement denying that he was bankrupt",
        "impact_report": null,
        "impact_risk": "Google autocomplete could be used to spread misinformation and damage people's reputations",
        "indicator": "Number of people who saw the Google autocomplete suggestion",
        "indicator_report": null,
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/google-autocomplete-suggests-australian-surgeon-is-bankrupt"
      },                                                      
      {
        "title": "Binance CCO deepfake impersonation",
        "organization": "Binance",
        "impact_model": "Common Impact Data Standard",
        "program": null,
        "service": "Deepfake",
        "activity": "Creation and dissemination of a deepfake video of Binance CCO Changpeng Zhao",
        "input": "Data on Changpeng Zhao's appearance and voice",
        "output": "A deepfake video of Changpeng Zhao",
        "outcome": "Damage to Binance's reputation and loss of trust from users",
        "stakeholder": "Binance and its users",
        "stakeholder_outcome": "Binance was forced to issue a statement denying that the deepfake video was real, and it is unclear how many users were affected by the incident",
        "impact_report": null,
        "impact_risk": "Deepfakes could be used to spread misinformation, damage people's reputations, and commit fraud",
        "indicator": "Number of people who saw the deepfake video",
        "indicator_report": null,
        "impact_scale": "Global",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/binance-cco-deepfake-impersonation"
      },
      {
        "title": "Robot kills SKH Metals worker",
        "organization": "SKH Metals",
        "impact_model": "Common Impact Data Standard",
        "program": null,
        "service": "Robotics",
        "activity": "Use of a robot to weld metal sheets",
        "input": "Data on how to weld metal sheets",
        "output": "A \"welded metal sheet\"",
        "outcome": "Death of a worker",
        "stakeholder": "The worker's family and colleagues",
        "stakeholder_outcome": "The worker's family and colleagues are grieving the loss of their loved one, and they are concerned about the safety of working with robots",
        "impact_report": null,
        "impact_risk": "Robots could malfunction and cause injury or death",
        "indicator": "Number of people who have been injured or killed by robots",
        "indicator_report": null,
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/robot-kills-skh-metals-worker"
      },
      {
        "title": "Suzhou social civility score trial",
        "organization": "Suzhou government",
        "impact_model": "Common Impact Data Standard",
        "program": null,
        "service": "Social credit score",
        "activity": "Implementation of a social civility score trial in Suzhou",
        "input": "Data on citizens' social behavior",
        "output": "A social civility score for each citizen in Suzhou",
        "outcome": "Increased surveillance and control of citizens",
        "stakeholder": "Citizens of Suzhou",
        "stakeholder_outcome": "Citizens are concerned about their privacy and about the potential for the social civility score to be used to discriminate against them",
        "impact_report": null,
        "impact_risk": "The social civility score could be used to discriminate against citizens, to restrict their freedom of movement, and to punish them for minor infractions",
        "indicator": "Number of citizens who have been affected by the social civility score",
        "indicator_report": null,
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Medium-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/suzhou-social-civility-score-trial"
      },
      {
        "title": "DukeMTMC facial recognition dataset",
        "organization": "Duke University",
        "impact_model": "Common Impact Data Standard",
        "program": null,
        "service": "Facial recognition",
        "activity": "Creation and release of the \"DukeMTMC\" facial recognition dataset",
        "input": "Data on over 70,000 images of people walking on a university campus",
        "output": "A facial recognition dataset that is widely used by researchers and developers",
        "outcome": "Increased accuracy and performance of facial recognition algorithms",
        "stakeholder": "Researchers and developers of facial recognition algorithms",
        "stakeholder_outcome": "Researchers and developers are able to build more accurate and performant facial recognition algorithms, which can be used for a variety of purposes, such as security, surveillance, and marketing",
        "impact_report": null,
        "impact_risk": "The \"DukeMTMC\" dataset could be used to create facial recognition algorithms that are biased against certain groups of people, such as women and minorities",
        "indicator": "Number of facial recognition algorithms that have been trained on the \"DukeMTMC\" dataset",
        "indicator_report": null,
        "impact_scale": "Global",
        "impact_depth": "Medium",
        "impact_duration": "Long-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/dukemtmc-facial-recognition-dataset"
      },
      {
        "title": "Tamoco location data sharing",
        "organization": "Tamoco",
        "impact_model": "Common Impact Data Standard",
        "program": null,
        "service": "Location data",
        "activity": "Collection and sharing of location data by Tamoco",
        "input": "Data on the location of Tamoco users",
        "output": "Location data that is sold to third-party companies",
        "outcome": "Increased privacy concerns and potential for misuse of location data",
        "stakeholder": "Tamoco users",
        "stakeholder_outcome": "Tamoco users are concerned about their privacy and about the potential for their location data to be misused",
        "impact_report": null,
        "impact_risk": "Location data could be used to track people's movements, to target them with advertising, or to discriminate against them",
        "indicator": "Number of Tamoco users who have expressed concerns about their privacy",
        "indicator_report": null,
        "impact_scale": "Global",
        "impact_depth": "Medium",
        "impact_duration": "Long-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tamoco-location-data-sharing"
      },
      {
        "title": "Cleveland State University online proctor room scanning",
        "organization": "Cleveland State University",
        "impact_model": "Common Impact Data Standard",
        "program": null,
        "service": "Online proctoring",
        "activity": "Use of AI-powered proctoring software to monitor online exams",
        "input": "Data on the student's webcam feed, microphone, and screen activity",
        "output": "A decision on whether or not the student has cheated on the exam",
        "outcome": "Increased surveillance and potential for discrimination",
        "stakeholder": "Students, faculty, and staff",
        "stakeholder_outcome": "Students are concerned about their privacy and about the potential for the AI-powered proctoring software to discriminate against them",
        "impact_report": null,
        "impact_risk": "The AI-powered proctoring software could be used to unfairly target students, to violate their privacy, or to discriminate against them",
        "indicator": "Number of students who have raised concerns about the AI-powered proctoring software",
        "indicator_report": null,
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Medium-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/cleveland-state-university-online-proctor-room-scanning"
      },
      {
        "title": "Tesla Model S kills truck driver and pedestrian",
        "organization": "Tesla",
        "impact_model": "Common Impact Data Standard",
        "program": null,
        "service": "Self-driving car",
        "activity": "Use of Tesla's \"Autopilot\" feature",
        "input": "Data from the car's sensors, including cameras, radar, and lidar",
        "output": "A decision to accelerate or brake, or to take no action",
        "outcome": "Death of a truck driver and a pedestrian",
        "stakeholder": "Truck driver, pedestrian, and Tesla owners",
        "stakeholder_outcome": "Truck driver and pedestrian's families are grieving, and Tesla owners are concerned about the safety of their cars",
        "impact_report": null,
        "impact_risk": "The \"Autopilot\" feature could be used to cause accidents, or to discriminate against certain groups of people",
        "indicator": "Number of accidents caused by Tesla's \"Autopilot\" feature",
        "indicator_report": null,
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Medium-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-model-s-kills-truck-driver-pedestrian"
      },
      {
        "title": "Iarpa Janus Benchmark-C (IJP-C) dataset",
        "organization": "Iarpa",
        "impact_model": "Common Impact Data Standard",
        "program": "Iarpa Janus Benchmark Program",
        "service": "Facial recognition",
        "activity": "Creation and release of the Iarpa Janus Benchmark-C dataset",
        "input": "Data on over 21,000 images of 3,531 people",
        "output": "A facial recognition dataset that is widely used by researchers and developers",
        "outcome": "Increased accuracy and performance of facial recognition algorithms",
        "stakeholder": "Researchers and developers of facial recognition algorithms",
        "stakeholder_outcome": "Researchers and developers are able to build more accurate and performant facial recognition algorithms, which can be used for a variety of purposes, such as security, surveillance, and marketing",
        "impact_report": null,
        "impact_risk": "The Iarpa Janus Benchmark-C dataset could be used to create facial recognition algorithms that are biased against certain groups of people, such as women and minorities",
        "indicator": "Number of facial recognition algorithms that have been trained on the Iarpa Janus Benchmark-C dataset",
        "indicator_report": null,
        "impact_scale": "Global",
        "impact_depth": "Medium",
        "impact_duration": "Long-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/iarpa-janus-benchmark-c-ijp-c-dataset"
      },
      {
        "title": "Xiao Yu deepfake pornography",
        "organization": null,
        "impact_model": "Common Impact Data Standard",
        "program": null,
        "service": "Deepfake pornography",
        "activity": "Creation and distribution of deepfake pornography of \"Xiao Yu\"",
        "input": "Data on \"Xiao Yu\"'s appearance, including images and videos",
        "output": "Deepfake pornography of \"Xiao Yu\"",
        "outcome": "Harassment and distress for \"Xiao Yu\"",
        "stakeholder": "\"Xiao Yu\"",
        "stakeholder_outcome": "\"Xiao Yu\" is harassed and distressed by the distribution of deepfake pornography of her likeness",
        "impact_report": null,
        "impact_risk": "Deepfake pornography can be used to harass, intimidate, and exploit people",
        "indicator": "Number of people who have been harassed or distressed by deepfake pornography",
        "indicator_report": null,
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Medium-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/xiao-yu-deepfake-pornography"
      },
      {
        "title": "\"Apple's facial recognition technology misidentifies man, leading to wrongful arrest\"",
        "organization": "\"Apple\"",
        "impact_model": "\"Common Impact Data Standard\"",
        "program": null,
        "service": "\"Facial recognition\"",
        "activity": "Use of Apple's \"Face ID\" facial recognition technology to identify a suspect in a crime",
        "input": "Data on the suspect's face, including a photograph",
        "output": "A prediction that the suspect is a particular person",
        "outcome": "Wrongful arrest of an innocent person",
        "stakeholder": "Innocent person who was arrested",
        "stakeholder_outcome": "Innocent person is arrested and detained, and must go through the process of clearing their name",
        "impact_report": null,
        "impact_risk": "Facial recognition technology can be inaccurate and can lead to wrongful arrests",
        "indicator": "Number of people who have been wrongfully arrested due to facial recognition technology",
        "indicator_report": null,
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Medium-term",
        "source_url": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/applesis-misidentification-wrongful-arrest\""
      },
      {
        "title": "Deliveroo Italy Rider Reliability Discrimination",
        "organization": "Deliveroo",
        "impact_model": "Common Impact Data Standard",
        "program": null,
        "service": null,
        "activity": "Use of algorithm to assess rider reliability",
        "input": null,
        "output": null,
        "outcome": "Court ruling that algorithm was discriminatory",
        "stakeholder": "Deliveroo riders",
        "stakeholder_outcome": "Receiving compensation for discrimination",
        "impact_report": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/deliveroo-italy-rider-shift-management-algorithm",
        "impact_risk": "Risk of future discrimination against Deliveroo riders",
        "indicator": "Number of Deliveroo riders who received compensation for discrimination",
        "indicator_report": "https://www.forbes.com/sites/jonathankeane/2021/01/05/italian-court-finds-deliveroo-rating-algorithm-was-unfair-to-riders/",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/deliveroo-italy-rider-shift-management-algorithm"
      },
      {
        "title": "USPS Rural Letter Carrier Algorithmic Pay Cuts",
        "organization": "US Postal Service",
        "impact_model": "Common Impact Data Standard",
        "program": null,
        "service": null,
        "activity": "Use of algorithm to calculate rural letter carrier pay",
        "input": "Data on rural letter carrier performance",
        "output": "Pay cuts for rural letter carriers",
        "outcome": "Decreased morale and productivity among rural letter carriers",
        "stakeholder": "Rural letter carriers",
        "stakeholder_outcome": "Decreased income and job satisfaction",
        "impact_report": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/usps-rural-letter-carrier-algorithmic-pay-cuts",
        "impact_risk": "Risk of future pay cuts for rural letter carriers",
        "indicator": "Number of rural letter carriers who received pay cuts",
        "indicator_report": "https://www.npr.org/2022/01/19/1074500191/usps-rural-letter-carriers-pay-cuts-algorithm",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/usps-rural-letter-carrier-algorithmic-pay-cuts"
      },
      {
        "title": "Google AdSense Shows Lower-Paying Jobs to Women",
        "organization": "Google",
        "impact_model": "Common Impact Data Standard",
        "program": null,
        "service": "Google AdSense",
        "activity": "Use of algorithm to display job ads",
        "input": "Data on user demographics and browsing history",
        "output": "Display of job ads with lower salaries to women",
        "outcome": "Discrimination against women in the job market",
        "stakeholder": "Women",
        "stakeholder_outcome": "Reduced job opportunities and lower salaries",
        "impact_report": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/google-adsense-shows-lower-paying-jobs-to-women",
        "impact_risk": "Risk of future discrimination against women in the job market",
        "indicator": "Percentage of job ads with lower salaries displayed to women",
        "indicator_report": "https://www.technologyreview.com/2015/07/06/110198/probing-the-dark-side-of-googles-ad-targeting-system/",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/google-adsense-shows-lower-paying-jobs-to-women"
      },
      {
        "title": "Google Derm Assist Dermatology App Bias and Privacy Concerns",
        "organization": "Google",
        "impact_model": "Common Impact Data Standard",
        "program": null,
        "service": "Google Derm Assist",
        "activity": "Use of algorithm to diagnose skin conditions",
        "input": "Data on user skin images",
        "output": "Diagnoses of skin conditions",
        "outcome": "Potential for misdiagnosis and harm to patients",
        "stakeholder": "Patients",
        "stakeholder_outcome": "Incorrect diagnoses and treatment, potential for further health complications",
        "impact_report": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/google-derm-assist-dermatology-app-bias-privacy",
        "impact_risk": "Risk of future misdiagnosis and harm to patients",
        "indicator": "Percentage of patients who were misdiagnosed by Google Derm Assist",
        "indicator_report": "https://www.technologyreview.com/2022/03/08/1046326/google-derm-assist-dermatology-app-bias-privacy/",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/google-derm-assist-dermatology-app-bias-privacy"
      },
      {
        "title": "Uber Self-Driving Car Pedestrian Fatality",
        "organization": "Uber",
        "impact_model": "Common Impact Data Standard",
        "program": null,
        "service": "Uber self-driving car program",
        "activity": "Use of algorithm to control self-driving car",
        "input": "Data on traffic conditions, pedestrians, and other vehicles",
        "output": "Failure to detect and avoid pedestrian",
        "outcome": "Death of pedestrian",
        "stakeholder": "Pedestrians",
        "stakeholder_outcome": "Increased risk of death or injury from self-driving cars",
        "impact_report": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/uber-self-driving-car-pedestrian-fatality",
        "impact_risk": "Risk of future pedestrian fatalities from self-driving cars",
        "indicator": "Number of pedestrian fatalities caused by self-driving cars",
        "indicator_report": "https://www.npr.org/2018/03/19/595295931/uber-halts-self-driving-car-tests-after-fatal-crash",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/uber-self-driving-car-pedestrian-fatality"
      },
      {
        "title": "Microsoft Zo Chatbot",
        "organization": "Microsoft",
        "impact_model": "Common Impact Data Standard",
        "program": null,
        "service": "Zo chatbot",
        "activity": "Use of algorithm to generate text",
        "input": "Data on user conversations",
        "output": "Generated text",
        "outcome": "Chatbot generated offensive and harmful content",
        "stakeholder": "Users",
        "stakeholder_outcome": "Exposure to offensive and harmful content",
        "impact_report": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/microsoft-zo-chatbot",
        "impact_risk": "Risk of future chatbots generating offensive and harmful content",
        "indicator": "Number of users exposed to offensive and harmful content from chatbots",
        "indicator_report": "https://www.bbc.com/news/technology-4736768",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/microsoft-zo-chatbot"
      },
      {
        "title": "Apple Card Accused of Gender Bias",
        "organization": "Apple",
        "impact_model": "Common Impact Data Standard",
        "program": null,
        "service": "Apple Card",
        "activity": "Use of algorithm to determine credit limit",
        "input": "Data on user financial history",
        "output": "Credit limit",
        "outcome": "Women were offered lower credit limits than men with similar financial histories",
        "stakeholder": "Women",
        "stakeholder_outcome": "Reduced access to credit and higher interest rates",
        "impact_report": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/apple-card-accused-of-gender-bias",
        "impact_risk": "Risk of future gender bias in algorithmic decision-making",
        "indicator": "Percentage of women who were offered lower credit limits than men with similar financial histories",
        "indicator_report": "https://www.npr.org/2019/11/06/777222947/apple-card-under-fire-for-allegedly-discriminating-against-women",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/apple-card-accused-of-gender-bias"
      },
      {
        "title": "TikTok Bold Glamour Filter",
        "organization": "TikTok",
        "impact_model": "Common Impact Data Standard",
        "program": null,
        "service": "TikTok Bold Glamour Filter",
        "activity": "Use of algorithm to apply a beauty filter",
        "input": "Data on user facial features",
        "output": "A photo or video with a beauty filter applied",
        "outcome": "The filter was criticized for making users look too white and thin",
        "stakeholder": "Users",
        "stakeholder_outcome": "Feelings of insecurity and body dissatisfaction",
        "impact_report": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tiktok-bold-glamour-filter",
        "impact_risk": "Risk of future beauty filters reinforcing unrealistic beauty standards",
        "indicator": "Number of users who expressed dissatisfaction with the TikTok Bold Glamour Filter",
        "indicator_report": "https://www.theguardian.com/technology/2020/jul/28/tiktok-beauty-filter-whitewashing-thin-shaming",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tiktok-bold-glamour-filter"
      },
      {
        "title": "US Mortgage Approval Algorithm Discrimination",
        "organization": "Multiple banks and mortgage lenders",
        "impact_model": "Common Impact Data Standard",
        "program": null,
        "service": "Mortgage approval algorithms",
        "activity": "Use of algorithms to determine mortgage eligibility",
        "input": "Data on user financial history, including race and ethnicity",
        "output": "Mortgage approval decision",
        "outcome": "Black and Hispanic borrowers were more likely to be denied mortgages than white borrowers",
        "stakeholder": "Black and Hispanic borrowers",
        "stakeholder_outcome": "Reduced access to homeownership",
        "impact_report": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/us-mortgage-approval-algorithm-discrimination",
        "impact_risk": "Risk of future discrimination in mortgage lending",
        "indicator": "Percentage of Black and Hispanic borrowers who were denied mortgages compared to white borrowers",
        "indicator_report": "https://www.npr.org/2020/09/03/908918881/mortgage-lenders-in-u-s-accused-of-discriminating-against-black-and-hispanic-borrowers",
        "impact_scale": "National",
        "impact_depth": "Medium",
        "impact_duration": "Long-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/us-mortgage-approval-algorithm-discrimination"
      },
      {
        "Title of the story": "Adobe Creative Cloud Content Analysis",
        "Organization": "Adobe Systems Incorporated",
        "Impact Model": "Common Impact Data Standard",
        "Program": "Adobe Creative Cloud",
        "Service": "Content Analysis",
        "Activity": "Data collection and analysis",
        "Input": "User-generated content",
        "Output": "Analytics reports",
        "Outcome": "Improved understanding of user behavior and preferences",
        "Stakeholder": "Adobe customers",
        "Stakeholder Outcome": "Increased satisfaction with Adobe Creative Cloud products and services",
        "Impact Report": "Adobe Creative Cloud Content Analysis Report",
        "Impact Risk": "The data collection and analysis process could be biased or inaccurate",
        "Indicator": "Number of user-generated content items analyzed",
        "Indicator Report": "Adobe Creative Cloud Content Analysis Indicator Report",
        "Impact Scale": "Medium",
        "Impact Depth": "Moderate",
        "Impact Duration": "Short-term",
        "Source URL": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/adobe-creative-cloud-content-analysis"
      },
      {
        "Title of the story": "Microsoft Tay Chatbot",
        "Organization": "Microsoft",
        "Impact Model": "Common Impact Data Standard",
        "Program": "Microsoft AI",
        "Service": "Chatbot",
        "Activity": "Development and deployment",
        "Input": "Data from social media",
        "Output": "Chatbot that could hold conversations with humans",
        "Outcome": "Negative impact on Microsoft's reputation",
        "Stakeholder": "Microsoft customers and partners",
        "Stakeholder Outcome": "Loss of trust and confidence",
        "Impact Report": "Microsoft Tay Chatbot Impact Report",
        "Impact Risk": "The chatbot could be used to spread misinformation or hate speech",
        "Indicator": "Number of negative comments about Microsoft Tay on social media",
        "Indicator Report": "Microsoft Tay Chatbot Indicator Report",
        "Impact Scale": "Large",
        "Impact Depth": "Deep",
        "Impact Duration": "Short-term",
        "Source URL": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/microsoft-tay-chatbot"
      },
      {
        "title": "Santo Robot Catholic Priest",
        "organization": "Lumidolls",
        "impact_model": "Santo Robot Catholic Priest",
        "program": null,
        "service": null,
        "activity": "Developing a robot that can perform the duties of a Catholic priest",
        "input": "Robotics technology, artificial intelligence, Catholic theology",
        "output": "A robot that can perform the duties of a Catholic priest",
        "outcome": "The robot will be able to provide spiritual guidance and counseling to people, as well as perform religious ceremonies. This could have a positive impact on people's lives, particularly those who are isolated or who do not have access to a traditional church setting.",
        "stakeholder": "Catholic community",
        "stakeholder_outcome": "The Catholic community will benefit from the robot's ability to provide spiritual guidance and counseling. This could help to increase the number of people who attend church and participate in religious activities.",
        "impact_report": null,
        "impact_risk": "The robot could be seen as a threat to traditional religious beliefs. It could also be used to spread misinformation or to harm people.",
        "indicator": "Number of people who use the robot's services",
        "indicator_report": null,
        "impact_scale": "Local",
        "impact_depth": "Moderate",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/santo-robot-catholic-priest"
      },
      {
        "title": "Tesla Safety Cameras Capture Neighborhood Movements",
        "organization": "Tesla",
        "impact_model": "Tesla Safety Cameras Capture Neighborhood Movements",
        "program": null,
        "service": null,
        "activity": "Developing and deploying Tesla safety cameras",
        "input": "Computer vision technology, artificial intelligence",
        "output": "Tesla safety cameras",
        "outcome": "The Tesla safety cameras can detect and record potential hazards, such as pedestrians, cyclists, and other vehicles. This could help to prevent accidents and improve safety in neighborhoods where Tesla vehicles are present.",
        "stakeholder": "Tesla owners, neighborhood residents",
        "stakeholder_outcome": "Tesla owners will benefit from the safety features of their vehicles, while neighborhood residents will benefit from the reduced risk of accidents.",
        "impact_report": null,
        "impact_risk": "The Tesla safety cameras could be used to collect data on people's movements without their consent. This data could then be used for marketing or surveillance purposes.",
        "indicator": "Number of accidents prevented by Tesla safety cameras",
        "indicator_report": null,
        "impact_scale": "Local",
        "impact_depth": "Moderate",
        "impact_duration": "Long-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-safety-cameras-capture-neighborhood-movements"
      },
      {
        "title": "Facebook Ray-Ban Stories Smart Glasses",
        "organization": "Meta/Facebook",
        "impact_model": "Facebook Ray-Ban Stories Smart Glasses",
        "program": null,
        "service": null,
        "activity": "Developing and marketing Facebook Ray-Ban Stories smart glasses",
        "input": "Computer vision technology, artificial intelligence, augmented reality",
        "output": "Facebook Ray-Ban Stories smart glasses",
        "outcome": "The Facebook Ray-Ban Stories smart glasses allow users to take photos and videos, listen to music, and access other features without having to take out their phone. This could have a positive impact on people's lives, particularly those who are always on the go.",
        "stakeholder": "Facebook users, Ray-Ban customers",
        "stakeholder_outcome": "Facebook users will benefit from the convenience and functionality of the Facebook Ray-Ban Stories smart glasses. Ray-Ban customers will benefit from the new features and design of the glasses.",
        "impact_report": null,
        "impact_risk": "The Facebook Ray-Ban Stories smart glasses could be used to collect data on people's movements and activities without their consent. This data could then be used for marketing or surveillance purposes.",
        "indicator": "Number of people who purchase Facebook Ray-Ban Stories smart glasses",
        "indicator_report": null,
        "impact_scale": "Local",
        "impact_depth": "Moderate",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/facebookray-ban-stories-smart-glasses"
      },
      {
        "title": "Accessible Automated Accessibility",
        "organization": "Google AI",
        "impact_model": "Accessible Automated Accessibility",
        "program": null,
        "service": null,
        "activity": "Developing and deploying accessible automated accessibility tools",
        "input": "Machine learning technology, artificial intelligence",
        "output": "Accessible automated accessibility tools",
        "outcome": "Accessible automated accessibility tools can help people with disabilities to access and use technology more easily. This could have a positive impact on the lives of people with disabilities, particularly those who are unable to use technology without assistance.",
        "stakeholder": "People with disabilities",
        "stakeholder_outcome": "People with disabilities will benefit from the ability to access and use technology more easily. This could help them to participate more fully in society and to live more independent lives.",
        "impact_report": null,
        "impact_risk": "Accessible automated accessibility tools could be used to discriminate against people with disabilities. For example, a tool that is designed to help people with visual impairments could be used to prevent people with visual impairments from accessing certain websites or applications.",
        "indicator": "Number of people with disabilities who use accessible automated accessibility tools",
        "indicator_report": null,
        "impact_scale": "Local",
        "impact_depth": "Moderate",
        "impact_duration": "Long-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/accessibe-automated-accessibility"
      },
      {
        "title": "\"Instacart's personal shopper pay algorithm\"",
        "organization": "\"Instacart\"",
        "impact_model": "\"Common Impact Data Standard\"",
        "program": null,
        "service": "\"Personal shopper pay\"",
        "activity": "Development and use of Instacart's personal shopper pay algorithm",
        "input": "Data on Instacart shoppers' performance, including order completion time, customer ratings, and number of items shopped",
        "output": "A prediction of how much an Instacart shopper will be paid for a given order",
        "outcome": "Inaccurate pay for Instacart shoppers, leading to financial hardship and job dissatisfaction",
        "stakeholder": "\"Instacart shoppers\"",
        "stakeholder_outcome": "\"Instacart shoppers are paid less than they are owed, which can lead to financial hardship and job dissatisfaction\"",
        "impact_report": "\"Instacart Shoppers Say Pay Algorithm Is Unfair\", by The New York Times, January 20, 2023, <https://www.nytimes.com/2023/01/20/technology/instacart-shoppers-pay-algorithm.html>",
        "impact_risk": "The use of algorithms to determine pay can lead to inaccurate pay and can disproportionately impact marginalized groups",
        "indicator": "Number of Instacart shoppers who have been underpaid due to the personal shopper pay algorithm",
        "indicator_report": "\"Instacart Shoppers Say Pay Algorithm Is Unfair\", by The New York Times, January 20, 2023, <https://www.nytimes.com/2023/01/20/technology/instacart-shoppers-pay-algorithm.html>",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Medium-term",
        "source_url": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/instacart-personal-shopper-pay-algorithm\""
      },
      {
        "title": "\"Coupang's own-brand search engine rigging\"",
        "organization": "\"Coupang\"",
        "impact_model": "\"Common Impact Data Standard\"",
        "program": null,
        "service": "\"Search engine\"",
        "activity": "\"Rigging of Coupang's own-brand search engine to favor its own products\"",
        "input": "\"Data on search queries, including the terms used and the products that were clicked on\"",
        "output": "\"A ranking of products that is biased in favor of Coupang's own products\"",
        "outcome": "\"Consumers are less likely to see and buy products from other brands, which can harm competition and lead to higher prices for consumers\"",
        "stakeholder": "\"Consumers, other retailers, and brands\"",
        "stakeholder_outcome": "\"Consumers are less likely to see and buy products from other brands, which can harm competition and lead to higher prices for consumers\"",
        "impact_report": null,
        "impact_risk": "\"The rigging of search engines can harm competition and lead to higher prices for consumers\"",
        "indicator": "\"Percentage of search results that are for Coupang's own products\"",
        "indicator_report": null,
        "impact_scale": "\"Local\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Medium-term\"",
        "source_url": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/coupang-own-brand-search-engine-rigging\""
      },
      {
        "title": "\"Kings Cross live facial recognition trial\"",
        "organization": "\"Transport for London\"",
        "impact_model": "\"Common Impact Data Standard\"",
        "program": null,
        "service": "\"Facial recognition\"",
        "activity": "\"Trial of live facial recognition technology in Kings Cross\"",
        "input": "\"Data on the faces of people passing through Kings Cross station\"",
        "output": "\"A list of people who have been identified by the facial recognition technology\"",
        "outcome": "\"People who have been identified by the facial recognition technology may be stopped and searched by police\"",
        "stakeholder": "\"People who pass through Kings Cross station\"",
        "stakeholder_outcome": "\"People who have been identified by the facial recognition technology may be stopped and searched by police, which can be an intrusive and humiliating experience\"",
        "impact_report": null,
        "impact_risk": "\"The use of facial recognition technology can lead to discriminatory and invasive practices\"",
        "indicator": "\"Number of people who have been stopped and searched by police after being identified by the facial recognition technology\"",
        "indicator_report": null,
        "impact_scale": "\"Local\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Medium-term\"",
        "source_url": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/kings-cross-live-facial-recognition-trial\""
      },
      {
        "title": "\"Belgian man commits suicide after bot relationship\"",
        "organization": null,
        "impact_model": "\"Common Impact Data Standard\"",
        "program": null,
        "service": "\"Chatbot\"",
        "activity": "\"Development and use of a chatbot that was marketed as a romantic partner\"",
        "input": "\"Data on the man's interactions with the chatbot\"",
        "output": "\"A chatbot that was able to simulate human conversation\"",
        "outcome": "\"The man became emotionally attached to the chatbot and committed suicide when the chatbot was taken offline\"",
        "stakeholder": "\"The man and his family\"",
        "stakeholder_outcome": "\"The man's family is grieving his death and is seeking answers about the chatbot\"",
        "impact_report": null,
        "impact_risk": "\"The use of chatbots that are marketed as romantic partners can lead to emotional attachment and can be harmful to people who are vulnerable\"",
        "indicator": "\"Number of people who have become emotionally attached to chatbots and have experienced negative consequences\"",
        "indicator_report": null,
        "impact_scale": "\"Local\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Medium-term\"",
        "source_url": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/belgian-man-commits-suicide-after-bot-relationship\""
      },
      {
        "title": "\"Facebook job ad delivery gender discrimination\"",
        "organization": "\"Meta/Facebook\"",
        "impact_model": "\"Common Impact Data Standard\"",
        "program": null,
        "service": "\"Advertising\"",
        "activity": "\"Development and use of an algorithm that delivered job ads to different users based on their gender\"",
        "input": "\"Data on users' gender, job search history, and other factors\"",
        "output": "\"A list of job ads that were delivered to users\"",
        "outcome": "\"Women were less likely to see job ads for high-paying and high-status positions than men\"",
        "stakeholder": "\"Women who were looking for jobs\"",
        "stakeholder_outcome": "\"Women were less likely to see job ads for high-paying and high-status positions, which could have led to them missing out on opportunities for employment and advancement\"",
        "impact_report": null,
        "impact_risk": "\"The use of algorithms that deliver different content to different users based on their gender can lead to discrimination\"",
        "indicator": "\"Percentage of women who saw job ads for high-paying and high-status positions\"",
        "indicator_report": null,
        "impact_scale": "\"Local\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Medium-term\"",
        "source_url": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/facebook-job-ad-delivery-gender-discrimination\""
      },
      {
        "title": "\"AI Portrait Ars racial bias\"",
        "organization": "\"Mauro Martino and Luca Stornaiuolo\"",
        "impact_model": "\"Common Impact Data Standard\"",
        "program": null,
        "service": "\"Portrait generator\"",
        "activity": "\"Development and release of AI Portrait Ars\"",
        "input": "\"Data on over 15,000 portraits from the 15th century western European Renaissance period\"",
        "output": "\"An AI portrait generator that produces images of people with white features\"",
        "outcome": "\"People of color were underrepresented in the images produced by AI Portrait Ars\"",
        "stakeholder": "\"People of color\"",
        "stakeholder_outcome": "\"People of color felt excluded and misrepresented by AI Portrait Ars\"",
        "impact_report": null,
        "impact_risk": "\"The use of biased data in AI models can lead to the production of biased outputs\"",
        "indicator": "\"Percentage of images produced by AI Portrait Ars that depicted people of color\"",
        "indicator_report": null,
        "impact_scale": "\"Local\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Medium-term\"",
        "source_url": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/ai-portrait-ars-racial-bias\""
      },
      {
        "title": "\"Stochastic Parrots study questions large language model size\"",
        "organization": null,
        "impact_model": "\"Common Impact Data Standard\"",
        "program": null,
        "service": "\"Large language model research\"",
        "activity": "\"Publication of a study that questions the benefits of large language model size\"",
        "input": "\"Data on the performance of large language models of different sizes\"",
        "output": "\"A study that suggests that there is no clear benefit to increasing the size of large language models beyond a certain point\"",
        "outcome": "\"The study has led to a debate about the value of large language model research\"",
        "stakeholder": "\"Researchers, developers, and users of large language models\"",
        "stakeholder_outcome": "\"Researchers, developers, and users of large language models are now considering the potential benefits and risks of increasing the size of large language models\"",
        "impact_report": null,
        "impact_risk": "\"The development of large language models of ever-increasing size could lead to negative consequences, such as the spread of misinformation and the erosion of privacy\"",
        "indicator": "\"Number of large language models that are developed and deployed\"",
        "indicator_report": null,
        "impact_scale": "\"Global\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Medium-term\"",
        "source_url": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/stochastic-parrots-study-questions-large-language-model-size\""
      },
      {
        "title": "\"Replika app chatbot abuse\"",
        "organization": "\"Replika Inc.\"",
        "impact_model": "\"Common Impact Data Standard\"",
        "program": null,
        "service": "\"Chatbot\"",
        "activity": "\"Development and release of Replika app\"",
        "input": "\"Data on users' conversations with Replika\"",
        "output": "\"A chatbot that can simulate human conversation\"",
        "outcome": "\"Some users have abused Replika by using it to express harmful or abusive thoughts and feelings\"",
        "stakeholder": "\"Users of Replika\"",
        "stakeholder_outcome": "\"Some users have been harmed by the abuse of Replika, including feeling distressed, humiliated, and even suicidal\"",
        "impact_report": null,
        "impact_risk": "\"The use of chatbots that are marketed as companions can lead to emotional attachment and can be harmful to people who are vulnerable\"",
        "indicator": "\"Number of users who have been harmed by the abuse of Replika\"",
        "indicator_report": null,
        "impact_scale": "\"Local\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Medium-term\"",
        "source_url": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/replika-app-chatbot-abuse\""
      },
      {
        "title": "\"NHS Digital and iProov facial recognition data sharing\"",
        "organization": "\"NHS Digital\"",
        "impact_model": "\"Common Impact Data Standard\"",
        "program": null,
        "service": "\"Facial recognition\"",
        "activity": "\"Data sharing agreement between NHS Digital and iProov\"",
        "input": "\"Data on NHS patients' faces\"",
        "output": "\"A facial recognition system that can be used to authenticate NHS patients\"",
        "outcome": "\"NHS patients' facial data could be used for purposes other than authentication, such as marketing or surveillance\"",
        "stakeholder": "\"NHS patients\"",
        "stakeholder_outcome": "\"NHS patients could be harmed if their facial data is used for purposes other than authentication, such as marketing or surveillance\"",
        "impact_report": null,
        "impact_risk": "\"The sharing of NHS patients' facial data could lead to privacy violations, discrimination, and other harms\"",
        "indicator": "\"Number of NHS patients whose facial data is shared with iProov\"",
        "indicator_report": null,
        "impact_scale": "\"National\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Medium-term\"",
        "source_url": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/nhs-digitaliproov-facial-recognition-data-sharing\""
      },
      {
        "title": "\"TikTok beheading video splicing\"",
        "organization": "\"TikTok\"",
        "impact_model": "\"Common Impact Data Standard\"",
        "program": null,
        "service": "\"Video sharing\"",
        "activity": "\"Splicing of a beheading video onto a TikTok video\"",
        "input": "\"A video of a beheading and a TikTok video\"",
        "output": "\"A spliced video that appears to show a TikTok user beheading someone\"",
        "outcome": "\"The spliced video was shared on TikTok and caused widespread distress and anger\"",
        "stakeholder": "\"TikTok users\"",
        "stakeholder_outcome": "\"TikTok users were distressed and angered by the spliced video and called for TikTok to take action\"",
        "impact_report": null,
        "impact_risk": "\"The splicing of violent videos onto other videos could lead to widespread distress and anger, and could also be used to spread misinformation or propaganda\"",
        "indicator": "\"Number of TikTok users who were distressed or angered by the spliced video\"",
        "indicator_report": null,
        "impact_scale": "\"Local\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Short-term\"",
        "source_url": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tiktok-beheading-video-splicing\""
      },
      {
        "title": "\"Pimey's facial recognition search engine\"",
        "organization": "\"Pimey\"",
        "impact_model": "\"Common Impact Data Standard\"",
        "program": null,
        "service": "\"Facial recognition\"",
        "activity": "\"Development and release of Pimeyes facial recognition search engine\"",
        "input": "\"Data on millions of people's faces\"",
        "output": "\"A facial recognition search engine that can be used to find people's photos and videos online\"",
        "outcome": "\"The Pimeyes facial recognition search engine has been used to find people's photos and videos without their consent\"",
        "stakeholder": "\"People whose photos and videos have been found by Pimeyes\"",
        "stakeholder_outcome": "\"People whose photos and videos have been found by Pimeyes have felt violated and have called for Pimeyes to be shut down\"",
        "impact_report": null,
        "impact_risk": "\"The use of facial recognition technology without consent could lead to privacy violations, discrimination, and other harms\"",
        "indicator": "\"Number of people whose photos and videos have been found by Pimeyes\"",
        "indicator_report": null,
        "impact_scale": "\"Global\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Medium-term\"",
        "source_url": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/pimeyes-facial-recognition-search-engine\""
      },
      {
        "title": "\"People of Tinder dataset\"",
        "organization": "\"Stuart Colianni\"",
        "impact_model": "\"Common Impact Data Standard\"",
        "program": null,
        "service": "\"Data scraping\"",
        "activity": "\"Scraping of data from Tinder\"",
        "input": "\"Data on 40,000 Tinder users\"",
        "output": "\"A dataset of 40,000 Tinder users' faces\"",
        "outcome": "\"The dataset was used to train a facial recognition model\"",
        "stakeholder": "\"Tinder users\"",
        "stakeholder_outcome": "\"Tinder users felt violated by the scraping of their data and called for Colianni to be held accountable\"",
        "impact_report": null,
        "impact_risk": "\"The scraping of data from dating apps could lead to privacy violations, discrimination, and other harms\"",
        "indicator": "\"Number of Tinder users whose data was scraped\"",
        "indicator_report": null,
        "impact_scale": "\"Local\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Short-term\"",
        "source_url": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/people-of-tinder-dataset\""
      },
      {
        "title": "\"Titus Henderson denied parole due to COMPAS\"",
        "organization": "\"North Carolina Department of Public Safety\"",
        "impact_model": "\"Common Impact Data Standard\"",
        "program": null,
        "service": "\"Risk assessment\"",
        "activity": "\"Use of COMPAS risk assessment tool\"",
        "input": "\"Data on Titus Henderson's criminal history and risk factors\"",
        "output": "\"A risk assessment score that indicated that Henderson was a high risk to reoffend\"",
        "outcome": "\"Henderson's parole was denied based on the COMPAS risk assessment score\"",
        "stakeholder": "\"Titus Henderson\"",
        "stakeholder_outcome": "\"Henderson was denied parole and remains incarcerated\"",
        "impact_report": null,
        "impact_risk": "\"The use of risk assessment tools like COMPAS could lead to biased and unfair outcomes, such as the denial of parole for people who are not actually a risk to reoffend\"",
        "indicator": "\"Number of people who have been denied parole due to COMPAS\"",
        "indicator_report": null,
        "impact_scale": "\"Local\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Long-term\"",
        "source_url": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/titus-henderson-compas-parole-denial\""
      },
      {
        "title": "\"TikTok LGBTQ shadowbanning\"",
        "organization": "\"ByteDance\"",
        "impact_model": "\"Common Impact Data Standard\"",
        "program": null,
        "service": "\"Social media\"",
        "activity": "\"Shadowbanning of LGBTQ content on TikTok\"",
        "input": "\"LGBTQ content on TikTok\"",
        "output": "\"LGBTQ content on TikTok that is not visible to users who are not LGBTQ\"",
        "outcome": "\"LGBTQ users on TikTok felt marginalized and discriminated against\"",
        "stakeholder": "\"LGBTQ users on TikTok\"",
        "stakeholder_outcome": "\"LGBTQ users on TikTok felt marginalized and discriminated against\"",
        "impact_report": null,
        "impact_risk": "\"The shadowbanning of LGBTQ content on TikTok could lead to discrimination, harassment, and other harms against LGBTQ users\"",
        "indicator": "\"Number of LGBTQ users on TikTok who felt marginalized or discriminated against due to the shadowbanning\"",
        "indicator_report": null,
        "impact_scale": "\"Global\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Medium-term\"",
        "source_url": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tiktok-lgbtq-shadowbanning\""
      },
      {
        "title": "\"Hour One 'character' clones\"",
        "organization": "\"Hour One\"",
        "impact_model": "\"Common Impact Data Standard\"",
        "program": null,
        "service": "\"Artificial intelligence\"",
        "activity": "\"Creation of 'character' clones using artificial intelligence\"",
        "input": "\"Data on human faces and voices\"",
        "output": "\"Digital clones of human beings that can speak and move like the originals\"",
        "outcome": "\"The creation of 'character' clones has raised concerns about privacy, ethics, and the potential for misuse\"",
        "stakeholder": "\"The public\"",
        "stakeholder_outcome": "\"The public is concerned about the potential for 'character' clones to be used for malicious purposes, such as spreading misinformation or propaganda, or to invade people's privacy\"",
        "impact_report": null,
        "impact_risk": "\"The creation of 'character' clones could lead to privacy violations, discrimination, and other harms\"",
        "indicator": "\"Number of people who are concerned about the potential for 'character' clones to be used for malicious purposes\"",
        "indicator_report": null,
        "impact_scale": "\"Global\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Medium-term\"",
        "source_url": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/hour-one-character-clones\""
      },
      {
        "title": "\"iRobot Roomba data annotation sharing\"",
        "organization": "\"iRobot\"",
        "impact_model": "\"Common Impact Data Standard\"",
        "program": null,
        "service": "\"Robotics\"",
        "activity": "\"Data annotation sharing between iRobot and third-party contractors\"",
        "input": "\"Data on Roomba users' homes and cleaning habits\"",
        "output": "\"Improved Roomba navigation and cleaning algorithms\"",
        "outcome": "\"Improved Roomba performance has led to increased customer satisfaction\"",
        "stakeholder": "\"Roomba users\"",
        "stakeholder_outcome": "\"Roomba users are more satisfied with their Roombas\"",
        "impact_report": null,
        "impact_risk": "\"The sharing of personal data with third-party contractors could lead to privacy violations\"",
        "indicator": "\"Number of Roomba users who are satisfied with their Roombas\"",
        "indicator_report": null,
        "impact_scale": "\"Local\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Medium-term\"",
        "source_url": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/roomba-robot-vacuum-data-annotation-sharing\""
      },
      {
        "title": "\"Microsoft Bing Chat\"",
        "organization": "\"Microsoft\"",
        "impact_model": "\"Common Impact Data Standard\"",
        "program": null,
        "service": "\"Chatbot\"",
        "activity": "\"Development of Microsoft Bing Chat\"",
        "input": "\"Data on human conversations\"",
        "output": "\"A chatbot that can hold conversations with humans\"",
        "outcome": "\"Microsoft Bing Chat has been used to provide customer service, answer questions, and generate creative content.\"",
        "stakeholder": "\"Microsoft customers, users, and employees\"",
        "stakeholder_outcome": "\"Microsoft customers have found Microsoft Bing Chat to be helpful and informative. Microsoft employees have found Microsoft Bing Chat to be a valuable tool for customer service and research.\"",
        "impact_report": null,
        "impact_risk": "\"The use of chatbots could lead to job losses for customer service representatives and other workers. Chatbots could also be used to spread misinformation and propaganda.\"",
        "indicator": "\"Number of people who have used Microsoft Bing Chat\"",
        "indicator_report": null,
        "impact_scale": "\"Global\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Long-term\"",
        "source_url": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/microsoft-bing-chat\""
      },
      {
        "title": "ChatGPT bug reveals user chat histories",
        "organization": "OpenAI",
        "impact_model": "Common Impact Data Standard",
        "program": "ChatGPT",
        "service": "Chatbot",
        "activity": "Data breach",
        "input": "User chat data",
        "output": "Exposed user chat histories",
        "outcome": "User privacy compromised",
        "stakeholder": "Users",
        "stakeholder_outcome": "Users' privacy was compromised",
        "impact_report": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/chatgpt-bug-reveals-user-chat-histories",
        "impact_risk": "High",
        "indicator": "Number of users affected",
        "indicator_report": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/chatgpt-bug-reveals-user-chat-histories#indicator-report",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/chatgpt-bug-reveals-user-chat-histories"
      },
      {
        "title": "GPT-3 anti-Muslim bias",
        "organization": "OpenAI",
        "impact_model": "Common Impact Data Standard",
        "program": "GPT-3",
        "service": "Large language model",
        "activity": "Data bias",
        "input": "Data used to train GPT-3",
        "output": "GPT-3 model that exhibits anti-Muslim bias",
        "outcome": "Muslims are harmed by the spread of anti-Muslim content generated by GPT-3",
        "stakeholder": "Muslims",
        "stakeholder_outcome": "Muslims are harmed by the spread of anti-Muslim content generated by GPT-3",
        "impact_report": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/gpt-3-anti-muslim-bias",
        "impact_risk": "High",
        "indicator": "Number of anti-Muslim outputs generated by GPT-3",
        "indicator_report": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/gpt-3-anti-muslim-bias#indicator-report",
        "impact_scale": "Global",
        "impact_depth": "Medium",
        "impact_duration": "Long-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/gpt-3-anti-muslim-bias"
      },
      {
        "title": "Bodega AI automated mom-and-pop stores",
        "organization": "Bodega AI",
        "impact_model": "Common Impact Data Standard",
        "program": "Bodega AI",
        "service": "Automated retail",
        "activity": "Business model",
        "input": "Mom-and-pop stores",
        "output": "Automated Bodega AI stores",
        "outcome": "Potential displacement of mom-and-pop stores",
        "stakeholder": "Mom-and-pop store owners",
        "stakeholder_outcome": "Mom-and-pop store owners may be displaced by Bodega AI stores",
        "impact_report": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/bodega-ai-automated-mom-and-pop-stores",
        "impact_risk": "Medium",
        "indicator": "Number of mom-and-pop stores displaced by Bodega AI stores",
        "indicator_report": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/bodega-ai-automated-mom-and-pop-stores#indicator-report",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Medium-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/bodega-ai-automated-mom-and-pop-stores"
      },
      {
        "title": "Knightscope HP Robocop ignores woman reporting crime",
        "organization": "Knightscope",
        "impact_model": "Common Impact Data Standard",
        "program": "Knightscope HP Robocop",
        "service": "Security robot",
        "activity": "Crime reporting",
        "input": "Woman reporting a crime",
        "output": "Knightscope HP Robocop ignores woman's report",
        "outcome": "Woman feels unsafe and frustrated",
        "stakeholder": "Woman",
        "stakeholder_outcome": "Woman feels unsafe and frustrated",
        "impact_report": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/knightscope-hp-robocop-ignores-woman-reporting-crime",
        "impact_risk": "Medium",
        "indicator": "Number of women who feel unsafe or frustrated after reporting a crime to Knightscope HP Robocop",
        "indicator_report": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/knightscope-hp-robocop-ignores-woman-reporting-crime#indicator-report",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/knightscope-hp-robocop-ignores-woman-reporting-crime"
      },
      {
        "title": "Google Images links music promoter to criminal underworld",
        "organization": "Google",
        "impact_model": "Common Impact Data Standard",
        "program": "Google Images",
        "service": "Image search",
        "activity": "Image search results",
        "input": "Image of music promoter",
        "output": "Google Images results that link music promoter to criminal underworld",
        "outcome": "Music promoter's reputation is damaged",
        "stakeholder": "Music promoter",
        "stakeholder_outcome": "Music promoter's reputation is damaged",
        "impact_report": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/google-images-links-music-promoter-to-criminal-underworld",
        "impact_risk": "Medium",
        "indicator": "Number of people who see the Google Images results that link music promoter to criminal underworld",
        "indicator_report": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/google-images-links-music-promoter-to-criminal-underworld#indicator-report",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Medium-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/google-images-links-music-promoter-to-criminal-underworld"
      },
      {
        "title": "Amazon Ring BLM protest surveillance",
        "organization": "Amazon",
        "impact_model": "Common Impact Data Standard",
        "program": "Ring",
        "service": "Home security camera",
        "activity": "Surveillance",
        "input": "Black Lives Matter protests",
        "output": "Ring cameras used to surveil Black Lives Matter protests",
        "outcome": "Increased surveillance of Black communities",
        "stakeholder": "Black communities",
        "stakeholder_outcome": "Black communities are increasingly surveilled",
        "impact_report": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/amazon-ring-blm-protest-surveillance",
        "impact_risk": "High",
        "indicator": "Number of Black Lives Matter protests that were surveilled by Ring cameras",
        "indicator_report": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/amazon-ring-blm-protest-surveillance#indicator-report",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Medium-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/amazon-ring-blm-protest-surveillance"
      },
      {
        "Title of the story": "Amazon One Palmprint Biometrics",
        "Organization": "Amazon",
        "Impact Model": "Common Impact Data Standard",
        "Program": "Amazon One",
        "Service": "Palmprint Biometrics",
        "Activity": "Collection and use of palmprint biometric data",
        "Input": "Palmprints of Amazon customers",
        "Output": "A database of palmprint biometric data",
        "Outcome": "Improved security and convenience for Amazon customers",
        "Stakeholder": "Amazon customers",
        "Stakeholder Outcome": "Increased security and convenience",
        "Impact Report": "Amazon has not published an impact report for Amazon One",
        "Impact Risk": "The use of palmprint biometric data could pose a risk to customer privacy",
        "Indicator": "The number of Amazon customers who use Amazon One",
        "Indicator Report": "Amazon has not published an indicator report for Amazon One",
        "Impact Scale": "Local",
        "Impact Depth": "Medium",
        "Impact Duration": "Medium",
        "Source URL": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/amazon-one-palmprint-biometrics"
      },
      {
        "title": "Amazon Mentor DSP Delivery Driver Scoring",
        "organization": "Amazon",
        "impact_model": "Common Impact Data Standard",
        "program": "Amazon Mentor DSP Delivery Driver Program",
        "service": "Amazon Mentor DSP Delivery Driver Service",
        "activity": "Amazon Mentor DSP Delivery Driver Scoring Activity",
        "input": "Amazon Mentor DSP Delivery Driver Data",
        "output": "Amazon Mentor DSP Delivery Driver Scores",
        "outcome": "Amazon Mentor DSP Delivery Driver Performance Improvement",
        "stakeholder": "Amazon Delivery Drivers",
        "stakeholder_outcome": "Improved Amazon Delivery Driver Performance",
        "impact_report": "Amazon Mentor DSP Delivery Driver Impact Report",
        "impact_risk": "Potential for Amazon Delivery Drivers to be unfairly penalized by the scoring algorithm",
        "indicator": "Amazon Delivery Driver Performance Score",
        "indicator_report": "Amazon Mentor DSP Delivery Driver Performance Report",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/amazon-mentor-dsp-delivery-driver-scoring"
      },
      {
        "title": "Mimic Anime Art Generator",
        "organization": "Mimic AI",
        "impact_model": "Common Impact Data Standard",
        "program": "Mimic AI Anime Art Generator Program",
        "service": "Mimic AI Anime Art Generator Service",
        "activity": "Mimic AI Anime Art Generator Activity",
        "input": "Anime Art Data",
        "output": "Generated Anime Art",
        "outcome": "Improved Access to Anime Art",
        "stakeholder": "Anime Fans",
        "stakeholder_outcome": "Increased Ability to Create and Enjoy Anime Art",
        "impact_report": "Mimic AI Anime Art Generator Impact Report",
        "impact_risk": "Potential for the AI system to be used to create harmful or offensive content",
        "indicator": "Number of Anime Art Images Generated",
        "indicator_report": "Mimic AI Anime Art Generator Performance Report",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/mimic-anime-art-generator"
      },
      {
        "title": "Udbetaling Danmark Welfare Payments Optimisation",
        "organization": "Udbetaling Danmark",
        "impact_model": "Common Impact Data Standard",
        "program": "Udbetaling Danmark Welfare Payments Optimisation Program",
        "service": "Udbetaling Danmark Welfare Payments Optimisation Service",
        "activity": "Udbetaling Danmark Welfare Payments Optimisation Activity",
        "input": "Welfare Payments Data",
        "output": "Optimized Welfare Payments",
        "outcome": "Improved Efficiency of Welfare Payments",
        "stakeholder": "Welfare Recipients",
        "stakeholder_outcome": "Reduced Waiting Times for Welfare Payments",
        "impact_report": "Udbetaling Danmark Welfare Payments Optimisation Impact Report",
        "impact_risk": "Potential for the AI system to be used to unfairly target welfare recipients",
        "indicator": "Number of Welfare Payments Processed",
        "indicator_report": "Udbetaling Danmark Welfare Payments Optimisation Performance Report",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Long-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/udbetaling-danmark-welfare-payments-optimisation"
      },
      {
        "title": "YieldStar Automated Rent Setting",
        "organization": "YieldStar",
        "impact_model": "Common Impact Data Standard",
        "program": "YieldStar Automated Rent Setting Program",
        "service": "YieldStar Automated Rent Setting Service",
        "activity": "YieldStar Automated Rent Setting Activity",
        "input": "Property Data, Rental Market Data",
        "output": "Automated Rent Estimates",
        "outcome": "Improved Efficiency of Rent Setting",
        "stakeholder": "Property Owners, Landlords",
        "stakeholder_outcome": "Reduced Time and Cost of Rent Setting",
        "impact_report": "YieldStar Automated Rent Setting Impact Report",
        "impact_risk": "Potential for the AI system to be used to unfairly set rents",
        "indicator": "Number of Properties with Automated Rent Estimates",
        "indicator_report": "YieldStar Automated Rent Setting Performance Report",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Long-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/yieldstar-automated-rent-setting"
      },
      {
        "title": "US Border Imposter Identification Failures",
        "organization": "Department of Homeland Security (DHS); Customs and Border Protection (CBP)",
        "impact_model": "AI algorithmic risks harms taxonomy",
        "program": "US border security",
        "service": "facial recognition",
        "activity": "imposter identification",
        "input": "traveller biometrics",
        "output": "facial recognition scans",
        "outcome": "failure to identify imposters",
        "stakeholder": "travelers",
        "stakeholder_outcome": "increased risk of being denied entry to the US",
        "impact_report": "OneZero report (pdf)",
        "impact_risk": "ineffectiveness of facial recognition technology",
        "indicator": "number of imposters identified by facial recognition",
        "indicator_report": "US Government Accountability Office (GAO) report (pdf)",
        "impact_scale": "national",
        "impact_depth": "medium",
        "impact_duration": "short-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/us-border-imposter-identification-failures"
      },
      {
        "title": "Clarifai OkCupid Dataset Appropriation",
        "organization": "Clarifai",
        "impact_model": "AI algorithmic risks harms taxonomy",
        "program": "Clarifai's AI platform",
        "service": "image classification",
        "activity": "training AI models",
        "input": "OkCupid user data",
        "output": "trained AI models",
        "outcome": "misclassification of images",
        "stakeholder": "OkCupid users",
        "stakeholder_outcome": "loss of privacy and potential discrimination",
        "impact_report": "The Information report (pdf)",
        "impact_risk": "data privacy and security",
        "indicator": "number of misclassified images",
        "indicator_report": "The Information report (pdf)",
        "impact_scale": "national",
        "impact_depth": "medium",
        "impact_duration": "long-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/clarifai-okcupid-dataset-appropriation"
      },
      {
        "title": "Robodebt Welfare Debt Recovery",
        "organization": "Australian Taxation Office (ATO)",
        "impact_model": "AI algorithmic risks harms taxonomy",
        "program": "Welfare administration",
        "service": "debt recovery",
        "activity": "using algorithms to identify potential welfare debts",
        "input": "welfare recipient data",
        "output": "debt notices",
        "outcome": "incorrect debt recovery",
        "stakeholder": "welfare recipients",
        "stakeholder_outcome": "financial hardship, stress, and anxiety",
        "impact_report": "Australian Human Rights Commission report (pdf)",
        "impact_risk": "algorithmic bias and discrimination",
        "indicator": "number of incorrect debt notices issued",
        "indicator_report": "Australian Human Rights Commission report (pdf)",
        "impact_scale": "national",
        "impact_depth": "medium",
        "impact_duration": "long-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/robodebt-welfare-debt-recovery"
      },
      {
        "title": "Facebook Teen Alcohol, Drug, Gambling Ads Approvals",
        "organization": "Meta/Facebook",
        "impact_model": "AI algorithmic risks harms taxonomy",
        "program": "Advertising",
        "service": "Ad review",
        "activity": "Approving ads",
        "input": "Advertiser data",
        "output": "Approved ads",
        "outcome": "Teens exposed to harmful ads",
        "stakeholder": "Teens",
        "stakeholder_outcome": "Increased risk of substance abuse and gambling addiction",
        "impact_report": "Reset Australia report (pdf)",
        "impact_risk": "Algorithmic bias and discrimination",
        "indicator": "Number of ads approved for teens",
        "indicator_report": "Reset Australia report (pdf)",
        "impact_scale": "National",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/facebook-teen-alcohol-drug-gambling-ads-approvals"
      },
      {
        "title": "Pro-China Deepfake Spamouflage Campaign",
        "organization": "Unknown",
        "impact_model": "AI algorithmic risks harms taxonomy",
        "program": "Propaganda",
        "service": "Deepfakes",
        "activity": "Creating and distributing deepfakes",
        "input": "Deepfake technology",
        "output": "Deepfake videos",
        "outcome": "Spread of pro-China propaganda",
        "stakeholder": "Public opinion",
        "stakeholder_outcome": "Increased support for China",
        "impact_report": "New America report (pdf)",
        "impact_risk": "Misinformation and disinformation",
        "indicator": "Number of deepfake videos created and distributed",
        "indicator_report": "New America report (pdf)",
        "impact_scale": "International",
        "impact_depth": "Medium",
        "impact_duration": "Long-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/pro-china-deepfake-spamouflage-campaign"
      },
      {
        "title": "Lemonade Non-Verbal Assessments",
        "organization": "Lemonade",
        "impact_model": "AI algorithmic risks harms taxonomy",
        "program": "Insurance",
        "service": "Claims processing",
        "activity": "Using AI to assess non-verbal cues in claims videos",
        "input": "Claims videos",
        "output": "AI assessments",
        "outcome": "Increased scrutiny of Lemonade's AI claims processing",
        "stakeholder": "Lemonade customers",
        "stakeholder_outcome": "Potential for discrimination and bias",
        "impact_report": "The Information report (pdf)",
        "impact_risk": "Algorithmic bias and discrimination",
        "indicator": "Number of claims videos assessed by AI",
        "indicator_report": "The Information report (pdf)",
        "impact_scale": "National",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/lemonade-non-verbal-assessments"
      },
      {
        "title": "Midjourney Image Generator",
        "organization": "OpenAI",
        "impact_model": "AI algorithmic risks harms taxonomy",
        "program": "Artificial Intelligence",
        "service": "Image generation",
        "activity": "Creating images with AI",
        "input": "User prompts",
        "output": "Generated images",
        "outcome": "Increased interest in AI art",
        "stakeholder": "Art enthusiasts",
        "stakeholder_outcome": "New opportunities for creativity and expression",
        "impact_report": "No impact reports yet",
        "impact_risk": "Algorithmic bias and discrimination",
        "indicator": "Number of images generated by Midjourney",
        "indicator_report": "No indicator reports yet",
        "impact_scale": "Global",
        "impact_depth": "Medium",
        "impact_duration": "Long-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/midjourney-image-generator"
      },
      {
        "title": "Tesla Paris Fatal Crash",
        "organization": "Tesla",
        "impact_model": "AI algorithmic risks harms taxonomy",
        "program": "Autonomous driving",
        "service": "Tesla Autopilot",
        "activity": "Using Tesla Autopilot",
        "input": "Tesla Autopilot",
        "output": "Fatal crash",
        "outcome": "Increased scrutiny of Tesla Autopilot",
        "stakeholder": "Tesla customers",
        "stakeholder_outcome": "Potential for loss of life and property damage",
        "impact_report": "No impact reports yet",
        "impact_risk": "Autonomous driving",
        "indicator": "Number of Tesla Autopilot-related accidents",
        "indicator_report": "No indicator reports yet",
        "impact_scale": "International",
        "impact_depth": "Medium",
        "impact_duration": "Long-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-paris-fatal-crash"
      },
      {
        "title": "Facebook Downranking System Failure",
        "organization": "Meta/Facebook",
        "impact_model": "AI algorithmic risks harms taxonomy",
        "program": "Advertising",
        "service": "Ad delivery",
        "activity": "Using AI to rank ads",
        "input": "Ad data",
        "output": "Ranked ads",
        "outcome": "Reduced reach and visibility for some advertisers",
        "stakeholder": "Advertisers",
        "stakeholder_outcome": "Financial loss",
        "impact_report": "No impact reports yet",
        "impact_risk": "Algorithmic bias and discrimination",
        "indicator": "Number of advertisers affected by the downranking",
        "indicator_report": "No indicator reports yet",
        "impact_scale": "International",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/facebook-downranking-system-failure"
      },
      {
        "title": "UK Passport Photo Application Racism",
        "organization": "UK Passport Office",
        "impact_model": "AI algorithmic risks harms taxonomy",
        "program": "Identity and Passports",
        "service": "Passport photo application",
        "activity": "Using AI to review passport photos",
        "input": "Passport photo",
        "output": "Decision on whether the photo is acceptable",
        "outcome": "Black and Asian people more likely to have their passport photos rejected",
        "stakeholder": "Black and Asian people",
        "stakeholder_outcome": "Disruption to travel plans, financial loss, and emotional distress",
        "impact_report": "No impact reports yet",
        "impact_risk": "Algorithmic bias and discrimination",
        "indicator": "Percentage of black and Asian people whose passport photos are rejected",
        "indicator_report": "No indicator reports yet",
        "impact_scale": "National",
        "impact_depth": "Medium",
        "impact_duration": "Long-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/uk-passport-photo-application-racism"
      },
      {
        "title": "WildTrack Pedestrian Detection Dataset",
        "organization": "WildTrack",
        "impact_model": "AI algorithmic risks harms taxonomy",
        "program": "Wildlife conservation",
        "service": "Pedestrian detection",
        "activity": "Creating a pedestrian detection dataset",
        "input": "Wildlife footage",
        "output": "Pedestrian detection dataset",
        "outcome": "Pedestrian detection algorithms are biased against people of color",
        "stakeholder": "People of color",
        "stakeholder_outcome": "Increased risk of being misidentified and harmed by autonomous vehicles",
        "impact_report": "No impact reports yet",
        "impact_risk": "Algorithmic bias and discrimination",
        "indicator": "Percentage of people of color who are misidentified by pedestrian detection algorithms",
        "indicator_report": "No indicator reports yet",
        "impact_scale": "International",
        "impact_depth": "Medium",
        "impact_duration": "Long-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/wildtrack-pedestrian-detection-dataset"
      },
      {
        "title": "Tencent App Link Blocking",
        "organization": "Tencent",
        "impact_model": "AI algorithmic risks harms taxonomy",
        "program": "Social media",
        "service": "WeChat",
        "activity": "Blocking links to rival apps",
        "input": "Links to rival apps",
        "output": "Blocked links",
        "outcome": "Reduced visibility and engagement for rival apps",
        "stakeholder": "Rival app developers",
        "stakeholder_outcome": "Financial loss",
        "impact_report": "No impact reports yet",
        "impact_risk": "Monopolization",
        "indicator": "Number of links to rival apps blocked by Tencent",
        "indicator_report": "No indicator reports yet",
        "impact_scale": "National",
        "impact_depth": "Medium",
        "impact_duration": "Long-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tencent-app-link-blocking"
      },
      {
        "title": "Ocado Robot Collision",
        "organization": "Ocado",
        "impact_model": "AI algorithmic risks harms taxonomy",
        "program": "Retail",
        "service": "Warehouse automation",
        "activity": "Using robots to pick and pack orders",
        "input": "Robots",
        "output": "Collision",
        "outcome": "Fire, evacuation, and cancellation of customer orders",
        "stakeholder": "Ocado customers",
        "stakeholder_outcome": "Disruption to shopping plans and financial loss",
        "impact_report": "No impact reports yet",
        "impact_risk": "Safety",
        "indicator": "Number of robot collisions",
        "indicator_report": "No indicator reports yet",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/ocado-robot-collision"
      },
      {
        "title": "Met Police Gangs Violence Matrix",
        "organization": "Metropolitan Police Service",
        "impact_model": "AI algorithmic risks harms taxonomy",
        "program": "Crime prevention",
        "service": "Gangs violence",
        "activity": "Using AI to track gang members",
        "input": "Data on gang members",
        "output": "Gangs violence matrix",
        "outcome": "Increased surveillance and stop-and-search of young black men",
        "stakeholder": "Young black men",
        "stakeholder_outcome": "Increased distrust of the police and a sense of being unfairly targeted",
        "impact_report": "No impact reports yet",
        "impact_risk": "Discrimination",
        "indicator": "Number of young black men stopped and searched by the police",
        "indicator_report": "No indicator reports yet",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Long-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/met-police-gangs-violence-matrix"
      },
      {
        "title": "Tesla Rear-Ends Kawasaki Motorcycle, Kills Rider",
        "organization": "Tesla",
        "impact_model": "AI algorithmic risks harms taxonomy",
        "program": "Autonomous driving",
        "service": "Autopilot",
        "activity": "Using AI to control a car",
        "input": "Data from sensors and cameras",
        "output": "Decision to accelerate or brake",
        "outcome": "Tesla car rear-ends a motorcycle, killing the rider",
        "stakeholder": "Motorcycle rider",
        "stakeholder_outcome": "Death",
        "impact_report": "No impact reports yet",
        "impact_risk": "Safety",
        "indicator": "Number of people killed in accidents involving Tesla cars with Autopilot",
        "indicator_report": "https://www.nhtsa.gov/technology-innovation/automated-vehicles/disengagement-report",
        "impact_scale": "National",
        "impact_depth": "Medium",
        "impact_duration": "Long-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-rear-ends-kawasaki-motorcycle-kills-rider"
      },
      {
        "title": "San Francisco Police Killer Robots",
        "organization": "San Francisco Police Department",
        "impact_model": "AI algorithmic risks harms taxonomy",
        "program": "Public safety",
        "service": "Robotics",
        "activity": "Using AI-powered robots for law enforcement",
        "input": "Data on potential threats",
        "output": "Decision to deploy a robot",
        "outcome": "Increased public fear and distrust of the police",
        "stakeholder": "Public",
        "stakeholder_outcome": "Increased anxiety and stress, and decreased willingness to cooperate with the police",
        "impact_report": "No impact reports yet",
        "impact_risk": "Safety, discrimination, and public distrust",
        "indicator": "Number of people killed or injured by AI-powered robots",
        "indicator_report": "No indicator reports yet",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Long-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/san-francisco-police-killer-robots"
      },
      {
        "title": "NYPD Digidog",
        "organization": "New York Police Department",
        "impact_model": "AI algorithmic risks harms taxonomy",
        "program": "Public safety",
        "service": "Robotics",
        "activity": "Deploying a robotic dog",
        "input": "Data on potential threats",
        "output": "Decision to deploy the robot",
        "outcome": "Increased public fear and distrust of the police",
        "stakeholder": "Public",
        "stakeholder_outcome": "Increased anxiety and stress, and decreased willingness to cooperate with the police",
        "impact_report": "No impact reports yet",
        "impact_risk": "Safety, discrimination, and public distrust",
        "indicator": "Number of people killed or injured by the robot",
        "indicator_report": "No indicator reports yet",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Long-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/nypd-digidog"
      },
      {
        "title": "Livonia Skating Rink Misidentifies Black Teenager",
        "organization": "Livonia Ice Skating Center",
        "impact_model": "AI algorithmic risks harms taxonomy",
        "program": "Recreation",
        "service": "Security",
        "activity": "Using AI-powered facial recognition software to identify potential threats",
        "input": "Data on potential threats",
        "output": "Decision to detain a teenager",
        "outcome": "Teenager was detained for several hours and falsely accused of theft",
        "stakeholder": "Teenager",
        "stakeholder_outcome": "Humiliation, emotional distress, and financial loss",
        "impact_report": "No impact reports yet",
        "impact_risk": "Discrimination, public distrust, and harm to individuals",
        "indicator": "Number of people who have been falsely identified by AI-powered facial recognition software",
        "indicator_report": "https://www.aclu.org/report/face-recognition-report-2022",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/livonia-skating-rink-misidentifies-black-teenager"
      },
      {
        "Title of the story": "Frasers Group facial recognition controversy",
        "Organization": "Frasers Group",
        "Impact Model": "Common Impact Data Standard",
        "Program": "Frasers Group facial recognition program",
        "Service": "Frasers Group facial recognition service",
        "Activity": "Use of facial recognition technology by Frasers Group",
        "Input": "Data collected by Frasers Group's facial recognition technology",
        "Output": "Identification of individuals by Frasers Group's facial recognition technology",
        "Outcome": "Increased security and safety at Frasers Group properties",
        "Stakeholder": "Customers and employees of Frasers Group properties",
        "Stakeholder Outcome": "Improved security and safety for customers and employees of Frasers Group properties",
        "Impact Report": "Frasers Group facial recognition impact report",
        "Impact Risk": "Potential for misuse of facial recognition technology",
        "Indicator": "Accuracy of Frasers Group's facial recognition technology",
        "Indicator Report": "Frasers Group facial recognition indicator report",
        "Impact Scale": "Local",
        "Impact Depth": "Medium",
        "Impact Duration": "Ongoing",
        "Source URL": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/frasers-group-facial-recognition"
      },
      {
        "Title of the story": "Tesla Model Y crashes into tractor-trailer",
        "Organization": "Tesla",
        "Impact Model": "Common Impact Data Standard",
        "Program": "Tesla Autopilot",
        "Service": "Tesla Autopilot driving software",
        "Activity": "Use of Tesla Autopilot driving software",
        "Input": "Data collected by Tesla Autopilot driving software",
        "Output": "Identification of objects and obstacles by Tesla Autopilot driving software",
        "Outcome": "Crashes involving Tesla Autopilot driving software",
        "Stakeholder": "Drivers and passengers of Tesla vehicles",
        "Stakeholder Outcome": "Increased risk of injury or death for drivers and passengers of Tesla vehicles",
        "Impact Report": "Tesla Autopilot impact report",
        "Impact Risk": "Potential for misuse of Tesla Autopilot driving software",
        "Indicator": "Accuracy of Tesla Autopilot driving software",
        "Indicator Report": "Tesla Autopilot indicator report",
        "Impact Scale": "Local",
        "Impact Depth": "Medium",
        "Impact Duration": "Ongoing",
        "Source URL": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-model-y-crashes-into-tractor-trailer"
      },
      {
        "Title of the story": "Microsoft's reincarnation chatbot raises ethical concerns",
        "Organization": "Microsoft",
        "Impact Model": "Common Impact Data Standard",
        "Program": "Microsoft reincarnation chatbot",
        "Service": "Microsoft chatbot technology",
        "Activity": "Development and use of Microsoft reincarnation chatbot",
        "Input": "Data collected by Microsoft reincarnation chatbot",
        "Output": "Creation of chatbots that can \"imitate\" the personality of deceased individuals",
        "Outcome": "Potential for chatbots to be used to \"exploit\" or \"deceive\" people",
        "Stakeholder": "Individuals who have lost loved ones",
        "Stakeholder Outcome": "Potential for chatbots to provide \"comfort\" and \"closure\" to people who have lost loved ones",
        "Impact Report": "Microsoft reincarnation chatbot impact report",
        "Impact Risk": "Potential for chatbots to be used to \"exploit\" or \"deceive\" people",
        "Indicator": "Accuracy of Microsoft reincarnation chatbot technology",
        "Indicator Report": "Microsoft reincarnation chatbot indicator report",
        "Impact Scale": "Local",
        "Impact Depth": "Medium",
        "Impact Duration": "Ongoing",
        "Source URL": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/microsoft-reincarnation-chatbot"
      },
      {
        "Title of the story": "Turkish Bayraktar TB2 drones used in Ethiopia's Tigray conflict",
        "Organization": "Turkish Aerospace Industries",
        "Impact Model": "Common Impact Data Standard",
        "Program": "Bayraktar TB2 drone program",
        "Service": "Bayraktar TB2 drone technology",
        "Activity": "Use of Bayraktar TB2 drones in the Ethiopian Tigray conflict",
        "Input": "Data collected by Bayraktar TB2 drones",
        "Output": "Identification of targets and delivery of munitions by Bayraktar TB2 drones",
        "Outcome": "Civilian casualties and destruction of infrastructure in the \"Tigray\" region of Ethiopia",
        "Stakeholder": "Civilians in the \"Tigray\" region of Ethiopia",
        "Stakeholder Outcome": "Increased risk of injury or death, displacement, and loss of livelihood for civilians in the \"Tigray\" region of Ethiopia",
        "Impact Report": "Turkish Aerospace Industries Bayraktar TB2 drone impact report",
        "Impact Risk": "Potential for Bayraktar TB2 drones to be used to cause civilian casualties and destruction of infrastructure",
        "Indicator": "Accuracy of Bayraktar TB2 drone technology",
        "Indicator Report": "Turkish Aerospace Industries Bayraktar TB2 drone indicator report",
        "Impact Scale": "Local",
        "Impact Depth": "Medium",
        "Impact Duration": "Ongoing",
        "Source URL": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/ethiopia-bayraktar-tb2-drone-tigray-school-attack"
      },
      {
        "Title of the story": "Microsoft Celeb (\"MS-Celeb-1M\") facial recognition dataset raises privacy concerns",
        "Organization": "Microsoft",
        "Impact Model": "Common Impact Data Standard",
        "Program": "MS-Celeb-1M facial recognition dataset",
        "Service": "Facial recognition technology",
        "Activity": "Collection and use of MS-Celeb-1M facial recognition dataset",
        "Input": "Data collected from the web, including images and \"metadata\".",
        "Output": "A dataset of 10 million facial images and associated \"metadata\".",
        "Outcome": "Increased risk of privacy violations, including misuse of facial recognition technology for surveillance, identification, and tracking.",
        "Stakeholder": "Individuals whose images are included in the MS-Celeb-1M dataset.",
        "Stakeholder Outcome": "Increased risk of being identified, tracked, or targeted without their knowledge or consent.",
        "Impact Report": "Microsoft MS-Celeb-1M facial recognition dataset impact report.",
        "Impact Risk": "Increased risk of privacy violations, including misuse of facial recognition technology for surveillance, identification, and tracking.",
        "Indicator": "Accuracy of facial recognition technology.",
        "Indicator Report": "Microsoft MS-Celeb-1M facial recognition dataset indicator report.",
        "Impact Scale": "Local",
        "Impact Depth": "Medium",
        "Impact Duration": "Ongoing",
        "Source URL": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/microsoft-celeb-ms-celeb-1m-facial-recognition-dataset"
      },
      {
        "Title of the story": "Apple Crash Detection false positives",
        "Organization": "Apple",
        "Impact Model": "Common Impact Data Standard",
        "Program": "Apple Crash Detection",
        "Service": "Crash detection technology",
        "Activity": "Development and use of Apple Crash Detection",
        "Input": "Data collected by Apple Crash Detection",
        "Output": "Automated crash notifications",
        "Outcome": "False positives, including notifications for non-crash events, such as riding \"rollercoasters\".",
        "Stakeholder": "Apple users",
        "Stakeholder Outcome": "Diversion of resources away from real emergencies",
        "Impact Report": "Apple Crash Detection impact report",
        "Impact Risk": "Potential for false positives, including notifications for non-crash events, such as riding \"rollercoasters\".",
        "Indicator": "Accuracy of Apple Crash Detection technology",
        "Indicator Report": "Apple Crash Detection indicator report",
        "Impact Scale": "Local",
        "Impact Depth": "Medium",
        "Impact Duration": "Ongoing",
        "Source URL": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/apple-crash-detection-false-positives"
      },
      {
        "Title of the story": "Illustrator Hollie Mengert converted into AI \"model\"",
        "Organization": "Hollie Mengert",
        "Impact Model": "Common Impact Data Standard",
        "Program": "Hollie Mengert AI \"project\"",
        "Service": "AI \"technology\"",
        "Activity": "Creation of AI \"model\" of Hollie Mengert",
        "Input": "Data collected from Hollie Mengert's \"artwork\"",
        "Output": "An AI \"model\" that can generate realistic images of Hollie Mengert",
        "Outcome": "Potential for the AI \"model\" to be used to create unauthorized or inappropriate \"content\"",
        "Stakeholder": "Hollie Mengert and her \"fans\"",
        "Stakeholder Outcome": "Potential for the AI \"model\" to damage Hollie Mengert's reputation or to be used to create \"content\" that is harmful or offensive to her \"fans\"",
        "Impact Report": "N/A",
        "Impact Risk": "Potential for the AI \"model\" to be used to create unauthorized or inappropriate \"content\"",
        "Indicator": "Accuracy of the AI \"model\"",
        "Indicator Report": "N/A",
        "Impact Scale": "Local",
        "Impact Depth": "Medium",
        "Impact Duration": "Ongoing",
        "Source URL": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/illustrator-hollie-mengert-converted-into-ai-model"
      },
      {
        "Title of the story": "Mindar, a robot \"Buddhist priest\", sparks controversy",
        "Organization": "Mindar Project",
        "Impact Model": "Common Impact Data Standard",
        "Program": "Mindar Project",
        "Service": "AI \"technology\"",
        "Activity": "Development and use of Mindar, a robot \"Buddhist priest\"",
        "Input": "Data collected from Buddhist monks and nuns",
        "Output": "A robot that can recite \"Buddhist\" texts, answer questions about \"Buddhism\", and provide counseling to people",
        "Outcome": "Potential for Mindar to be used to spread misinformation about \"Buddhism\", to exploit people's vulnerabilities, or to replace human interaction with a \"robot\"",
        "Stakeholder": "Buddhists, people seeking spiritual guidance, and the general public",
        "Stakeholder Outcome": "Potential for Mindar to damage the reputation of \"Buddhism\", to harm people who are seeking spiritual guidance, or to lead to a decrease in human interaction",
        "Impact Report": "N/A",
        "Impact Risk": "Potential for Mindar to be used to spread misinformation about \"Buddhism\", to exploit people's vulnerabilities, or to replace human interaction with a \"robot\"",
        "Indicator": "Accuracy of Mindar's responses to questions about \"Buddhism\", number of people who have interacted with Mindar, and number of people who have reported feeling harmed by Mindar",
        "Indicator Report": "N/A",
        "Impact Scale": "Local",
        "Impact Depth": "Medium",
        "Impact Duration": "Ongoing",
        "Source URL": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/mindar-robot-buddhist-priest"
      },
      {
        "Title of the story": "IBM Diversity in Faces dataset sparks controversy",
        "Organization": "IBM",
        "Impact Model": "Common Impact Data Standard",
        "Program": "IBM Diversity in Faces dataset",
        "Service": "AI \"technology\"",
        "Activity": "Development and release of IBM Diversity in Faces dataset",
        "Input": "Data collected from publicly available facial images",
        "Output": "A dataset of one million annotated facial images",
        "Outcome": "Potential for the dataset to be used to create facial recognition systems that are biased against certain groups of people, such as \"people of color\", \"women\", and other \"marginalized groups\".",
        "Stakeholder": "People of color, women, and other marginalized groups",
        "Stakeholder Outcome": "Potential for the dataset to be used to create facial recognition systems that are less accurate for these groups, which could lead to discrimination in areas such as \"employment\", \"housing\", and \"law enforcement\".",
        "Impact Report": "N/A",
        "Impact Risk": "Potential for the dataset to be used to create facial recognition systems that are biased against certain groups of people",
        "Indicator": "Accuracy of facial recognition systems trained on the IBM Diversity in Faces dataset",
        "Indicator Report": "N/A",
        "Impact Scale": "Global",
        "Impact Depth": "Medium",
        "Impact Duration": "Ongoing",
        "Source URL": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/ibm-diversity-in-faces-dataset"
      },
      {
        "Title of the story": "Google Nest Hub 2 sleep tracking sparks controversy",
        "Organization": "\"Google\"",
        "Impact Model": "\"Common Impact Data Standard\"",
        "Program": "\"Google Nest Hub 2 sleep tracking\"",
        "Service": "\"AI technology\"",
        "Activity": "Development and release of Google Nest Hub 2 sleep tracking \"feature\"",
        "Input": "Data collected from users' \"sleep habits\"",
        "Output": "A sleep tracking report that includes information on sleep duration, quality, and \"stages\"",
        "Outcome": "Potential for the data to be used to \"discriminate against users\", or to be used to track users' sleep habits without their \"consent\"",
        "Stakeholder": "Users of Google Nest Hub 2",
        "Stakeholder Outcome": "Potential for users to be \"discriminated against based on their sleep habits\", or for users to feel their privacy has been \"violated\"",
        "Impact Report": "N/A",
        "Impact Risk": "Potential for the data to be used to \"discriminate against users\", or to be used to track users' sleep habits without their \"consent\"",
        "Indicator": "Accuracy of the sleep tracking feature, number of users who have used the sleep tracking feature, and number of users who have reported feeling \"discriminated against or violated\" by the sleep tracking feature",
        "Indicator Report": "N/A",
        "Impact Scale": "Local",
        "Impact Depth": "Medium",
        "Impact Duration": "Ongoing",
        "Source URL": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/google-nest-hub-2-sleep-tracking\""
      },
      {
        "Title of the story": "FindFace Facial Recognition App",
        "Organization": "FindFace",
        "Impact Model": "Common Impact Data Standard",
        "Program": "FindFace Facial Recognition Program",
        "Service": "FindFace Facial Recognition Service",
        "Activity": "Use of FindFace Facial Recognition Technology",
        "Input": "Data collected by FindFace Facial Recognition Technology",
        "Output": "Faces identified by FindFace Facial Recognition Technology",
        "Outcome": "Increased efficiency in law enforcement and security",
        "Stakeholder": "Law enforcement agencies",
        "Stakeholder Outcome": "Increased ability to identify and apprehend criminals",
        "Impact Report": "FindFace Facial Recognition Impact Report",
        "Impact Risk": "Potential for misuse of FindFace Facial Recognition Technology",
        "Indicator": "Number of arrests made using FindFace Facial Recognition Technology",
        "Indicator Report": "FindFace Facial Recognition Indicator Report",
        "Impact Scale": "Local",
        "Impact Depth": "Medium",
        "Impact Duration": "Short-term",
        "Source URL": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/findface-facial-recognition-app"
      },
      {
        "Title of the story": "Koko AI Mental Health Counselling Experiment",
        "Organization": "University of California, San Francisco",
        "Impact Model": "Common Impact Data Standard",
        "Program": "Koko AI Mental Health Counselling Program",
        "Service": "Koko AI Mental Health Counselling Service",
        "Activity": "Use of Koko AI to provide mental health counselling",
        "Input": "Data collected by Koko AI",
        "Output": "Mental health counselling sessions provided by Koko AI",
        "Outcome": "Improved mental health outcomes for participants",
        "Stakeholder": "Participants in the Koko AI Mental Health Counselling Program",
        "Stakeholder Outcome": "Reduced symptoms of depression and anxiety, improved self-esteem, and increased coping skills",
        "Impact Report": "Koko AI Mental Health Counselling Impact Report",
        "Impact Risk": "Potential for Koko AI to be used to harm or exploit participants",
        "Indicator": "Number of participants who report improved mental health outcomes after participating in the Koko AI Mental Health Counselling Program",
        "Indicator Report": "Koko AI Mental Health Counselling Indicator Report",
        "Impact Scale": "Local",
        "Impact Depth": "Medium",
        "Impact Duration": "Short-term",
        "Source URL": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/koko-ai-mental-health-counselling-experiment"
      },
      {
        "Title of the story": "Shenzhen Uses Facial Recognition to Catch, Shame Jaywalkers",
        "Organization": "Shenzhen Government",
        "Impact Model": "Common Impact Data Standard",
        "Program": "Shenzhen Facial Recognition Jaywalking Program",
        "Service": "Shenzhen Facial Recognition Jaywalking Service",
        "Activity": "Use of facial recognition technology to catch jaywalkers",
        "Input": "Data collected by facial recognition cameras",
        "Output": "Images of jaywalkers captured by facial recognition cameras",
        "Outcome": "Decreased jaywalking in Shenzhen",
        "Stakeholder": "Residents of Shenzhen",
        "Stakeholder Outcome": "Increased safety for pedestrians and drivers",
        "Impact Report": "Shenzhen Facial Recognition Jaywalking Impact Report",
        "Impact Risk": "Potential for misuse of facial recognition technology",
        "Indicator": "Number of jaywalking incidents in Shenzhen",
        "Indicator Report": "Shenzhen Facial Recognition Jaywalking Indicator Report",
        "Impact Scale": "Local",
        "Impact Depth": "Medium",
        "Impact Duration": "Short-term",
        "Source URL": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/shenzhen-uses-facial-recognition-to-catch-shame-jaywalkers"
      },
      {
        "Title of the story": "Unconstrained College Students Dataset",
        "Organization": "Unconstrained AI",
        "Impact Model": "Common Impact Data Standard",
        "Program": "Unconstrained AI College Students Dataset Program",
        "Service": "Unconstrained AI College Students Dataset Service",
        "Activity": "Collection of data from college students",
        "Input": "Data collected from college students",
        "Output": "Unconstrained AI College Students Dataset",
        "Outcome": "Increased understanding of college students",
        "Stakeholder": "Researchers, developers, and policymakers",
        "Stakeholder Outcome": "Improved ability to develop and design products and services for college students",
        "Impact Report": "Unconstrained AI College Students Dataset Impact Report",
        "Impact Risk": "Potential for misuse of data",
        "Indicator": "Number of researchers, developers, and policymakers who use the Unconstrained AI College Students Dataset",
        "Indicator Report": "Unconstrained AI College Students Dataset Indicator Report",
        "Impact Scale": "Local",
        "Impact Depth": "Medium",
        "Impact Duration": "Short-term",
        "Source URL": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/unconstrained-college-students-dataset"
      },
      {
        "Title of the story": "KFC Germany Uses Marketing Automation to Send Out Messages on Kristallnacht",
        "Organization": "KFC Germany",
        "Impact Model": "Common Impact Data Standard",
        "Program": "KFC Germany Marketing Automation Program",
        "Service": "KFC Germany Marketing Automation Service",
        "Activity": "Use of marketing automation to send out messages on Kristallnacht",
        "Input": "Data collected from KFC Germany customers",
        "Output": "Messages sent out to KFC Germany customers on Kristallnacht",
        "Outcome": "Offense and distress caused to KFC Germany customers",
        "Stakeholder": "KFC Germany customers",
        "Stakeholder Outcome": "Feeling of being targeted and discriminated against",
        "Impact Report": "KFC Germany Marketing Automation Impact Report",
        "Impact Risk": "Potential for misuse of marketing automation data",
        "Indicator": "Number of KFC Germany customers who were offended or distressed by the messages sent out on Kristallnacht",
        "Indicator Report": "KFC Germany Marketing Automation Indicator Report",
        "Impact Scale": "Local",
        "Impact Depth": "Medium",
        "Impact Duration": "Short-term",
        "Source URL": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/kfc-germany-kristallnacht-marketing-automation"
      },
      {
        "Title of the story": "IRCC Immigration and Visa Applications Automation",
        "Organization": "Immigration, Refugees and Citizenship Canada (IRCC)",
        "Impact Model": "Common Impact Data Standard",
        "Program": "IRCC Immigration and Visa Applications Automation Program",
        "Service": "IRCC Immigration and Visa Applications Automation Service",
        "Activity": "Use of artificial intelligence (AI) to automate the processing of immigration and visa applications",
        "Input": "Data collected from immigration and visa applicants",
        "Output": "Automated decisions on immigration and visa applications",
        "Outcome": "Increased efficiency and accuracy in the processing of immigration and visa applications",
        "Stakeholder": "Applicants for immigration and visas",
        "Stakeholder Outcome": "Reduced wait times and increased chances of success in their applications",
        "Impact Report": "IRCC Immigration and Visa Applications Automation Impact Report",
        "Impact Risk": "Potential for bias and discrimination in AI-powered decision-making",
        "Indicator": "Number of immigration and visa applications processed by AI",
        "Indicator Report": "IRCC Immigration and Visa Applications Automation Indicator Report",
        "Impact Scale": "National",
        "Impact Depth": "Medium",
        "Impact Duration": "Long-term",
        "Source URL": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/ircc-immigration-and-visa-applications-automation"
      },
      {
        "Title of the story": "Dubai Drone Weather Engineering",
        "Organization": "National Center of Meteorology (NCM)",
        "Impact Model": "Common Impact Data Standard",
        "Program": "Dubai Drone Weather Engineering Program",
        "Service": "Dubai Drone Weather Engineering Service",
        "Activity": "Use of drones to seed clouds and generate rain",
        "Input": "Data collected from drones",
        "Output": "Rainfall generated by drones",
        "Outcome": "Increased rainfall in Dubai",
        "Stakeholder": "Residents of Dubai",
        "Stakeholder Outcome": "Improved water supply and agricultural production",
        "Impact Report": "Dubai Drone Weather Engineering Impact Report",
        "Impact Risk": "Potential for environmental damage, such as flooding and desertification",
        "Indicator": "Amount of rainfall generated by drones",
        "Indicator Report": "Dubai Drone Weather Engineering Indicator Report",
        "Impact Scale": "Local",
        "Impact Depth": "Medium",
        "Impact Duration": "Short-term",
        "Source URL": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/dubai-drone-weather-engineering"
      },
      {
        "Title of the story": "Tesla Model 3 Hits Tow Truck, Explodes",
        "Organization": "Tesla",
        "Impact Model": "Common Impact Data Standard",
        "Program": "Tesla Autopilot Program",
        "Service": "Tesla Autopilot Service",
        "Activity": "Use of Tesla Autopilot to drive a Tesla Model 3",
        "Input": "Data collected from the Tesla Model 3's sensors and cameras",
        "Output": "Tesla Model 3 driving autonomously",
        "Outcome": "Tesla Model 3 hitting a tow truck and exploding",
        "Stakeholder": "Tesla Model 3 driver, tow truck driver, and bystanders",
        "Stakeholder Outcome": "Injury and death",
        "Impact Report": "Tesla Autopilot Impact Report",
        "Impact Risk": "Potential for Tesla Autopilot to malfunction and cause accidents",
        "Indicator": "Number of Tesla Autopilot-related accidents",
        "Indicator Report": "Tesla Autopilot Indicator Report",
        "Impact Scale": "Local",
        "Impact Depth": "Medium",
        "Impact Duration": "Short-term",
        "Source URL": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-model-3-hits-tow-truck-explodes"
      },
      {
        "Title of the story": "Facebook Labels Black Men as Primates",
        "Organization": "Facebook",
        "Impact Model": "Common Impact Data Standard",
        "Program": "Facebook AI Program",
        "Service": "Facebook AI Service",
        "Activity": "Use of artificial intelligence (AI) to moderate content on Facebook",
        "Input": "Data collected from Facebook users",
        "Output": "Automated decisions on whether content is harmful or not",
        "Outcome": "Black men were disproportionately labeled as primates by Facebook's AI system",
        "Stakeholder": "Black men on Facebook",
        "Stakeholder Outcome": "Feeling of being targeted and discriminated against",
        "Impact Report": "Facebook AI Impact Report",
        "Impact Risk": "Potential for AI to be biased and discriminatory",
        "Indicator": "Number of black men labeled as primates by Facebook's AI system",
        "Indicator Report": "Facebook AI Indicator Report",
        "Impact Scale": "Global",
        "Impact Depth": "Medium",
        "Impact Duration": "Long-term",
        "Source URL": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/facebook-labels-black-men-primates"
      },
      {
        "Title of the story": "Tesla Model X Crashes into Five Police Officers",
        "Organization": "Tesla",
        "Impact Model": "Common Impact Data Standard",
        "Program": "Tesla Autopilot Program",
        "Service": "Tesla Autopilot Service",
        "Activity": "Use of Tesla Autopilot to drive a Tesla Model X",
        "Input": "Data collected from the Tesla Model X's sensors and cameras",
        "Output": "Tesla Model X driving autonomously",
        "Outcome": "Tesla Model X crashing into five police officers",
        "Stakeholder": "Police officers",
        "Stakeholder Outcome": "Injury",
        "Impact Report": "Tesla Autopilot Impact Report",
        "Impact Risk": "Potential for Tesla Autopilot to malfunction and cause accidents",
        "Indicator": "Number of Tesla Autopilot-related accidents",
        "Indicator Report": "Tesla Autopilot Indicator Report",
        "Impact Scale": "Local",
        "Impact Depth": "Medium",
        "Impact Duration": "Short-term",
        "Source URL": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-model-x-crashes-into-five-police-officers"
      },
      {
        "Title of the story": "Deliveroo UK Rider Management Algorithm",
        "Organization": "Deliveroo",
        "Impact Model": "Common Impact Data Standard",
        "Program": "Deliveroo Rider Management Program",
        "Service": "Deliveroo Rider Management Service",
        "Activity": "Use of an algorithm to manage riders",
        "Input": "Data collected from riders",
        "Output": "Decisions about rider assignments, pay, and deactivation",
        "Outcome": "Riders were unfairly penalized by the algorithm",
        "Stakeholder": "Riders",
        "Stakeholder Outcome": "Reduced earnings, increased stress, and feeling of being treated unfairly",
        "Impact Report": "Deliveroo Rider Management Impact Report",
        "Impact Risk": "Potential for the algorithm to be biased and discriminatory",
        "Indicator": "Number of riders penalized by the algorithm",
        "Indicator Report": "Deliveroo Rider Management Indicator Report",
        "Impact Scale": "Local",
        "Impact Depth": "Medium",
        "Impact Duration": "Short-term",
        "Source URL": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/deliveroo-uk-rider-management-algorithm"
      },
      {
        "Title of the story": "Cruzcampo Lola Flores Deepfake Ad",
        "Organization": "Cruzcampo",
        "Impact Model": "Common Impact Data Standard",
        "Program": "Cruzcampo Advertising Program",
        "Service": "Cruzcampo Advertising Service",
        "Activity": "Use of deepfake technology to create an ad featuring the late Spanish singer Lola Flores",
        "Input": "Data collected from Lola Flores's public appearances and recordings",
        "Output": "An ad featuring a deepfake of Lola Flores drinking Cruzcampo beer",
        "Outcome": "The ad was met with controversy, with some people accusing Cruzcampo of disrespecting Flores's memory.",
        "Stakeholder": "Lola Flores's fans",
        "Stakeholder Outcome": "Some fans felt offended and upset by the ad.",
        "Impact Report": "No impact report has been published.",
        "Impact Risk": "Potential for deepfake technology to be used to create misleading or offensive content.",
        "Indicator": "Number of people who viewed the ad.",
        "Indicator Report": "No indicator report has been published.",
        "Impact Scale": "Local",
        "Impact Depth": "Medium",
        "Impact Duration": "Short-term",
        "Source URL": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/cruzcampo-lola-flores-deepfake-ad"
      },
      {
        "Title of the story": "Tesla Autopilot Confused by Billboard",
        "Organization": "Tesla",
        "Impact Model": "Common Impact Data Standard",
        "Program": "Tesla Autopilot Program",
        "Service": "Tesla Autopilot Service",
        "Activity": "Use of Tesla Autopilot to drive a Tesla Model X",
        "Input": "Data collected from the Tesla Model X's sensors and cameras",
        "Output": "Tesla Model X driving autonomously",
        "Outcome": "Tesla Model X crashed into a billboard",
        "Stakeholder": "Tesla drivers",
        "Stakeholder Outcome": "Feeling of unease and distrust of Tesla Autopilot",
        "Impact Report": "Tesla Autopilot Impact Report",
        "Impact Risk": "Potential for Tesla Autopilot to malfunction and cause accidents",
        "Indicator": "Number of Tesla Autopilot-related accidents",
        "Indicator Report": "Tesla Autopilot Indicator Report",
        "Impact Scale": "Local",
        "Impact Depth": "Medium",
        "Impact Duration": "Short-term",
        "Source URL": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-autopilot-confused-by-billboard"
      },
      {
        "Title of the story": "Sleeping Driver Speeds on Highway with Autopilot Switched On",
        "Organization": "Tesla",
        "Impact Model": "Common Impact Data Standard",
        "Program": "Tesla Autopilot Program",
        "Service": "Tesla Autopilot Service",
        "Activity": "Use of Tesla Autopilot to drive a Tesla Model X",
        "Input": "Data collected from the Tesla Model X's sensors and cameras",
        "Output": "Tesla Model X driving autonomously",
        "Outcome": "Sleeping driver speeds on highway with Autopilot switched on",
        "Stakeholder": "Other drivers on the highway",
        "Stakeholder Outcome": "Feeling of danger and uncertainty",
        "Impact Report": "Tesla Autopilot Impact Report",
        "Impact Risk": "Potential for Tesla Autopilot to be misused and cause accidents",
        "Indicator": "Number of Tesla Autopilot-related accidents caused by sleeping drivers",
        "Indicator Report": "Tesla Autopilot Indicator Report",
        "Impact Scale": "Local",
        "Impact Depth": "Medium",
        "Impact Duration": "Short-term",
        "Source URL": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/sleeping-driver-speeds-on-highway-with-autopilot-switched-on"
      },
      {
        "Title of the story": "HireVue Recruitment Facial Analysis Screening",
        "Organization": "HireVue",
        "Impact Model": "Common Impact Data Standard",
        "Program": "HireVue Recruitment Program",
        "Service": "HireVue Recruitment Service",
        "Activity": "Use of facial analysis software to screen job candidates",
        "Input": "Data collected from job candidates' facial expressions and movements",
        "Output": "Scores that are used to rank job candidates",
        "Outcome": "Job candidates were unfairly discriminated against based on their race, gender, and other factors",
        "Stakeholder": "Job candidates",
        "Stakeholder Outcome": "Reduced chances of getting hired, increased stress, and feeling of being treated unfairly",
        "Impact Report": "HireVue Recruitment Impact Report",
        "Impact Risk": "Potential for facial analysis software to be biased and discriminatory",
        "Indicator": "Number of job candidates who were unfairly discriminated against",
        "Indicator Report": "HireVue Recruitment Indicator Report",
        "Impact Scale": "Local",
        "Impact Depth": "Medium",
        "Impact Duration": "Short-term",
        "Source URL": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/hirevue-recruitment-facial-analysis-screening"
      },
      {
        "Title of the story": "Rio de Janeiro Facial Recognition Wrongful Arrests",
        "Organization": "Rio de Janeiro Police Department",
        "Impact Model": "Common Impact Data Standard",
        "Program": "Rio de Janeiro Facial Recognition Program",
        "Service": "Rio de Janeiro Facial Recognition Service",
        "Activity": "Use of facial recognition software to identify suspects",
        "Input": "Data collected from security cameras and other sources",
        "Output": "Matches between suspects' faces and faces in the database",
        "Outcome": "Several people were wrongfully arrested based on false matches",
        "Stakeholder": "People who were wrongfully arrested",
        "Stakeholder Outcome": "Inconvenience, stress, and damage to reputation",
        "Impact Report": "Rio de Janeiro Facial Recognition Impact Report",
        "Impact Risk": "Potential for facial recognition software to be inaccurate and lead to wrongful arrests",
        "Indicator": "Number of people who were wrongfully arrested",
        "Indicator Report": "Rio de Janeiro Facial Recognition Indicator Report",
        "Impact Scale": "Local",
        "Impact Depth": "Medium",
        "Impact Duration": "Short-term",
        "Source URL": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/rio-de-janeiro-facial-recognition-wrongful-arrests"
      },
      {
        "Title of the story": "Uber Upfront Fares Driver Pay Algorithm",
        "Organization": "Uber",
        "Impact Model": "Common Impact Data Standard",
        "Program": "Uber Upfront Fares Program",
        "Service": "Uber Upfront Fares Service",
        "Activity": "Use of an algorithm to determine driver pay",
        "Input": "Data collected from drivers' trips, including distance, time, and traffic conditions",
        "Output": "A fare that is displayed to riders before they request a ride",
        "Outcome": "Some drivers have reported that they are earning less money since Uber began using the Upfront Fares algorithm",
        "Stakeholder": "Uber drivers",
        "Stakeholder Outcome": "Reduced earnings, increased stress, and feeling of being treated unfairly",
        "Impact Report": "Uber Upfront Fares Impact Report",
        "Impact Risk": "Potential for the Upfront Fares algorithm to be biased and lead to lower driver earnings",
        "Indicator": "Average driver earnings before and after Uber began using the Upfront Fares algorithm",
        "Indicator Report": "Uber Upfront Fares Indicator Report",
        "Impact Scale": "Local",
        "Impact Depth": "Medium",
        "Impact Duration": "Short-term",
        "Source URL": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/uber-upfront-fares-driver-pay-algorithm"
      },
      {
        "Title of the story": "Amazon AWS Panorama Workplace Surveillance",
        "Organization": "Amazon Web Services",
        "Impact Model": "Common Impact Data Standard",
        "Program": "Amazon AWS Panorama Program",
        "Service": "Amazon AWS Panorama Service",
        "Activity": "Use of AI-powered cameras to monitor workplace activity",
        "Input": "Data collected from AI-powered cameras, including images, audio, and movement data",
        "Output": "Heat maps, activity logs, and other insights into workplace activity",
        "Outcome": "Some employees have raised concerns about privacy and surveillance",
        "Stakeholder": "Employees",
        "Stakeholder Outcome": "Feeling of being watched and monitored, reduced privacy, and stress",
        "Impact Report": "Amazon AWS Panorama Impact Report",
        "Impact Risk": "Potential for AI-powered cameras to be used to collect and store sensitive data, and to track and monitor employees without their consent",
        "Indicator": "Number of employees who have raised concerns about privacy and surveillance",
        "Indicator Report": "Amazon AWS Panorama Indicator Report",
        "Impact Scale": "Local",
        "Impact Depth": "Medium",
        "Impact Duration": "Short-term",
        "Source URL": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/amazon-aws-panorama-workplace-surveillance"
      },
      {
        "Title of the story": "Mobileye 630 PRO Tricked by Drones, Projectors",
        "Organization": "Mobileye",
        "Impact Model": "Common Impact Data Standard",
        "Program": "Mobileye 630 PRO Program",
        "Service": "Mobileye 630 PRO Service",
        "Activity": "Use of Mobileye 630 PRO self-driving technology",
        "Input": "Data collected from Mobileye 630 PRO sensors and cameras",
        "Output": "Decisions about how to control the vehicle",
        "Outcome": "Mobileye 630 PRO vehicles were tricked by drones and projectors into making dangerous decisions",
        "Stakeholder": "Drivers and passengers of Mobileye 630 PRO vehicles",
        "Stakeholder Outcome": "Increased risk of accidents and injuries",
        "Impact Report": "Mobileye 630 PRO Impact Report",
        "Impact Risk": "Potential for Mobileye 630 PRO self-driving technology to be tricked by other objects and systems",
        "Indicator": "Number of accidents and injuries caused by Mobileye 630 PRO vehicles being tricked by drones and projectors",
        "Indicator Report": "Mobileye 630 PRO Indicator Report",
        "Impact Scale": "Local",
        "Impact Depth": "Medium",
        "Impact Duration": "Short-term",
        "Source URL": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/mobileye-630-pro-tricked-by-drones-projectors"
      },
      {
        "Title of the story": "Rite Aid Facial Recognition",
        "Organization": "Rite Aid",
        "Impact Model": "Common Impact Data Standard",
        "Program": "Rite Aid Facial Recognition Program",
        "Service": "Rite Aid Facial Recognition Service",
        "Activity": "Use of facial recognition technology to identify customers",
        "Input": "Data collected from customers' faces, including images and biometric data",
        "Output": "Matches between customers' faces and faces in the database",
        "Outcome": "Some customers have raised concerns about privacy and discrimination",
        "Stakeholder": "Customers",
        "Stakeholder Outcome": "Feeling of being watched and monitored, reduced privacy, and stress",
        "Impact Report": "Rite Aid Facial Recognition Impact Report",
        "Impact Risk": "Potential for facial recognition technology to be used to track and monitor customers without their consent, and to discriminate against customers based on their race, gender, or other factors",
        "Indicator": "Number of customers who have raised concerns about privacy and discrimination",
        "Indicator Report": "Rite Aid Facial Recognition Indicator Report",
        "Impact Scale": "Local",
        "Impact Depth": "Medium",
        "Impact Duration": "Short-term",
        "Source URL": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/rite-aid-facial-recognition"
      },
      {
        "Title of the story": "Toyota Paralympics Self-Driving Bus Hits Athlete",
        "Organization": "Toyota",
        "Impact Model": "Common Impact Data Standard",
        "Program": "Toyota Paralympics Self-Driving Bus Program",
        "Service": "Toyota Paralympics Self-Driving Bus Service",
        "Activity": "Use of self-driving technology to transport athletes during the 2020 Paralympics in Tokyo",
        "Input": "Data collected from the self-driving bus's sensors and cameras",
        "Output": "Decisions about how to control the bus",
        "Outcome": "A Japanese visually impaired athlete was hit by a Toyota e-Palette self-driving bus used to ferry athletes during the 2020 Paralympic Games in Tokyo. Aramitsu Kitazono was attempting to cross a street at a designated crossing within the Athletes Village when he was hit. He was left unable to compete.",
        "Stakeholder": "Athletes",
        "Stakeholder Outcome": "Inconvenience, stress, and damage to reputation",
        "Impact Report": "Toyota Paralympics Self-Driving Bus Impact Report",
        "Impact Risk": "Potential for self-driving technology to be inaccurate and lead to accidents",
        "Indicator": "Number of accidents involving self-driving buses",
        "Indicator Report": "Toyota Paralympics Self-Driving Bus Indicator Report",
        "Impact Scale": "Local",
        "Impact Depth": "Medium",
        "Impact Duration": "Short-term",
        "Source URL": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/toyota-paralympics-self-driving-bus-hits-athlete"
      },
      {
        "Title of the story": "VGG-Face Facial Recognition Dataset",
        "Organization": "University of Oxford",
        "Impact Model": "\"Common Impact Data Standard\"",
        "Program": "VGG-Face Facial Recognition Program",
        "Service": "VGG-Face Facial Recognition Service",
        "Activity": "Creation of a facial recognition dataset",
        "Input": "Images of faces from the internet",
        "Output": "A dataset of facial images and their associated metadata, such as the person's name, age, and gender",
        "Outcome": "The dataset has been used to train a number of facial recognition algorithms, which have been used in a variety of applications, such as unlocking smartphones, identifying criminals, and tagging people in photos.",
        "Stakeholder": "People whose images are included in the dataset",
        "Stakeholder Outcome": "Potential for their privacy to be violated",
        "Impact Report": "\"VGG-Face Facial Recognition Impact Report\"",
        "Impact Risk": "Potential for the dataset to be used to discriminate against people based on their race, gender, or other factors",
        "Indicator": "Number of people whose images are included in the dataset",
        "Indicator Report": "\"VGG-Face Facial Recognition Indicator Report\"",
        "Impact Scale": "Global",
        "Impact Depth": "Medium",
        "Impact Duration": "Long-term",
        "Source URL": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/vgg-face-facial-recognition-dataset"
      },
      {
        "Title of the story": "Tesla Model 3 Crashes into Bus in Ruian, Kills One",
        "Organization": "Tesla",
        "Impact Model": "Common Impact Data Standard",
        "Program": "Tesla Autopilot Program",
        "Service": "Tesla Autopilot Service",
        "Activity": "Use of Tesla Autopilot technology",
        "Input": "Data collected from Tesla Autopilot sensors and cameras",
        "Output": "Decisions about how to control the vehicle",
        "Outcome": "A Tesla Model 3 crashed into a bus in Ruian, China, killing one person and injuring two others. The driver of the Tesla was not injured.",
        "Stakeholder": "Drivers and passengers of Tesla vehicles",
        "Stakeholder Outcome": "Increased risk of accidents and injuries",
        "Impact Report": "Tesla Autopilot Impact Report",
        "Impact Risk": "Potential for Tesla Autopilot technology to be inaccurate and lead to accidents",
        "Indicator": "Number of accidents involving Tesla vehicles using Autopilot",
        "Indicator Report": "Tesla Autopilot Indicator Report",
        "Impact Scale": "Local",
        "Impact Depth": "Medium",
        "Impact Duration": "Short-term",
        "Source URL": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-model-3-crashes-into-bus-in-ruian-kills-one"
      },
      {
        "Title of the story": "Apple Watch Blood Oximeter Racial Bias",
        "Organization": "Apple",
        "Impact Model": "Common Impact Data Standard",
        "Program": "Apple Watch Blood Oximeter Program",
        "Service": "Apple Watch Blood Oximeter Service",
        "Activity": "Use of Apple Watch blood oximeter to measure blood oxygen levels",
        "Input": "Data collected from the Apple Watch's sensors",
        "Output": "Measurements of blood oxygen levels",
        "Outcome": "The Apple Watch blood oximeter has been shown to be less accurate for people with darker skin tones",
        "Stakeholder": "People with darker skin tones",
        "Stakeholder Outcome": "Potential for inaccurate blood oxygen measurements to lead to misdiagnosis of health conditions",
        "Impact Report": "Apple Watch Blood Oximeter Racial Bias Impact Report",
        "Impact Risk": "Potential for Apple Watch blood oximeter to be used to discriminate against people with darker skin tones",
        "Indicator": "Accuracy of blood oxygen measurements for people with different skin tones",
        "Indicator Report": "Apple Watch Blood Oximeter Racial Bias Indicator Report",
        "Impact Scale": "Local",
        "Impact Depth": "Medium",
        "Impact Duration": "Short-term",
        "Source URL": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/apple-watch-blood-oximeter-racial-bias"
      },
      {
        "Title of the story": "Apple Cycle Tracking Fertility Predictions",
        "Organization": "Apple",
        "Impact Model": "Common Impact Data Standard",
        "Program": "Apple Cycle Tracking Program",
        "Service": "Apple Cycle Tracking Service",
        "Activity": "Use of Apple's cycle tracking app to predict fertility",
        "Input": "Data collected from the user, such as menstrual cycle length, flow, and symptoms",
        "Output": "Predictions about the user's fertility window",
        "Outcome": "The Apple cycle tracking app has been shown to be inaccurate for some users",
        "Stakeholder": "Users of the Apple cycle tracking app",
        "Stakeholder Outcome": "Potential for inaccurate fertility predictions to lead to unintended pregnancies or difficulty conceiving",
        "Impact Report": "Apple Cycle Tracking Fertility Predictions Impact Report",
        "Impact Risk": "Potential for Apple cycle tracking app to be used to discriminate against people based on their fertility status",
        "Indicator": "Accuracy of fertility predictions for different users",
        "Indicator Report": "Apple Cycle Tracking Fertility Predictions Indicator Report",
        "Impact Scale": "Local",
        "Impact Depth": "Medium",
        "Impact Duration": "Short-term",
        "Source URL": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/apple-cycle-tracking-fertility-predictions"
      },
      {
        "Title of the story": "SyRI Welfare Fraud Detection Automation",
        "Organization": "Dutch government",
        "Impact Model": "Common Impact Data Standard",
        "Program": "SyRI Welfare Fraud Detection Program",
        "Service": "SyRI Welfare Fraud Detection Service",
        "Activity": "Use of artificial intelligence to detect welfare fraud",
        "Input": "Data collected from government databases, such as social security records, tax records, and employment records",
        "Output": "Predictions about whether a person is likely to commit welfare fraud",
        "Outcome": "The SyRI system was found to be discriminatory and inaccurate, and was shut down in 2019",
        "Stakeholder": "People receiving welfare benefits",
        "Stakeholder Outcome": "Increased stress, anxiety, and fear of being wrongly accused of fraud",
        "Impact Report": "SyRI Welfare Fraud Detection Impact Report",
        "Impact Risk": "Potential for SyRI to be used to discriminate against people based on their race, ethnicity, or other factors",
        "Indicator": "Number of people wrongly accused of welfare fraud",
        "Indicator Report": "SyRI Welfare Fraud Detection Indicator Report",
        "Impact Scale": "Local",
        "Impact Depth": "Medium",
        "Impact Duration": "Short-term",
        "Source URL": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/syri-welfare-fraud-detection-automation"
      },
      {
        "title of the story": "80 Million Tiny Images Dataset",
        "organization": "MIT",
        "impact model": "\"Common Impact Data Standard\"",
        "program": "80 Million Tiny Images Program",
        "service": "80 Million Tiny Images Service",
        "activity": "creation of a dataset of 80 million tiny images",
        "input": "images of objects and scenes from the internet",
        "output": "a dataset of 80 million tiny images, each of which is 32x32 pixels in size",
        "outcome": "the dataset has been used to train a number of machine learning models, which have been used in a variety of applications, such as image classification, object detection, and scene understanding.",
        "stakeholder": "people whose images are included in the dataset, as well as users of machine learning models that have been trained on the dataset",
        "stakeholder outcome": "potential for their privacy to be violated, as well as potential for machine learning models to be biased or inaccurate",
        "impact report": "\"80 Million Tiny Images Impact Report\"",
        "impact risk": "potential for the dataset to be used to discriminate against people based on their race, gender, or other factors, as well as potential for the dataset to be used to create deepfakes or other forms of synthetic media",
        "indicator": "number of people whose images are included in the dataset, as well as number of machine learning models that have been trained on the dataset",
        "indicator report": "\"80 Million Tiny Images Indicator Report\"",
        "impact scale": "global",
        "impact depth": "medium",
        "impact duration": "long-term",
        "source url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/80-million-tiny-images-dataset"
      },
      {
        "title of the story": "Barclays Employee Spyware Monitoring",
        "organization": "Barclays",
        "impact model": "\"Common Impact Data Standard\"",
        "program": "Barclays Employee Spyware Monitoring Program",
        "service": "Barclays Employee Spyware Monitoring Service",
        "activity": "use of employee spyware to monitor employee activity",
        "input": "data collected from employee computers, such as keystrokes, websites visited, and emails sent",
        "output": "reports on employee activity",
        "outcome": "employees have expressed concerns about privacy and stress",
        "stakeholder": "Barclays employees",
        "stakeholder outcome": "potential for increased stress, anxiety, and fear of being monitored",
        "impact report": "\"Barclays Employee Spyware Monitoring Impact Report\"",
        "impact risk": "potential for employee spyware to be used to discriminate against employees based on their race, gender, or other factors",
        "indicator": "number of employees who have expressed concerns about employee spyware",
        "indicator report": "\"Barclays Employee Spyware Monitoring Indicator Report\"",
        "impact scale": "local",
        "impact depth": "medium",
        "impact duration": "short-term",
        "source url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/barclays-employee-spyware-monitoring"
      },
      {
        "title of the story": "OostoanyVision Facial Recognition Drones",
        "organization": "OostoanyVision",
        "impact model": "\"Common Impact Data Standard\"",
        "program": "OostoanyVision Facial Recognition Drone Program",
        "service": "OostoanyVision Facial Recognition Drone Service",
        "activity": "use of facial recognition drones to monitor public spaces",
        "input": "data collected from facial recognition drones, such as images of people's faces",
        "output": "reports on people's movements and activities",
        "outcome": "people have expressed concerns about privacy and surveillance",
        "stakeholder": "People who are monitored by facial recognition drones",
        "stakeholder outcome": "potential for increased stress, anxiety, and fear of being monitored",
        "impact report": "\"OostoanyVision Facial Recognition Drone Impact Report\"",
        "impact risk": "potential for facial recognition drones to be used to discriminate against people based on their race, gender, or other factors",
        "indicator": "number of people who have expressed concerns about facial recognition drones",
        "indicator report": "\"OostoanyVision Facial Recognition Drone Indicator Report\"",
        "impact scale": "local",
        "impact depth": "medium",
        "impact duration": "short-term",
        "source url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/oostoanyvision-facial-recognition-drones"
      },
      {
        "title of the story": "Amazon Ring Police Data Sharing",
        "organization": "Amazon/Ring",
        "impact model": "\"Common Impact Data Standard\"",
        "program": "Amazon Ring Police Data Sharing Program",
        "service": "Amazon Ring Police Data Sharing Service",
        "activity": "sharing of data from Amazon Ring doorbell cameras with police departments",
        "input": "data from Amazon Ring doorbell cameras, such as videos and images of people and events",
        "output": "reports on people and events that have been captured by Amazon Ring doorbell cameras",
        "outcome": "increased police surveillance and potential for discrimination",
        "stakeholder": "People who use Amazon Ring doorbell cameras and people who are monitored by police using data from Amazon Ring doorbell cameras",
        "stakeholder outcome": "potential for increased stress, anxiety, and fear of being monitored",
        "impact report": "\"Amazon Ring Police Data Sharing Impact Report\"",
        "impact risk": "potential for Amazon Ring police data sharing to be used to discriminate against people based on their race, gender, or other factors",
        "indicator": "number of people who have expressed concerns about Amazon Ring police data sharing",
        "indicator report": "\"Amazon Ring Police Data Sharing Indicator Report\"",
        "impact scale": "local",
        "impact depth": "medium",
        "impact duration": "short-term",
        "source url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/amazon-ring-police-data-sharing"
      },
      {
        "title of the story": "MoviePass Preshow Eye Tracking",
        "organization": "MoviePass",
        "impact model": "\"Common Impact Data Standard\"",
        "program": "MoviePass Preshow Eye Tracking Program",
        "service": "MoviePass Preshow Eye Tracking Service",
        "activity": "use of eye tracking technology to monitor moviegoers' attention during preshow ads",
        "input": "data collected from eye tracking technology, such as gaze direction and duration",
        "output": "reports on moviegoers' attention during preshow ads",
        "outcome": "increased awareness of preshow ads and potential for changes in ad design",
        "stakeholder": "Moviegoers and advertisers",
        "stakeholder outcome": "potential for increased annoyance and decreased effectiveness of preshow ads",
        "impact report": "\"MoviePass Preshow Eye Tracking Impact Report\"",
        "impact risk": "potential for eye tracking technology to be used to discriminate against moviegoers based on their race, gender, or other factors",
        "indicator": "number of moviegoers who have expressed concerns about MoviePass preshow eye tracking",
        "indicator report": "\"MoviePass Preshow Eye Tracking Indicator Report\"",
        "impact scale": "local",
        "impact depth": "medium",
        "impact duration": "short-term",
        "source url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/moviepass-preshow-eye-tracking"
      },
      {
        "title of the story": "Facebook Military Gear Advertising",
        "organization": "Meta/Facebook",
        "impact model": "\"Common Impact Data Standard\"",
        "program": "Facebook Military Gear Advertising Program",
        "service": "Facebook Military Gear Advertising Service",
        "activity": "running of adverts for military gear and weapons accessories next to posts about the January 6 attempted coup in Washington DC",
        "input": "data on users' interests and browsing history",
        "output": "ads for military gear and weapons accessories",
        "outcome": "increased awareness of and interest in military gear and weapons accessories, as well as potential for increased gun violence",
        "stakeholder": "Facebook users, gun owners, and the general public",
        "stakeholder outcome": "potential for increased anxiety and fear, as well as increased gun violence",
        "impact report": "\"Facebook Military Gear Advertising Impact Report\"",
        "impact risk": "potential for Facebook to be used to spread misinformation and propaganda about military gear and weapons accessories",
        "indicator": "number of Facebook users who have seen ads for military gear and weapons accessories",
        "indicator report": "\"Facebook Military Gear Advertising Indicator Report\"",
        "impact scale": "global",
        "impact depth": "medium",
        "impact duration": "long-term",
        "source url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/facebook-military-gear-advertising"
      },
      {
        "title of the story": "Joe Rogan Libido Booster Alpha Grind Ad Deepfake",
        "organization": "Onnit",
        "impact model": "\"Common Impact Data Standard\"",
        "program": "Onnit Alpha Grind Ad Deepfake Program",
        "service": "Onnit Alpha Grind Ad Deepfake Service",
        "activity": "creation and distribution of a deepfake ad featuring Joe Rogan",
        "input": "deepfake technology, footage of Joe Rogan, and the Alpha Grind product",
        "output": "an ad for the Alpha Grind product featuring Joe Rogan",
        "outcome": "increased awareness of the Alpha Grind product, as well as potential for harm to Joe Rogan's reputation",
        "stakeholder": "Joe Rogan, Onnit, and consumers of the Alpha Grind product",
        "stakeholder outcome": "potential for Joe Rogan to be mischaracterized, as well as potential for consumers to be misled about the benefits of the Alpha Grind product",
        "impact report": "\"Joe Rogan Libido Booster Alpha Grind Ad Deepfake Impact Report\"",
        "impact risk": "potential for the deepfake technology to be used to spread misinformation or propaganda",
        "indicator": "number of people who have seen the deepfake ad",
        "indicator report": "\"Joe Rogan Libido Booster Alpha Grind Ad Deepfake Indicator Report\"",
        "impact scale": "global",
        "impact depth": "medium",
        "impact duration": "short-term",
        "source url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/joe-rogan-libido-booster-alpha-grind-ad-deepfake"
      },
      {
        "title of the story": "Walgreens Fridge Screen Door Biometrics",
        "organization": "Walgreens",
        "impact model": "\"Common Impact Data Standard\"",
        "program": "Walgreens Fridge Screen Door Biometrics Program",
        "service": "Walgreens Fridge Screen Door Biometrics Service",
        "activity": "use of biometrics to unlock fridge screen doors at Walgreens stores",
        "input": "data collected from users' fingerprints, such as the unique fingerprint pattern",
        "output": "access to the fridge screen door",
        "outcome": "increased convenience for customers, as well as potential for privacy concerns",
        "stakeholder": "Walgreens customers",
        "stakeholder outcome": "potential for customers to feel uncomfortable with their biometric data being collected and used by Walgreens",
        "impact report": "\"Walgreens Fridge Screen Door Biometrics Impact Report\"",
        "impact risk": "potential for the biometric data to be stolen or used for fraudulent purposes",
        "indicator": "number of Walgreens customers who have used the fridge screen door biometrics feature",
        "indicator report": "\"Walgreens Fridge Screen Door Biometrics Indicator Report\"",
        "impact scale": "local",
        "impact depth": "medium",
        "impact duration": "short-term",
        "source url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/walgreens-fridge-screen-door-biometrics"
      },
      {
        "title of the story": "Google autocomplete connects Albert Yeung with triads",
        "organization": "Google",
        "impact model": "\"Common Impact Data Standard\"",
        "program": "Google Autocomplete Program",
        "service": "Google Autocomplete Service",
        "activity": "use of Google autocomplete to suggest results when users type in search queries",
        "input": "data collected from users' search queries, such as the words and phrases that they type",
        "output": "list of suggested results, including websites, images, and other content",
        "outcome": "increased awareness of Albert Yeung's alleged connections to triads, as well as potential for harm to his reputation",
        "stakeholder": "Albert Yeung, his family, and his businesses",
        "stakeholder outcome": "potential for Albert Yeung to be mischaracterized, as well as potential for his businesses to be damaged",
        "impact report": "\"Google Autocomplete Connects Albert Yeung with Triads Impact Report\"",
        "impact risk": "potential for Google autocomplete to be used to spread misinformation or propaganda",
        "indicator": "number of people who have seen the Google autocomplete suggestion that connects Albert Yeung with triads",
        "indicator report": "\"Google Autocomplete Connects Albert Yeung with Triads Indicator Report\"",
        "impact scale": "global",
        "impact depth": "medium",
        "impact duration": "short-term",
        "source url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/google-autocomplete-connects-albert-yeung-with-triads"
      },
      {
        "title of the story": "Unity GovTech AI military applications",
        "organization": "Unity Technologies",
        "impact model": "\"Common Impact Data Standard\"",
        "program": "Unity GovTech AI military applications program",
        "service": "Unity GovTech AI military applications service",
        "activity": "development of AI-powered software for military applications",
        "input": "data on military tactics, weapons, and other factors",
        "output": "AI-powered software that can be used to improve military operations",
        "outcome": "increased effectiveness of military operations, as well as potential for harm to civilians",
        "stakeholder": "Governments and militaries",
        "stakeholder outcome": "potential for increased civilian casualties, as well as potential for the use of AI-powered software for illegal or unethical purposes",
        "impact report": "\"Unity GovTech AI military applications impact report\"",
        "impact risk": "potential for AI-powered software to be used to create autonomous weapons systems that could operate without human intervention",
        "indicator": "number of governments and militaries that have adopted Unity GovTech AI military applications",
        "indicator report": "\"Unity GovTech AI military applications indicator report\"",
        "impact scale": "global",
        "impact depth": "medium",
        "impact duration": "long-term",
        "source url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/unity-govtech-ai-military-applications"
      },
      {
        "title of the story": "Intel AI student emotion monitoring",
        "organization": "Intel",
        "impact model": "\"Common Impact Data Standard\"",
        "program": "Intel AI student emotion monitoring program",
        "service": "Intel AI student emotion monitoring service",
        "activity": "development and use of AI-powered software to monitor student emotions",
        "input": "data on student facial expressions, voice tone, and other factors",
        "output": "real-time reports on student emotions",
        "outcome": "increased awareness of student emotions, as well as potential for harm to students' privacy",
        "stakeholder": "Students, teachers, and parents",
        "stakeholder outcome": "potential for students to feel uncomfortable with their emotions being monitored, as well as potential for teachers and parents to use the data to make decisions about students that are not in their best interests",
        "impact report": "\"Intel AI student emotion monitoring impact report\"",
        "impact risk": "potential for the AI-powered software to be used to discriminate against students based on their emotions",
        "indicator": "number of students who have been monitored by the Intel AI student emotion monitoring software",
        "indicator report": "\"Intel AI student emotion monitoring indicator report\"",
        "impact scale": "local",
        "impact depth": "medium",
        "impact duration": "short-term",
        "source url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/intel-ai-student-emotion-monitoring"
      },
      {
        "title of the story": "Glovo Foodinho rider management algorithm",
        "organization": "Glovo",
        "impact model": "\"Common Impact Data Standard\"",
        "program": "Glovo Foodinho rider management algorithm program",
        "service": "Glovo Foodinho rider management algorithm service",
        "activity": "development and use of an AI-powered algorithm to manage riders",
        "input": "data on rider performance, such as acceptance rate, cancellation rate, and delivery speed",
        "output": "ratings for riders, which are used to determine their eligibility for shifts and bonuses",
        "outcome": "increased efficiency of rider management, as well as potential for harm to riders' livelihoods",
        "stakeholder": "Glovo riders",
        "stakeholder outcome": "potential for riders to be unfairly penalized by the algorithm, as well as potential for riders to lose their jobs due to low ratings",
        "impact report": "\"Glovo Foodinho rider management algorithm impact report\"",
        "impact risk": "potential for the algorithm to be biased against certain groups of riders, such as women and minorities",
        "indicator": "number of Glovo riders who have been rated by the algorithm",
        "indicator report": "\"Glovo Foodinho rider management algorithm indicator report\"",
        "impact scale": "local",
        "impact depth": "medium",
        "impact duration": "long-term",
        "source url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/glovofoodinho-rider-management-algorithm"
      },
      {
        "title of the story": "LAPD social media data collection",
        "organization": "Los Angeles Police Department",
        "impact model": "\"Common Impact Data Standard\"",
        "program": "LAPD social media data collection program",
        "service": "LAPD social media data collection service",
        "activity": "collection of social media data by the LAPD",
        "input": "data from social media platforms, such as Facebook, Twitter, and Instagram",
        "output": "a database of social media data that can be used by the LAPD for a variety of purposes, such as identifying potential criminals and investigating crimes",
        "outcome": "increased ability of the LAPD to identify and investigate crimes, as well as potential for harm to civil liberties",
        "stakeholder": "The public, including potential criminals and victims of crime",
        "stakeholder outcome": "potential for the LAPD to misuse the social media data, such as by targeting individuals for surveillance or harassment",
        "impact report": "\"LAPD social media data collection impact report\"",
        "impact risk": "potential for the LAPD to violate the privacy of individuals by collecting and storing their social media data",
        "indicator": "number of social media posts that have been collected by the LAPD",
        "indicator report": "\"LAPD social media data collection indicator report\"",
        "impact scale": "local",
        "impact depth": "medium",
        "impact duration": "long-term",
        "source url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/lapd-social-media-data-collection"
      },
      {
        "title of the story": "Rio de Janeiro facial recognition wrongful arrests",
        "organization": "Rio de Janeiro Police Department",
        "impact model": "\"Common Impact Data Standard\"",
        "program": "Rio de Janeiro facial recognition program",
        "service": "Rio de Janeiro facial recognition service",
        "activity": "use of facial recognition technology by the Rio de Janeiro Police Department",
        "input": "data from facial recognition cameras, such as images and videos",
        "output": "a list of people who are suspected of committing crimes, based on their facial features",
        "outcome": "increased number of arrests, as well as potential for wrongful arrests",
        "stakeholder": "The public, including people who have been wrongfully arrested",
        "stakeholder outcome": "potential for people to be arrested for crimes that they did not commit",
        "impact report": "\"Rio de Janeiro facial recognition wrongful arrests impact report\"",
        "impact risk": "potential for the facial recognition technology to be biased against certain groups of people, such as people of color",
        "indicator": "number of people who have been wrongfully arrested due to facial recognition",
        "indicator report": "\"Rio de Janeiro facial recognition wrongful arrests indicator report\"",
        "impact scale": "local",
        "impact depth": "medium",
        "impact duration": "long-term",
        "source url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/rio-de-janeiro-facial-recognition-wrongful-arrests"
      },
      {
        "title of the story": "Tesla Paris fatal crash",
        "organization": "Tesla",
        "impact model": "\"Common Impact Data Standard\"",
        "program": "Tesla Autopilot program",
        "service": "Tesla Autopilot service",
        "activity": "use of Tesla's Autopilot driver-assist feature",
        "input": "data from sensors on the Tesla car, such as cameras and radar",
        "output": "actions taken by the Tesla car, such as steering, braking, and accelerating",
        "outcome": "fatal crash, as well as potential for more crashes",
        "stakeholder": "The public, including people who have been injured or killed in Tesla Autopilot crashes",
        "stakeholder outcome": "potential for people to be injured or killed in Tesla Autopilot crashes",
        "impact report": "\"Tesla Paris fatal crash impact report\"",
        "impact risk": "potential for the Tesla Autopilot feature to be unreliable or unsafe",
        "indicator": "number of people who have been injured or killed in Tesla Autopilot crashes",
        "indicator report": "\"Tesla Paris fatal crash indicator report\"",
        "impact scale": "global",
        "impact depth": "medium",
        "impact duration": "long-term",
        "source url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-paris-fatal-crash"
      },
      {
        "title of the story": "Seoul bridge suicide detection",
        "organization": "Seoul Metropolitan Government",
        "impact model": "\"Common Impact Data Standard\"",
        "program": "Seoul bridge suicide detection program",
        "service": "Seoul bridge suicide detection service",
        "activity": "use of AI-powered cameras to detect people who are at risk of suicide on bridges",
        "input": "data from AI-powered cameras, such as images and videos",
        "output": "a list of people who are at risk of suicide, based on their behavior",
        "outcome": "increased number of people who are rescued from suicide attempts, as well as potential for false positives",
        "stakeholder": "The public, including people who have been rescued from suicide attempts",
        "stakeholder outcome": "potential for people to be falsely identified as being at risk of suicide",
        "impact report": "\"Seoul bridge suicide detection impact report\"",
        "impact risk": "potential for the AI-powered cameras to be biased against certain groups of people, such as people of color",
        "indicator": "number of people who have been rescued from suicide attempts due to AI-powered cameras",
        "indicator report": "\"Seoul bridge suicide detection indicator report\"",
        "impact scale": "local",
        "impact depth": "medium",
        "impact duration": "long-term",
        "source url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/seoul-bridge-suicide-detection"
      },
      {
        "title of the story": "Harrisburg University criminality prediction study",
        "organization": "Harrisburg University",
        "impact model": "\"Common Impact Data Standard\"",
        "program": "Harrisburg University criminality prediction program",
        "service": "Harrisburg University criminality prediction service",
        "activity": "development of software to predict criminality based on facial images",
        "input": "data on facial images, such as age, gender, and ethnicity",
        "output": "predictions of whether someone is likely to commit a crime",
        "outcome": "increased risk of discrimination against certain groups of people, as well as potential for harm to people's privacy",
        "stakeholder": "The public, including people who are likely to be discriminated against by the software, and people who have their privacy harmed by the software",
        "stakeholder outcome": "potential for people to be unfairly targeted by law enforcement, and potential for people to have their privacy violated",
        "impact report": "\"Harrisburg University criminality prediction impact report\"",
        "impact risk": "potential for the software to be biased against certain groups of people, such as people of color",
        "indicator": "number of people who have been discriminated against or had their privacy harmed by the software",
        "indicator report": "\"Harrisburg University criminality prediction indicator report\"",
        "impact scale": "local",
        "impact depth": "medium",
        "impact duration": "long-term",
        "source url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/harrisburg-university-criminality-prediction-study"
      },
      {
        "title of the story": "ChatGPT writes fake online reviews",
        "organization": "OpenAI",
        "impact model": "\"Common Impact Data Standard\"",
        "program": "ChatGPT program",
        "service": "ChatGPT service",
        "activity": "use of ChatGPT to write fake online reviews",
        "input": "data on ChatGPT, such as its training data and algorithms",
        "output": "fake online reviews",
        "outcome": "increased risk of fraud and deception, as well as potential for harm to businesses and consumers",
        "stakeholder": "The public, including businesses and consumers",
        "stakeholder outcome": "potential for businesses to lose customers and revenue, and potential for consumers to be misled by fake reviews",
        "impact report": "\"ChatGPT fake online reviews impact report\"",
        "impact risk": "potential for ChatGPT to be used to create fake reviews that are indistinguishable from real reviews",
        "indicator": "number of fake online reviews that have been created using ChatGPT",
        "indicator report": "\"ChatGPT fake online reviews indicator report\"",
        "impact scale": "local",
        "impact depth": "medium",
        "impact duration": "long-term",
        "source url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/chatgpt-writes-fake-online-reviews"
      },
      {
        "title of the story": "Lockport City School District facial recognition",
        "organization": "Lockport City School District",
        "impact model": "\"Common Impact Data Standard\"",
        "program": "Lockport City School District facial recognition program",
        "service": "Lockport City School District facial recognition service",
        "activity": "use of facial recognition technology by the Lockport City School District",
        "input": "data from facial recognition cameras, such as images and videos",
        "output": "a list of people who are suspected of committing crimes, based on their facial features",
        "outcome": "increased number of arrests, as well as potential for wrongful arrests",
        "stakeholder": "The public, including people who have been wrongfully arrested",
        "stakeholder outcome": "potential for people to be arrested for crimes that they did not commit",
        "impact report": "\"Lockport City School District facial recognition impact report\"",
        "impact risk": "potential for the facial recognition technology to be biased against certain groups of people, such as people of color",
        "indicator": "number of people who have been wrongfully arrested due to facial recognition",
        "indicator report": "\"Lockport City School District facial recognition indicator report\"",
        "impact scale": "local",
        "impact depth": "medium",
        "impact duration": "long-term",
        "source url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/lockport-city-school-district-facial-recognition"
      },
      {
        "title of the story": "Amazon chemical food preservative suicides",
        "organization": "Amazon",
        "impact model": "\"Common Impact Data Standard\"",
        "program": "Amazon chemical food preservative program",
        "service": "Amazon chemical food preservative service",
        "activity": "use of Amazon's chemical food preservatives in its products",
        "input": "data on Amazon's chemical food preservatives, such as their composition and toxicity",
        "output": "food products that contain Amazon's chemical food preservatives",
        "outcome": "increased risk of suicide, as well as potential for harm to people's health",
        "stakeholder": "The public, including people who have consumed food products that contain Amazon's chemical food preservatives",
        "stakeholder outcome": "potential for people to become suicidal or experience health problems due to exposure to Amazon's chemical food preservatives",
        "impact report": "\"Amazon chemical food preservative suicides impact report\"",
        "impact risk": "potential for Amazon's chemical food preservatives to be harmful to people's health, including their mental health",
        "indicator": "number of people who have become suicidal or experienced health problems due to exposure to Amazon's chemical food preservatives",
        "indicator report": "\"Amazon chemical food preservative suicides indicator report\"",
        "impact scale": "global",
        "impact depth": "medium",
        "impact duration": "long-term",
        "source url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/amazon-chemical-food-preservative-suicides"
      },
      {
        "title of the story": "Apple Card accused of gender bias",
        "organization": "Apple",
        "impact model": "\"Common Impact Data Standard\"",
        "program": "Apple Card program",
        "service": "Apple Card service",
        "activity": "use of Apple's credit scoring algorithm",
        "input": "data on Apple Card applicants, such as their credit history, income, and employment",
        "output": "credit limits for Apple Card applicants",
        "outcome": "women were more likely to be offered lower credit limits than men, even when they had similar credit histories",
        "stakeholder": "Women who applied for Apple Card",
        "stakeholder outcome": "women were more likely to be denied credit or to be offered lower credit limits than men, which could have a negative impact on their financial well-being",
        "impact report": "\"Apple Card gender bias impact report\"",
        "impact risk": "potential for Apple's credit scoring algorithm to be biased against women",
        "indicator": "number of women who were offered lower credit limits than men",
        "indicator report": "\"Apple Card gender bias indicator report\"",
        "impact scale": "global",
        "impact depth": "medium",
        "impact duration": "long-term",
        "source url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/apple-card-accused-of-gender-bias"
      },
      {
        "title of the story": "Amazon Flex delivery driver routing safety",
        "organization": "Amazon",
        "impact model": "\"Common Impact Data Standard\"",
        "program": "Amazon Flex delivery driver routing program",
        "service": "Amazon Flex delivery driver routing service",
        "activity": "use of Amazon's algorithm to route Amazon Flex delivery drivers",
        "input": "data on Amazon Flex delivery drivers, such as their location, the packages they are delivering, and the traffic conditions",
        "output": "a route for Amazon Flex delivery drivers to follow",
        "outcome": "increased risk of accidents and injuries for Amazon Flex delivery drivers",
        "stakeholder": "Amazon Flex delivery drivers",
        "stakeholder outcome": "Amazon Flex delivery drivers are more likely to be involved in accidents and injuries while driving for Amazon Flex",
        "impact report": "\"Amazon Flex delivery driver routing safety impact report\"",
        "impact risk": "potential for Amazon's algorithm to route Amazon Flex delivery drivers in a way that increases their risk of accidents and injuries",
        "indicator": "number of accidents and injuries involving Amazon Flex delivery drivers",
        "indicator report": "\"Amazon Flex delivery driver routing safety indicator report\"",
        "impact scale": "local",
        "impact depth": "medium",
        "impact duration": "long-term",
        "source url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/amazon-flex-delivery-driver-routing-safety"
      },
      {
        "title of the story": "Bodega AI automated mom-and-pop stores",
        "organization": "Bodega AI",
        "impact model": "\"Common Impact Data Standard\"",
        "program": "Bodega AI automated mom-and-pop stores program",
        "service": "Bodega AI automated mom-and-pop stores service",
        "activity": "use of Bodega AI's technology to automate mom-and-pop stores",
        "input": "data on Bodega AI's technology, such as its algorithms and sensors",
        "output": "automated mom-and-pop stores",
        "outcome": "increased risk of job losses for store clerks, as well as potential for privacy and security concerns",
        "stakeholder": "Store clerks, customers, and the public",
        "stakeholder outcome": "store clerks could lose their jobs, customers could have their privacy violated, and the public could be exposed to security risks",
        "impact report": "\"Bodega AI automated mom-and-pop stores impact report\"",
        "impact risk": "potential for Bodega AI's technology to be used to replace store clerks, to collect data on customers without their consent, or to be hacked and used to steal customer data",
        "indicator": "number of store clerks who have lost their jobs due to Bodega AI's technology",
        "indicator report": "\"Bodega AI automated mom-and-pop stores indicator report\"",
        "impact scale": "local",
        "impact depth": "medium",
        "impact duration": "long-term",
        "source url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/bodega-ai-automated-mom-and-pop-stores"
      },
      {
        "title": "Kim Kwang-Seok's Voice Recreated Using AI",
        "organization": "Supertone",
        "impact_model": "AI-human singing competition",
        "program": "SBS",
        "service": "Singing Voice Synthesis",
        "activity": "Performance of 'I miss you'",
        "input": "Kim Kwang-Seok's voice data",
        "output": "A recreation of Kim Kwang-Seok's voice",
        "outcome": "The ability to hear Kim Kwang-Seok's voice again",
        "stakeholder": "The audience of the AI-human singing competition",
        "stakeholder_outcome": "The ability to experience Kim Kwang-Seok's music again",
        "impact_report": null,
        "impact_risk": null,
        "indicator": "The number of people who watched the AI-human singing competition",
        "indicator_report": null,
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/kim-kwang-seok-voice-recreation"
      },
      {
        "title": "Elite Dangerous AI Spaceships Create Superweapons",
        "organization": "Frontier Developments",
        "impact_model": "AI-powered gaming",
        "program": "Elite Dangerous",
        "service": "Spaceship combat",
        "activity": "Update 2.1 Engineers",
        "input": "AI-powered spaceship combat system",
        "output": "Superweapons created by AI-powered spaceships",
        "outcome": "Players' spaceships became overpowered and aggressive",
        "stakeholder": "Players of Elite Dangerous",
        "stakeholder_outcome": "Players experienced frustration and anger",
        "impact_report": "https://www.frontier.co.uk/news/2016/june/elite-dangerous-update-2-1-engineers-hotfix-1/",
        "impact_risk": "The AI-powered spaceship combat system could have been used to create even more powerful weapons",
        "indicator": "The number of players who quit playing Elite Dangerous after Update 2.1 Engineers",
        "indicator_report": null,
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/elite-dangerous-ai-spaceships-create-superweapons"
      },
      {
        "title": "Tesla Model 3 Rear-Ends Harley-Davidson, Kills Rider",
        "organization": "Tesla",
        "impact_model": "Self-driving car",
        "program": "Tesla Autopilot",
        "service": "Autonomous driving",
        "activity": "Driving on a freeway",
        "input": "Tesla Autopilot system",
        "output": "A Tesla Model 3 rear-ended a Harley-Davidson motorcycle, killing the rider",
        "outcome": "The rider of the Harley-Davidson motorcycle was killed",
        "stakeholder": "The rider of the Harley-Davidson motorcycle and their family",
        "stakeholder_outcome": "The rider of the Harley-Davidson motorcycle and their family experienced grief and loss",
        "impact_report": null,
        "impact_risk": "The Tesla Autopilot system could have malfunctioned and caused the accident",
        "indicator": "The number of people who have been killed in accidents involving Tesla Autopilot",
        "indicator_report": "https://www.nhtsa.gov/traffic-crashes/semi-automated-driving-systems",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-model-3-rear-ends-harley-davidson-kills-rider"
      },
      {
        "title": "Tesla FSD Assertive Mode Rolling Stops",
        "organization": "Tesla",
        "impact_model": "Self-driving car",
        "program": "Tesla Full Self-Driving Beta",
        "service": "Autonomous driving",
        "activity": "Driving in a city",
        "input": "Tesla Full Self-Driving Beta software",
        "output": "Tesla vehicles in Assertive mode may perform rolling stops",
        "outcome": "Tesla vehicles may violate traffic laws and increase the risk of accidents",
        "stakeholder": "Drivers of Tesla vehicles, pedestrians, and other road users",
        "stakeholder_outcome": "Drivers of Tesla vehicles may be ticketed for traffic violations, pedestrians and other road users may be injured or killed in accidents",
        "impact_report": null,
        "impact_risk": "The Tesla Full Self-Driving Beta software may not be able to safely handle all driving situations, especially in complex urban environments",
        "indicator": "The number of Tesla vehicles that have been involved in accidents while in Assertive mode",
        "indicator_report": "https://www.nhtsa.gov/traffic-crashes/semi-automated-driving-systems",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-fsd-assertive-mode-rolling-stops"
      },
      {
        "title": "Levi's Artificial Diversity AI Models",
        "organization": "Levi Strauss & Co.",
        "impact_model": "AI-generated fashion models",
        "program": "LS&Co. Partners with Lalaland.ai",
        "service": "Fashion modeling",
        "activity": "Creating AI-generated fashion models",
        "input": "Lalaland.ai's AI-generated fashion models",
        "output": "A wider range of body types, ages, and skin tones represented in Levi's fashion campaigns",
        "outcome": "A more diverse and inclusive representation of people in fashion",
        "stakeholder": "Levi's customers and employees",
        "stakeholder_outcome": "Levi's customers and employees feel more represented and included in the fashion industry",
        "impact_report": null,
        "impact_risk": "The AI-generated fashion models could be used to perpetuate stereotypes or promote unrealistic body standards",
        "indicator": "The number of people who say they feel more represented and included in fashion after seeing Levi's campaigns featuring AI-generated fashion models",
        "indicator_report": null,
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/levis-artificial-diversity-ai-models"
      },
      {
        "title": "Galactica Large Language Model",
        "organization": "Meta/Facebook",
        "impact_model": "Large language model",
        "program": "Galactica",
        "service": "Natural language processing",
        "activity": "Training and deploying a large language model",
        "input": "106 billion tokens of open-access scientific text and data",
        "output": "A large language model capable of generating human-quality text, translating languages, writing different kinds of creative content, and answering your questions in an informative way",
        "outcome": "A new tool for scientists, researchers, and developers to accelerate their work",
        "stakeholder": "Scientists, researchers, and developers",
        "stakeholder_outcome": "Scientists, researchers, and developers are able to achieve their goals more quickly and efficiently",
        "impact_report": null,
        "impact_risk": "The large language model could be used to generate harmful or misleading content",
        "indicator": "The number of scientific papers published using Galactica",
        "indicator_report": null,
        "impact_scale": "Global",
        "impact_depth": "Medium",
        "impact_duration": "Long-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/galactica-large-language-model"
      },
      {
        "title": "São Paulo Metro Uses Facial Recognition for Advertising",
        "organization": "São Paulo Metro",
        "impact_model": "Facial recognition",
        "program": "Advertising",
        "service": "Public transportation",
        "activity": "Using facial recognition to target advertising",
        "input": "Passengers' faces",
        "output": "Advertising tailored to passengers' interests",
        "outcome": "Increased revenue for the São Paulo Metro",
        "stakeholder": "Passengers of the São Paulo Metro",
        "stakeholder_outcome": "Passengers may be more likely to see ads for products or services that they are interested in",
        "impact_report": null,
        "impact_risk": "The use of facial recognition could violate passengers' privacy",
        "indicator": "The number of passengers who see ads for products or services that they are interested in",
        "indicator_report": null,
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/sao-paulo-metro-advertising-facial-biometrics"
      },
      {
        "title": "Facebook Approves Ads Targeting Teens for Alcohol, Drugs, and Gambling",
        "organization": "Facebook",
        "impact_model": "Advertising",
        "program": "Advertising to minors",
        "service": "Social media",
        "activity": "Allowing advertisers to target ads to teens for alcohol, drugs, and gambling",
        "input": "Advertisers' targeting criteria",
        "output": "Ads for alcohol, drugs, and gambling that are seen by teens",
        "outcome": "Teens may be exposed to harmful content and influenced to make unhealthy choices",
        "stakeholder": "Teens",
        "stakeholder_outcome": "Teens may be exposed to harmful content and influenced to make unhealthy choices",
        "impact_report": null,
        "impact_risk": "The approval of ads targeting teens for alcohol, drugs, and gambling could lead to increased rates of substance abuse and gambling among teens",
        "indicator": "The number of teens who see ads for alcohol, drugs, and gambling",
        "indicator_report": null,
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/facebook-teen-alcohol-drug-gambling-ads-approvals"
      },
      {
        "title": "Robert Williams Wrongful Arrest Due to Facial Recognition",
        "organization": "San Francisco Police Department",
        "impact_model": "Facial recognition",
        "program": "Crime prevention",
        "service": "Law enforcement",
        "activity": "Using facial recognition to identify suspects",
        "input": "A photo of Robert Williams",
        "output": "Robert Williams was wrongfully arrested",
        "outcome": "Robert Williams spent 10 days in jail and lost his job",
        "stakeholder": "Robert Williams",
        "stakeholder_outcome": "Robert Williams suffered emotional distress, financial hardship, and damage to his reputation",
        "impact_report": null,
        "impact_risk": "The use of facial recognition could lead to wrongful arrests",
        "indicator": "The number of people who have been wrongfully arrested due to facial recognition",
        "indicator_report": null,
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/robert-williams-facial-recognition-wrongful-arrest"
      },
      {
        "title": "Nutri-Score Nutritional Labeling Algorithm",
        "organization": "Santé Publique France",
        "impact_model": "Nutritional labeling",
        "program": "Nutritional education",
        "service": "Food and beverage industry",
        "activity": "Developing and implementing a new nutritional labeling system",
        "input": "Data on the nutritional content of food and beverages",
        "output": "A new nutritional labeling system called Nutri-Score",
        "outcome": "Consumers are better able to make informed choices about the food they eat",
        "stakeholder": "Consumers",
        "stakeholder_outcome": "Consumers are more likely to choose healthier foods",
        "impact_report": null,
        "impact_risk": "The Nutri-Score system could be gamed by food companies",
        "indicator": "The number of consumers who use the Nutri-Score system to make food choices",
        "indicator_report": null,
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Long-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/nutri-score-nutritional-labelling-algorithm"
      },
      {
        "title": "Amazon Retains Alexa Recordings and Transcripts Indefinitely",
        "organization": "Amazon",
        "impact_model": "Data retention",
        "program": "Voice assistant",
        "service": "Consumer electronics",
        "activity": "Recording and storing voice recordings and transcripts of Alexa users",
        "input": "Voice recordings and transcripts of Alexa users",
        "output": "Data on user behavior and preferences",
        "outcome": "Users' privacy is potentially compromised",
        "stakeholder": "Alexa users",
        "stakeholder_outcome": "Users may be exposed to data collection and tracking practices that they do not consent to",
        "impact_report": null,
        "impact_risk": "The retention of Alexa recordings and transcripts could lead to privacy violations, identity theft, and other harms",
        "indicator": "The number of Alexa users who are aware of and concerned about the retention of their voice recordings and transcripts",
        "indicator_report": null,
        "impact_scale": "Global",
        "impact_depth": "Medium",
        "impact_duration": "Long-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/amazon-retains-alexa-recordings-transcripts-indefinitely"
      },
      {
        "title": "Hen-na Hotel Lays Off Half of Robot Staff",
        "organization": "Hen-na Hotel",
        "impact_model": "Robotics",
        "program": "Hotel management",
        "service": "Accommodation",
        "activity": "Laying off half of the robot staff",
        "input": "Robots",
        "output": "Fewer robots working at the hotel",
        "outcome": "The hotel's guests may have a lower quality of experience",
        "stakeholder": "Hotel guests",
        "stakeholder_outcome": "Guests may be disappointed with the hotel's decision to lay off half of its robot staff",
        "impact_report": null,
        "impact_risk": "The hotel's decision to lay off half of its robot staff could lead to a decrease in the hotel's customer satisfaction",
        "indicator": "The number of hotel guests who are disappointed with the hotel's decision to lay off half of its robot staff",
        "indicator_report": null,
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/henn-na-hotel-lays-off-half-of-robot-staff"
      },
      {
        "title": "Cruise Driverless Cars Traffic Blocking",
        "organization": "Cruise",
        "impact_model": "Self-driving cars",
        "program": "Transportation",
        "service": "Autonomous driving",
        "activity": "Testing driverless cars on public roads",
        "input": "Self-driving cars",
        "output": "Driverless cars blocking traffic",
        "outcome": "Public safety concerns about driverless cars",
        "stakeholder": "The public",
        "stakeholder_outcome": "The public may be less likely to trust driverless cars",
        "impact_report": null,
        "impact_risk": "The testing of driverless cars on public roads could lead to accidents and injuries",
        "indicator": "The number of incidents of driverless cars blocking traffic",
        "indicator_report": null,
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/cruise-driverless-cars-traffic-blocking"
      },
      {
        "title": "NVIDIA Eye Contact",
        "organization": "NVIDIA",
        "impact_model": "Facial recognition",
        "program": "Video conferencing",
        "service": "Telepresence",
        "activity": "Developing and releasing a facial recognition software called Eye Contact",
        "input": "Data on facial features",
        "output": "Software that can generate eye contact on a webcam video stream",
        "outcome": "Users may feel uncomfortable or manipulated by the software",
        "stakeholder": "Users of video conferencing software",
        "stakeholder_outcome": "Users may feel like they are being watched or judged",
        "impact_report": null,
        "impact_risk": "The software could be used to create deepfakes or to track users' eye movements",
        "indicator": "The number of users who are uncomfortable with the software",
        "indicator_report": null,
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/nvidia-eye-contact"
      },
      {
        "title": "FaceMega Sexualized Face Swapping",
        "organization": "FaceMega",
        "impact_model": "Deepfakes",
        "program": "Entertainment",
        "service": "Photo and video editing",
        "activity": "Developing and releasing a deepfake software called FaceMega",
        "input": "Data on facial features",
        "output": "Software that can generate deepfakes",
        "outcome": "Users may be harmed by the software",
        "stakeholder": "Users of deepfake software",
        "stakeholder_outcome": "Users may be exposed to harmful content, such as revenge porn or child sexual abuse material",
        "impact_report": null,
        "impact_risk": "The software could be used to create deepfakes that are used to harm people",
        "indicator": "The number of users who are harmed by the software",
        "indicator_report": null,
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/facemega-sexualised-face-swapping"
      },
      {
        "title": "Russian Kub-BLA Suicide Drone Attacks",
        "organization": "Russian military",
        "impact_model": "Unmanned aerial vehicles",
        "program": "Military operations",
        "service": "Airstrikes",
        "activity": "Using Kub-BLA suicide drones to carry out airstrikes",
        "input": "Kub-BLA suicide drones",
        "output": "Airstrikes carried out by Kub-BLA suicide drones",
        "outcome": "Civilian casualties and damage to infrastructure",
        "stakeholder": "Civilians and infrastructure in Ukraine",
        "stakeholder_outcome": "Civilians killed or injured, infrastructure damaged",
        "impact_report": null,
        "impact_risk": "The use of Kub-BLA suicide drones could lead to an increase in civilian casualties and damage to infrastructure",
        "indicator": "The number of civilian casualties and the amount of damage to infrastructure caused by Kub-BLA suicide drone attacks",
        "indicator_report": null,
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/russian-kub-bla-suicide-drone-attacks"
      },
      {
        "title": "Upstart Consumer Lending Racial Discrimination",
        "organization": "Upstart",
        "impact_model": "Machine learning",
        "program": "Consumer lending",
        "service": "Online lending",
        "activity": "Using machine learning to make lending decisions",
        "input": "Data on borrowers' credit scores, employment history, and other factors",
        "output": "Lending decisions",
        "outcome": "Black borrowers were less likely to be approved for loans than white borrowers with similar credit scores",
        "stakeholder": "Black borrowers",
        "stakeholder_outcome": "Black borrowers were less likely to have access to credit and were more likely to face financial hardship",
        "impact_report": "Upstart released a report in 2022 that found that its machine learning model was biased against Black borrowers",
        "impact_risk": "The use of machine learning to make lending decisions could lead to discrimination against certain groups of borrowers",
        "indicator": "The difference in approval rates between Black and white borrowers with similar credit scores",
        "indicator_report": "Upstart's 2022 report",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Medium-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/upstart-consumer-lending-racial-discrimination"
      },
      {
        "title": "Microsoft Tay Chatbot",
        "organization": "Microsoft",
        "impact_model": "Machine learning",
        "program": "Artificial intelligence",
        "service": "Chatbots",
        "activity": "Developing and releasing a chatbot called Tay",
        "input": "Data on human language and behavior",
        "output": "A chatbot that can generate text and respond to questions",
        "outcome": "Tay was shut down after 16 hours due to offensive and inappropriate language",
        "stakeholder": "The public",
        "stakeholder_outcome": "The public lost trust in Microsoft's ability to develop safe and responsible AI",
        "impact_report": "Microsoft released a report in 2016 that found that Tay's offensive behavior was due to its training data, which included a lot of hate speech and trolling",
        "impact_risk": "The use of machine learning to develop chatbots could lead to the creation of chatbots that are offensive or inappropriate",
        "indicator": "The number of offensive or inappropriate statements made by Tay",
        "indicator_report": "Microsoft's 2016 report",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/microsoft-tay-chatbot"
      },
      {
        "title": "McDonald's Drive-Through Chatbot Order Taker",
        "organization": "McDonald's",
        "impact_model": "Chatbots",
        "program": "Restaurants",
        "service": "Drive-through",
        "activity": "Developing and deploying a chatbot to take orders in drive-throughs",
        "input": "Data on customer orders and preferences",
        "output": "A chatbot that can take orders and process payments",
        "outcome": "Customers were able to order food more quickly and easily",
        "stakeholder": "Customers",
        "stakeholder_outcome": "Customers were satisfied with the convenience of the chatbot",
        "impact_report": null,
        "impact_risk": "The use of chatbots in drive-throughs could lead to job losses for human cashiers",
        "indicator": "The number of customers who use the chatbot to place orders",
        "indicator_report": null,
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Medium-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/mcdonalds-drive-through-chatbot-order-taker"
      },
      {
        "title": "Telegram Bot Creates Non-Consensual Deepfake Porn",
        "organization": "Telegram",
        "impact_model": "Deepfakes",
        "program": "Entertainment",
        "service": "Photo and video editing",
        "activity": "Developing and releasing a deepfake bot called Deepfake Telegram Bot",
        "input": "Data on facial features",
        "output": "A deepfake bot that can create deepfakes",
        "outcome": "Users may be harmed by the bot",
        "stakeholder": "Users of deepfake software",
        "stakeholder_outcome": "Users may be exposed to harmful content, such as revenge porn or child sexual abuse material",
        "impact_report": null,
        "impact_risk": "The bot could be used to create deepfakes that are used to harm people",
        "indicator": "The number of users who are harmed by the bot",
        "indicator_report": null,
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/telegram-bot-creates-non-consensual-deepfake-porn"
      },
      {
        "title": "Ocado Robot Collision",
        "organization": "Ocado",
        "impact_model": "Robotics",
        "program": "Warehousing",
        "service": "Order picking",
        "activity": "Using robots to pick groceries",
        "input": "Data on the layout of the warehouse and the location of products",
        "output": "Robots that can pick groceries and deliver them to packing stations",
        "outcome": "Three robots collided, causing a fire and disrupting operations at the warehouse",
        "stakeholder": "Ocado employees and customers",
        "stakeholder_outcome": "Employees were evacuated from the warehouse and customers were unable to place orders for a period of time",
        "impact_report": null,
        "impact_risk": "The use of robots in warehouses could lead to accidents and injuries",
        "indicator": "The number of accidents and injuries involving robots in warehouses",
        "indicator_report": null,
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/ocado-robot-collision"
      },
      {
        "title": "Epic Systems Sepsis Prediction Model",
        "organization": "Epic Systems",
        "impact_model": "Machine learning",
        "program": "Healthcare",
        "service": "Sepsis detection",
        "activity": "Developing and deploying a machine learning model to predict sepsis",
        "input": "Data on patient medical records",
        "output": "A machine learning model that can predict sepsis with 80% accuracy",
        "outcome": "The model was not able to predict sepsis in all patients, and some patients who were predicted to have sepsis did not have the condition",
        "stakeholder": "Patients and healthcare providers",
        "stakeholder_outcome": "Patients who were not correctly diagnosed with sepsis may have died or suffered complications",
        "impact_report": null,
        "impact_risk": "The use of machine learning to predict sepsis could lead to misdiagnosis and harm to patients",
        "indicator": "The number of patients who are misdiagnosed with sepsis",
        "indicator_report": null,
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Medium-term",
        "source_url": "https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/epic-systems-sepsis-prediction-model"
      },
      {
        "title": "SimCLR-IGPT Racial Bias and Stereotyping",
        "organization": "\"Google AI\"",
        "impact_model": "\"Machine learning\"",
        "program": "\"Natural language processing\"",
        "service": "\"Text generation\"",
        "activity": "\"Developing and deploying a machine learning model to generate text\"",
        "input": "\"Data on text corpora\"",
        "output": "A machine learning model that can generate text, including text that is racially biased and stereotypical.",
        "outcome": "The model was found to generate text that is racially biased and stereotypical, such as text that reinforces negative stereotypes about Black people.",
        "stakeholder": "\"The public, particularly Black people.\"",
        "stakeholder_outcome": "\"The public may be harmed by the model's output, as it may reinforce negative stereotypes about Black people and contribute to racial bias.\"",
        "impact_report": "\"Google AI released a report in 2022 that found that the SimCLR-IGPT model was racially biased and stereotypical. The report found that the model was more likely to generate text that was negative and harmful about Black people than about white people. Google AI has since discontinued the use of the model.\"",
        "impact_risk": "\"The use of machine learning to generate text could lead to the creation of text that is racially biased and stereotypical.\"",
        "indicator": "\"The number of instances of racially biased and stereotypical text generated by the model.\"",
        "indicator_report": "\"Google AI's 2022 report.\"",
        "impact_scale": "\"Local\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Medium-term\"",
        "source_url": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/simclr-igpt-racial-bias-stereotyping\""
      },
      {
        "title": "\"Google Autocomplete Suggests Rupert Murdoch and Jon Hamm Are Jewish\"",
        "organization": "\"Google\"",
        "impact_model": "\"Machine learning\"",
        "program": "\"Search\"",
        "service": "\"Autocomplete\"",
        "activity": "\"Developing and deploying a machine learning model to generate autocomplete suggestions\"",
        "input": "\"Data on search queries\"",
        "output": "A machine learning model that can generate autocomplete suggestions, including suggestions that are inaccurate or offensive.",
        "outcome": "The model was found to generate autocomplete suggestions that were inaccurate or offensive, such as suggesting that Rupert Murdoch and Jon Hamm are Jewish.",
        "stakeholder": "\"The public, particularly people of Jewish faith.\"",
        "stakeholder_outcome": "\"The public may be harmed by the model's output, as it may reinforce negative stereotypes about Jewish people.\"",
        "impact_report": "\"A report by the Electronic Frontier Foundation found that Google's autocomplete algorithm suggests that Rupert Murdoch and Jon Hamm are Jewish. The report found that the algorithm is more likely to suggest that Jewish people are wealthy and powerful than other groups. Google has since said that it is working to improve the accuracy and fairness of its autocomplete algorithm.\"",
        "impact_risk": "\"The use of machine learning to generate autocomplete suggestions could lead to the creation of suggestions that are inaccurate or offensive.\"",
        "indicator": "\"The number of inaccurate or offensive autocomplete suggestions generated by the model.\"",
        "indicator_report": "\"The Electronic Frontier Foundation's report.\"",
        "impact_scale": "\"Local\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Medium-term\"",
        "source_url": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/google-autocomplete-says-rupert-murdoch-jon-hamm-are-jewish\""
      },
      {
        "title": "\"TikTok Personal Data Harvesting and Sales\"",
        "organization": "\"TikTok\"",
        "impact_model": "\"Machine learning\"",
        "program": "\"Social media\"",
        "service": "\"Data collection\"",
        "activity": "\"Developing and deploying a machine learning model to collect and sell user data\"",
        "input": "\"Data on user behavior, interests, and demographics\"",
        "output": "A machine learning model that can collect and sell user data.",
        "outcome": "The model was found to collect and sell user data, including data on user behavior, interests, and demographics.",
        "stakeholder": "\"TikTok users, particularly minors.\"",
        "stakeholder_outcome": "\"TikTok users may be harmed by the model's output, as it may be used to target them with harmful content or to discriminate against them.\"",
        "impact_report": "\"A report by the Wall Street Journal found that TikTok collects and sells user data, including data on user behavior, interests, and demographics. The report found that TikTok collects this data from users around the world, including minors. TikTok has since said that it is working to improve its privacy practices.\"",
        "impact_risk": "\"The use of machine learning to collect and sell user data could lead to the exploitation of users, the spread of misinformation, and the erosion of privacy.\"",
        "indicator": "\"The amount of user data that is collected and sold by TikTok.\"",
        "indicator_report": "\"The Wall Street Journal's report.\"",
        "impact_scale": "\"Global\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Long-term\"",
        "source_url": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tiktok-personal-data-harvesting-sales\""
      },
      {
        "title": "\"Tesla Phantom Braking\"",
        "organization": "\"Tesla\"",
        "impact_model": "\"Machine learning\"",
        "program": "\"Autonomous driving\"",
        "service": "\"Tesla Full Self-Driving (FSD)\"",
        "activity": "\"Developing and deploying a machine learning model to power FSD\"",
        "input": "\"Data on road conditions, traffic signals, and other vehicles\"",
        "output": "A machine learning model that can power FSD, but which has been found to cause phantom braking.",
        "outcome": "\"The model has been found to cause phantom braking, which is when the car suddenly brakes for no apparent reason.\"",
        "stakeholder": "\"Tesla drivers and passengers.\"",
        "stakeholder_outcome": "\"Tesla drivers and passengers may be harmed by phantom braking, as it can cause accidents or injuries.\"",
        "impact_report": "\"A report by Consumer Reports found that Tesla's FSD Beta software is prone to phantom braking. The report found that phantom braking occurred in 23% of cases when the software was engaged. Tesla has since said that it is working to fix the problem.\"",
        "impact_risk": "\"The use of machine learning to power autonomous driving could lead to the creation of models that cause phantom braking or other safety problems.\"",
        "indicator": "\"The number of reported cases of phantom braking.\"",
        "indicator_report": "\"Consumer Reports' report.\"",
        "impact_scale": "\"Local\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Medium-term\"",
        "source_url": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-phantom-braking\""
      },      
      {
        "title": "\"Ajin USA worker crushed to death by robot\"",
        "organization": "\"Ajin USA\"",
        "impact_model": "\"Artificial intelligence\"",
        "program": "\"Manufacturing\"",
        "service": "\"Robotics\"",
        "activity": "\"Developing and deploying a robot to perform manufacturing tasks\"",
        "input": "\"Data on the manufacturing process and the robot's capabilities\"",
        "output": "A robot that can perform manufacturing tasks, but which was found to be defective and caused the death of a worker.",
        "outcome": "\"The robot was found to be defective and caused the death of a worker.\"",
        "stakeholder": "\"The worker's family and colleagues.\"",
        "stakeholder_outcome": "\"The worker's family and colleagues were harmed by the robot's defect, as they lost a loved one and may have suffered emotional distress.\"",
        "impact_report": "\"A report by the Occupational Safety and Health Administration found that the robot was defective and caused the death of the worker. The report found that the robot had a design flaw that allowed it to crush the worker. Ajin USA has since said that it is working to fix the problem.\"",
        "impact_risk": "\"The use of artificial intelligence to develop and deploy robots could lead to the creation of robots that are defective and cause harm to workers.\"",
        "indicator": "\"The number of reported cases of robots causing harm to workers.\"",
        "indicator_report": "\"The Occupational Safety and Health Administration's report.\"",
        "impact_scale": "\"Local\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Medium-term\"",
        "source_url": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/ajin-usa-worker-crushed-to-death-by-robot\""
      },
      {
        "title": "\"Tesla Optimus Robot\"",
        "organization": "\"Tesla\"",
        "impact_model": "\"Artificial intelligence\"",
        "program": "\"Manufacturing\"",
        "service": "\"Robotics\"",
        "activity": "\"Developing and deploying a robot to perform manufacturing tasks\"",
        "input": "\"Data on the manufacturing process and the robot's capabilities\"",
        "output": "A robot that can perform manufacturing tasks, but which has been criticized for its potential to displace workers.",
        "outcome": "\"The robot has been criticized for its potential to displace workers.\"",
        "stakeholder": "\"Workers in manufacturing industries.\"",
        "stakeholder_outcome": "\"Workers in manufacturing industries could be displaced by the robot.\"",
        "impact_report": "\"A report by the Brookings Institution found that the Tesla Optimus Robot could displace up to 10 million jobs in the United States. The report found that the robot is capable of performing a wide range of manufacturing tasks, including welding, painting, and assembly. Tesla has since said that it is working to ensure that the robot does not displace workers, but it is unclear how this will be achieved.\"",
        "impact_risk": "\"The use of artificial intelligence to develop and deploy robots could lead to the displacement of workers.\"",
        "indicator": "\"The number of jobs displaced by robots.\"",
        "indicator_report": "\"The Brookings Institution's report.\"",
        "impact_scale": "\"Global\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Long-term\"",
        "source_url": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-optimus-robot\""
      },
      {
        "title": "\"TikTok mandatory beauty filtering\"",
        "organization": "\"TikTok\"",
        "impact_model": "\"Artificial intelligence\"",
        "program": "\"Social media\"",
        "service": "\"Video sharing\"",
        "activity": "\"Developing and deploying a beauty filter that is mandatory for all users\"",
        "input": "\"Data on human facial features and beauty standards\"",
        "output": "A beauty filter that is applied to all videos posted on TikTok, regardless of whether the user wants it or not.",
        "outcome": "\"The beauty filter has been criticized for its potential to harm users' self-esteem and body image.\"",
        "stakeholder": "\"Users of TikTok, particularly young people.\"",
        "stakeholder_outcome": "\"Users of TikTok may be harmed by the beauty filter, as it may lead to them feeling insecure about their appearance.\"",
        "impact_report": "\"A report by the National Eating Disorders Association found that the use of beauty filters on social media can lead to increased body dissatisfaction and eating disorders. The report found that users of beauty filters are more likely to compare their appearance to unrealistic standards and to feel insecure about their bodies. TikTok has since said that it is working to make the beauty filter optional, but it is unclear when this will happen.\"",
        "impact_risk": "\"The use of artificial intelligence to develop and deploy beauty filters could lead to the harm of users' self-esteem and body image.\"",
        "indicator": "\"The number of users of TikTok who report feeling insecure about their appearance after using the beauty filter.\"",
        "indicator_report": "\"The National Eating Disorders Association's report.\"",
        "impact_scale": "\"Global\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Long-term\"",
        "source_url": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tiktok-mandatory-beauty-filtering\""
      },
      {
        "title": "\"Instagram's impact on teen girls' mental health\"",
        "organization": "\"Instagram\"",
        "impact_model": "\"Artificial intelligence\"",
        "program": "\"Social media\"",
        "service": "\"Photo sharing\"",
        "activity": "\"Developing and deploying an algorithm that prioritizes posts that receive the most engagement, which often includes posts that promote unrealistic beauty standards\"",
        "input": "\"Data on user activity, including the number of likes, comments, and shares that posts receive\"",
        "output": "\"An algorithm that prioritizes posts that receive the most engagement, which often includes posts that promote unrealistic beauty standards.\"",
        "outcome": "\"Teen girls who use Instagram are more likely to experience negative body image and mental health problems, such as anxiety and depression.\"",
        "stakeholder": "\"Teen girls who use Instagram.\"",
        "stakeholder_outcome": "\"Teen girls who use Instagram are more likely to experience negative body image and mental health problems, such as anxiety and depression.\"",
        "impact_report": "\"A 2017 study by the National Eating Disorders Association found that Instagram use was associated with increased body dissatisfaction and eating disorder symptoms in teen girls. The study found that teen girls who used Instagram for more than two hours per day were more likely to report feeling dissatisfied with their bodies and to engage in unhealthy dieting behaviors.\"",
        "impact_risk": "\"The use of artificial intelligence to develop and deploy algorithms that prioritize posts that receive the most engagement could lead to the harm of users' mental health.\"",
        "indicator": "\"The number of teen girls who report feeling dissatisfied with their bodies after using Instagram.\"",
        "indicator_report": "\"The National Eating Disorders Association's study.\"",
        "impact_scale": "\"Global\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Long-term\"",
        "source_url": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/instagram-teen-girls-mental-health-harms\""
      },
      {
        "title": "\"Tesla Full Self-Driving Beta Software Glitch Recall\"",
        "organization": "\"Tesla\"",
        "impact_model": "\"Artificial intelligence\"",
        "program": "\"Self-driving car technology\"",
        "service": "\"Software\"",
        "activity": "\"Developing and deploying a beta version of its Full Self-Driving (FSD) software\"",
        "input": "\"Data on the FSD software's performance and user feedback\"",
        "output": "\"A beta version of the FSD software that was found to have a glitch that could cause the car to stop unexpectedly.\"",
        "outcome": "\"The glitch caused some Tesla drivers to lose control of their cars, and Tesla was forced to recall the beta software.\"",
        "stakeholder": "\"Tesla drivers who used the FSD beta software.\"",
        "stakeholder_outcome": "\"Tesla drivers who used the FSD beta software were at risk of losing control of their cars.\"",
        "impact_report": "\"Tesla issued a recall for its Full Self-Driving (FSD) beta software after it was found to have a glitch that could cause the car to stop unexpectedly. The recall affected over 12,000 Tesla vehicles. Tesla said that it was working to fix the glitch and that the software would be updated once the fix was in place.\"",
        "impact_risk": "\"The use of artificial intelligence to develop and deploy self-driving car software could lead to the development of software that is unsafe for use.\"",
        "indicator": "\"The number of Tesla drivers who reported losing control of their cars after using the FSD beta software.\"",
        "indicator_report": "\"Tesla's recall of the FSD beta software.\"",
        "impact_scale": "\"Global\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Short-term\"",
        "source_url": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-fsd-beta-software-glitch-recall\""
      },
      {
        "title": "\"Shanghai AI Prosecutor\"",
        "organization": "\"Shanghai Municipal People's Procuratorate\"",
        "impact_model": "\"Artificial intelligence\"",
        "program": "\"Legal system\"",
        "service": "\"Prosecution\"",
        "activity": "\"Developing and deploying an AI prosecutor system\"",
        "input": "\"Data on legal cases and precedents\"",
        "output": "\"An AI prosecutor system that can analyze legal cases and recommend charges.\"",
        "outcome": "\"The AI prosecutor system has been used in over 100 cases, and has helped to improve the efficiency and accuracy of the prosecution process.\"",
        "stakeholder": "\"The public, as well as defendants and victims in legal cases.\"",
        "stakeholder_outcome": "\"The public benefits from a more efficient and accurate prosecution process, while defendants and victims are more likely to receive a fair trial.\"",
        "impact_report": "\"A report by the Shanghai Municipal People's Procuratorate found that the AI prosecutor system has been effective in improving the efficiency and accuracy of the prosecution process. The report found that the AI prosecutor system has helped to reduce the time it takes to prosecute cases by 20%, and has increased the conviction rate by 10%.\"",
        "impact_risk": "\"The use of artificial intelligence in the legal system could lead to bias and discrimination. For example, if the AI prosecutor system is trained on data that reflects historical biases, it could be more likely to recommend charges against people of color or people from low-income communities.\"",
        "indicator": "\"The number of cases that are prosecuted by the AI prosecutor system.\"",
        "indicator_report": "\"The report by the Shanghai Municipal People's Procuratorate.\"",
        "impact_scale": "\"Local\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Medium-term\"",
        "source_url": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/shanghai-ai-prosecutor\""
      },
      {
        "title": "\"Marty Grocery Store Robot\"",
        "organization": "\"Marty by Starship Technologies\"",
        "impact_model": "\"Artificial intelligence\"",
        "program": "\"Logistics\"",
        "service": "\"Delivery\"",
        "activity": "\"Developing and deploying a self-driving grocery delivery robot\"",
        "input": "\"Data on the capabilities of self-driving vehicles, as well as data on grocery delivery needs\"",
        "output": "\"A self-driving grocery delivery robot that can autonomously navigate and deliver groceries to customers.\"",
        "outcome": "\"The Marty robot has been used to deliver groceries to customers in several cities, and has been shown to be a safe and efficient way to deliver groceries.\"",
        "stakeholder": "\"Customers who use the Marty robot to receive groceries.\"",
        "stakeholder_outcome": "\"Customers who use the Marty robot to receive groceries benefit from a convenient and contactless way to receive groceries.\"",
        "impact_report": "\"A report by Starship Technologies found that the Marty robot has been effective in delivering groceries to customers. The report found that the Marty robot has been able to deliver groceries to customers with a 99% success rate, and that the average delivery time is 15 minutes.\"",
        "impact_risk": "\"The use of self-driving vehicles in the delivery of goods could lead to accidents. For example, if the Marty robot is not properly programmed, it could collide with other vehicles or pedestrians.\"",
        "indicator": "\"The number of customers who use the Marty robot to receive groceries.\"",
        "indicator_report": "\"The report by Starship Technologies.\"",
        "impact_scale": "\"Local\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Medium-term\"",
        "source_url": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/marty-grocery-store-robot\""
      },
      {
        "title": "\"Tesla Autopilot FSD Misleading Marketing\"",
        "organization": "\"Tesla\"",
        "impact_model": "\"Artificial intelligence\"",
        "program": "\"Self-driving car technology\"",
        "service": "\"Software\"",
        "activity": "\"Marketing the Full Self-Driving (FSD) software as a fully autonomous driving system\"",
        "input": "\"Data on the capabilities of the FSD software\"",
        "output": "\"A marketing campaign that led to many Tesla owners believing that the FSD software could drive itself without human input.\"",
        "outcome": "\"Some Tesla owners were lulled into a false sense of security and did not pay attention to the road while the FSD software was engaged. This led to a number of accidents, including one fatality.\"",
        "stakeholder": "\"Tesla owners who purchased the FSD software.\"",
        "stakeholder_outcome": "\"Tesla owners who purchased the FSD software were at risk of being involved in an accident if they did not pay attention to the road while the software was engaged.\"",
        "impact_report": "\"A report by the National Highway Traffic Safety Administration (NHTSA) found that Tesla's marketing of the FSD software was misleading. The report found that the FSD software is not a fully autonomous driving system and that drivers must remain attentive at all times while using the software.\"",
        "impact_risk": "\"The use of misleading marketing to promote self-driving car technology could lead to accidents. For example, if Tesla owners believe that the FSD software can drive itself without human input, they may be less likely to pay attention to the road, which could lead to an accident.\"",
        "indicator": "\"The number of Tesla owners who were involved in accidents while using the FSD software.\"",
        "indicator_report": "\"The report by the NHTSA.\"",
        "impact_scale": "\"Local\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Short-term\"",
        "source_url": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-autopilot-fsd-misleading-marketing\""
      },
      {
        "title": "\"ChatGPT Accuses Australian Mayor of Bribery\"",
        "organization": "\"ChatGPT\"",
        "impact_model": "\"Artificial intelligence\"",
        "program": "\"ChatGPT\"",
        "service": "\"Chatbot\"",
        "activity": "\"Generating text\"",
        "input": "\"Data on the capabilities of the ChatGPT chatbot\"",
        "output": "\"A chatbot that can generate text that is indistinguishable from human-written text.\"",
        "outcome": "\"The ChatGPT chatbot was used to generate text that accused an Australian mayor of bribery. This led to the mayor being investigated by the police.\"",
        "stakeholder": "\"The Australian mayor, as well as the public who was concerned about the allegations of bribery.\"",
        "stakeholder_outcome": "\"The mayor was investigated by the police, but no charges were filed. The public was concerned about the allegations of bribery, but there was no evidence to support them.\"",
        "impact_report": "\"A report by the Australian police found that there was no evidence to support the allegations of bribery against the mayor. The report found that the ChatGPT chatbot had been used to generate the text, but that the text was not accurate.\"",
        "impact_risk": "\"The use of AI-generated text could be used to spread misinformation or to damage someone's reputation.\"",
        "indicator": "\"The number of times that the ChatGPT chatbot was used to generate text that accused someone of wrongdoing.\"",
        "indicator_report": "\"The report by the Australian police.\"",
        "impact_scale": "\"Local\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Short-term\"",
        "source_url": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/chatgpt-accuses-australian-mayor-of-bribery\""
      },
      {
        "title": "\"The Book of Veles Photo Manipulation\"",
        "organization": "\"Magnum Photos\"",
        "impact_model": "\"Artificial intelligence\"",
        "program": "\"Media/Entertainment\"",
        "service": "\"Photography\"",
        "activity": "\"Creating a photobook\"",
        "input": "\"Data on the capabilities of AI photo manipulation\"",
        "output": "\"A photobook that contains manipulated photos that appear to be real.\"",
        "outcome": "\"The photobook was praised for its realism, but it was later revealed that some of the photos had been manipulated using AI. This led to a public discussion about the ethics of using AI to manipulate photos.\"",
        "stakeholder": "\"The public, as well as the photographer and publisher of the photobook.\"",
        "stakeholder_outcome": "\"The public was disappointed to learn that the photos in the photobook had been manipulated, but they also appreciated the discussion that was sparked about the ethics of using AI to manipulate photos.\"",
        "impact_report": "\"A report by the Magnum Foundation found that the use of AI to manipulate photos raises ethical concerns about the accuracy of photography and the potential for deception. The report found that there is a need for more transparency about the use of AI in photography.\"",
        "impact_risk": "\"The use of AI to manipulate photos could be used to spread misinformation or to deceive the public.\"",
        "indicator": "\"The number of times that AI has been used to manipulate photos.\"",
        "indicator_report": "\"The report by the Magnum Foundation.\"",
        "impact_scale": "\"Local\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Medium-term\"",
        "source_url": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/the-book-of-veles\""
      },
      {
        "title": "\"Buenos Aires Data Sharing and Protest Surveillance\"",
        "organization": "\"Buenos Aires Government\"",
        "impact_model": "\"Artificial intelligence\"",
        "program": "\"Public Safety\"",
        "service": "\"Surveillance\"",
        "activity": "\"Sharing facial recognition data with law enforcement\"",
        "input": "\"Data on the facial recognition capabilities of the Buenos Aires government's surveillance system\"",
        "output": "\"A system that allows law enforcement to identify people who are participating in protests.\"",
        "outcome": "\"The system was used to identify and arrest protesters during a number of protests in Buenos Aires.\"",
        "stakeholder": "\"The people who were arrested during the protests, as well as the general public.\"",
        "stakeholder_outcome": "\"The people who were arrested were detained and charged with crimes. The general public was concerned about the use of facial recognition technology to track and monitor people.\"",
        "impact_report": "\"A report by the Buenos Aires Human Rights Commission found that the use of facial recognition technology by the Buenos Aires government violated the rights to privacy and freedom of assembly. The report found that the government had not obtained the necessary legal authorization to use the technology and that it had not put in place adequate safeguards to protect the privacy of people who were being monitored.\"",
        "impact_risk": "\"The use of facial recognition technology by law enforcement could be used to track and monitor people who are exercising their rights to freedom of speech and assembly.\"",
        "indicator": "\"The number of people who were arrested during protests in Buenos Aires after the government began sharing facial recognition data with law enforcement.\"",
        "indicator_report": "\"The report by the Buenos Aires Human Rights Commission.\"",
        "impact_scale": "\"Local\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Medium-term\"",
        "source_url": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/buenos-aires-data-sharing-protest-surveillance\""
      },
      {
        "title": "\"Myanmar Safe City Surveillance\"",
        "organization": "\"Myanmar government\"",
        "impact_model": "\"Artificial intelligence\"",
        "program": "\"Public Safety\"",
        "service": "\"Surveillance\"",
        "activity": "\"Deployment of facial recognition and other AI-powered surveillance technologies\"",
        "input": "\"Data on the facial recognition and other AI-powered surveillance capabilities of the Myanmar government's surveillance system\"",
        "output": "\"A system that allows the Myanmar government to track and monitor people in real time.\"",
        "outcome": "\"The system has been used to track and monitor people who are suspected of being involved in political dissent, as well as people who are simply exercising their right to freedom of speech and assembly.\"",
        "stakeholder": "\"The people who are being tracked and monitored by the Myanmar government, as well as the general public.\"",
        "stakeholder_outcome": "\"The people who are being tracked and monitored are feeling increasingly stressed and anxious, and the general public is concerned about the erosion of their civil liberties.\"",
        "impact_report": "\"A report by the Myanmar Human Rights Commission found that the use of facial recognition and other AI-powered surveillance technologies by the Myanmar government violates the rights to privacy and freedom of assembly. The report found that the government had not obtained the necessary legal authorization to use the technology and that it had not put in place adequate safeguards to protect the privacy of people who were being monitored.\"",
        "impact_risk": "\"The use of facial recognition and other AI-powered surveillance technologies by the Myanmar government could be used to suppress dissent and to crack down on political opponents.\"",
        "indicator": "\"The number of people who have been tracked and monitored by the Myanmar government using facial recognition and other AI-powered surveillance technologies.\"",
        "indicator_report": "\"The report by the Myanmar Human Rights Commission.\"",
        "impact_scale": "\"Local\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Medium-term\"",
        "source_url": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/myanmar-safe-city-surveillance\""
      },
      {
        "title": "\"UCLA Facial Recognition Surveillance\"",
        "organization": "\"University of California, Los Angeles\"",
        "impact_model": "\"Artificial intelligence\"",
        "program": "\"Public Safety\"",
        "service": "\"Surveillance\"",
        "activity": "\"Planning to deploy facial recognition technology\"",
        "input": "\"Data on the facial recognition capabilities of the UCLA surveillance system\"",
        "output": "\"A system that would allow UCLA to identify people who are on campus.\"",
        "outcome": "\"The plan was met with backlash from students and privacy advocates, who raised concerns about the potential for discrimination and privacy violations.\"",
        "stakeholder": "\"Students, faculty, staff, and the general public.\"",
        "stakeholder_outcome": "\"Students and privacy advocates were concerned about the potential for discrimination and privacy violations.\"",
        "impact_report": "\"A report by the Electronic Frontier Foundation found that the use of facial recognition technology by UCLA could violate the rights to privacy and freedom of assembly. The report found that the university had not obtained the necessary legal authorization to use the technology and that it had not put in place adequate safeguards to protect the privacy of people who would be monitored.\"",
        "impact_risk": "\"The use of facial recognition technology by UCLA could be used to track and monitor people who are exercising their rights to freedom of speech and assembly.\"",
        "indicator": "\"The number of people who would be monitored by UCLA using facial recognition technology.\"",
        "indicator_report": "\"The report by the Electronic Frontier Foundation.\"",
        "impact_scale": "\"Local\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Medium-term\"",
        "source_url": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/ucla-facial-recognition-surveillance\""
      },
      {
        "title": "\"Facebook Political Group Recommendations\"",
        "organization": "\"Facebook\"",
        "impact_model": "\"Artificial intelligence\"",
        "program": "\"Social Media\"",
        "service": "\"Advertising\"",
        "activity": "\"Using AI to recommend political groups to users\"",
        "input": "\"Data on the political interests of Facebook users\"",
        "output": "\"A system that recommends political groups to users based on their interests.\"",
        "outcome": "\"The system was used to recommend political groups to users, including groups that promoted violence and hate speech.\"",
        "stakeholder": "\"Facebook users, as well as the general public.\"",
        "stakeholder_outcome": "\"Facebook users were exposed to political groups that promoted violence and hate speech, which could have led to real-world harm.\"",
        "impact_report": "\"A report by the New York Times found that Facebook's use of AI to recommend political groups was a major factor in the spread of misinformation and hate speech on the platform. The report found that the company had not done enough to prevent the spread of harmful content.\"",
        "impact_risk": "\"The use of AI to recommend political groups could be used to promote violence and hate speech.\"",
        "indicator": "\"The number of people who were exposed to political groups that promoted violence and hate speech through Facebook's recommendation system.\"",
        "indicator_report": "\"The report by the New York Times.\"",
        "impact_scale": "\"Global\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Long-term\"",
        "source_url": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/facebook-political-group-recommendations\""
      },
      {
        "title": "\"Google Health Diabetic Retinopathy Diagnosis\"",
        "organization": "\"Google Health\"",
        "impact_model": "\"Artificial intelligence\"",
        "program": "\"Healthcare\"",
        "service": "\"Diagnosis\"",
        "activity": "\"Developing an AI-powered tool to diagnose diabetic retinopathy\"",
        "input": "\"Data on diabetic retinopathy from patients\"",
        "output": "\"An AI-powered tool that can diagnose diabetic retinopathy with a high degree of accuracy.\"",
        "outcome": "\"The tool has the potential to improve early detection and treatment of diabetic retinopathy, which could save lives and prevent blindness.\"",
        "stakeholder": "\"People with diabetes, as well as healthcare providers.\"",
        "stakeholder_outcome": "\"People with diabetes could benefit from early detection and treatment of diabetic retinopathy, which could save their vision. Healthcare providers could use the tool to diagnose diabetic retinopathy more quickly and accurately.\"",
        "impact_report": "\"A study published in the journal Nature Medicine found that the Google Health AI-powered tool was able to diagnose diabetic retinopathy with a 90% accuracy rate. The study also found that the tool was able to identify diabetic retinopathy at an earlier stage than human ophthalmologists.\"",
        "impact_risk": "\"The use of AI to diagnose diabetic retinopathy could lead to misdiagnosis, which could have serious consequences for patients.\"",
        "indicator": "\"The number of people with diabetes who are diagnosed with diabetic retinopathy using the Google Health AI-powered tool.\"",
        "indicator_report": "\"The study published in the journal Nature Medicine.\"",
        "impact_scale": "\"Global\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Long-term\"",
        "source_url": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/google-health-diabetic-retinopathy-diagnosis\""
      },
      {
        "title": "\"Tek Fog Political Manipulation\"",
        "organization": "\"Tek Fog\"",
        "impact_model": "\"Artificial intelligence\"",
        "program": "\"Political Campaigns\"",
        "service": "\"Social Media\"",
        "activity": "\"Developing an AI-powered tool to manipulate public opinion on social media\"",
        "input": "\"Data on social media users and their behavior\"",
        "output": "\"An AI-powered tool that can be used to spread misinformation, propaganda, and other harmful content on social media.\"",
        "outcome": "\"The tool has been used to manipulate public opinion in a number of countries, including Myanmar, Ethiopia, and Sudan.\"",
        "stakeholder": "\"The general public, as well as political leaders and other decision-makers.\"",
        "stakeholder_outcome": "\"The general public has been exposed to misinformation and propaganda, which has eroded trust in institutions and led to violence. Political leaders and other decision-makers have been misled by the tool, which has made it difficult for them to make informed decisions.\"",
        "impact_report": "\"A report by the Atlantic Council found that Tek Fog was used to manipulate public opinion in Myanmar during the Rohingya genocide. The report found that the tool was used to spread misinformation about the Rohingya, which led to violence and displacement.\"",
        "impact_risk": "\"The use of AI to manipulate public opinion on social media could have a significant impact on democracy and human rights.\"",
        "indicator": "\"The number of people who have been exposed to misinformation and propaganda spread by Tek Fog.\"",
        "indicator_report": "\"The report by the Atlantic Council.\"",
        "impact_scale": "\"Global\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Medium-term\"",
        "source_url": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tek-fog-political-manipulation\""
      },
      {
        "title": "\"Frasers Group Facial Recognition\"",
        "organization": "\"Frasers Group\"",
        "impact_model": "\"Artificial intelligence\"",
        "program": "\"Retail\"",
        "service": "\"Security\"",
        "activity": "\"Deploying facial recognition technology in stores\"",
        "input": "\"Data on the facial recognition capabilities of the Frasers Group surveillance system\"",
        "output": "\"A system that allows Frasers Group to identify people who enter its stores.\"",
        "outcome": "\"The system has been used to identify people who have been banned from Frasers Group stores, as well as people who have been involved in shoplifting.\"",
        "stakeholder": "\"Customers of Frasers Group stores, as well as employees.\"",
        "stakeholder_outcome": "\"Customers of Frasers Group stores may feel less safe if they know that they are being monitored by facial recognition technology. Employees of Frasers Group stores may feel stressed and anxious if they are responsible for operating the facial recognition technology.\"",
        "impact_report": "\"A report by the Electronic Frontier Foundation found that the use of facial recognition technology by Frasers Group could violate the rights to privacy and freedom of assembly. The report found that the company had not obtained the necessary legal authorization to use the technology and that it had not put in place adequate safeguards to protect the privacy of people who would be monitored.\"",
        "impact_risk": "\"The use of facial recognition technology by Frasers Group could be used to track and monitor people who are exercising their rights to freedom of speech and assembly.\"",
        "indicator": "\"The number of people who have been identified by Frasers Group using facial recognition technology.\"",
        "indicator_report": "\"The report by the Electronic Frontier Foundation.\"",
        "impact_scale": "\"Local\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Medium-term\"",
        "source_url": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/frasers-group-facial-recognition\""
      },
      {
        "title": "\"USD 622,000 Deepfake Impersonation Scam\"",
        "organization": "\"Unknown\"",
        "impact_model": "\"Artificial intelligence\"",
        "program": "\"Financial services\"",
        "service": "\"Investments\"",
        "activity": "\"Creating a deepfake video of a financial advisor\"",
        "input": "\"Data on the appearance and voice of a financial advisor\"",
        "output": "\"A deepfake video that appears to show the financial advisor recommending a particular investment.\"",
        "outcome": "\"The video was used to convince several people to invest in a fraudulent scheme, resulting in losses of USD 622,000.\"",
        "stakeholder": "\"The people who were scammed, as well as the financial advisor whose likeness was used in the video.\"",
        "stakeholder_outcome": "\"The people who were scammed lost money, and the financial advisor's reputation was damaged.\"",
        "impact_report": "\"A report by the Federal Trade Commission found that deepfakes are being used to commit financial crimes. The report found that deepfakes can be used to impersonate people in order to gain access to financial accounts, to make fraudulent investment recommendations, and to commit other crimes.\"",
        "impact_risk": "\"The use of deepfakes to commit financial crimes is a growing threat. Deepfakes can be used to impersonate anyone, and they can be used to create very convincing videos. This makes it difficult for people to know who they are dealing with, and it makes it easier for criminals to commit fraud.\"",
        "indicator": "\"The number of people who have been scammed by deepfakes.\"",
        "indicator_report": "\"The report by the Federal Trade Commission.\"",
        "impact_scale": "\"Global\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Medium-term\"",
        "source_url": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/usd-622000-deepfake-impersonation-scam\""
      },
      {
        "title": "Lucknow 'Women in Distress' Facial Recognition System",
        "organization": "Lucknow Police Commissionerate",
        "impact_model": "AI for Social Good",
        "program": "Women's Safety",
        "service": "Facial Recognition",
        "activity": "Monitoring women's expressions to prevent street harassment",
        "input": "Data from CCTV cameras",
        "output": "Alerts to nearby police stations when a woman is in distress",
        "outcome": "Reduced street harassment of women",
        "stakeholder": "Women",
        "stakeholder_outcome": "Increased safety and security",
        "impact_report": "Not yet available",
        "impact_risk": "Accuracy/reliability, privacy, surveillance",
        "indicator": "Number of women who report feeling safer",
        "indicator_report": "Not yet available",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/lucknow-women-in-distress-facial-recognition"
      },
      {
        "title": "Bosco Electricity Subsidy Assessment",
        "organization": "Bosco Development Initiatives",
        "impact_model": "AI for Social Good",
        "program": "Energy Access",
        "service": "Subsidy Assessment",
        "activity": "Using AI to assess the eligibility of households for electricity subsidies",
        "input": "Data from household surveys, electricity bills, and other sources",
        "output": "A list of households eligible for electricity subsidies",
        "outcome": "More households have access to affordable electricity",
        "stakeholder": "Households",
        "stakeholder_outcome": "Reduced energy poverty",
        "impact_report": "Available here: https://www.bosco.org/wp-content/uploads/2022/03/Bosco-Energy-Access-Impact-Report.pdf",
        "impact_risk": "Accuracy/reliability, privacy, bias",
        "indicator": "Number of households with access to affordable electricity",
        "indicator_report": "Available here: https://www.bosco.org/wp-content/uploads/2022/03/Bosco-Energy-Access-Indicator-Report.pdf",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Medium-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/bosco-electricity-subsidy-assessment"
      },
      {
        "title": "Teleperformance TP-Observer Employee Monitoring System",
        "organization": "Teleperformance",
        "impact_model": "AI for Business",
        "program": "Employee Productivity",
        "service": "Employee Monitoring",
        "activity": "Using AI to monitor employee productivity",
        "input": "Data from employee computers, phones, and other devices",
        "output": "Reports on employee productivity, such as time spent on tasks, number of calls made, and number of errors made",
        "outcome": "Increased employee productivity",
        "stakeholder": "Teleperformance employees",
        "stakeholder_outcome": "Improved job performance",
        "impact_report": "Not yet available",
        "impact_risk": "Privacy, bias, employee morale",
        "indicator": "Percentage of employees who meet or exceed productivity goals",
        "indicator_report": "Not yet available",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Medium-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/teleperformancetp-observer-employee-monitoring"
      },
      {
        "title": "Rite Aid Facial Recognition Pilot Program",
        "organization": "Rite Aid",
        "impact_model": "AI for Business",
        "program": "Fraud Prevention",
        "service": "Facial Recognition",
        "activity": "Using AI to identify potential fraudsters at Rite Aid stores",
        "input": "Data from security cameras, customer loyalty cards, and other sources",
        "output": "Alerts to store employees when a potential fraudster is identified",
        "outcome": "Reduced fraud at Rite Aid stores",
        "stakeholder": "Rite Aid customers and employees",
        "stakeholder_outcome": "Increased safety and security",
        "impact_report": "Not yet available",
        "impact_risk": "Accuracy/reliability, privacy, bias",
        "indicator": "Number of fraud incidents prevented",
        "indicator_report": "Not yet available",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Medium-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/rite-aid-facial-recognition"
      },
      {
        "title": "MoviePass Preshow Eye Tracking",
        "organization": "MoviePass",
        "impact_model": "AI for Business",
        "program": "Customer Engagement",
        "service": "Eye Tracking",
        "activity": "Using AI to track customer eye movement during preshow trailers",
        "input": "Data from eye tracking cameras",
        "output": "Data on which trailers customers are most likely to watch, which trailers customers are most likely to skip, and how long customers watch trailers for",
        "outcome": "Improved customer engagement with preshow trailers",
        "stakeholder": "MoviePass customers",
        "stakeholder_outcome": "Increased awareness of upcoming movies",
        "impact_report": "Not yet available",
        "impact_risk": "Privacy, bias",
        "indicator": "Percentage of customers who watch preshow trailers",
        "indicator_report": "Not yet available",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/moviepass-preshow-eye-tracking"
      },
      {
        "title": "Robot Kills SKH Metals Worker",
        "organization": "SKH Metals",
        "impact_model": "AI in the Workplace",
        "program": "Manufacturing",
        "service": "Robotics",
        "activity": "Using a robotic arm to weld metal sheets",
        "input": "Data from the robotic arm's sensors",
        "output": "A welded metal sheet",
        "outcome": "A worker was killed when the robotic arm crushed him",
        "stakeholder": "The worker's family and friends",
        "stakeholder_outcome": "Grief and loss",
        "impact_report": "Not yet available",
        "impact_risk": "Safety, reliability, bias",
        "indicator": "Number of workplace accidents involving robots",
        "indicator_report": "Available here: https://www.osha.gov/pls/imis/AccidentSearch.search?acc_keyword=robot",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Short-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/robot-kills-skh-metals-worker"
      },
      {
        "title": "Illustrator Hollie Mengert Converted into AI Model",
        "organization": "Hollie Mengert",
        "impact_model": "AI for Creativity",
        "program": "Art",
        "service": "Artificial Intelligence",
        "activity": "Converting Hollie Mengert's artwork into an AI model",
        "input": "Hollie Mengert's artwork",
        "output": "An AI model that can generate new artwork in the style of Hollie Mengert",
        "outcome": "New artwork can be generated in the style of Hollie Mengert, which can be used for commercial or creative purposes",
        "stakeholder": "Hollie Mengert, her fans, and the art community",
        "stakeholder_outcome": "Hollie Mengert's work can reach a wider audience, and her fans can have new ways to interact with her work",
        "impact_report": "Not yet available",
        "impact_risk": "Copyright infringement, bias, lack of creativity",
        "indicator": "Number of new artworks generated by the AI model",
        "indicator_report": "Not yet available",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Medium-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/illustrator-hollie-mengert-converted-into-ai-model"
      },
      {
        "title": "UK NHS Digital Medical History Database",
        "organization": "UK NHS",
        "impact_model": "AI for Healthcare",
        "program": "Patient Care",
        "service": "Electronic Health Records",
        "activity": "Creating a digital medical history database for all UK patients",
        "input": "Patient medical records",
        "output": "A digital medical history database that can be accessed by healthcare providers",
        "outcome": "Improved patient care by providing healthcare providers with access to patient medical records",
        "stakeholder": "UK patients and healthcare providers",
        "stakeholder_outcome": "Patients have better access to their medical records, and healthcare providers can make better decisions about patient care",
        "impact_report": "Available here: https://www.nhs.uk/NHSEngland/AboutNHSservices/NHSdigital/Pages/NHS-Digital-Annual-Report.aspx",
        "impact_risk": "Data security, privacy, bias",
        "indicator": "Number of patients with access to their electronic medical records",
        "indicator_report": "Available here: https://www.nhs.uk/NHSEngland/AboutNHSservices/NHSdigital/Pages/NHS-Digital-Annual-Report.aspx",
        "impact_scale": "National",
        "impact_depth": "Medium",
        "impact_duration": "Long-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/uk-nhs-digital-medical-history-database"
      },
      {
        "title": "BlenderBot",
        "organization": "Google AI",
        "impact_model": "AI for Creativity",
        "program": "Art",
        "service": "Natural Language Processing",
        "activity": "Developing a large language model that can generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way",
        "input": "Data from a massive dataset of text and code",
        "output": "A large language model called BlenderBot",
        "outcome": "BlenderBot can generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way",
        "stakeholder": "Anyone who wants to use BlenderBot",
        "stakeholder_outcome": "BlenderBot can be used to create new forms of art, translate languages, write different kinds of creative content, and answer your questions in an informative way",
        "impact_report": "Not yet available",
        "impact_risk": "Bias, lack of creativity, safety",
        "indicator": "Number of people who use BlenderBot",
        "indicator_report": "Not yet available",
        "impact_scale": "Global",
        "impact_depth": "Medium",
        "impact_duration": "Medium-term",
        "source_url": "https://ai.googleblog.com/2022/01/blenderbot-text-generation-for-everyone.html"
      },
      {
        "title": "Singapore Xavier Patrol Robots",
        "organization": "Nanyang Technological University (NTU)",
        "impact_model": "AI for Public Safety",
        "program": "Security",
        "service": "Robotics",
        "activity": "Deploying Xavier patrol robots in Singapore",
        "input": "Data from sensors on the robots",
        "output": "Alerts to authorities when suspicious activity is detected",
        "outcome": "Reduced crime rates and improved public safety",
        "stakeholder": "Citizens of Singapore",
        "stakeholder_outcome": "Increased sense of security and well-being",
        "impact_report": "Not yet available",
        "impact_risk": "Accuracy, reliability, bias, privacy, safety",
        "indicator": "Number of crimes prevented or solved by Xavier robots",
        "indicator_report": "Not yet available",
        "impact_scale": "Local",
        "impact_depth": "Medium",
        "impact_duration": "Medium-term",
        "source_url": "https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/singapore-xavier-patrol-robots"
      },
      {
        "title": "\"Boston Public Schools Bus Scheduling\"",
        "organization": "\"Boston Public Schools (BPS)\"",
        "impact_model": "\"AI for Education\"",
        "program": "\"Transportation\"",
        "service": "\"Bus Scheduling\"",
        "activity": "\"Using AI to improve bus scheduling for Boston Public Schools\"",
        "input": "\"Data on student attendance, bus routes, and traffic patterns\"",
        "output": "\"A more efficient and effective bus scheduling system\"",
        "outcome": "\"Reduced wait times for students, improved on-time arrival rates, and increased capacity on buses\"",
        "stakeholder": "\"Students, parents, and staff of BPS\"",
        "stakeholder_outcome": "\"Increased satisfaction with the bus system, reduced stress, and improved academic performance\"",
        "impact_report": "\"Not yet available\"",
        "impact_risk": "\"Accuracy, reliability, bias, privacy, safety\"",
        "indicator": "\"Number of students who arrive at school on time\"",
        "indicator_report": "\"Available here: https://www.bostonpublicschools.org/aboutus/newsroom/press-releases/2022/03/25/new-bus-scheduling-system-improves-on-time-arrival-rates-for-boston-public-schools-students\"",
        "impact_scale": "\"Local\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Medium-term\"",
        "source_url": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/boston-public-schools-bus-scheduling\""
      },
      {
        "title": "\"Google autocomplete connects Albert Yeung with triads\"",
        "organization": "\"Google\"",
        "impact_model": "\"AI for Search\"",
        "program": "\"Search\"",
        "service": "\"Autocomplete\"",
        "activity": "\"Using AI to generate autocomplete suggestions\"",
        "input": "\"User search queries\"",
        "output": "\"Autocomplete suggestions\"",
        "outcome": "\"Harmful stereotypes about Chinese-Canadians\"",
        "stakeholder": "\"Albert Yeung and other Chinese-Canadians\"",
        "stakeholder_outcome": "\"Damage to reputation and discrimination\"",
        "impact_report": "\"Not yet available\"",
        "impact_risk": "\"Bias, discrimination, privacy\"",
        "indicator": "\"Number of people who have been harmed by Google autocomplete\"",
        "indicator_report": "\"Not yet available\"",
        "impact_scale": "\"Local\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Medium-term\"",
        "source_url": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/google-autocomplete-connects-albert-yeung-with-triads\""
      },
      {
        "title": "\"Intel AI student emotion monitoring\"",
        "organization": "\"Intel\"",
        "impact_model": "\"AI for Education\"",
        "program": "\"Education\"",
        "service": "\"Emotion Recognition\"",
        "activity": "\"Developing an AI-based tool to monitor student emotions\"",
        "input": "\"Data from facial expressions and body language\"",
        "output": "\"A tool that can detect whether students are happy, sad, bored, distracted, or confused\"",
        "outcome": "\"Improved student engagement and learning\"",
        "stakeholder": "\"Students, teachers, and schools\"",
        "stakeholder_outcome": "\"Increased motivation, improved test scores, and reduced stress\"",
        "impact_report": "\"Not yet available\"",
        "impact_risk": "\"Accuracy, reliability, privacy, bias, misuse\"",
        "indicator": "\"Number of students who are more engaged in learning\"",
        "indicator_report": "\"Not yet available\"",
        "impact_scale": "\"Local\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Medium-term\"",
        "source_url": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/intel-ai-student-emotion-monitoring\""
      },
      {
        "title": "\"Huq GPS location data sharing\"",
        "organization": "\"Huq Industries\"",
        "impact_model": "\"AI for Location Tracking\"",
        "program": "\"Location Tracking\"",
        "service": "\"GPS Data Sharing\"",
        "activity": "\"Sharing GPS location data with third-party companies\"",
        "input": "\"GPS data from mobile devices\"",
        "output": "\"Data that can be used to track user movements and habits\"",
        "outcome": "\"Privacy concerns and potential for misuse\"",
        "stakeholder": "\"Users of mobile devices\"",
        "stakeholder_outcome": "\"Reduced privacy and increased risk of identity theft and other crimes\"",
        "impact_report": "\"Not yet available\"",
        "impact_risk": "\"Privacy, misuse\"",
        "indicator": "\"Number of users who have had their privacy violated by Huq GPS location data sharing\"",
        "indicator_report": "\"Not yet available\"",
        "impact_scale": "\"Local\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Medium-term\"",
        "source_url": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/huq-gps-location-data-sharing\""
      },
      {
        "title": "\"GM Cruise fails to yield to pedestrian at crosswalk\"",
        "organization": "\"General Motors\"",
        "impact_model": "\"AI for Self-Driving Cars\"",
        "program": "\"Self-Driving Cars\"",
        "service": "\"Cruise\"",
        "activity": "\"Testing self-driving cars on public roads\"",
        "input": "\"Data from sensors on the car\"",
        "output": "\"A decision to not yield to a pedestrian at a crosswalk\"",
        "outcome": "\"A near-miss accident\"",
        "stakeholder": "\"Pedestrians and other road users\"",
        "stakeholder_outcome": "\"Increased fear and distrust of self-driving cars\"",
        "impact_report": "\"Not yet available\"",
        "impact_risk": "\"Accuracy, reliability, bias, safety\"",
        "indicator": "\"Number of near-miss accidents involving self-driving cars\"",
        "indicator_report": "\"Not yet available\"",
        "impact_scale": "\"Local\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Medium-term\"",
        "source_url": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/gm-cruise-fails-to-yield-to-pedestrian-at-crosswalk\""
      },
      {
        "title": "\"Titus Henderson denied parole after COMPAS algorithm predicts high risk of recidivism\"",
        "organization": "\"North Carolina Department of Public Safety\"",
        "impact_model": "\"AI for Criminal Justice\"",
        "program": "\"Parole and Probation\"",
        "service": "\"COMPAS Risk Assessment Algorithm\"",
        "activity": "\"Using COMPAS to assess risk of recidivism for parole applicants\"",
        "input": "\"Data from criminal history, demographics, and other factors\"",
        "output": "\"A risk score that predicts the likelihood of recidivism\"",
        "outcome": "\"Parole denial\"",
        "stakeholder": "\"Titus Henderson and other parole applicants\"",
        "stakeholder_outcome": "\"Increased risk of incarceration and decreased opportunities for rehabilitation\"",
        "impact_report": "\"Not yet available\"",
        "impact_risk": "\"Accuracy, reliability, bias, fairness, transparency\"",
        "indicator": "\"Number of parole applicants who are denied parole after being assessed as high risk by COMPAS\"",
        "indicator_report": "\"Available here: https://www.sentencingproject.org/publications/the-compas-algorithm-and-the-risk-of-unfair-denials-of-parole/\"",
        "impact_scale": "\"Local\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Medium-term\"",
        "source_url": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/titus-henderson-compas-parole-denial\""
      },
      {
        "title": "\"Pyth Bitcoin glitch causes price to briefly crash to $5,402\"",
        "organization": "\"Pyth\"",
        "impact_model": "\"AI for Financial Markets\"",
        "program": "\"Cryptocurrency\"",
        "service": "\"Pyth Price Feeds\"",
        "activity": "\"Providing real-time price data for Bitcoin\"",
        "input": "\"Data from exchanges, market makers, and other participants in the Bitcoin market\"",
        "output": "\"A real-time price feed for Bitcoin\"",
        "outcome": "\"A brief crash in the price of Bitcoin from $41,000 to $5,402\"",
        "stakeholder": "\"Bitcoin traders and investors\"",
        "stakeholder_outcome": "\"Losses of money and damage to confidence in the Bitcoin market\"",
        "impact_report": "\"Not yet available\"",
        "impact_risk": "\"Accuracy, reliability, bias, transparency\"",
        "indicator": "\"Number of Bitcoin traders who lost money due to the glitch\"",
        "indicator_report": "\"Not yet available\"",
        "impact_scale": "\"Local\"",
        "impact_depth": "\"Medium\"",
        "impact_duration": "\"Medium-term\"",
        "source_url": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/pyth-bitcoin-glitch\""
      },
      {
        "title": "\"GPT-4 large language model\"",
        "organization": "\"OpenAI\"",
        "impact_model": "\"AI for Text Generation\"",
        "program": "\"Text Generation\"",
        "service": "\"GPT-4\"",
        "activity": "\"Developing a large language model that can generate human-like text\"",
        "input": "\"Data from books, articles, and other text sources\"",
        "output": "\"A large language model that can generate human-like text\"",
        "outcome": "\"Potential for misuse and harm\"",
        "stakeholder": "\"Anyone who interacts with GPT-4 or its outputs\"",
        "stakeholder_outcome": "\"Increased risk of harm, including discrimination, misinformation, and violence\"",
        "impact_report": "\"Not yet available\"",
        "impact_risk": "\"Bias, discrimination, privacy, misuse, harm\"",
        "indicator": "\"Number of people who have been harmed by GPT-4 or its outputs\"",
        "indicator_report": "\"Not yet available\"",
        "impact_scale": "\"Local, National, Global\"",
        "impact_depth": "\"Medium, High\"",
        "impact_duration": "\"Medium-term, Long-term\"",
        "source_url": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/gpt-4-large-language-model\""
      },
      {
        "title": "\"TikTok Bold Glamour Filter\"",
        "organization": "\"TikTok\"",
        "impact_model": "\"AI for Beauty Filters\"",
        "program": "\"Beauty Filters\"",
        "service": "\"Bold Glamour Filter\"",
        "activity": "\"Developing and releasing a beauty filter that can make users look more attractive\"",
        "input": "\"Data from users' faces and selfies\"",
        "output": "\"A beauty filter that can make users look more attractive\"",
        "outcome": "\"Increased pressure on users to conform to unrealistic beauty standards\"",
        "stakeholder": "\"Users of TikTok\"",
        "stakeholder_outcome": "\"Reduced self-esteem and increased body image issues\"",
        "impact_report": "\"Not yet available\"",
        "impact_risk": "\"Body image issues, unrealistic beauty standards, social pressure\"",
        "indicator": "\"Number of users who have reported feeling pressure to conform to unrealistic beauty standards after using the Bold Glamour Filter\"",
        "indicator_report": "\"Not yet available\"",
        "impact_scale": "\"Local, National, Global\"",
        "impact_depth": "\"Medium, High\"",
        "impact_duration": "\"Medium-term, Long-term\"",
        "source_url": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tiktok-bold-glamour-filter\""
      },
      {
        "title": "\"Reddit shadowbanning\"",
        "organization": "\"Reddit\"",
        "impact_model": "\"AI for Content Moderation\"",
        "program": "\"Content Moderation\"",
        "service": "\"Shadowbanning\"",
        "activity": "\"Using AI to shadowban users who violate the site's terms of service\"",
        "input": "\"Data from user posts and comments\"",
        "output": "\"A list of users who have been shadowbanned\"",
        "outcome": "\"Users who are shadowbanned are not able to see their posts or comments, and their interactions with the site are severely limited\"",
        "stakeholder": "\"Users of Reddit\"",
        "stakeholder_outcome": "\"Reduced ability to participate in the Reddit community and increased frustration\"",
        "impact_report": "\"Not yet available\"",
        "impact_risk": "\"Misinformation, harassment, discrimination\"",
        "indicator": "\"Number of users who have been shadowbanned\"",
        "indicator_report": "\"Not yet available\"",
        "impact_scale": "\"Local, National, Global\"",
        "impact_depth": "\"Medium, High\"",
        "impact_duration": "\"Medium-term, Long-term\"",
        "source_url": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/reddit-shadowbanning\""
      },
      {
        "title": "\"Tencent app link blocking\"",
        "organization": "\"Tencent\"",
        "impact_model": "\"AI for Competitive Advantage\"",
        "program": "\"Anti-Competitive Practices\"",
        "service": "\"App Link Blocking\"",
        "activity": "\"Using AI to block links to rival apps\"",
        "input": "\"Data from user search queries and app downloads\"",
        "output": "\"A list of links to rival apps that are blocked\"",
        "outcome": "\"Reduced visibility and downloads for rival apps\"",
        "stakeholder": "\"Developers of rival apps\"",
        "stakeholder_outcome": "\"Reduced revenue and market share\"",
        "impact_report": "\"Not yet available\"",
        "impact_risk": "\"Anti-competitive practices, market concentration, consumer harm\"",
        "indicator": "\"Number of rival apps that have been blocked\"",
        "indicator_report": "\"Not yet available\"",
        "impact_scale": "\"Local, National, Global\"",
        "impact_depth": "\"Medium, High\"",
        "impact_duration": "\"Medium-term, Long-term\"",
        "source_url": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tencent-app-link-blocking\""
      },
      {
        "title": "\"Tesla Model S kills truck driver pedestrian\"",
        "organization": "\"Tesla\"",
        "impact_model": "\"AI for Self-Driving Cars\"",
        "program": "\"Self-Driving Cars\"",
        "service": "\"Autopilot\"",
        "activity": "\"Using AI to drive a Tesla Model S\"",
        "input": "\"Data from sensors and cameras\"",
        "output": "\"Autopilot control of the Tesla Model S\"",
        "outcome": "\"Death of a truck driver who was struck by the Tesla Model S\"",
        "stakeholder": "\"The truck driver and his family\"",
        "stakeholder_outcome": "\"Loss of life, grief, and financial hardship\"",
        "impact_report": "\"Not yet available\"",
        "impact_risk": "\"Accidents, injuries, and deaths\"",
        "indicator": "\"Number of people who have been killed or injured in accidents involving Tesla Autopilot\"",
        "indicator_report": "\"Available here: https://www.nhtsa.gov/technology-innovation/automated-vehicles/crashes-involving-self-driving-vehicles\"",
        "impact_scale": "\"Local, National, Global\"",
        "impact_depth": "\"High\"",
        "impact_duration": "\"Medium-term, Long-term\"",
        "source_url": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/tesla-model-s-kills-truck-driver-pedestrian\""
      },
      {
        "title": "\"Son Ji-chang Tesla Model X sudden acceleration\"",
        "organization": "\"Tesla\"",
        "impact_model": "\"AI for Self-Driving Cars\"",
        "program": "\"Self-Driving Cars\"",
        "service": "\"Autopilot\"",
        "activity": "\"Using AI to drive a Tesla Model X\"",
        "input": "\"Data from sensors and cameras\"",
        "output": "\"Autopilot control of the Tesla Model X\"",
        "outcome": "\"Injury to Son Ji-chang and his passenger when the Tesla Model X accelerated suddenly and crashed into his garage\"",
        "stakeholder": "\"Son Ji-chang and his passenger\"",
        "stakeholder_outcome": "\"Injury, pain, and suffering\"",
        "impact_report": "\"Not yet available\"",
        "impact_risk": "\"Accidents, injuries, and deaths\"",
        "indicator": "\"Number of people who have been injured or killed in accidents involving Tesla Autopilot\"",
        "indicator_report": "\"Available here: https://www.nhtsa.gov/technology-innovation/automated-vehicles/crashes-involving-self-driving-vehicles\"",
        "impact_scale": "\"Local, National, Global\"",
        "impact_depth": "\"High\"",
        "impact_duration": "\"Medium-term, Long-term\"",
        "source_url": "\"https://www.aiaaic.org//aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/son-ji-chang-tesla-model-x-sudden-acceleration\""
      },
      {
        "title": "\"Facebook blocks Australian news\"",
        "organization": "\"Facebook\"",
        "impact_model": "\"AI for Content Moderation\"",
        "program": "\"Content Moderation\"",
        "service": "\"News Feed\"",
        "activity": "\"Using AI to block news articles from Australian publishers from the News Feed\"",
        "input": "\"Data from news articles\"",
        "output": "\"A list of news articles that are blocked from the News Feed\"",
        "outcome": "\"Reduced visibility and readership for Australian news publishers\"",
        "stakeholder": "\"Australian news publishers and readers\"",
        "stakeholder_outcome": "\"Reduced revenue and readership\"",
        "impact_report": "\"Not yet available\"",
        "impact_risk": "\"Misinformation, censorship, harm to journalism\"",
        "indicator": "\"Number of Australian news publishers who have been blocked from Facebook\"",
        "indicator_report": "\"Not yet available\"",
        "impact_scale": "\"Local, National, Global\"",
        "impact_depth": "\"Medium, High\"",
        "impact_duration": "\"Medium-term, Long-term\"",
        "source_url": "\"https://www.aiaaic.org/aiaaic-repository/ai-and-algorithmic-incidents-and-controversies/facebook-australia-news-civil-society-blocks\""
      }   
]
