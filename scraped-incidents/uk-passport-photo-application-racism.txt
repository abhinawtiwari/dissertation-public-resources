- Occurred: April 2021
- Can you improve this page?Share your insights with us
- The UK Home Office stands accused of implicit racism after a Black man was prevented from registering for a UK passport on its website because the software was unable to recognise his skin colour from the grey wall behind him.
- Model and racial justice activist Joris Lechêne took to TikTok to complain that his photo 'was rejected because the artificial intelligence software wasn’t designed with people of my phenotype in mind.'
- The UK's Passport Office had earlier told the New Scientist that an update to the system had been available for more than a year but had not yet been rolled out.
- Furthermore, Home Office documents released under a Freedom of Information (FOI) request show it knew its passport photo system failed to work well for some ethnic minority people but decided to use it anyway.
- Operator: UK Home OfficeDeveloper:
- Country: UK
- Sector: Govt - immigration
- Purpose: Check photograph
- Technology: Facial detection; Facial analysis; Computer vision Issue: Accuracy/reliability; Bias/discrimination - race, ethnicity
- Transparency: Governance; Complaints/appeals
URL: https://www.tiktok.com/@joris_explains/video/6954036663580495109
- Log in to follow creators, like videos, and view comments.
- 37 comments
- You may like

URL: https://www.independent.co.uk/life-style/ai-racist-robots-algorithm-tiktok-b1838521.html
- Please refresh the page or navigate to another page on the site to be automatically logged inPlease refresh your browser to be logged in
- The Independent’s journalism is supported by our readers. When you purchase through links on our site, we may earn commission.
- ‘Automation is never diversity-friendly,’ video’s caption reads
- Find your bookmarks in your Independent Premium section, under my profile
- A TikTok user who educates people about the pervasiveness of racial bias has demonstrated how prejudice appears to be built into artificial intelligence.
- Joris Lechêne, a model and artist from London, recently uploaded a TikTok in which he explained that he tried to upload a passport photo only for it to be denied because the software could not accurately recognise him.
- “Don’t you love it when you train people to spot racist biases for a living and then it happens to you?” Lechêne, who goes by the username @joris_explains, began the video. “In the process of applying for a British passport, I had to upload a photo, so I followed every guideline to the T and submitted this melanated hotness.”
- While he is speaking, Lechêne, who is Black, showed viewers the photo in question that he submitted, which sees him wearing a black shirt and standing in front of a gray wall.
- “Lo and behold, that photo was rejected because the artificial intelligence software wasn’t designed with people of my phenotype in mind,” Lechêne continued. “It tends to get very confused by hairlines that don’t fall along the face and somehow mistake it for the background and it has a habit of thinking that people like me keep our mouths open.”
- According to the screenshot of the rejection shared on the screen, Lechêne’s photo “doesn’t meet all the rules and is unlikely to be suitable for a new passport,” with the government website suggesting that his mouth “may be open” and that it was “difficult” to tell the image and the backdrop apart.
- Automation is never diversity-friendly. ##LearnOnTikTok ##diversity ##technology ##artificialintelligence ##ukcitizen ##bias
- In the video, which has been viewed more than 156,000 times, Lechêne explained that he knew about the racial bias before being subjected to it because he uses similar examples in the prejudice training that he delivers.
- According to Lechêne, his own experience is a reminder that current software is not without prejudice, with the model stating: “This is just a reminder that, if you believe that automation and artificial intelligence can help us build a society without biases, you are terribly mistaken.”
- Rather, a more equitable society is only achievable through “political actions at every level of society,” Lechêne continued, adding: “because robots are just as racist as society is.”
- This is not the first time the subject of racism in AI has been raised, as the topic has come up frequently as more of the world becomes digitised and examples have come to light.
- Previously, the Google Photos app was found to be labelling Black people as gorillas, according to The New York Times, while an Amazon face service had trouble “identifying the sex of female and darker-​skinned faces”.
- “The service mistook women for men 19 per cent of the time and misidentified darker-​skinned women for men 31 per cent of the time. For lighter-​skinned males, the error rate was zero,” The Times states.
- The issue is due to biases in society that then end up ingrained in algorithms and artificial intelligence due to a lack of diverse training.
- “Lack of diversity in the data you work with, that’s exactly what we’re talking about,”Lechêne explained in a follow-up video. “Society is heavily skewed towards whiteness and that creates an unequal system. And that unequal system is carried through the algorithm.”
- Reply to @william.haggerty.phl I would like to apologise for the inconvenience my non-whiteness may have caused you in your work. ##data ##antiracism
- Lechêne’s video prompted numerous dismayed comments on his behalf, with one person writing: “We need more POC in STEM so they can write algorithms that aren’t biased, especially as our society automates processes like these more.”
- Join thought-provoking conversations, follow other Independent readers and see their replies
- TikTok / @joris_explains
- Want to bookmark your favourite articles and stories to read or reference later? Start your Independent Premium subscription today.
- Please refresh the page or navigate to another page on the site to be automatically logged inPlease refresh your browser to be logged in
- Log in
- New to The Independent?
- Or if you would prefer:
- Want an ad-free experience?
- 

URL: https://www.newsweek.com/black-man-tiktok-artificial-intelligence-racism-1587018

URL: https://www.dailydot.com/irl/black-tiktoker-passport-facial-recognition/

URL: https://www.unilad.co.uk/viral/man-proves-artificial-intelligence-is-racist-by-attempting-to-get-passport-picture-approved/
- To make sure you never miss out on your favourite NEW stories, we're happy to send you some reminders
- Click 'OK' then 'Allow' to enable notifications
- Published 12:30, 28 April 2021 BST| Last updated 14:02, 28 April 2021 BST
- A TikTok user has used his experience of trying to apply for a new passport to demonstrate how racial bias can also exist in artificial intelligence (AI).
- Joris Lechêne, a model and artist who also works to educate people on racial bias, explained how his passport photo was rejected by an automated identification software that struggles to distinguish Black faces.
- Sharing a screenshot of the rejected photo, Lechêne wrote, ‘In the process of applying for a British passport, I had to upload a photo, so I followed every guideline to the T and submitted this melanated hotness.’
- 
- ‘Lo and behold, that photo was rejected because the artificial intelligence software wasn’t designed with people of my phenotype in mind,’ he said, explaining, ‘It tends to get very confused by hairlines that don’t fall along the face and somehow mistake it for the background and it has a habit of thinking that people like me keep our mouths open.’
- According to information provided by the software, which is used by the UK government as part of its online passport application process, Lechêne’s photo ‘doesn’t meet all the rules and is unlikely to be suitable for a new passport’. Despite being clearly distinguishable from his grey background, and with his mouth closed, the software explained that the image had been rejected because Lechêne’s ‘mouth may be open’ and it was ‘difficult’ to tell him apart from the backdrop.
- Lechêne went on to say that his experience is just one example of how racial bias is built in to AI and automated software, and goes to show that ‘robots are just as racist as society is’.
- He said:
- This is just a reminder that, if you believe that automation and artificial intelligence can help us build a society without biases, you are terribly mistaken.
- While there is a common conception that automated software is inherently objective, researchers into ethical AI have warned that software in fact incorporates human biases due to the lack of diversity in data and programming. ‘Society is heavily skewed towards whiteness and that creates an unequal system.’ Lechêne explains. ‘And that unequal system is carried through the algorithm.’
- Lechêne’s experience is just one of a host of examples of facial recognition technology in particular struggling to distinguish Black faces. The New York Times has previously reported incidences of Google Photos identifying Black people as ‘gorillas’, while a 2018 ACLU investigation found that a facial recognition program developed by Amazon was more likely to wrongly identify Black people as criminals.
- If you have a story you want to tell, send it to UNILAD via [email protected]
- Topics: Technology, Artificial Intelligence, Facial Recognition, Now, Racism, TikTok
- The Independent
- BLACK MAN USES PASSPORT PHOTO AS EVIDENCE AI IS ‘RACIST’ IN VIRAL TIKTOK

URL: https://futurism.com/the-byte/uk-passport-ai-racist-dark-skin
- The U.K. government uses facial recognition AI to check travelers' photos when they apply for passports. It works just fine for white people, but like so many algorithms out there, it doesn't work well when presented with dark skin.
- Anti-black bias in tech is nothing new, unfortunately: algorithms trained on biased data have often resulted in software that perpetuates prejudice. What's particularly troubling about this passport photo AI is that the British government knew about the problem, according to New Scientist — but decided it was okay to deploy the system anyway.
- Newly-released internal documents revealed that the same racial disparities occurred during testing, resulting in the system telling darker-skinned people that their pictures didn't comply with passport guidelines, New Scientist reports.
- "User research was carried out with a wide range of ethnic groups and did identify that people with very light or very dark skin found it difficult to provide an acceptable passport photograph," read the documents. "However; the overall performance was judged sufficient to deploy."
- If someone applying for a passport is told that their photo isn't acceptable, they can still circumvent the AI system and submit it anyway — but they'll face warnings that it could interfere with their application.
- "Even with the user being able to override the selection, it is still creating a — largely racialized — disparity in experience between users," University of Washington engineer Os Keyes told New Scientist.
- READ MORE: UK launched passport photo checker it knew would fail with dark skin [New Scientist]
- More on facial recognition: Google Contractors Tricked Homeless Black People Into Face Scans
- DISCLAIMER(S)
- Articles may contain affiliate links which enable us to share in the revenue of any purchases made.
- Registration on or use of this site constitutes acceptance of our Terms of Service.
- © Recurrent Ventures Inc, All Rights Reserved.

URL: https://www.newscientist.com/article/2219284-uk-launched-passport-photo-checker-it-knew-would-fail-with-dark-skin/
- Advertisement
- Explore by section
- Explore by subject
- Explore our products and services
- By Adam Vaughan
- 9 October 2019
- Face recognition often struggles to recognise people with certain skin tonesIza habur/Getty Images
- Face recognition often struggles to recognise people with certain skin tones
- Iza habur/Getty Images
- The UK government went ahead with a face-detection system for its passport photo checking service, despite knowing the technology failed to work well for people in some ethnic minorities.
- Face recognition technology has a record of failing to recognise people with certain skin tones. For example, Google had to apologise in 2015 when its photos app labelled a black couple as gorillas.
- Now, documents released by the Home Office this week show it was aware of problems with its website’s passport photo checking service, but decided to use it regardless.
- Advertisement
- “User research was carried out with a wide range of ethnic groups and did identify that people with very light or very dark skin found it difficult to provide an acceptable passport photograph,” the department wrote in a document released in response to a freedom of information (FOI) request. “However; the overall performance was judged sufficient to deploy.”
- Since the service went live in June 2016, some users have had problems with the photo checking service. Joshua Bada, a black sports coach, was told by the system recently that his photo didn’t meet requirements after it mistook his lips for an open mouth.
- Cat Hallam, a black technology officer at Keele University, UK, found the service wrongly suggested her eyes were closed and her mouth was open. “What is very disheartening about all of this is they were aware of it,” Hallam told New Scientist.
- “A person’s race should not be a barrier to using technology for essential public services,” says a spokesperson for the UK’s Equality and Human Rights Commission. “We are disappointed that the government is proceeding with the implementation of this technology despite evidence that it is more difficult for some people to use it based on the colour of their skin.”
- Face detection software is normally trained on thousands of images. One way bias can enter the system is if the training data isn’t large enough or diverse enough to represent the group it will be used on.
- Sam Smith of campaign group MedConfidential, which submitted the FOI request, says: “Clearly, they deployed what they had anyway – they’ve taken the standard as ‘if no one else’s image analysis works for black people, then ours doesn’t have to either’.”
- The Home Office says users can override the photo checker and proceed with their passport application, but observers say that misses the point.
- “Even with the user being able to override the selection, it is still creating a – largely racialised – disparity in experience between users,” says Os Keyes at the University of Washington, Seattle.
- Users may be reluctant to use the override function given that the website warns people they may have a problem with their application if the photo doesn’t meet the rules, says Hallam.
- The government said that to mitigate the issue it would: “continue to conduct user research and usability testing with appropriate participants to ensure that users from different ethnicities can follow the photo guidance and provide a photo that passes the photo checks.”
- A Home Office spokesperson told New Scientist: “We are determined to make the experience of uploading a digital photograph as simple as possible, and will continue working to improve this process for all of our customers.”
- The government promise of “we’ll fix it later” is “a depressingly common response to people pointing out biases in technology”, says Keyes.
- Samir Jeraj at the Race Equality Foundation says: “It’s outrageous. It clearly shows it wasn’t a priority for them that it would work for people with black skin.” Jeraj called on the government to be clearer and more robust about what improvements it will make, and by when. In the meantime, he adds it would not cost the passport office anything to put a note on its website acknowledging the issue.
- Noel Sharkey, a computer scientist at the University of Sheffield, UK, says the case shows the need for new regulation. “It is well known that current face recognition systems are highly inaccurate for people with darker shades of skin. That is bad enough, but then using a technology that the passport office knows to be biased is unacceptable and normalises injustice. We urgently need new regulation to stop any inequalities.”
- Topics:
- Advertisement
- Explore the latest news, articles and features
- Regulars
- Subscriber-only
- Culture
- Subscriber-only
- News
- Subscriber-only
- Features
- Subscriber-only
- Trending New Scientist articles
- 1
- 2
- 3
- 4
- 5
- 6
- 7
- 8
- 9
- 10
- Advertisement
- Download the app

URL: https://thenextweb.com/news/black-man-says-racially-biased-ai-system-rejected-his-passport-photo-facial-recognition-tiktok
- You have been blacklisted, KTHXBAI
- XID: 5762028
- Varnish cache server

URL: https://www.blackenterprise.com/tiktok-model-exposes-how-artificial-intelligence-is-biased-toward-black-people/
- TikTok user and model Joris Lechêne, aka @Joris_Explains, demonstrated how artificial intelligence software is racially biased toward Black people.
- Last Friday, Lechêne, a London influencer, uploaded a TikTok to explain how AI prevented him from registering for a UK passport because the software did not recognize his skin color, The Independent reported.
- Related stories: THE DEBATE OVER VACCINE PASSPORTS, SOME LIKE IT, OTHERS THINK IT’S INVASIVE
- “Don’t you love it when you train people to spot racist biases for a living and then it happens to you?” Lechêne said. “In the process of applying for a British passport, I had to upload a photo, so I followed every guideline to the T and submitted this melanated hotness.”
- The photo Lechêne used showed him posing in a black shirt with a grey background.
- “Lo and behold, that photo was rejected because the artificial intelligence software wasn’t designed with people of my phenotype in mind,” Lechêne continued. “It tends to get very confused by hairlines that don’t fall along the face and somehow mistake it for the background and it has a habit of thinking that people like me keep our mouths open.”
- Lechêne said his photo was rejected on the government website due to either his mouth being open or it was difficult to tell the image and gray wall apart.
- The video has garnered more than 156,000 views.
- The New York Times also did a piece on AI technology going wrong.The Google Photos app labeled Black people as gorillas, while an Amazon face service had trouble “identifying the sex of female and darker-​skinned faces.”
- In addition, ‘the service mistook women for men 19% of the time and misidentified darker-​skinned women for men 31% of the time. For lighter-​skinned males, the error rate was zero,” The Times reported.
- “Lack of diversity in the data you work with, that’s exactly what we’re talking about,” Lechêne explained in a follow-up video. “Society is heavily skewed towards whiteness and that creates an unequal system. And that unequal system is carried through the algorithm.”
- 
- 
- 
- 
- 
- EVENTS
Women of Power Summit
Entrepreneurs Summit
Black Men XCEL
Women of Power TECH
- PODCASTS
SistersInc.
Your Money, Your Life
- LATEST LISTS
BE 100s
B.E. Registry of Corporate Directors
- ABOUT
BLACK ENTERPRISE is the premier business, investing, and wealth-building resource for African Americans. Since 1970, BLACK ENTERPRISE has provided essential business information and advice to professionals, corporate executives, entrepreneurs, and decision makers.
- INFORMATION
Management
Sales Partnership Solutions
Privacy Policy
About
Contact
Subscribe to Newsletter
Masthead
- 

URL: https://www.dailyadvent.com/news/cc7c48203ddde869d5f4c37b4f356a5c-TikTok-Model-Exposes-How-Artificial-Intelligence-is-Biased-Toward-Black-People
- Download Opera News APP
- Free cookies 🍪
- We are always working to improve your experience on our website. Part of this involves using cookies to collect anonymous data for statistics and personalization. We may also use your data to tailor ads for you and our partners will collect data and use cookies for ad personalisation and measurement. Further information on how we and our partners collect and use data can be found in our
            
                Privacy Statement
            
            and
            
                Cookies Policy
            
            , as well as the common used
            
              Ad Technology Providers list
            
            .
- Page Not Found
- Perhaps the page was deleted,the URL is incorrect,or some other error occured.
- © 2021 Opera News. All Rights Reserved.

- Met Police Gangs Violence Matrix
- England footballers' racism Instagram moderation
- Page infoType: IncidentPublished: February 2023
