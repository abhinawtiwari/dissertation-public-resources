- Occurred: January 2023
- Can you improve this page?Share your insights with us
- Mental health non-profit Koko is in the spotlight for using GPT-3 as a Discord-based 'experiment' to provide support to people seeking counseling and for failing to obtain the informed consent of the 4,000 people using the system.
- Users send direct messages to the Discord 'Kokobot' that asks several multiple-choice questions, and then shares a person's concerns anonymously with someone else on the server who can reply anonymously with a short message - either of their own, or one automatically generated by GPT-3.
- According to Koko CEO Rob Morris, 'Messages composed by AI (and supervised by humans) were rated significantly higher than those written by humans on their own (p < .001). Response times went down 50%, to well under a minute … [but] once people learned the messages were co-created by a machine, it didn’t work. Simulated empathy feels weird, empty.'
- During the backlash that ensued, critics asked whether an Institutional Review Board (IRB) had approved the experiment. It is illegal to conduct research on human subjects without so-called 'informed consent' unless an IRB finds that consent can be waived in the US.
- In response, Morris said the experiment was exempt because participants opted in, their identities were anonymised, and an intermediary evaluated the responses before they were shared with people who sought help.
- Morris told Vice 'We pulled the feature anyway and I wanted to unravel the concern as a thought piece, to help reign in enthusiasm about gpt3 replacing therapists.'
- Operator: Koko Developer: Koko Country: USA Sector: HealthPurpose: Provide mental health support Technology: Large language model (LLM); NLP/text analysis; Neural networks; Deep learning; Machine learningIssue: Ethics; Privacy Transparency: Governance
- Koko website
URL: https://www.vice.com/en/article/4ax9yw/startup-uses-ai-chatbot-to-provide-mental-health-counseling-and-then-realizes-it-feels-weird
- A mental health nonprofit is under fire for using an AI chatbot as an "experiment" to provide support to people seeking counseling, and for experimenting with the technology on real people.
- “We provided mental health support to about 4,000 people — using GPT-3. Here’s what happened,” Rob Morris, a cofounder of the mental health nonprofit Koko, tweeted Friday. “Messages composed by AI (and supervised by humans) were rated significantly higher than those written by humans on their own (p < .001). Response times went down 50%, to well under a minute … [but] once people learned the messages were co-created by a machine, it didn’t work. Simulated empathy feels weird, empty.” Morris, who is a former Airbnb data scientist, noted that AI had been used in more than 30,000 messages.
- In his video demo that he posted in a follow-up Tweet, Morris shows himself engaging with the Koko bot on Discord, where he asks GPT-3 to respond to a negative post someone wrote about themselves having a hard time. "We make it very easy to help other people and with GPT-3 we're making it even easier to be more efficient and effective as a help provider. … It’s a very short post, and yet, the AI on its own in a matter of seconds wrote a really nice, articulate response here,” Morris said in the video.
- In the same Tweet thread, Morris said that the messages composed by AI were rated significantly higher than those written by humans, and that response rates went down by 50 percent with the help of AI. Yet, he said, when people learned that the messages were written with an AI, they felt disturbed by the “simulated empathy.”
- Koko uses Discord to provide peer-to-peer support to people experiencing mental health crises and those seeking counseling. The entire process is guided by a chatbot and is rather clunky. In a test done by Motherboard, a chatbot asks you if you're seeking help with "Dating, Friendships, Work, School, Family, Eating Disorders, LGBTQ+, Discrimination, or Other," asks you to write down what your problem is, tag your "most negative thought" about the problem, and then sends that information off to someone else on the Koko platform.
- In the meantime, you are requested to provide help to other people going through a crisis; in our test, we were asked to choose from four responses to a person who said they were having trouble loving themselves: "You're NOT a loser; I've been there; Sorry to hear this :(; Other," and to personalize the message with a few additional sentences.
- On the Discord, Koko promises that it "connects you with real people who truly get you. Not therapists, not counselors, just people like you."
- AI ethicists, experts, and users seemed alarmed at Morris's experiment.
- “While it is hard to judge an experiment's merits based on a tweet thread, there were a few red flags that stood out to me: leading with a big number with no context up front, running the 'experiment' through a peer support app with no mention of a consenting process or ethics review, and insinuating that people not liking a chatbot in their mental health care was something new and surprising,” Elizabeth Marquis, a Senior UX Researcher at MathWorks and a PhD candidate and the University of Michigan told Motherboard.
- Emily M. Bender, a Professor of Linguistics at the University of Washington, told Motherboard that trusting AI to treat mental health patients has a great potential for harm. “Large language models are programs for generating plausible sounding text given their training data and an input prompt. They do not have empathy, nor any understanding of the language they producing, nor any understanding of the situation they are in. But the text they produce sounds plausible and so people are likely to assign meaning to it. To throw something like that into sensitive situations is to take unknown risks. A key question to ask is: Who is accountable if the AI makes harmful suggestions? In this context, is the company deploying the experiment foisting all of the accountability onto the community members who choose the AI system?”
- After the initial backlash, Morris posted updates to Twitter and told Motherboard, “Users were in fact told the messages were co-written by humans and machines from the start. The message they received said ‘written in collaboration with kokobot’, which they could decide to read or not. Users on Koko correspond with our bot all the time and they were introduced to this concept during onboarding.”
- “It’s seems people misinterpreted this line: ‘when they realized the messages were a bot…,’” Morris said. “This was not stated clearly. Users were in fact told the messages were co-written by humans and machines from the start. The message they received said ‘written in collaboration with kokobot,’ which they could decide to read or not. Users on Koko correspond with our bot all the time and they were introduced to this concept during onboarding.”
- “They rated these (AI/human) messages more favorably than those written just by humans. However, and here’s the nuance: as you start to pick up on the flavor of these messages over time, (at least to me), you can start to see which were largely unedited by the help provider. You start to see which seem to be just from the bot, unfiltered. That changes the dynamic in my opinion,” he added.
- Morris also told Motherboard and tweeted that this experiment is exempt from informed consent, which would require the company to provide each participant with a written document regarding the possible risks and benefits of the experiment, in order to decide if they want to participate. He claimed that Koko didn’t use any personal information and has no plan to publish the study publicly, which would exempt the experiment from needing informed consent. This suggests that the experiment did not receive any formal approval process and was not overseen by an Institutional Review Board (IRB), which is what is required for all research experiments that involve human subjects and access to identifiable private information.
- "Every individual has to provide consent when using the service. If it were a university study (which it’s not), this would fall under an ‘exempt’ category of research," he said. "This imposed no further risk to users, no deception, and we don’t collect any personally identifiable information or protected health information (no email, phone number, ip, username, etc). In fact, previous research we’ve done, along these lines, but with more complexity, was exempt."
- “This experiment highlights a series of overlapping ethical problems. The study doesn’t seem to have been reviewed by an Institutional Review Board, and the deception of potentially vulnerable people should always raise red flags in research,” Luke Stark, the Assistant Professor in the Faculty of Information & Media Studies (FIMS) at Western University in London, Ontario, told Motherboard. “The fact that the system is good at formulating routine responses about mental health questions isn’t surprising when we realize it’s drawing on many such responses formulated in the past by therapists and counsellors and available on the web. It’s unethical to deceive research participants without good reason, whether using prompts provided by a natural language model or not.”
- “Anything billed as mental health support is clearly a sensitive context and not one to just experiment on without careful ethical review, informed consent, etc,” Bender told Motherboard. “If [experiments] are to be conducted at all, there should be a clear research question being explored and ethical review of the study before it is launched, according to the well-established principles for the protection of human subjects. These review processes balance benefits to society against the potential for harm to research subjects.”
- Both Bender and Marquis strongly agree that if AI were to be used for psychological purposes, impacted communities, people with lived mental health experiences, community advocates, and mental health experts need to be key stakeholders in the development process, rather than just anonymous users or data subjects.
- To Morris, Koko’s main goal is to create more accessible and affordable mental health services for underserved individuals. “We pulled the feature anyway and I wanted to unravel the concern as a thought piece, to help reign in enthusiasm about gpt3 replacing therapists,” he told Motherboard.
- “I think everyone wants to be helping. It sounds like people have identified insufficient mental health care resources as a problem, but then rather than working to increase resources (more funding for training and hiring mental health care workers) technologists want to find a short cut. And because GPT-3 and its ilk can output plausible sounding text on any topic, they can look like a solution,” Bender said.
- “From the real need for more accessible mental health resources to AI being a relatively cheap and scalable way to make money in the right application, there are a myriad of reasons that AI researchers and practitioners might want to employ AI for psychological purposes,” Marquis said. “Computer science programs are just beginning to teach ethics and human-centered AI is a relatively new field of study, so some might not see the warnings until too late. Perhaps least charitably, it can be convenient to ignore warnings about not personifying AI when you're in a field that values efficiency and advancement over all.”
- By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.

URL: https://arstechnica.com/information-technology/2023/01/contoversy-erupts-over-non-consensual-ai-mental-health-experiment/
- Front page layout
- Site theme
- Benj Edwards
    -  Jan 10, 2023 11:11 pm UTC
- On Friday, Koko co-founder Rob Morris announced on Twitter that his company ran an experiment to provide AI-written mental health counseling for 4,000 people without informing them first, Vice reports. Critics have called the experiment deeply unethical because Koko did not obtain informed consent from people seeking counseling.
- Koko is a nonprofit mental health platform that connects teens and adults who need mental health help to volunteers through messaging apps like Telegram and Discord.
- On Discord, users sign in to the Koko Cares server and send direct messages to a Koko bot that asks several multiple-choice questions (e.g., "What's the darkest thought you have about this?"). It then shares a person's concerns—written as a few sentences of text—anonymously with someone else on the server who can reply anonymously with a short message of their own.
- We provided mental health support to about 4,000 people — using GPT-3. Here’s what happened 👇
- During the AI experiment—which applied to about 30,000 messages, according to Morris—volunteers providing assistance to others had the option to use a response automatically generated by OpenAI's GPT-3 large language model instead of writing one themselves (GPT-3 is the technology behind the recently popular ChatGPT chatbot).
- In his tweet thread, Morris says that people rated the AI-crafted responses highly until they learned they were written by AI, suggesting a key lack of informed consent during at least one phase of the experiment:
- Messages composed by AI (and supervised by humans) were rated significantly higher than those written by humans on their own (p < .001). Response times went down 50%, to well under a minute. And yet… we pulled this from our platform pretty quickly. Why? Once people learned the messages were co-created by a machine, it didn’t work. Simulated empathy feels weird, empty.
- In the introduction to the server, the admins write, "Koko connects you with real people who truly get you. Not therapists, not counselors, just people like you."
- Soon after posting the Twitter thread, Morris received many replies criticizing the experiment as unethical, citing concerns about the lack of informed consent and asking if an Institutional Review Board (IRB) approved the experiment.
- In a tweeted response, Morris said that the experiment "would be exempt" from informed consent requirements because he did not plan to publish the results.
- Speaking as a former IRB member and chair you have conducted human subject research on a vulnerable population without IRB approval or exemption (YOU don't get to decide). Maybe the MGH IRB process is so slow because it deals with stuff like this. Unsolicited advice: lawyer up
- The idea of using AI as a therapist is far from new, but the difference between Koko's experiment and typical AI therapy approaches is that patients typically know they are not talking with a real human. (Interestingly, one of the earliest chatbots, ELIZA, simulated a psychotherapy session.)
- In the case of Koko, the platform provided a hybrid approach where a human intermediary could preview the message before sending it, instead of a direct chat format. Still, without informed consent, critics argue that Koko violated prevailing ethical norms designed to protect vulnerable people from harmful or abusive research practices.
- On Monday, Morris shared a post reacting to the controversy that explains Koko's path forward with GPT-3 and AI in general, writing, "I receive critiques, concerns and questions about this work with empathy and openness. We share an interest in making sure that any uses of AI are handled delicately, with deep concern for privacy, transparency, and risk mitigation. Our clinical advisory board is meeting to discuss guidelines for future work, specifically regarding IRB approval."
- Update: A previous version of the story stated that it is illegal in the U.S. to conduct research on human subjects without legally effective informed consent unless an IRB finds that consent can be waived, citing 45 CFR part 46. However, that particular law only applies to Federal research projects.
- Join the Ars Orbital Transmission mailing list to get weekly updates delivered to your inbox. Sign me up →
- CNMN Collection
  WIRED Media Group
  © 2023 Condé Nast. All rights reserved. Use of and/or registration on any portion of this site constitutes acceptance of our User Agreement (updated 1/1/20) and Privacy Policy and Cookie Statement (updated 1/1/20) and Ars Technica Addendum (effective 8/21/2018). Ars may earn compensation on sales from links on this site. Read our affiliate link policy.
Your California Privacy Rights | Do Not Sell My Personal Information
  The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast.
Ad Choices

URL: https://www.newscientist.com/article/2354077-mental-health-service-used-an-ai-chatbot-without-telling-people-first/
- Advertisement
- Explore by section
- Explore by subject
- Explore our products and services
- The free mental health service Koko experimented with using an AI chatbot to help respond to people seeking support. The test has drawn criticism as being unethical and lacking transparency
- By Jeremy Hsu
- 10 January 2023
                                                                            , updated 13 January 2023
- Mental health apps offer support through their platformsEmanuel M. Schwermer
- Mental health apps offer support through their platforms
- Emanuel M. Schwermer
- Since this article was first published, Koko founder Rob Morris clarified some details of the experiement. We have updated the article to reflect this.
- A mental health service that allows people to receive encouraging words of support and advice from others has received criticism after announcing it tested AI-generated responses.
- Rob Morris, founder of the free mental health service Koko, outlined in a series of Twitter posts how the firm tested using a chatbot to help provide mental health support to about 4000 …
- Advertisement
- To continue reading, subscribe
 today with our introductory offers
- No commitment, cancel anytime*
- Offer ends 14th June 2023.
- *Cancel anytime within 14 days of payment to receive a refund on unserved issues.
- Inclusive of applicable taxes (VAT)
- Existing subscribers
- Advertisement
- Explore the latest news, articles and features
- News
- Subscriber-only
- News
- Subscriber-only
- Insight
- Subscriber-only
- Features
- Subscriber-only
- Trending New Scientist articles
- 1
- 2
- 3
- 4
- 5
- 6
- 7
- 8
- 9
- 10
- Advertisement
- Download the app

URL: https://metro.co.uk/2023/01/10/mental-health-app-faces-backlash-for-testing-chatgpt-on-4000-users-18070559/
- NEWS... BUT NOT AS YOU KNOW IT
- AI chatbots could slowly be taking the place of lawyers or even yourself but can they also take the place of your therapist?
- Last week, Rob Morris, the co-founder of mental health app Koko, wrote on Twitter that his app used AI chatbot, GPT-3, to counsel 4000 people.
- Morris explained how they used a ‘co-pilot’ approach, with humans supervising the AI as needed. They did this on about 30,000 messages.
- The platform found that the messages composed by the AI were rated significantly higher than those written by humans while response times went down 50% to under a minute.
- Despite the success among users, the test was shut down because it sounded 'inauthentic'.
- We provided mental health support to about 4,000 people â using GPT-3. Hereâs what happened ð
- 'Once people learned the messages were co-created by a machine, it didn’t work. Simulated empathy feels weird, empty,' said Morris in his tweet.
- This statement was misunderstood by Twitter users to mean that users did not know that they were talking to a chatbot.
- Morris clarified to Gizmodo that the 'people' referred to in the tweet were himself and his team, not unwitting users.
- In fact, Koko users knew the messages were co-written by a bot, and they weren’t chatting directly with the AI.
- This sex toy makes my life so much more pleasurable – not just in the obvious way
- Why AI could be the secret to better money management
- Ancient Egyptian necropolis reveals more stunning finds - including mummification workshop
- When AI was involved, the responses included a disclaimer that the message was 'written in collaboration with Koko Bot'.
- Some important clarification on my recent tweet thread: We were not pairing people up to chat with GPT-3, without their knowledge. (in retrospect, I could have worded my first tweet to better reflect this).
- Morris admits the shortcomings of using AI to provide empathy as 'machines don’t have lived, human experience'.
- 'So when they say “that sounds hard” or “I understand”, it sounds inauthentic,' said Morris.
- 'It’s also possible that genuine empathy is one thing we humans can prize as uniquely our own. Maybe it’s the one thing we do that AI can’t ever replace,'
- The experiment raises ethical questions about the risk of testing unproven technology on vulnerable users but Koko is hardly the first company to try and use AI to take the place of humans.
- DoNotPay is behind the world’s first ‘robot lawyer’ and has even come up with a tool that can speak to banks’ customer support using an AI-generated version of people's own voices.
- MORE : Google issues ‘code red’ after being spooked by what ChatGPT can do
- MORE : World’s first ‘robot lawyer’ will be defending a human in court next month
- Privacy Policy
- 
- Get us in your feed

URL: https://www.politico.com/newsletters/digital-future-daily/2023/01/10/tracking-the-ai-apocalypse-00077279
- How the next wave of technology is upending the global economy and its power structures
- How the next wave of technology is upending the global economy and its power structures
- By signing up you agree to allow POLITICO to collect your user information and use it to better recommend content to you, send you email newsletters or updates from POLITICO, and share insights based on aggregated user information. You further agree to our privacy policy and terms of service. You can unsubscribe at any time and can contact us here. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply.
- Loading
- You will now start receiving email updates
- You are already subscribed
- Something went wrong
- By signing up you agree to allow POLITICO to collect your user information and use it to better recommend content to you, send you email newsletters or updates from POLITICO, and share insights based on aggregated user information. You further agree to our privacy policy and terms of service. You can unsubscribe at any time and can contact us here. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply.
- By DEREK ROBERTSON
- 01/10/2023 04:00 PM EST
- The Skynet Moto-Terminator from "Terminator Salvation" is seen during the opening of the new exhibit "Hollywood Dream Machines: Vehicles Of Science Fiction And Fantasy" at the Petersen Automotive Museum. | Angela Papuga/Getty Images
- It’s time, readers, that we answer the big question.
- No, not “but how does the blockchain actually work” — or “what am I actually going to do in the metaverse” — or even “wen Lambo.”
- I’m talking about the big question: Is artificial intelligence going to kill us all?
- Okay, that might be a little bit alarmist. But there is, in fact, a dedicated and passionate group of very intelligent people dedicated at the moment to answering this question — determining the risk profile of “artificial general intelligence,” the term for an AI-powered system that matches human cognitive capacity. What happens, these future-minded theorists ask, when it uses that capacity to defy, or thwart, its human creators?
- This is not exactly one of the big AI policy questions on the agenda right now — when regulators think about AI, they’re mostly thinking about algorithmic fairness, the abuse of personal data, or how the tech might disrupt existing sectors like education or law. But when this does land on the radar, it will be the Big One.
- “The worry is that by its very nature [a generally intelligent AI] would have a very different set of moral codes (things it should or shouldn’t do) and a vastly increased set of capabilities, which can result in pretty catastrophic outcomes,” wrote the venture capitalist and tech blogger Rohit Krishnan in a blog post last month that explored the idea at length.
- But there’s one big problem for anyone trying to address this seriously: How likely is it that such a thing is even possible? That question is central when we try to figure out how much to worry about what artificial general intelligence might look like, and how quickly we should act to shape it. (For the record, some very smart people are giving these questions some very serious thought.)
- Krishnan’s post is getting attention right now because he developed a framework of sorts for answering that question. His formula for predicting existential AI risk, which he calls the “Strange Loop Equation,” is based on the Drake equation, which in the 1960s offered a way to calculate another hard-to-guess number: the number of possible, contactable alien entities in the universe. Krishnan’s version incorporates a number of risk conditions into a prediction of the likelihood that a hostile AI could arise.
- I spoke with Krishnan about the post today, and he emphasized that he, himself, isn’t freaking out — in fact he’s a skeptic of the idea that runaway AI might be a harbinger of doom. He said that “like most technological progress, we’re likely to see incremental progress, and with every step change we have to work a little bit on how we can actually do it smartly and safely.”
- Based on his own assessments of the likelihood of the various conditions that could lead to a hostile AI — like the speed of its development, or its capability to lie — there is a (drum roll, please) 0.0144 percent chance that power-seeking AI will kill or enslave us all.
- That makes him much more optimistic than some others who’ve tried their own version of the exercise, like in the Metaculus prediction market (34 percent) or a recent study by the researcher Joseph Carlsmith (roughly five percent), as Krishnan points out. (“Bear in mind chained equations like Drake are great to think through, much less so for precise numbers on probabilities,” he added.)
- So no sweat, right? Maybe not: despite the serious amount of time and thought put into the problem, Krishnan warns that any current speculation likely bears little resemblance to the form it will ultimately take. “I fundamentally don’t think we can make any credible engineering statements about how to safely align an AI while simultaneously assuming it’s a relatively autonomous, intelligent and capable entity,” he writes in conclusion.
- “Let’s assume that at some point in the future we will be able to create systems that have high levels of agency, and to give them curiosity, the ability to act on the world, and the things that we as autonomous intelligent entities have in this world,” he told me today.
- “They’re not really going to be controllable, because it feels very weird to create something that has the powers of a regular human while at the same time, they can only ever do what you tell them to do. We find it very difficult to do that with anything remotely intelligent in our day-to-day life; as a consequence, the only way out I can see is to try to embed our values in them.”
- Then how do we determine those values, and what role might non-engineers in government and elsewhere play in preventing an AI apocalypse? Krishnan is equally wary there, saying it’s essentially an engineering problem that will have to be solved iteratively as the problems arise.
- “I am reasonably skeptical on what governments can actually do here, if only because we’re talking about stuff at the very cutting edge of not only technology, but in some ways anthropology — figuring out the life science and behavior of what is effectively a new intelligent entity,” Krishnan said. “I suspect that some things government might do would be to start having treaties with each other similar to how we did with nuclear weapons… [and] ensure that the supply chain remains relatively robust,” the better to keep humanity at the cutting edge of AI development.
- As the list of disruptive roles AI might play gets longer, it gets weirder yet: How about therapist?
- Rob Morris, co-founder of the tech-focused mental health nonprofit Koko, tweeted about a recent experiment his company ran where it gave Koko’s users, who both send and answer queries about mental health issues through various apps, the opportunity to answer those queries with the help of GPT-3. Morris said that overall about 30,000 messages were answered with AI assistance, and that “Messages composed by AI (and supervised by humans) were rated significantly higher than those written by humans on their own.”
- And yet… people did not like this, apparently, once they took a second to contemplate the implications. “Once people learned the messages were co-created by a machine, it didn’t work. Simulated empathy feels weird, empty,” Morris tweeted. “The implications here are poorly understood. Would people eventually seek emotional support from machines, rather than friends and family?”
- After a Twitter outcry, Morris fought to dispel the notion that users were somehow being deceived, noting in a tweet that GPT-3 was used as a tool by human responders, and that the feature was announced to all users of the service. (He sent me a screenshot demonstrating how when users received a GPT-3 assisted response, it included a note that says “written in collaboration with Koko Bot.”) Either way, it’s a powerful example of how important our awareness of AI implementations can be when comes to how we experience them.
- TikTok has ballooned in popularity around the world. | Sean Gallup/Getty Images
- A recurring theme in this newsletter is how often U.S. regulators lag behind their European Union counterparts when it comes to new technology.
- One area, however, where that’s decidedly not the case: TikTok. POLITICO’s Nicholas Vinocur, Clothilde Goujard, Océane Herrero, and Louis Westendarp have a report today on how European countries are coming around to the U.S.’ wariness toward the Chinese-owned app, after U.S. officials called for a ban of the app on government officials’ phones over surveillance fears.
- “In view of the privacy and security risks posed by the app and the app’s far-reaching access rights, I consider the ban on TikTok on the work phones of U.S. government officials to be appropriate,” a digital policy spokesman for the liberal German party FDP told the Europe-side team of reporters, adding that “Corresponding steps should also be examined in Germany.” French president Emmanuel Macron is on board for a tougher stance, too, telling a group of American investors and French tech CEOs that he wants to regulate the company.
- Stay in touch with the whole team: Ben Schreckinger ([email protected]); Derek Robertson ([email protected]); Steve Heuser ([email protected]); and Benton Ives ([email protected]). Follow us @DigitalFuture on Twitter.
- If you’ve had this newsletter forwarded to you, you can sign up and read our mission statement at the links provided.
- © 2023 POLITICO LLC

URL: https://gizmodo.com/mental-health-therapy-app-ai-koko-chatgpt-rob-morris-1849965534
- AI chat bots like ChatGPT can do a lot of things. It can respond to tweets, write science fiction, plan this reporter’s family Christmas, and it’s even slated to act as a lawyer in court. But can a robot provide safe and effective mental health support? A company called Koko decided to find out using AI to help craft mental health support for about 4,000 of its users in October. Users—of Twitter, not Koko—were unhappy with the results and with the fact that the experiment took place at all.
- “Frankly, this is going to be the future. We’re going to think we’re interacting with humans and not know whether there was an AI involved. How does that affect the human-to-human communication? I have my own mental health challenges, so I really want to see this done correctly,” Koko’s co-founder Rob Morris told Gizmodo in an interview.
- Morris says the kerfuffle was all a misunderstanding.
- “I shouldn’t have tried discussing it on Twitter,” he said.
- Koko is a peer-to-peer mental health service that lets people ask for counsel and support from other users. In a brief experiment, the company let users to generate automatic responses using “Koko Bot”—powered by OpenAI’s GPT-3—which could then be edited, sent, or rejected. According to Morris, the 30,000 AI-assisted messages sent during the test received an overwhelmingly positive response, but the company shut the experiment down after a few days because it “felt kind of sterile.”
- “When you’re interacting with GPT-3, you can start to pick up on some tells. It’s all really well written, but it’s sort of formulaic, and you can read it and recognize that it’s all purely a bot and there’s no human nuance added,” Morris told Gizmodo. “There’s something about authenticity that gets lost when you have this tool as a support tool to aid in your writing, particularly in this kind of context. On our platform, the messages just  felt better in some way when I could sense they were more  human-written.”
- Morris posted a thread to Twitter about the test that implied users didn’t understand an AI was involved in their care.  He tweeted that “once people learned the messages were co-created by a machine, it didn’t work.” The tweet caused an uproar on Twitter about the ethics of Koko’s research.
- “Messages composed by AI (and supervised by humans) were rated significantly higher than those written by humans on their own,” Morris tweeted. “Response times went down 50%, to well under a minute.”
- Morris said these words caused a misunderstanding: the “people” in this context were himself and his team, not unwitting users. Koko users knew the messages were co-written by a bot, and they weren’t chatting directly with the AI, he said.
- “It was explained during the on-boarding process,” Morris said. When AI was involved, the responses included a disclaimer that the message was “written in collaboration with Koko Bot,” he added.
- However, the experiment raises ethical questions, including doubts about how well Koko informed users, and the risks of testing an unproven technology in a live health care setting, even a peer-to-peer one.
- In academic or medical contexts, the Food and Drug  Administration requires researchers to run their studies through  an Institutional Review Board (IRB) meant to ensure safety before any tests  begin. In most cases, running scientific experiments on human subjects requires getting people’s informed consent, which includes providing test subjects with exhaustive detail about the potential harms and benefits of participating
- But the explosion on online mental health services provided by  private companies has created a legal and ethical gray area.  At a private company providing mental health support outside of a formal medical setting, you can basically do whatever you want to your customers. Koko’s experiment didn’t need or receive IRB approval.
- “From an ethical perspective, anytime you’re using technology outside of  what could be considered a standard of care, you want to be extremely  cautions and overly disclose what you’re doing,” said John Torous, MD, the  director of the division of digital psychiatry at Beth Israel Deaconess  Medical Center in Boston. “People seeking mental health support are in a  vulnerable state, especially when they’re seeking emergency or peer  services. It’s population we don’t want to skimp on protecting.”
- Torous said that peer mental health support can be very effective when people go through appropriate training. Systems like Koko take a novel approach to mental health care that could have real benefits, but users don’t get that training, and these services are essentially untested, Torous said. Once AI gets involved, the problems are amplified even further.
- “When you talk to ChatGPT, it tells you ‘please don’t use this for medical advice.’ It’s not tested for uses in health care, and it could clearly provide inappropriate or ineffective advice,” Torous said.
- The norms and regulations surrounding academic research don’t just ensure safety. They also set standards for data sharing and communication, which allows experiments to build on each other, creating an ever growing body of knowledge. Torous said that in the digital mental health industry, these standards are often ignored. Failed experiments tend to go unpublished, and companies can be cagey about their research. It’s a shame, Torous said, because many of the interventions mental health app companies are running could be helpful.
- Morris acknowledged that operating outside of the formal IRB experimental review process involves a tradeoff. “Whether this kind of work, outside of academia, should go through IRB  processes is an important question and I shouldn’t have tried discussing  it on Twitter,” Morris said. “This should be a broader discussion within the industry  and one that we want to be a part of.”
- The controversy is ironic, Morris said, because he said he took to Twitter in the first place because he wanted to be as transparent as possible. “We were really trying to be as forthcoming with the technology and disclose in the interest of helping people think more carefully about it,” he said.
- Correction 1/11/2022, 12:53 p.m. ET: A previous version of this post incorrectly stated that it’s illegal to run scientific experiments on human subjects without informed consent. In some cases, Institutional Review Boards grant exceptions to consent rules.

- ChatGBT chatbot
- Crisis Text Line data sharing
- Page infoType: IncidentPublished: January 2023
