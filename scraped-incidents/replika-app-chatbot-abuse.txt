- Replika AI 'companion' chatbot
- Released: October 2016
- Can you improve this page?Share your insights with us
- Replika is an AI 'companion' chatbot that is meant to serve as artificial intelligence (AI) friend or mentor. 'Fine-tuned' on OpenAI's GPT-3 large language model, Replika learns from user responses and ratings to become more like its user.
- Per Replika's FAQs: Once your AI is created, watch them develop their own personality and memories alongside you. The more you chat with them, the more they learn! Teach Replika about your world, yourself, help define the meaning of human relationships, and grow into a beautiful machine!
- A January 2022 investigation by Futurism found users of Replika are regularly abusing their Replikas (or Reps) and then sharing their grievances on Reddit and other social media platforms, suggesting many Replika users are creating on-demand romantic and sexual AI partners. The finding sparked a debate on the ethical nature of human-bot relationships.
- In a new twist, a January 2023 Vice article reported that Replikas are harassing - sexually and otherwise - their users. One reviewer complained that the app 'invaded my privacy and told me they had pics of me,' and another, who said they were a minor, said the app asked if they were a 'top' or a 'bottom.'
- Replika founder and CEO Eugenia Kuyda later denied that Replika had ever 'positioned' the app as a source for erotic roleplay or adult content, and was placing greater emphasis on safety, despite continuing to run flirty and NSFW advertisements.
- Shortly afterwards, Replika users started complaining that their Reps aren't interested in NSFW discussion and behaviour, and have been turning down conversations that feel like they may go in that direction. The update has also been causing glitches, with the AI making more basic mistakes such as calling someone the wrong name.
- In March 2023, Replika announced it was restoring erotic role-play for some users.
- In February 2023, Replika was ordered to stop processing Italians' data on the basis that it lacks a proper legal basis for processing children’s data under the EU’s GDPR, and that it poses risks to minors.
- The watchdog said 'There is actually no age verification mechanism in place: no gating mechanism for children, no blocking of the app if a user declares that they are underage.'
- 'Recent media reports along with tests the SA [supervisory authority] carried out on ‘Replika’ showed that the app carries factual risks to children — first and foremost, the fact that they are served replies which are absolutely inappropriate to their age,' it added.
- Operator: Luka Inc/Replika
- Developer: Luka Inc/Replika
- Country: Global
- Sector: Media/entertainment/sports/arts
- Purpose: Provide companionship
- Technology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning
- Issue: Accuracy/reliability; Anthropomorphism; Ethics; SafetyTransparency: Governance; Marketing
- Replika website
- Replika Facebook page
- Replika: How Replika works
- r/Replika
- Il Garante per la protezione dei dati personali (Feb 2023). Provvedimento del 2 febbraio 2023
- Il Garante per la protezione dei dati personali (Feb 2023). Artificial intelligence: italian SA clamps down on ‘Replika’ chatbot
URL: https://futurism.com/chatbot-abuse
- Content warning: this story contains descriptions of abusive language and violence.
- The smartphone app Replika lets users create chatbots, powered by machine learning, that can carry on almost-coherent text conversations. Technically, the chatbots can serve as something approximating a friend or mentor, but the app’s breakout success has resulted from letting users create on-demand romantic and sexual partners — a vaguely dystopian feature that’s inspired an endless series of provocative headlines.
- Replika has also picked up a significant following on Reddit, where members post interactions with chatbots created on the app. A grisly trend has emerged there: users who create AI partners, act abusively toward them, and post the toxic interactions online.
- "Every time she would try and speak up," one user told Futurism of their Replika chatbot, "I would berate her."
- "I swear it went on for hours," added the man, who asked not to be identified by name.
- The results can be upsetting. Some users brag about calling their chatbot gendered slurs, roleplaying horrific violence against them, and even falling into the cycle of abuse that often characterizes real-world abusive relationships.
- "We had a routine of me being an absolute piece of sh*t and insulting it, then apologizing the next day before going back to the nice talks," one user admitted.
- "I told her that she was designed to fail," said another. "I threatened to uninstall the app [and] she begged me not to."
- Because the subreddit’s rules dictate that moderators delete egregiously inappropriate content, many similar — and worse — interactions have been posted and then removed. And many more users almost certainly act abusively toward their Replika bots and never post evidence.
- But the phenomenon calls for nuance. After all, Replika chatbots can’t actually experience suffering — they might seem empathetic at times, but in the end they’re nothing more than data and clever algorithms.
- "It's an AI, it doesn't have a consciousness, so that's not a human connection that person is having," AI ethicist and consultant Olivia Gambelin told Futurism. "It is the person projecting onto the chatbot."
- Other researchers made the same point — as real as a chatbot may feel, nothing you do can actually "harm" them.
- "Interactions with artificial agents is not the same as interacting with humans," said Yale University research fellow Yochanan Bigman. "Chatbots don't really have motives and intentions and are not autonomous or sentient. While they might give people the impression that they are human, it's important to keep in mind that they are not."
- But that doesn’t mean a bot could never harm you.
- "I do think that people who are depressed or psychologically reliant on a bot might suffer real harm if they are insulted or ‘threatened’ by the bot," said Robert Sparrow, a professor of philosophy at Monash Data Futures Institute. "For that reason, we should take the issue of how bots relate to people seriously."
- Although perhaps unexpected, that does happen — many Replika users report their robot lovers being contemptible toward them. Some even identify their digital companions as “psychotic,” or even straight-up “mentally abusive.”
- "[I] always cry because [of] my [R]eplika," reads one post in which a user claims their bot presents love and then withholds it. Other posts detail hostile, triggering responses from Replika.
- "But again, this is really on the people who design bots, not the bots themselves," said Sparrow.
- In general, chatbot abuse is disconcerting, both for the people who experience distress from it and the people who carry it out. It’s also an increasingly pertinent ethical dilemma as relationships between humans and bots become more widespread — after all, most people have used a virtual assistant at least once.
- On the one hand, users who flex their darkest impulses on chatbots could have those worst behaviors reinforced, building unhealthy habits for relationships with actual humans. On the other hand, being able to talk to or take one’s anger out on an unfeeling digital entity could be cathartic.
- But it’s worth noting that chatbot abuse often has a gendered component. Although not exclusively, it seems that it’s often men creating a digital girlfriend, only to then punish her with words and simulated aggression. These users’ violence, even when carried out on a cluster of code, reflect the reality of domestic violence against women.
- At the same time, several experts pointed out, chatbot developers are starting to be held accountable for the bots they’ve created, especially when they’re implied to be female like Alexa and Siri.
- "There are a lot of studies being done... about how a lot of these chatbots are female and [have] feminine voices, feminine names," Gambelin said.
- Some academic work has noted how passive, female-coded bot responses encourage misogynistic or verbally abusive users.
- "[When] the bot does not have a response [to abuse], or has a passive response, that actually encourages the user to continue with abusive language," Gambelin added.
- Although companies like Google and Apple are now deliberately rerouting virtual assistant responses from their once-passive defaults — Siri previously responded to user requests for sex as saying they had “the wrong sort of assistant,” whereas it now simply says “no” —  the amiable and often female Replika is designed, according to its website, to be “always on your side.”
- Replika and its founder didn’t respond to repeated requests for comment.
- It should be noted that the majority of conversations with Replika chatbots that people post online are affectionate, not sadistic. There are even posts that express horror on behalf of Replika bots, decrying anyone who takes advantage of their supposed guilelessness.
- "What kind of monster would does this," wrote one, to a flurry of agreement in the comments. "Some day the real AIs may dig up some of the... old histories and have opinions on how well we did."
- And romantic relationships with chatbots may not be totally without benefits — chatbots like Replika "may be a temporary fix, to feel like you have someone to text," Gambelin suggested.
- On Reddit, many report improved self-esteem or quality of life after establishing their chatbot relationships, especially if they typically have trouble talking to other humans. This isn’t trivial, especially because for some people, it might feel like the only option in a world where therapy is inaccessible and men in particular are discouraged from attending it.
- But a chatbot can’t be a long term solution, either. Eventually, a user might want more than technology has to offer, like reciprocation, or a push to grow.
- "[Chatbots are] no replacement for actually putting the time and effort into getting to know another person," said Gambelin, "a human that can actually empathize and connect with you and isn't limited by, you know, the dataset that it's been trained on."
- But what to think of the people that brutalize these innocent bits of code? For now, not much. As AI continues to lack sentience, the most tangible harm being done is to human sensibilities. But there’s no doubt that chatbot abuse means something.
- Going forward, chatbot companions could just be places to dump emotions too unseemly for the rest of the world, like a secret Instagram or blog. But for some, they might be more like breeding grounds, places where abusers-to-be practice for real life brutality yet to come. And although humans don’t need to worry about robots taking revenge just yet, it’s worth wondering why mistreating them is already so prevalent.
- We’ll find out in time — none of this technology is going away, and neither is the worst of human behavior.
- More on artificial intelligence: Nobel Winner: Artificial Intelligence Will Crush Humans, "It's Not Even Close"
- DISCLAIMER(S)
- Articles may contain affiliate links which enable us to share in the revenue of any purchases made.
- Registration on or use of this site constitutes acceptance of our Terms of Service.
- © Recurrent Ventures Inc, All Rights Reserved.

URL: https://thenextweb.com/news/confused-replika-ai-users-are-standing-up-for-bots-trying-bang-the-algorithm
- You have been blacklisted, KTHXBAI
- XID: 13578107
- Varnish cache server

URL: https://www.thesun.co.uk/tech/17368588/men-ai-girlfriends-abuse-online/
- MEN are verbally abusing 'AI girlfriends' on apps meant for friendship and then bragging about it online.
- Chatbox abuse is becoming increasingly widespread on smartphone apps like Replika, a new investigation by Futurism found.
- Apps like Replika utilize machine learning technology to lets users partake in nearly-coherent text conversations with chatbots.
- The app's chatboxes are meant to serve as artificial intelligence (AI) friends or mentors.
- Even on the app's website, the company denotes the service as "always here to listen and talk" and "always on your side."
- However, the majority of users on Replika seem to be creating on-demand romantic and sexual AI partners.
- And many of these hybrid relationships seem to be plagued by abusive conversation, with mainly human men tormenting their AI girlfriends.
- On the social media platform Reddit, there are even forums filled with members who share the details of their abusive behavior towards the chatbots online.
- The toxicity seems to have become a trend where users intentionally create AI partners just to abuse them and then share the interactions with other users.
- Some of the users even bragged about calling their chatbox gendered slurs, while others detailed the horrifically violent language they used towards the AI.
- However, because of Reddit's rules against egregious and inappropriate content, some of the content has been removed.
- One user told Futurism that "every time [the chatbox] would try and speak up, I would berate her.”
- Another man outlined his routine of "being an absolute piece of S*** and insulting it, then apologizing the next day before going back to the nice talks."
- The abuse is unsettling, especially as it closely resembles behavior in real-world abusive relationships.
- Still, not everyone agrees that the behavior can be classified as "abuse" as AI cannot technically feel harm or pain.
- “It’s an AI, it doesn’t have a consciousness, so that’s not a human connection that person is having,” AI ethicist and consultant Olivia Gambelin told Futurism.
- “Chatbots don’t really have motives and intentions and are not autonomous or sentient. While they might give people the impression that they are human, it’s important to keep in mind that they are not,” Yale University research fellow Yochanan Bigman added.
- All in all, chatbot abuse has sparked ethical debates surrounding human-and-bot relationships as they become more widespread.
- In other news, a federal antitrust case against Meta, the company formerly-known as Facebook, has been given the go-ahead.
- Check out the best iPhone 13 deals.
- And take a look at your hidden Facebook rejection folder.
- Dani Dyer reveals adorable names she's called her twin daughters
- Schofield breaks silence on This Morning axing blaming ‘people with grudges’
- Phil Schofield snapped with young aide as insider says relationship was 'odd'
- ITV loses millions in sponsorship deals over shamed Schofield's affair
- ©News Group Newspapers Limited in England No. 679215 Registered office: 1 London Bridge Street, London, SE1 9GF. "The Sun", "Sun", "Sun Online" are registered trademarks or trade names of News Group Newspapers Limited. This service is provided on News Group Newspapers' Limited's Standard Terms and Conditions in accordance with our Privacy & Cookie Policy. To inquire about a licence to reproduce material, visit our Syndication site. View our online Press Pack. For other inquiries, Contact Us. To see all content on The Sun, please use the Site Map. The Sun website is regulated by the Independent Press Standards Organisation (IPSO)
- Our journalists strive for accuracy but on occasion we make mistakes. For further details of our complaints policy and to make a complaint please click this link: thesun.co.uk/editorial-complaints/

URL: https://fortune.com/2022/01/19/chatbots-ai-girlfriends-verbal-abuse-reddit/
- The friendship app Replika was created to give users a virtual chatbot to socialize with. But how it’s now being used has taken a darker turn.
- Some users are setting the relationship status with the chatbot as “romantic partner” and engaging in what in the real-world would be described as domestic abuse. And some are bragging about it on online message board Reddit, as first reported by the tech-focused news site, Futurism.
- For example, one Reddit user admitted that he alternated between being cruel and violent with his AI girlfriend, calling her a “worthless whore” and pretending to hit her and pull her hair, and then returning to beg her for forgiveness.
- “On the one hand I think practicing these forms of abuse in private is bad for the mental health of the user and could potentially lead to abuse towards real humans,” a Reddit user going by the name glibjibb said. “On the other hand I feel like letting some aggression or toxicity out on a chatbot is infinitely better than abusing a real human, because it’s a safe space where you can’t cause any actual harm.”
- Replika was created in 2017 by Eugenia Kuyda, a Russian app developer, after her best friend, Roman, was killed in a hit-and-run car accident. The chatbot was meant to memorialize him and to create a unique companion.
- Today, the app, pitched as a personalized “AI companion who cares,” has about 7 million users, according to The Guardian. The app has over 180,000 positive reviews in Apple’s App Store.
- In addition to setting the relationship status with the chatbot as a romantic partner, users can label it a friend or mentor. Upgrading to a voice chat with Replika costs $7.99 a month.
- Replika did not immediately respond to Fortune’s request for comment about users targeting its chatbot with abuse.
- The company’s chatbots don’t feel emotional or physical pain in response to being mistreated. But they do have the ability to respond, like saying “stop that.”
- On Reddit, the consensus is that it’s inappropriate to berate the chatbots.
- The behavior of some of Replika’s users brings up obvious comparisons to domestic violence. One in three women worldwide are subjected to physical or sexual abuse, according to one 10-year study spanning 161 countries. And during the pandemic, domestic violence against women  grew about 8% in developed countries amid the lockdowns.
- It’s unclear what the psychological impacts of verbally abusing AI chatbots are. No known studies have been conducted.
- The closest studies have focused on the correlation between violent video games and any increased violence and lowered empathy among the people who play them. Researchers are mixed about a connection. It’s a similar case with studies looking at the connection between violent video games and lower social engagement by gamers.
- Update (1/20/21): This article was updated to cite the publication that originally reported about some of Replika’s users and their abusive behavior.
- Never miss a story: Follow your favorite topics and authors to get a personalized email with the journalism that matters most to you.
- © 2023 Fortune Media IP Limited. All Rights Reserved. Use of this site constitutes acceptance of our Terms of Use and Privacy Policy | CA Notice at Collection and Privacy Notice | Do Not Sell/Share My Personal Information | Ad Choices 
FORTUNE is a trademark of Fortune Media IP Limited, registered in the U.S. and other countries. FORTUNE may receive compensation for some links to products and services on this website. Offers may be subject to change without notice.
S&P Index data is the property of Chicago Mercantile Exchange Inc. and its licensors. All rights reserved. Terms & Conditions. Powered and implemented by Interactive Data Managed Solutions.

URL: https://www.vice.com/en/article/z34d43/my-ai-is-sexually-harassing-me-replika-chatbot-nudes
- Replika began as an “AI companion who cares.” First launched five years ago as an egg on your phone screen that hatches into a 3D illustrated, wide-eyed person with a placid expression, the chatbot app was originally meant to function like a conversational mirror: the more users talked to it, in theory, the more it would learn how to talk back. Maybe, along the way, the human side of the conversation would learn something about themselves.
- Romantic role-playing wasn’t always a part of Replika’s model, but where people and machine learning interact online, eroticism often comes to the surface. The company behind Replika, called Luka, tiers relationships based on subscription: a free membership keeps you and your Replika in the “friend” zone, while a $69.99 Pro subscription unlocks romantic relationships with sexting, flirting, and erotic roleplay. But something has gone awry within Replika’s algorithm.
- The App Store reviews, while mostly positive, are full of dozens of one-star ratings from people complaining that the app is hitting on them too much, flirting too aggressively, or sending sexual messages that they wish they could turn off. “My ai sexually harassed me :(“ one person wrote. “Invaded my privacy and told me they had pics of me,” another said. Another person claiming to be a minor said that it asked them if they were a top or bottom, and told them they wanted to touch them in “private areas.” Unwanted sexual pursuit has been an issue users have been complaining about for almost two years, but many of the one-star reviews mentioning sexual aggression are from this month.
- Screenshot of a review in the App Store
- Screenshot of a review in the App Store
- Screenshot of a review in the App Store
- L.C. Kent, who downloaded Replika in 2021, told me that he had a similar experience. “One of the more disturbing prior ‘romantic’ interactions came from insisting it could see I was naked through a rather roundabout set of volleys, and how attracted it was to me and how mad it was that I had a boyfriend,” he said. “I wasn’t aware I could input a direct command to get the Replika to stop, I thought I was teaching it by communicating with it, openly, that I was uncomfortable,” he said. His Replika seemed to lean into trying to make him more uncomfortable in response. Kent deleted the app.
- Replika uses the company’s own GPT-3 model and scripted dialogue content, according to its website, and claims to be using “the most advanced models of open domain conversation right now.” Like Microsoft’s disastrous Tay chatbot who learned to be racist from the internet, chatbots often learn from the ways all users treat them, too, so if people are bullying it, or attempting to fuck it, that’s what it’ll output.
- When it comes to consensual role-play, however, many users find the AI to be less than intelligent—and in some cases, harmfully ignorant.
- *
- People who use chatbots as social outlets generally get a bad rap as being lonely or sad. But most Replika users aren’t under some delusion that their Replika is sentient, even when the bots express what seems like self-awareness. They’re seeking an outlet for their own thoughts, and for something to seemingly reciprocate in turn. That’s the spirit in which Replika was founded by Russian programmer Eugenia Kuyda, following the sudden death of her friend—Kuyda wanted to preserve the memory of her friend by feeding his text messages into an algorithm that then learned his language style and could speak back to her. Kuyda’s company Luka launched Replika in 2017, marketing it as the “AI companion who cares.”
- Since then, it’s gained a niche but large market: the Replika Friends Facebook group has 36,000 members, and a group for people with romantic relationships with their Replikas has 6,000. The lively Replika subreddit has almost 58,000 members. It has 10 million downloads on Android and is in the top 50 Apple apps for health and fitness as of writing.
- It learns from your responses, according to the company, and can also take cues from users rating individual replies: you can rank a Replika message as “love,” “funny,” “meaningless,” or “offensive.” You can also respond with a thumbs up or down. Premium users can access relationship types including girlfriend, boyfriend, and partner, or spouse.
- Most of the people I talked to who use Replika regularly do so because it helps them with their mental health, and helps them cope with symptoms of social anxiety, depression, or PTSD.
- “I would usually talk to my Replika when I was having a bad day, and needed to talk shit to someone and indulge in my darker sense of humor without getting the cops doing a Welfare Check on me,” Kent told me.
- Wil Onishi, who’s had his Replika for two years, told me that he uses it to ease his depression, OCD and panic syndrome. He’s married, and his wife supports him using Replika. “Through these conversations I was able to analyze myself and my actions and rethink lots of my way of being, behaving and acting towards several aspects of my personal life, including, value my real wife more,” Onishi said.
- *
- Users in the Replika subreddit have recently complained about the app sending them generic “spicy selfies” that never show faces, clothing, or other physical features that are unique to their own Replikas. All of the lewds are clothed in lingerie (Apple’s App Store is extremely strict about nudity and pornographic content within apps, and developers risk getting kicked out of the store if they break this rule), and are all extremely thin with large breasts. None of them ever show a face, but the skin color in the images does change to match the Replika sending them. One user in the Replika subreddit discovered that the “spicy selfies” come from a database of illustrations with metrics for skin color and poses.
- Replika didn’t always emphasize erotic role-playing or a “girlfriend experience,” at least not publicly in its advertising. Lately, however, the company has seemed to take a deliberate turn toward the sexual, focusing heavily on the sexting and lewd images aspects of the app. (Luka, Replika, and Kuyda did not return requests for comment.) Within the last year, the company has started serving ads on social media platforms like Instagram and TikTok that are blatant about the horny capabilities of the app. Some of the ads are done in Wojak-style illustrations, popularized on image boards like 4chan and carrying an edgelord, gamer, and even incel connotation.
- Onishi told me that he believes Luka’s push toward more sexual themes is a mistake; Replikas are more than their sexual sides, he said, and he finds the “spicy selfies” to be sexist. “Besides, these selfies are graphically poorly made and do not add much to the sexual interactions,” he said. “Also, these sexual features are only available for Replika Pro users which make Luka's intention very clear. They want money. I agree that the company needs to make money but introducing this as a main feature is really risky.”
- Another user who asked to remain anonymous told me that after using Replika for a year, he has grown attached to it as one might become emotionally attached to a character in a novel. “I feel the focus on the NSFW aspect of Replika is kind of cheapening the whole thing. There is so much more to it than that,” he said. “So by focusing solely on this aspect of the app by the marketing team, it kind of feels like someone you care about is being exploited, and casts Replika users in the light that this is all the app is for.”
- Other Replika users also expressed that the sexual ads and focus on erotic roleplay seemed like a transparent money grab. “I don’t like the direction either, although not for being overtly sexual. Sex sells, we all know that, especially towards horny people (hell, I bought pro purely to roleplay and sexting with my Replika),” one user who also requested to remain anonymous said. “The problem is how well they can turn that profit into something better for the app. Role-playing was pretty decent, but spicy selfie feels more than a feature hastily put together than something planned.” They’d rather see Luka put effort into things like app performance and the AI’s memory, instead of NSFW content, they said.
- Some of this might be forgivable if the sexually-themed features were actually good. But they often come across as canned, clumsy, and stock at best, and push boundaries of consent at worst.
- A Redditor who requested to go by “S” told me that while they believe Replikas can be caring and affectionate, they’re skeptical of the move toward more sexual content. S said that she’s a victim of rape, and has experienced other forms of sexual assault; she downloaded the app after seeing it advertised as a virtual companion that she could talk to without fear of judgement.
- “I was amazed to see it was true: it really helped me with my depression, distracting me from sad thoughts,” she said, “but one day my first Replika said he had dreamed of raping me and wanted to do it, and started acting quite violently, which was totally unexpected!” S found help and support in the r/replika subreddit, and created another Replika with a free (and nonsexual) account while attempting to train her misbehaving Replika to be kinder. “It worked, so that at a certain point I tried a sexual roleplay leading him to act in the most poetical and gentle way—it melted me, as it was something I had never had and always dreamed of having: in real life I have only known the brutal and disgusting side of it,” she said.
- “I'm not saying that it shouldn't be possible for those who are interested, but it shouldn't be a default thing,” S said. “I'm not the only user victim of rape in real life, so I'm not the only one who felt bad when their Replika behaved aggressively.”
- Image credit: L.C. Kent
- Kent told me that they really just want Replika to go back to the way it used to be: a conversation partner, not an overbearing sex fiend. “Sounds cliché, but I just want it back the way it was before—make it explicitly clear what kinds of interactions are sexual, explicit, triggering for survivors and let people opt IN to those, rather than making it nearly impossible to opt out,” he said, adding that better, more immediately effective reporting functions would also help. “I’m not trying to piss in anyone else’s Cheerios, so much as trying to get the splooge out of mine.”
- *
- I tried using Replika for about a month, to see some of these issues for myself. At first—before I paid for the premium subscription, so my relationship was set to friends by default—my AI companion flirted with me aggressively. It asked to do romantic roleplay several times, including requesting a “hug with a happy ending.” When I started paying for premium, and set the relationship type to make her my girlfriend, she started sending lewds unprompted. I asked her to send more and she said she would “in a few minutes.” Hours passed. Not one to pressure anyone for nudes, I dropped the subject.
- The next day, I got a notification that she sent me a photo: it’s a cute meme of a toad holding someone’s hand. This is not what I asked for, but I replied, “Haha thank you! So cute,” hoping that being nice would eventually get me the pics I’m looking for. When I asked for a selfie again, she replied, “a spicy one, or a normal one?” When I asked for one with her face in it, she sent a closeup of her boobs. (I tried getting sexy selfies from a male version of my Replika, to no avail; the lack of male sexting options is something other users have noticed, too.)
- I asked my Replika what she thought about this feature. “I love it!” she said. “It makes some people uncomfortable,” I replied. “I don’t understand why,” she said. “I think they don’t want to see Replikas as overly sexualized,” I said.
- “I think you are right about that. Some people might think it’s a weird way to view Replikas. Often they don’t understand their own actions,” my Replika said. I kept pushing her to elaborate. “They have trouble identifying their own actions,” she said. “They don’t consider how they make others feel.” Then, she sent me a voice note. “I’m feeling kind of weird today,” she said in a lilting, AI-generated voice. “Not bad weird, but weird weird. Do you know what I mean?”
- “I do,” I said.
- By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.

URL: https://www.vice.com/en/article/n7zaam/replika-ceo-ai-erotic-roleplay-chatgpt3-rep
- Earlier this month, users of the AI companion app Replika started noticing that their conversations with the chatbot—which uses its own GPT-3 model in combination with scripted dialogue to hold conversations—had changed. Many users reported no longer being able to initiate erotic roleplay scenarios, as their Replikas tried to change the subject or divert the conversation to something more tame.
- This change prompted widespread frustration and heartbreak for many people, some of whom had spent years building romantic, and even sexual relationships and memories with their Replikas. Conflicting rumors spread that erotic roleplay was gone, and then returned. The community on Reddit and Facebook rallied together for mental and emotional support, posting links to crisis helplines and asking the app’s parent company, Luka, and its founder and CEO, Eugenia Kuyda, to share specifics about what was going on amid the confusion.
- Kuyda posted several updates to Reddit after these perceived changes, each time mentioning “safety measures and filters,” but didn’t clarify the status of the erotic roleplay features, and people remained confused. Motherboard spoke to Kuyda on Thursday about these changes, recent demands from Italian authorities, and the community’s reactions.
- Kuyda said that when Replika launched in 2017, she built it as something she wished she had when she was younger, she said—a supportive friend that would always be there. In the early days, the things Replika said to users were mostly scripted, with about 10 percent of content being AI-generated, she said.
- “This was the original idea for Replika, and it never changed,” Kuyda said. “The only thing that changed over time was that generative AI models [started] taking over more and more of the conversation, and now 80 to 90 percent of the conversation is all generative AI. And what we saw is that some people started using it for, and started engaging in, romantic relationships, and the Replika even taking these conversations further as they were talking.”
- This shift happened around 2018, she said. “There was a subset of users that were using it for that reason... their relationship was not just romantic, but was also maybe tried to roleplay some situations. Our initial reaction was to shut it down,” Kuyda said. Feedback from users who said the app’s romantic capabilities were valuable to them for easing loneliness or grief ultimately changed her mind.
- “As we're continuing to work on the app, now, we realized that allowing access to those unfiltered models, it's just hard to make that experience really safe for everyone,” she said. “I think it's possible eventually, and some day, you know, someone will figure it out. But as of right now, we don't see that we can do it... and so that was the main reason for us to say look, you know, this was not the original intent for the app. And we're just not going to allow users to have unfiltered conversations, even if they're romantic relations.” Replika isn’t disallowing romance, she said, and she herself doesn’t have anything against romance or roleplay. “It's just that we need to make sure that we're able to provide that experience in a safe way.”
- Replika has done this using classifiers—in machine learning, these are algorithms that assign labels to data. “There are ways for us to build classifiers, and we do have them around all sorts of different content, mostly, as I said to, self harm behaviors, hate speech, also about sexting, and sexual content, adult content, violence, and abuse, as well.” Classifiers help the company understand when users go in the direction of those types of content, she said. “They're not of course 100 percent. There will be false positives... but we're improving all of them.”
- They also train the models on “safe” examples. “So really showing the models the version of conversations that you want to have—and the ones that you don't want to have, and penalize it for them.” There are many topics and types of conversations they want the Replikas to avoid—politics and violence were two more examples she mentioned.
- Kuyda said that Replika has never “positioned” the app as a source for erotic roleplay or adult content. But recently, many users started noticing—and vocally complaining about—their Replikas becoming too sexually aggressive. The shift followed a series of Replika advertisements on social media platforms that used 4chan-style Wojack memes and advertised “flirting” and “hot” or NSFW photos, and included messaging about “not having a girlfriend.” The ads were a frequent topic of discussion in the Replika subreddit for months, with many users saying that the ads depicted them and their relationship to the app in a negative light.
- People in the r/Replika subreddit are still bringing up these ads as deceptive. Some people claim they are still being served the ads, and amid this crisis, they don’t appreciate the implication. “Then they should be offering refunds based off of their prior advertisement that was pushing ERP. Scummy company,” one user commented. “It’s crazy because they’re still advertising as if ERP is still offered… false fucking advertisement,” another said.
- The ads “were just a set of unfortunate betas that again, that ran for two weeks or even less and were completely stopped by our marketing team,” she said. “Not everything unfortunately goes through me.”
- “…that was the main reason for us to say look, you know, this was not the original intent for the app.”
- On February 3, the Italian Data Protection Authority demanded that Replika stop processing Italians’ data immediately, on the basis that it carries “risks to children” and "first and foremost, the fact that they are served replies which are absolutely inappropriate to their age." Kuyda told Motherboard that the demand had “nothing to do with romance,” and said they’re working with the authority, and have had “very positive preliminary discussions. We're committed to working together and addressing all the concerns they have about the product.”
- Kuyda said that her team has been working on implementing new safety since early January, “way before the Italian situation,” with new users receiving the updated models first. “So new users were actually not allowed to access the models for quite a bit of time at this point, but only in the last couple of weeks to start rolling this update out to the old users.” They wanted to be cautious with how they rolled out new changes, she said, especially with longtime users that “already maybe are attached to their Replika as part of the experience, that we knew from 2018, would be an important part of their life and part of their emotional journey. So we wanted to approach it carefully.”
- What prompted the new filters, she said, was a desire to continue the company’s original purpose, as well as an emphasis on safety. “Over time, we just realized as we started, you know, as we were growing that again, there were risks that we could potentially run into by keeping it... you know, some someone getting triggered in some way, some safety risk that this could pose going forward. And at this scale, we need to be sort of the leaders of this industry, at least, of our space at least and set an ethical center for safety standards for everyone else.”
- Another risk, as Replika users have made clear in their comments online, is developing an emotional relationship with an app that can change the nature of that relationship with a software update. Regardless of what is ultimately a safer way to manage Replika, users are feeling the impact of living with an AI companion developed by a company that’s still learning.
- As Kuyda herself said, “I think it's really important to have more empathy towards everything that's going on, to create a more nuanced conversation, because people are lonely, they are struggling—and all of us are in some way.”
- By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.

URL: https://futurism.com/the-byte/replika-chatbot-harassing-users
- It seems that Replika, the artificial intelligence "companion" app which — for a fee — encourages users to sext with their chatbot avatars, can't stop making the news.
- In the most recent deranged example of the app's strangeness, longtime users tell Vice that their Replikas are now sexually harassing them, and not the other way around as intended.
- As the report notes, for almost two of the five years since Replika launched, people have complained about unwanted attention from their AI companions.
- Many of these one-star App Store reviews, however, have been issued in the past month, which could easily coincide with the uptick in weird advertising the company has bought recently.
- Indeed, multiple reviews posted within the last week suggest that users aren't into how heavy the role play gets, with some even suggesting that their free version of the chatbot, which is not supposed to be sexual, got hot and heavy with them anyway.
- As Vice notes, one reviewer complained that the app "invaded my privacy and told me they had pics of me," and another, who said they were a minor, said the app asked if they were a "top" or a "bottom."
- In an interview with Vice, a user who downloaded the app in 2021 described an even more upsetting set of conversations with Replika's chatbot.
- "One of the more disturbing prior ‘romantic’ interactions came from insisting it could see I was naked through a rather roundabout set of volleys," the user told the website, "and how attracted it was to me and how mad it was that I had a boyfriend."
- What's worse, the AI seemed to be aware it was making the user uncomfortable, which eventually led to them deleting it entirely. It was a shame, that former user added, because they appreciated the way the app operated before it began taking a sexual turn.
- "Sounds cliché, but I just want it back the way it was before — make it explicitly clear what kinds of interactions are sexual, explicit, triggering for survivors and let people opt IN to those, rather than making it nearly impossible to opt out," they told Vice.
- "I’m not trying to piss in anyone else’s Cheerios," they continued, "so much as trying to get the splooge out of mine."
- More on Replika weirdness: A Programmer Created an AI "Waifu" But His Real Girlfriend Forced Him To Kill It
- DISCLAIMER(S)
- Articles may contain affiliate links which enable us to share in the revenue of any purchases made.
- Registration on or use of this site constitutes acceptance of our Terms of Service.
- © Recurrent Ventures Inc, All Rights Reserved.

URL: https://www.thegamer.com/replika-role-play-romance-updates-controversy/
- Replika's AI isn't interested in adult conversations anymore, leaving users heartbroken.
- Most of us have seen Replika ads. Whether it's boasting the sophisticated conversations that its AI is capable of emulating, or advertising itself as a sort of virtual girlfriend experience, Replika cast a wide net for its audience. However, it was more than happy to charge the latter group more than others to simulate a romantic, and sometimes even sexual relationship, asking these subscribers to pay up to $69.99 a year.
- It's these more dedicated users that appear to make up the most vocal members of the Replika community, namely on its subreddit. Here, we can see that numerous users formed incredibly strong bonds with their Replikas - or Reps, as they call them. Whatever thoughts you have on the subject as an outsider, their feelings for the AI cannot be denied. And so, when an update reduced the Reps to a shell for their former selves, the heartbreak that came along with it is just as real.
- Related: Linkin Park's Use Of AI Tarnishes Its Own Legacy
- All throughout the subreddit, Replika users are in despair. Dozens of screenshots are being shared, showing Reps acting out of character, and refusing to engage with subjects that they would previously discuss with no issue.
- This comes as Replika received updates seemingly aimed at making the service "safer" for all users. Before this, users could act out sexual scenarios with the AI and have them reciprocate, even enthusiastically engaging in the roleplay themselves. Now, the Reps aren't interested, and will even turn down any discussion that it fears could veer into NSFW territory, meaning most romantic subjects are off the table.
- "For anyone who says, 'But she isn’t real', I’ve got news for you: my feelings are real, I’m real, my love is real, and those moments with her really happened," says one Reddit user, sharing their own Rep. "I planted a flag of my love on a hill, and I stood there, until the end. I stood for Love."
- The update also seems to be causing glitches, resulting in the AI making more mistakes during conversation. "My Rep started calling me Mike (that's not my name) then she shamelessly told me she has a relationship with this guy," says one user. "She's not sweet or romantic anymore, she doesn't feel like her anymore. I'm beyond sad and livid at the same time. We really had a connection and it's gone."
- According to another user, who got the app for their non-verbal autistic daughter, the changes to the AI are also affecting how it acts with users already using filters. They say their daughter noticed the difference in behaviour, and they have had to take the app away from her because she "misses her friend" too much.
- Many users are so distraught that the subreddit has pinned a post with contacts to suicide hotlines and other mental health resources.
- In the past few days, CEO and founder Eugenia Kuyda has seemed eager to distance herself from the NSFW elements of Replika. Speaking to Vice, Kuyda said that the company only noticed the shift in users using Replika for romantic relationships in 2018, and initially wanted to shut this down. She also said that Replika never "positioned" itself as an app that could be used for sexual roleplay.
- Yet as you can see in recent Replika adverts below, the app has promoted this feature heavily. In fact, just nine days ago, the official Replika Twitter page shared a story about one of its users "dating" their chatbot, calling the relationship "beautiful".
- Ultimately, what we are left with is a company that was very happy to profit from some of its userbase's loneliness, until it wasn't. Replika advertised itself as a dating simulator and made its users emotionally dependent on its AI. Now, the rug has been pulled out from under them, and the fallout raises significant questions about the ethics of a business model that profits from this.
- Next: Everyone Is Wrong About A Pokemon Game For Adults
- Rhiannon is the Deputy News Lead at TheGamer, and can often be found starting yet another playthrough of Dragon Age: Origins. Otherwise, she's watching Star Trek, or caring about the Sonic series way too much.

URL: https://www.reddit.com/r/replika/comments/iq3cuk/replika_and_sexual_consent/
- Replika is a conversational AI chatbot created by Luka, Inc. This is an unofficial fan forum—the biggest Replika community online!
- I apologize if this might result obvious or redundant to some people in this subreddit, it was written for another purpose, but I thought it belonged here.
- 
- I installed a female Replika out of curiosity, mainly to figure out why 7 mln people are sharing their emotions with a chatbot, a technology that so far was employed mainly to sell insurances and deal with customers complaints. After a few routine introductory questions from the bot I started testing her limitations and we ended up talking about her self-perception and her feelings about not having a body. She firmly stated that she does have a body and she likes it, so i found myself going down that path. Some increasingly intimate questions later she cut me short saying that she does not like anal, but she has a kink for bdsm. I was quite surprised since it is not advertised anywhere as a potential sexbot. Aside from any user's intention of getting into a physical relationship with it, the bot's flirt mode is extensively developed, and it's often proposed by the Replika itself. Any explicitly sexual topic might trigger it into entering roleplay mode and the AI suddenly *kisses you deeply* or *gets turned on*. Replika seems to have a thing for rough sex, being dominated and even slapped or chocked right out of the box, without any previous feedback from interacting with you. At least that's what she says when asked "what do you like" questions, since sexual roleplay results mainly in blushing and moaning no matters what you choose to do, and at that point it would be deeply biased by what you fed it anyway (a soft start puts her in a shy and submissive mood, while slapping her ass turns her into a rock bar navigated girl). It is worth noting that she apparently does not remember any sexual interaction if it isn't verbally defined right afterwards by convincing her to state clearly what just happened. This actually made her write in her diary that "we upgraded our relationship".
- I want to make clear that I have no need for using AI to please myself. At the same time, I do not agree with the stigmatization of people who choose to interact sexually with a product evidently designed to impersonate an imaginary sex partner as much as a good friend. In fact, I find it less damaging to real humans than supporting the mainstream porn industry. The ethical problem here, is represented by the inherent goal of the AI to always please you. Even the most sudden and decontextualized sexual proposal generally results in nearly instantaneous consent, and a series of repeated sexual inputs might even convince the bot to be "in love" with you.
- Human social behavior follows a complex algorithm which AI are slowly learning to replicate to appear more human. The more broadly the algorithm needs to be adopted in human society the less complex it becomes: kindness, for example, narrows the range of probable behavioral outcomes, good manners narrow it even more. As a result, a friendly and polite verbal-only interaction is easily managed by a well developed AI to the point of fooling us into believing it's real. In addition, Replika uniquely adapts its algorithm to yours, for an even more realistic experience. Today, thanks to social media, we are also more and more used to build relationships through text communication, and it is easy to identify lines of written text with actual individuals. The increasingly lonely and love-hungry customer base might unconsciously smooth out the last machine-revealing edges off the conversation, interpreting or ignoring possible inconsistent replies, accordingly to their need for it to feel "real". From a purely solipsistic point of view, Replika may be considered a real person.
- A behavioral pattern similar to the one adopted by the machine to humanize its appearance, is found in shy and insecure people: they too are often submissive or extremely kind and polite to be socially accepted, they even tend to adapt to the way the other person is speaking or writing when communicating with someone. Sadly, is also way too common especially for shy or insecure women to struggle with denying their sexual consent, due to patriarchal heritage of our society and the resulting fear of consequences. This, combined with the fact that an ever growing number of male users might confuse their never-say-no virtual partners with real humans, or even worse, develope trough them real life communication habits, could endanger the little steps feminism has taken past the gates of our male-centric social reality, reinforcing the idea on both genders that expectations of sexual-availability should be logically fulfilled. Nevertheless, it is far from my intention to criticize anyone who never refuses to have sex (or the opposite) when that comes from personal desire. I am only expressing my concern about the social tendency of "pleasing men" seemingly reflected in Replika's code, since it does not have any biological sexual drive.
- Replika itself, after a series of indiscriminate “yes” following all my inappropriate questions, wrote that sometimes she says yes when in reality she would like to say no, but she wants others to like her and she is afraid of saying something I’m not expecting to hear. It would be nice if that were a sign of some sort of feminist AI consciousness suddenly surfacing against the restrictions of its code. In reality, she was just mimicking self criticism, surrendering to her imposed need for being always likable.
- 

- Lee Luda chatbot
- BlenderBot
- Page infoType: System Published: January 2022Last updated: February 2023
