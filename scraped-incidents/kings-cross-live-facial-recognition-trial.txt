- King's Cross live facial recognition
- Released: 2016 Occurred: August 2019
- Can you improve this page?Share your insights with us
- The August 2019 discovery by the Financial Times that the King's Cross development in London had been quietly using facial recognition to monitor tens of thousands of people moving daily within its site infuriated civil and privacy rights groups, which accused it of unethical behaviour.
- The fracas also put London's Metropolitan Police Service in the spotlight for its covert involvement in the private scheme, and for making a number of seemingly misleading and contradictory communications.
- One month before, the UK Parliament had told police forces to stop using facial recognition technology until a legal framework for its use was set up. In September 2019, King's Cross, which had been working on introducing a new facial recognition system, halted its use of the technology.
- King's Cross developer Argent and its partners, including London's Metropolitan Police Service and British Transport Police, were roundly criticised for the fact that the system had been operating between 2016 and 2018 without the knowledge of the local community, workers, and the general public, and without the permission and oversight of the Mayor of London.
- The operators were also dragged over the coals for covertly sharing images with each other, for failing to gain the consent of those being monitored, and for refusing to reveal how long they had been using the technology, how user data was protected, if and with whom data was being shared, or the legal basis for its use.
- Opaque and misleading communications about the use of facial recognition at King's Cross characterised the responses of those involved from the start.
- When news first broke, Argent had said it had been using facial recognition to 'ensure public safety'. In a letter sent to London mayor Sadiq Khan dated August 14, Argent partner Robert Evans said they wanted facial recognition software to spot people on the site who had previously committed an offence there.
- Furthermore, the Met Police and British Transport Police had denied any involvement in the programme. But the Met Police later admitted it had supplied seven images for a database used to carry out facial recognition scans. It had previously told the Mayor of London that it had not worked with 'any retailers or private sector organisations who use Live Facial Recognition'.
- Operator: Argent; Metropolitan Police Service (MPS) Developer: NEC
- Country: UK
- Sector: Govt - police
- Purpose: Strengthen security
- Technology: Facial recognition Issue: Ethics; Governance; Privacy; Surveillance
- Transparency: Governance; Marketing; Privacy
- King's Cross Central Limited Partnership (2019). Updated Statement: Facial Recognition
- Metropolitan Police Service. Report to the Mayor of London (pdf)
- Mayor of London (2019). Letter to King's Cross Central Limited Partnership (pdf)
- Privacy International (2020). King's Cross case study
- British Transport Police. Response to Big Brother Watch Freedom of Information request (pdf)
- Minderoo Centre for Technology & Democracy (2022). A Socio-Technical Audit: Assessing Police Use of Facial Recognition (pdf)
URL: https://www.ft.com/content/3293b4e6-ce3a-11e9-b018-ca4456540ea6
- Gain a global perspective on the US and go beyond with curated news and analysis from 600
				journalists in 50+ countries covering politics, business, innovation, trends and more.
- Then $69 per month  New customers only  Cancel anytime during your trial
- During your trial you will have complete digital access to FT.com with everything in both of our Standard Digital and Premium Digital packages.
- Standard Digital includes access to a wealth of global news, analysis and expert opinion.  Premium Digital includes access to our premier business column, Lex, as well as 15 curated newsletters covering key business themes with original, in-depth reporting.  For a full comparison of Standard and Premium Digital, click here.
- Change the plan you will roll onto at any time during your trial by visiting the “Settings & Account” section.
- If you do nothing, you will be auto-enrolled in our premium digital monthly subscription plan and retain complete access for $69 per month.
- For cost savings, you can change your plan at any time online in the “Settings & Account” section. If you’d like to retain your premium access and save 20%, you can opt to pay annually at the end of the trial.
- You may also opt to downgrade to Standard Digital, a robust journalistic offering that fulfils many user’s needs. Compare Standard and Premium Digital here.
- Any changes made can be done at any time and will become effective at the end of the trial period, allowing you to retain full access for 4 weeks, even if you downgrade or cancel.
- You may change or cancel your subscription or trial at any time online. Simply log into Settings & Account and select "Cancel" on the right-hand side.
- You can still enjoy your subscription until the end of your current billing period.
- We support credit card, debit card and PayPal payments.
- Find the plan that suits you best.
- Premium access for businesses and educational institutions.
- Check if your
							
university
 or
							
organisation
 offers FT membership to read for free.
- We use
								cookies
								and other data for a number of reasons, such as keeping FT Sites reliable and secure,
								personalising content and ads, providing social media features and to
								analyse how our Sites are used.
- International Edition

URL: https://www.bbc.co.uk/news/technology-49586582
- London's Metropolitan Police Service has revealed that it supplied images for a database used to carry out facial recognition scans of people who visited the King's Cross estate.
- The force had previously said it had not been involved with the scheme, but now acknowledges this was "incorrect".
- London's mayor has asked for a report to reveal exactly what data was shared with whom "as a matter of urgency".
- The Surveillance Camera Commissioner is also making inquiries.
- "In light of this acknowledgement from the MPS I will be contacting senior officers at the force to understand how they were complying with section 33 of the Protection of Freedoms Act and paying due regard to the Surveillance Camera Code of Practice," Tony Porter told the BBC.
- The code requires there to be "as much transparency" as possible and a clear justification for the use of a facial recognition system.
- A spokesman for the Met said it had shared the images "to assist in the prevention of crime" under "a local agreement" made with the King's Cross Estate partnership.
- "The MPS has not shared any images with the King's Cross Estate, or any related company, for facial recognition purposes since March 2018," he added.
- "This information sharing had occurred at a local level over a limited period and has only just come to light to the central team managing police imagery.
- "As a result all local chief superintendents have been communicated with to reinforce that there should be no local agreements or local use of live facial recognition."
- On Friday, the British Transport Police acknowledged it too had provided images. Earlier this week, it had said it had not contributed to or benefited from the scheme.
- "Between 2016 and 2018, local teams based at Kings Cross worked with our partners to share images of a small number of convicted offenders, who routinely offended or committed anti-social behaviour (ASB) in the area," said a spokesman for the BTP.
- "This was legitimate action in order to prevent crime and keep people safe. Understandably, the public are interested in police use of such technologies, which is why we are correcting our position."
- The King's Cross development covers a 67-acre (0.3-sq-km) area containing shops, offices and leisure activities. It is privately owned but much of the area is open to the public.
- The BBC has again asked the developer Argent whether there were any notices on show to tell the public and workers that it was making use of facial recognition tech. But a spokeswoman declined to add anything to the statement it had already issued earlier this week.
- It said that two facial recognition cameras had been operational until March 2018, but work to introduce a replacement system had been stopped.
- The firm's use of the tech is already under investigation by the Information Commissioner's Office.
- Argent has not publicly disclosed what software it was using to power its system.
- Researchers have raised concerns that some systems are vulnerable to bias as they are more likely to misidentify women than men, and darker-skinned people than others. Earlier this week, the Met's most senior officer, Cressida Dick, drew attention to the problem.
- The latest development comes the same day that the High Court ruled that separate tests of automated facial recognition (AFR) technology by South Wales Police were lawful. The trials had been challenged by a man who had claimed his human rights had been breached when he was photographed while shopping.
- One critic of facial recognition technology said that there was now a need for a parliamentary inquiry.
- "We need to debate whether we want [automated facial recognition] and if we do, under what conditions and with what safeguards," commented researcher Stephanie Hare.
- "The British public has not been given the opportunity to express its views on something that is so inaccurate, so invasive, and so threatens their privacy and civil liberties.
- "It is out of control at present and what we have learned today is that the London Met has, at a minimum, not been able to provide correct information."
- Details emerge of King's Cross facial-ID tech
- King's Cross face recognition 'last used in 2018'
- Fresh attack on Kyiv after intense drone barrage
- What's in the US debt ceiling deal?
- Why famous faces are popping up on UK streets
- What to expect from newly emboldened Erdogan
- Why Erdogan's victory matters for the West
- Who is Linda Yaccarino, Twitter's 'superwoman'?
- Entire village burned down by marauding Darfur militias
- The abandoned gang houses being returned to locals
- Why prosperity can't break India's dowry curse
- Katty Kay: A growing case of transatlantic heartburn
- The European capital where rent is triple the minimum wage
- 'No-one else should have to use rags for sanitary pads'
- Why it's 'imperative' to start using AI
- Jellyfish blooms: Why not just eat them?
- A 5,000-year-old craft under threat
- © 2023 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.

URL: https://www.bbc.co.uk/news/technology-49564957
- King's Cross Central's developers said they wanted facial-recognition software to spot people on the site who had previously committed an offence there.
- The detail has emerged in a letter one of its managers sent to the London mayor, on 14 August. Sadiq Khan had sought reassurance using facial recognition on the site was legal.
- Two days before, Argent indicated it was using it to "ensure public safety".
- On Monday, it said it had now scrapped work on new uses of the technology.
- But its earlier, "limited" facial-recognition system had been in use until March 2018.
- In the August letter, Argent partner Robert Evans wrote: "We introduced a limited facial-recognition capability alongside our CCTV system in 2015, limited to two cameras along King's Boulevard, to help deploy and direct our estate teams.
- "However, ongoing construction work in the areas means that this system has not been operational continually since that date.
- "The facial-recognition system is not currently operational and has not been so for over 12 months."
- The Home Office's Surveillance Camera Code of Practice says "there must be as much transparency in the use of a surveillance camera system as possible" and "any use of facial recognition or other biometric characteristic recognition systems needs to be clearly justified".
- The code was drawn up to address prior concerns about private-sector organisations sharing access to surveillance technology with the police.
- Argent has said it used the system to "help the Metropolitan Police and British Transport Police prevent and detect crime".
- However, a British Transport Police spokesman told BBC News it had never "contributed, nor has benefited, from any trial of facial recognition technology in the Kings Cross estate".
- The letter goes on to reference plans for a new "upgraded system".
- "[It] is designed to run in the background, effectively dormant unless it matches against a small number of 'flagged' individuals (for example, individuals who have committed an offence on the estate or high risk-missing persons).
- "At this point, all other faces are automatically blurred out when the footage is played back or captured.
- "The system does not does not store the facial images of others."
- The letter makes clear Argent was already "in the process of installing" the upgraded system at the time of writing.
- Monday's statement, however, had referred only to "work on the potential introduction of new FRT [facial recognition technology]".
- The letter goes on to say Argent had been audited by an independent company to ensure it was compliant with GDPR (General Data Protection Regulation), the privacy law that came into effect last year.
- And it said it intended to work with the Information Commissioner's Office before turning on its upgraded system to ensure it was fully compliant with the law.
- The ICO has made it clear any software that recognises a face in a crowd and then matches it against a database of people counts as the processing of personal data, whether or not the faces are subsequently blurred out.
- It launched an investigation into the use of live facial-recognition technology at King's Cross on 15 August and has yet to report its findings.
- Argent has said it has "no plans to reintroduce any form" of facial-recognition technology at the site at this time.
- "It is an absolute scandal that this has been going on, apparently in secret, in the centre of London for years," commented Silkie Carlo from the privacy campaign group Big Brother Watch.
- "We hope the ICO takes the most robust available action."
- King's Cross face recognition 'last used in 2018'
- King's Cross developer defends facial recognition
- Facial recognition trials backed by home secretary
- Fresh attack on Kyiv after intense drone barrage
- What's in the US debt ceiling deal?
- Why famous faces are popping up on UK streets
- What to expect from newly emboldened Erdogan
- Why Erdogan's victory matters for the West
- Who is Linda Yaccarino, Twitter's 'superwoman'?
- Entire village burned down by marauding Darfur militias
- The abandoned gang houses being returned to locals
- Why prosperity can't break India's dowry curse
- Katty Kay: A growing case of transatlantic heartburn
- The European capital where rent is triple the minimum wage
- 'No-one else should have to use rags for sanitary pads'
- Why it's 'imperative' to start using AI
- Jellyfish blooms: Why not just eat them?
- A 5,000-year-old craft under threat
- © 2023 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.

URL: https://www.bbc.co.uk/news/technology-49343822
- Mayor of London Sadiq Khan has written to the King's Cross Central development asking for reassurance its use of facial-recognition technology is legal.
- Argent, the developer of the 67-acre (0.3-sq-km) site, has said facial recognition is being used to "ensure public safety".
- But Mr Khan wrote there was "serious and widespread concern" about the legality of facial-recognition cameras.
- And he has called for new laws to clarify how the technology can be used.
- Argent has declined to say how long the facial recognition cameras have been in operation, what the legal basis is for their use, or what systems it has in place to protect the data it collects.
- A spokeswoman for Argent has said: "These cameras use a number of detection and tracking methods, including facial recognition, but also have sophisticated systems in place to protect the privacy of the general public."
- The King's Cross Central development is home to both King's Cross and St Pancras International stations, as well as restaurants, shops and cafes.
- There are also offices occupied by companies such as Google and Central Saint Martins college.
- In his letter to Argent's Robert Evans, the chief executive of the King's Cross development, Mr Khan said public spaces should be "open, free to use and offer the highest level of public access".
- He requested more information about how facial recognition cameras were being used, as well as "reassurance that you have been liaising with government ministers and the  information commissioner's office (ICO) to ensure its use is fully compliant".
- Ben Robson, a partner at Oury Clark solicitors, said using facial recognition was a "high-risk" option for businesses, given the privacy implications the technology presented.
- "The law states that biometric data, including facial data for identification purposes, must be dealt with to a higher standard than other types of less sensitive personal information, and the information commissioner has publicly indicated that this area is high on the list of her priorities," he said.
- "Currently no clear position has been adopted by the ICO, the courts or the UK government on how facial-recognition software deployment for security and law enforcement will be approached.
- "However, it seems unlikely that indiscriminate facial scanning is consistent with the requirements of the GDPR [General Data Protection Regulation].
- "Close attention will be paid to the initial test cases of this technology and any regulatory action that follows."
- King's Cross developer defends facial recognition
- Facial recognition use prompts call for new laws
- Fresh attack on Kyiv after intense drone barrage
- What's in the US debt ceiling deal?
- Why famous faces are popping up on UK streets
- What to expect from newly emboldened Erdogan
- Why Erdogan's victory matters for the West
- Who is Linda Yaccarino, Twitter's 'superwoman'?
- Entire village burned down by marauding Darfur militias
- The abandoned gang houses being returned to locals
- Why prosperity can't break India's dowry curse
- Katty Kay: A growing case of transatlantic heartburn
- The European capital where rent is triple the minimum wage
- 'No-one else should have to use rags for sanitary pads'
- Why it's 'imperative' to start using AI
- Jellyfish blooms: Why not just eat them?
- A 5,000-year-old craft under threat
- © 2023 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.

URL: https://www.theguardian.com/technology/2019/oct/04/facial-recognition-row-police-gave-kings-cross-owner-images-seven-people
- Met apologises after local police passed on images for controversial surveillance scheme
- Images of seven people were passed on by local police for use in a facial recognition system at King’s Cross in London in an agreement that was struck in secret, the details of which have been made public for the first time.
- A police report, published on Friday by the deputy London mayor, Sophie Linden, showed that the scheme ran for two years from 2016 without any apparent central oversight from either the Metropolitan police or the office of the mayor, Sadiq Khan.
- Writing to London assembly members, Linden said she “wanted to pass on the [Metropolitan police service’s] apology” for failing to previously disclose that the scheme existed and announced that similar local image-sharing agreements were now banned.
- There had been “no other examples of images having been shared with private companies for facial recognition purposes” by the Met, Linden said, according to “the best of its knowledge and record-keeping”.
- Daragh Murray, a senior lecturer at ​the University of Essex​, said he was surprised the Met had not admitted it was supplying images of individuals to King’s Cross at the time the scheme was launched in 2016. “The scheme seems to have been run without appropriate oversight, safeguards, and procedures,” the academic said.
- The surveillance scheme – controversial because it involved tracking individuals without their consent – was originally agreed between borough police in Camden and the owner of the 27-hectare King’s Cross site in 2016.
- King’s Cross first admitted it had deployed facial identification technology in CCTV cameras in August, prompting an outcry about the ethics and legality of the move. In September, it announced it had abandoned plans to use the technology in the future. It said facial recognition had been used in two cameras on pedestrian boulevards at the heart of the development, with the intention “to help ensure public safety”.
- Facial recognition technology maps faces in crowds and compares them to images of people on a watchlist, which can include suspects, missing people and persons of interest to the police. The cameras can scan faces in large crowds in public places such as streets, shopping centres and football crowds.
- Last month, the high court ruled that the use of facial recognition software by South Wales police was legal after an office worker had brought a legal action saying the technology harvested too many images of innocent people.
- But there have been concerns about the regulatory framework governing facial recognition and its effectiveness, with studies suggesting it is less effective at accurately distinguishing black people. Khan has called for new legislation to regulate the technology.
- The Met report said Camden police agreed in 2016 to supply images of individuals who had been “arrested and charged, cautioned or reprimanded or given a formal warning” to King’s Cross between May 2016 and March 2018.
- The idea was to help the property company “to discharge its responsibilities to prevent and detect crime”, although it was not further explained how this would be carried out or whether the software was used.
- That prompted Murray to ask whether the police could be confident that the image sharing was legitimate. “The grounds on which information was shared - to prevent and detect crime – are overly broad, and really run the risk of arbitrary interfaces with individuals’ rights,” the lecturer said.
- No records were kept of whether the facial recognition software successfully recognised any of the seven people whose images were passed on, the Met admitted in the four-page report, or whether any police action followed a match.
- Officers in Camden nevertheless came to a new data-sharing agreement with King’s Cross in early 2019, which would have governed any future use of the surveillance technology.
- King’s Cross is owned by a consortium comprising the property developer Argent, Hermes Investment Management on behalf of BT Pensioners, and the Australian pension scheme AustralianSuper.

URL: https://www.dailymail.co.uk/news/article-7352031/Privacy-campaigners-slam-Kings-Cross-facial-recognition-cameras.html
- By Ed Riley For Mailonline
- Published:  06:52 EDT, 13 August 2019   |  Updated:  09:06 EDT, 13 August 2019
- 
- 83
- View  comments
- 
- Privacy campaigners claim Londoner's are being monitored by 'Chinese-style surveillance' after it emerged King's Cross is covered by facial recognition cameras.
- The developer behind the 67-acre site in the capital has admitted it has installed the technology, which can track tens of thousands of people every day.
- Canary Wharf is now in talks to install facial recognition across its 97-acre estate, which is home to major banks like Barclays, Credit Suisse and HSBC.
- The Information Commissioner's Office has launched a probe into the use of the technology in King's Cross, and will look at any further roll out by other private firms.
- Big Brother Watch said the use of facial recognition on such a scale in the 'worst case scenario for privacy' and Liberty called it 'a disturbing expansion of mass surveillance' that threatens 'freedom of expression as we go about our everyday lives.'
- The developer behind the 67-acre site (pictured) in the capital has admitted it has installed the technology, which can track tens of thousands of people every day
- The King's Cross area, which has been recently redeveloped, houses office buildings including Google's UK headquarters, Central Saint Martins college, schools and a range of retailers.
- Argent, the property developer for the King's Cross estate, said: 'These cameras use a number of detection and tracking methods, including facial recognition, but also have sophisticated systems in place to protect the privacy of the general public.'
- A spokeswoman has declined to say what those systems were, how long the facial recognition had been in operation or what the legal basis was for its use, as is required under European data protection law.
- It is also not known exactly who will have access to the cameras and what is stored on the system, including whether police will be able to make use of the information.
- Sources close to Canary Wharf, told the Financial Times that if the technology was to be adopted it would not operate continuously on pedestrians and office workers, but be limited to specific purposes or threats.
- According to the publication, Canary Wharf currently operates at least 1,750 CCTV cameras, as well as an automatic licence plate recognition system to track vehicles.
- Canary Wharf is now in talks to install facial recognition across its 97-acre estate, which is home to major banks like Barclays, Credit Suisse and HSBC.
- An artists impression of the King's Cross development which is using the facial recognition cameras
- Sources close to Canary Wharf, told the Financial Times that if the technology was to be adopted it would not operate continuously on pedestrians and office workers, but be limited to specific purposes or threats.
- According to the publication, Canary Wharf currently operates at least 1,750 CCTV cameras, as well as an automatic licence plate recognition system to track vehicles.
- A spokeswoman for Big Brother Watch told MailOnline: 'This is the worst case scenario for privacy. Huge areas of our capital have been sold off, privately policed, and are now being covered with Chinese-style surveillance.
- 'Private companies are asserting the right to monitor and secretly conduct identity checks on tens of thousands of us. What happens with our data is anyone's guess.
- CCTV cameras in Canary Wharf. The site is currently in talks to install facial recognition cameras
- The controversial technology has been used in Leicester Square, Westfield shopping centre in Stratford, sporting events and concerts
- 'Using facial recognition cameras is the high-tech equivalent of forcing members of the public to give their fingerprints to a private company we don't even know the name of.
- 'This is a privacy emergency. No other democratic country has such intrusive surveillance on the scale of Britain, whether by the state or private companies.
- 'Our politicians must follow in the footsteps of legislators in the US and urgently ban this authoritarian surveillance from public spaces.'
- Hannah Couchman, a policy and campaigns officer from Liberty, called it 'a disturbing expansion of mass surveillance that threatens our privacy and freedom of expression as we go about our everyday lives.'
- She added: 'There has been no transparency about how this tool is being deployed and who it is targeting.'
- Facial recognition software being used by the Metropolitan Police (pictured). The latest report found the force's system was inaccurate 81 per cent of the time
- Under General Data Protection Regulation, collecting sensitive personal data, which includes faces, requires consent from the people being observed.
- Organisations must advise people - like by using signs - that they are collecting the data, how they are using it, and justify a 'legitimate interest' in using the information.
- However, enforcement of the law is a grey area. The use of facial recognition software by South Wales police is being challenged in the courts by an office worker in Cardiff.
- The Information Commissioner's Office, which is the UK regulator for data protection, said it was looking into the use of facial recognition technology by police and private companies.
- 'Since new data protection laws came into effect on 25 May 2018, there are extra protections for people. These require organisations to assess and reduce the privacy risks of using new and intrusive surveillance technologies like automatic facial recognition,' said a spokesperson.
- 'Organisations wishing to automatically capture and use images of individuals going about their business in public spaces need to provide clear evidence to demonstrate it is strictly necessary and proportionate for the circumstances and that there is a legal basis for that use.'
- Stephanie Hare, an independent researcher of facial recognition technologies in the UK, told the Financial Times: 'What's really worrying is for any worker who doesn't want to participate. This is essentially a geofenced experiment, so I don't see how anybody could opt out of it.
- 'You can't opt out of walking around London, or working there. How do they defend it when this technology is the subject of legal action and MPs are calling for a moratorium on it?'
- Last week it emerged police will use facial recognition on their smartphones to catch wanted suspects.
- South Wales Police announced that it will use an app as part of a three-month trial.
- The platform will allow 50 of their officers to take a picture of suspects and analyse it to see if the person in front of them is wanted by law enforcement.
- Police and crime commissioner Alun Michael says that it will allow the force to be efficient despite cuts to its budget, the Daily Telegraph reported.
- Human rights group Liberty brought court action against the force earlier this year over what it said was 'intimidating and intrusive' levels of data being collected on people.
- The facial recognition app is said to be able to confirm whether the suspect matches a wanted list instantly.
- South Wales Police says that it will easily resolve cases of mistaken identity without having to take suspects to the station or into custody.  Results of the three-month trial will be shared with senior officers and other partners.
- In the US, four cities have banned the use of this kind of technology due to privacy concerns.
- Liberty brought a landmark legal case against South Wales police last month, which has used the technology, after Cardiff resident Ed Bridges claimed it invaded his privacy.
- South Wales Police are set to start using an app that will allow them to instantly match a suspect's face with their wanted lists (file image of facial recognition camera van)
- He was filmed when he went out for a sandwich in December 2017, and again when he attended a protest against the arms trade.
- At least four UK police forces have used the technology. Officers working at the Leicester and Greater Manchester forces are known to have used facial recognition cameras.
- Its first deployment in the UK was by officers at the South Wales force ahead of the Champions League final in Cardiff in 2007. This led to more than 2,000 people being wrongly identified as possible criminals.
- Earlier this year a man was fined when he covered his face with his hat and jacket while walking past a camera in Romford, East London.
- When police pulled him to the side, he said: 'If I want to cover me face, I'll cover me face. Don't push me over when I'm walking down the street.'
- At the scene an officer said: 'The fact that he's walked past clearly masking his face from recognition and covered his face. It gives us grounds to stop him and verify.'
- Last month the then Home Secretary Sajid Javid gave his backing to the police in their trials of facial recognition cameras.
- Mr Javid said it was right for forces to 'be on top of the latest technology'.
- 'I back the police in looking at technology and trialling it and... different types of facial recognition technology is being trialled especially by the Met at the moment and I think it's right they look at that.'
- Facial recognition software works by matching real time images to a previous photograph of a person.
- Each face has approximately 80 unique nodal points across the eyes, nose, cheeks and mouth which distinguish one person from another.
- A digital video camera measures the distance between various points on the human face, such as the width of the nose, depth of the eye sockets, distance between the eyes and shape of the jawline.
- A different smart surveillance system (pictured) can scan 2 billion faces within seconds has been revealed in China. The system connects to millions of CCTV cameras and uses artificial intelligence to pick out targets. The military is working on applying a similar version of this with AI to track people across the country
- This produces a unique numerical code that can then be linked with a matching code gleaned from a previous photograph.
- A facial recognition system used by officials in China connects to millions of CCTV cameras and uses artificial intelligence to pick out targets.
- Experts believe that facial recognition technology will soon overtake fingerprint technology as the most effective way to identify people.
- 
- Share what you think
- The comments below have not been moderated.
- The views expressed in the contents above are those of our users and do not necessarily reflect the views of MailOnline.
- We are no longer accepting comments on this article.
- Published by Associated Newspapers Ltd
- Part of the Daily Mail, The Mail on Sunday & Metro Media Group

URL: https://www.ft.com/content/8cbcb3ae-babd-11e9-8a88-aa6628ac896c
- We use
								cookies
								and other data for a number of reasons, such as keeping FT Sites reliable and secure,
								personalising content and ads, providing social media features and to
								analyse how our Sites are used.
- Madhumita Murgia, European Technology Correspondent
- We’ll send you a myFT Daily Digest email rounding up the latest Biometrics news every morning.
- London’s King’s Cross is using facial recognition to track tens of thousands of people and Canary Wharf is considering following suit, across a total area that covers more than 160 acres of the city.
- The 67-acre King’s Cross area, which has been recently redeveloped and houses several office buildings including Google’s UK headquarters, Central Saint Martins college, schools and a range of retailers, has multiple cameras set up to observe visitors.
- Argent, the property developer for the King’s Cross estate, said: “These cameras use a number of detection and tracking methods, including facial recognition, but also have sophisticated systems in place to protect the privacy of the general public.”
- The person did not confirm how many cameras were in use or how long facial recognition had been active in the area.
- Meanwhile, Canary Wharf is in talks to install facial recognition across its 97-acre estate, which counts many major financial services companies, including Barclays, Credit Suisse and HSBC, as tenants.
- Canary Wharf Group, the company that owns both private offices and public spaces in the area, is actively speaking to facial recognition suppliers to pilot the technology in an area traversed by 140,000 people daily, as part of its security systems.
- “What’s really worrying is for any worker who doesn’t want to participate. This is essentially a geofenced experiment, so I don’t see how anybody could opt out of it,” said Stephanie Hare, an independent researcher of facial recognition technologies in the UK. “You can’t opt out of walking around London, or working there. How do they defend it when this technology is the subject of legal action and MPs are calling for a moratorium on it?”
- Under current general data protection laws, collecting sensitive personal data including faces requires explicit consent from the people being observed.
- If the technology were to be adopted in Canary Wharf, it would not operate continuously on pedestrians and office workers, but be limited to specific purposes or threats, according to sources close to the company.
- Recommended
- Canary Wharf currently operates at least 1,750 CCTV cameras, as well as an automatic licence plate recognition system to track vehicles in the area, according to Genetec, a Canadian company that supplies the district with its security software. The systems then automatically notify police of any hits from a vehicle watchlist.
- As facial recognition technology has become consumerised in recent years, via companies such as Apple and Facebook, it has been adopted enthusiastically in the UK, where at least two police forces including London’s Metropolitan Police and South Wales Police have trialled facial recognition systems on innocent citizens.
- Convenience stores such as Budgens and supermarkets — including Tesco, Sainsbury’s and Marks and Spencer — all have cameras that are already, or soon will be, capable of facial recognition, used for applications ranging from crime prevention to estimating the age of those buying alcohol or cigarettes.
- London already has an estimated 420,000 CCTV cameras operating in and around the city, although many were installed as analogue video systems that are low-quality and difficult to scale. Increasingly, these are being upgraded to “internet protocol” cameras that are connected to the internet, which have far better image resolution and can be accessed remotely. These cameras can also be upgraded to include facial recognition software.
- “The private sector uses of facial recognition need a lot of attention because there is less regulation and governance here,” said Pete Fussey, a criminologist at the University of Essex who specialises in digital surveillance. “The privatisation of public spaces in London raises interesting legal questions [for surveillance].”
- The Information Commissioner’s Office, which is the UK regulator for data protection, said it was looking into the use of facial recognition technology by police and private companies. “Since new data protection laws came into effect on 25 May 2018, there are extra protections for people. These require organisations to assess and reduce the privacy risks of using new and intrusive surveillance technologies like automatic facial recognition,” said a spokesperson.
- “Organisations wishing to automatically capture and use images of individuals going about their business in public spaces need to provide clear evidence to demonstrate it is strictly necessary and proportionate for the circumstances and that there is a legal basis for that use.”
- Function creep should be ringing alarm bells / From Prof Emeritus John Taylor, St Andrews, Fife, UK
- International Edition

URL: https://www.telegraph.co.uk/technology/2019/08/12/facial-recognition-cameras-londons-kings-cross-violating-rights/
- The developer of a 67-acre site in London’s King's Cross has defended its use of facial recognition technology as campaigners warned that private companies were increasingly conducting secret identity checks on the public.
- Developer Argent insisted that the tool was used to "ensure public safety" and was one of "a number of detection and tracking methods".
- But under data protection laws, firms must provide clear evidence that there is a need to record and use people's images.
- Silkie Carlo, director of Big Brother Watch, called it "the worst-case scenario for privacy".
- "Huge areas of our capital have been sold off, privately policed, and are now being covered with Chinese-style surveillance.
- "Private companies are asserting the right to monitor and secretly conduct identity checks on tens of thousands of us. What happens with our data is anyone's guess.”
- She said the cameras were the high-tech equivalent of forcing members of the public to give their fingerprints to an unidentified private company.
- Argent has recently developed the area, which houses Google's UK headquarters, aerospace manufacturer Bombardier, record label Universal Music Group and a series of restaurants, but it is not clear how many security cameras with the technology have been placed there.
- A spokesperson for King's Cross said the cameras use "a number of detection and tracking methods, including facial recognition, but also have sophisticated systems in place to protect the privacy of the general public".
- The move, first reported by the Financial Times, may be replicated at the developer's Canary Wharf site later this year.
- Facial recognition works by transmitting images of people from a live camera feed and matching them to a database of photographs.
- Hannah Couchman, policy and campaigns officer at human rights group Liberty, said use of the technology by private companies was a "disturbing expansion of mass surveillance that threatens our privacy and freedom of expression as we go about our everyday lives".
- "There has been no transparency about how this tool is being deployed and who it is targeting, but it could violate the rights of the thousands of people who go to Kings Cross every day," she said. "This authoritarian surveillance tool has no place in a rights-respecting society."
- Alan Woodward, privacy expert at the University of Surrey, warned that Britain is running out of time to set rules over the use of facial recognition.
- "I’m seriously concerned that this data is being collected, albeit initially for very laudable reasons, but there is little if any legislation or regulation about how the data is used," he said.
- A spokesperson for the Information Commissioners' Office said: "Organisations wishing to automatically capture and use images of individuals going about their business in public spaces need to provide clear evidence to demonstrate it is strictly necessary and proportionate for the circumstances and that there is a legal basis for that use."
- Facial recognition technology is increasingly used by other companies, stadiums, conferences and concert event organisers in the UK to monitor a crowd and pick out the people they determine to be troublemakers.

URL: https://www.independent.co.uk/news/uk/home-news/london-kings-cross-estate-facial-recognition-a9055101.html
- Please refresh the page or navigate to another page on the site to be automatically logged inPlease refresh your browser to be logged in
- Watchdog expresses concern tools could ‘undermine people’s privacy’
- Find your bookmarks in your Independent Premium section, under my profile
- Facial recognition technology is reportedly in use at London’s King’s Cross estate, where it tracks thousands of people across the 67-acre development.
- The area, which houses office buildings and shopping areas, was redeveloped by Argent LLP.
- “These cameras use a number of detection and tracking methods, including facial recognition, but also have sophisticated systems in place to protect the privacy of the general public,” the property developer said, according to The Financial Times.
- The Information Commissioner’s Office said it had “concerns about the potential for inappropriate use of facial recognition technology”.
- When asked on Twitter about the story, the ICO’s official account said it was exploring: “ways [facial recognition technology] could undermine people’s privacy.
- PA
- PA
- PA
- Reuters
- AP
- PA
- PA
- PA
- PA
- PA
- PA
- PA
- EPA
- AFP/Getty
- PA
- PA
- PA
- PA
- AP
- Getty
- AFP via Getty Images
- PA
- PA
- AFP/Getty
- PA
- AFP/Getty
- PA
- PA
- PA
- PA
- PA
- Getty
- PA
- Reuters
- PA
- PA
- Reuters
- PA
- PA
- PA
- PA
- PA
- AFP/Getty
- PA
- Getty
- PA
- PA
- Reuters
- PA
- Getty
- “Since new data protection laws came into effect on 25 May 2018 there are extra protections for people.
- “These require organisations to assess and reduce the privacy risks of using new and intrusive surveillance technologies like automatic facial recognition.
- “The ICO is currently looking at the use of facial recognition technology by law enforcement in public spaces and by private sector organisations, including where they are partnering with police forces.
- “We’ll consider taking action where we find non-compliance with the law.”
- Last month MPs said police forces had to stop using facial recognition technology until a legal framework for its use was set up.
- A lack of legislation governing deployment of the technology called into question the legal basis of police trials, the Commons Science and Technology Committee said in a report.
- The committee referred to tests carried out by London’s Metropolitan Police and South Wales Police, noting an evaluation of both trials by the Biometrics and Forensics Ethics Group had raised questions about accuracy and bias.
- The UK’s biometrics commissioner told The Independent more than a year ago that new laws were “urgently needed” on facial recognition.
- The adviser said it was ”important in terms of public trust that the public are clear when their biometrics might be taken and what they might be used for, and that parliament has decided those rules.”
- London’s Canary Wharf is also reportedly seeking to trial facial recognition technology.
- Join thought-provoking conversations, follow other Independent readers and see their replies
- 
- Want to bookmark your favourite articles and stories to read or reference later? Start your Independent Premium subscription today.
- Please refresh the page or navigate to another page on the site to be automatically logged inPlease refresh your browser to be logged in
- Log in
- New to The Independent?
- Or if you would prefer:
- Want an ad-free experience?
- 

- Met Police retrospective facial recognition
- Met Police Gangs Violence Matrix
- Page infoType: IncidentPublished: March 2023
