- Occurred: January 2021
- Can you improve this page?Share your insights with us
- Reuters reports that digital experts are up in arms about a plan by Lucknow police to monitor women’s expressions with facial recognition to prevent street harassment.
- Lucknow police commissioner DK Thakur told The Times of India that authorities aim to install AI-enabled cameras at 200 'crime hotspots' across the city that will alert nearby police stations when they spot a woman's 'distress' due to harassment. The spots were determined by the 'presence of girls and women in the area,' and stalking and harassment complaints.
- Digital rights activists are concerned the system is likely to be inaccurate and may lead to intrusive policing, surveillance against vulnerable sections of society, and privacy violations. Some feel it may also lead to over-policing in the areas where facial and emotion tracking technologies are being deployed.
- Lucknow police also stand accused of inadequate transparency by failing to consult with the public, and by providing the public with inadequate information on how the technology works, how data is stored, and who can access the data.
- Privacy was declared to be a fundamental right by India's Supreme Court in a landmark ruling in 2017.
- Operator: Lucknow Police Commissionerate Developer: Staqu Technologies
- Country: India
- Sector: Govt - police
- Purpose: Reduce sexual harrassment
- Technology: CCTV; Facial recognition; Emotion recognition; Automated license plate/number recognition (ALPR/ANPR) Issue: Accuracy/reliability; Privacy; Surveillance
- Transparency: Governance; Black box; Privacy
URL: https://www.staqu.com
- Enhance customer experience, ensure safety, and improve operational efficiency with JARVIS, the best video analytics software.
- Fueled with Artificial Intelligence, patented technologies and unique capabilities, JARVIS offers innovation for security and the best possible solution for business.
- JARVIS is an audio and video analytics software as well as an audio-video management technology platform that has changed the way people think about security cameras. JARVIS helps transform long CCTV video footage into meaningful information. The Video Analytics Software enables us to provide short and crisp real-time alerts that are actionable.
- Fueled with Artificial Intelligence, computer vision, deep learning, patented technologies, and unique capabilities, JARVIS offers innovation for growth and security, making Staqu Technologies the best Intelligent video technology solution company.
- Explore JARVIS Retail Analytics Software to enable seamless integration of video analytics for retail stores into your existing infrastructure. Know about your retail store’s unique visitor count, demography analytics, and more and elevate your brick-and-mortar with video analytics software.
- Unify your operational processes by integrating Artificial Intelligence for Manufacturing, resulting in smart, cost-effective functioning of your manufacturing plants. Get easier centralized access to your manufacturing plants and decrease the risks of theft and pilferage. Automate operational processes by leveraging ANPR route history, get real-time alerts on the detection of fire, and several other insights.
- Enhance infrastructure goals by analyzing perimeter security with more than 99.9% accuracy while also reducing CAPEX with our video analytics solution. Use infrastructure intelligence to quantify unique visitors and enhance your business goals. Analyze and automate operational efficiency with the various features of JARVIS.
- Our state-of-the-art video analytics technology provides real-time insights and actionable data to help you optimize your business’s performance. Use video analytics in hospitality industry on your existing infrastructure and develop better business goals with minimal CAPEX.
- We help the public sector, including government agencies such as the Army, Police, Prisons, and more, to create a transparent, data-centric culture using our video analytics solution. Integrating artificial intelligence in public sector enables agencies to provide quick and efficient service.
- Insights-driven video analytics solution for smart cities to increase resident satisfaction, improve municipal services, and enhance public safety. Reduce false alarms to almost nil (unlike sensors) and the requirement for human oversight or video verification using JARVIS video analytics for smart cities.
- JARVIS provides a user-friendly dashboard that will help you monitor critical incidents and to take actions on them.
- Video Wall panel enables live centralized monitoring of all the cameras from multiple locations on a single screen.
- Get instant alerts on violations to detect, verify and act on critical events.
- Access live feeds of your cameras anytime, anywhere.
- A simple ticketing system with predefined escalation matrix to resolve compliance violations and reduce operational bottlenecks.
- Our products have received accolades from some of the greatest Industry Leaders. This encourages us to keep moving forward with the same energy and motivation.
- FICCI Smart Policing Award UP Prison Department For the Year 2021
- National Startups Awards Citizen Security For the Year 2020
- TiE Lumis Excellence Award For the Year 2020
- IAMAI Award Best use of ML For the Year 2019
- Tech Rocketship Award For the Year 2018
- NASSCOM Award AI Game Changer For the Year 2018
- NASSCOM Award League of 10 For the Year 2018
- Acknowledged under Top 17 Start-ups TOI For the Year 2017
- IBM GEP Award Winner For the Year 2017
- IBM GEP Award Winner For the Year 2016
- Read about the latest press releases and media events of Staqu published in Times of India (TOI), Economic Times(ET), CNBC, Mint, The Hindu, Money Control, Hindustan Times, Indian Express, Tech Circle etc. Explore our news section for insightful articles, impactful resources and the ideas that inspire us to bring revolution in the domain of visual analytics. Also have a glance at Staqu’s innovations, collaboration with global players and other milestones for making the world safer and smarter.
- To Explore Our Media Section
- AI startup Staqu has seen a significant increase in demand for its retail analytics solution.....
- Staqu has announced that it has joined hands with the Bihar State Election Commission to automate the vote-counting amid the.....
- In early March when the government issued a guideline that prison inmates and those visiting them must wear...
- The app, named Artificial Intelligence Based Human Efface Detection (ABHED) is helping law enforcers digitise...
- Video AI implementation company Staqu has announced COVID-19 combat suite of services for businesses. As companies...
- त्रिनेत्र ऐप में अपराधियों की सभी डिटेल अपलोड की जा सकती है और ज़रूरत पड़ने पर 'फेशियल रिकग्नीशन' यानी चेहरा पहचानने की तकनीक का इस्तेमाल कर...
- We are a team of innovators disrupting the AI sphere. Want to join in?
- Learn how you can transform your CCTV footage into actionable insights within 30 minutes.
- Privacy Policy | Terms Of Use | Copyright © 2023 Staqu Technologies Pvt. Ltd.

URL: https://www.staqu.com/solutions/public-sector
- By replacing paper and pencil and outdated systems, JARVIS can assist you in automating 98% of your human processes and saving up to 40% of the costs involved. On activities like manual CCTV video surveillance and related services, you'll be able to recover valuable time and resources, and the public sector will gain from the availability of services around-the-clock.
- Digitally transform operations, services, internal communications, and engagement with citizens by replacing critical manual processes with automated workflows, transparent information exchange, centralized command centers, etc.
- Instead of relying solely on statistics, you can observe the wider picture offered by data insights in real-time by integrating JARVIS' video analytics solution at the center of your government agency. This ultimately means that you may accomplish your policy objectives far more quickly.
- We’ll help you utilize your big data with our video analytics solution and provide robust government software solutions that turn insights into smarter planning and strategy.
- Utilizing cutting-edge technical innovation in the public sector, like artificial intelligence, video and audio data analytics, etc., provide quick and efficient public service.
- Maximize value addition in public sector services through technological advancements. We help government agencies better serve the citizens while lowering administrative costs and optimizing spending.
- Help law enforcement organizations in tracking any incident of violence within any specific area. A real-time identification of such an occurrence can assist the Police in mobilizing personnel to contain any potential dangers to human life and property and to prevent any further escalation of the event.
- Identify vehicles’ number plates automatically to improve security & manage vehicle access to government premises. Help analyze end users of JARVIS to tag a vehicle of interest, to identify their routes, last location, etc., through existing infrastructure such as CCTV cameras.
- Avoid potential threats in any location by analyzing video feeds through our AI-powered video management system and receive real-time alerts for the same. During the time of any potential protest building up at any location, get situational awareness in real-time.
- 
- Identify criminals flagged in the existing database at any location using FR and make criminal search significantly effortless. Also, analyze visitor management in prisons and other government spaces to avoid any unwanted activities.
- Specify an event as SOS detection and get alerted if there is a need for a call to action. Authorities can get notified instantly and make real-time decisions to save any person and avoid any unruly event.
- 
- Get real-time alerts for any intrusion along with footage of the incident allowing rapid verification of the source and taking action on the same. Enhance perimeter security by implementing artificial intelligence for public sector.
- Privacy Policy | Terms Of Use | Copyright © 2023 Staqu Technologies Pvt. Ltd.

URL: https://twitter.com/lkopolice/status/1352112490457427968
- We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using twitter.com. You can see a list of supported browsers in our Help Center.
- Help Center
- Terms of Service
Privacy Policy
Cookie Policy
Imprint
Ads info
      © 2023 X Corp.

URL: https://panoptic.in/uttar-pradesh/FRT-000004
- FRT-000004, Last updated on 12-10-2021
- Read about important updates and our detailed analysis of FRT Projects which is being deployed in
the country. The case studies will look at the background of the project, how many people it will
affect and its scope and legality.
- The petition is for calling on/asking the government to immediately put
a moratorium on the development and use of FRT by government authorities.
- Help us raise awareness about the cause among everyone. And, you can
start
by sharing this website with your peers :)
- A public interest community project | [email protected]
All contents licensed under CC-BY unless stated otherwise.

Built and maintained by Shivam Mishra and
designed by Shobhit Katikia.


Hosted on Frappe Cloud.
- All contents licensed under CC-BY unless stated otherwise.
- Built and maintained by Shivam Mishra and
designed by Shobhit Katikia.
- Hosted on Frappe Cloud.

URL: https://www.reuters.com/article/us-india-tech-women-trfn/privacy-fears-as-indian-city-readies-facial-recognition-to-spot-harassed-women-idUSKBN29R0X5
- Discover Thomson Reuters
- By Rina Chandran
- 4 Min Read
- (Thomson Reuters Foundation) - A plan to monitor women’s expressions with facial recognition technology to prevent street harassment in a north Indian city, will lead to intrusive policing and privacy violations, digital rights experts warned on Friday.
- In Lucknow, about 500 kilometres (310 miles) from the nation’s capital New Delhi, police identified some 200 harassment hotspots that women visit often and where most complaints are reported, said police commissioner D.K. Thakur.
- “We will set up five AI-based cameras which will be capable of sending an alert to the nearest police station,” he said, referring to the artificial intelligence-based technology.
- “These cameras will become active as soon as the expressions of a woman in distress change,” he told reporters this week, without giving further details on which expressions would trigger an alert.
- Facial recognition technology is being increasingly deployed in airports, railway stations and cafes across India, with plans for nationwide systems to modernise the police force and its information gathering and criminal identification processes.
- But technology analysts and privacy experts say the benefits are not clear and could breach people’s privacy or lead to greater surveillance, with little clarity on how the technology works, how the data is stored, and who can access the data.
- “The whole idea that cameras are going to monitor women’s expressions to see if they are in distress is absurd,” said Anushka Jain, an associate counsel at digital rights non-profit Internet Freedom Foundation.
- “What is the expression of someone in distress - is it fear, is it anger? I could be talking to my mother on the phone and get angry and make a face - will that trigger an alert and will they send a policeman?”
- A more feasible solution would be to increase police patrol numbers, Jain told the Thomson Reuters Foundation, adding that the technology is untested, and could lead to over-policing and the harassment of women who trigger alerts.
- India is one of the world’s most dangerous places for women, with a rape occurring every 15 minutes, according to government data. Uttar Pradesh, where Lucknow is located, is the least safe state, with the highest number of reported crimes against women in 2019.
- Police often turn away women who go to register complaints or fail to take action, said Roop Rekha Verma, a women’s rights activist in Lucknow.
- “And they want us to believe they will take action watching our facial expressions,” she said.
- India launched a slew of legal reforms after a fatal 2012 gang rape, including easier mechanisms to report sex crimes, fast-track courts and a tougher rape law with the death penalty, but conviction rates remain low.
- While there is a growing backlash against facial recognition technology in the United States and in Europe, Indian officials have said it is needed to bolster a severely under-policed country, and to stop criminals and find missing children.
- But digital rights activists say its use is problematic without a data protection law, and that it threatens the right to privacy, which was declared to be a fundamental right by the Supreme Court in a landmark ruling in 2017.
- “The police are using the technology to solve a problem without considering that this will simply become a new form of surveillance, a new form of exercising power over women,” said Vidushi Marda, a researcher at human rights group Article 19.
- “AI is not a silver bullet, and no amount of ‘fancy’ tech can fix societal problems,” she said.
- Reporting by Rina Chandran, with additional reporting by Saurabh Sharma in Lucknow @rinachandran; Editing by Michael Taylor. Please credit the Thomson Reuters Foundation, the charitable arm of Thomson Reuters, that covers the lives of people around the world who struggle to live freely or fairly. Visit news.trust.org
- Our Standards: The Thomson Reuters Trust Principles.
- All quotes delayed a minimum of 15 minutes. See here for a complete list of exchanges and delays.

URL: https://timesofindia.indiatimes.com/india/up-cops-to-use-ai-to-read-faces-and-help-women-in-distress/articleshow/80396572.cms
- 'I want it Mumbai's way': Backstreet Boys back in India after 13 years
- In MP's Bandhavgarh, signs of 2,000-year-old 'modern society'
- Pictures: Pakistan foreign minister Bilawal Bhutto in India​
- Cyclone Mocha to form over southeast Bay of Bengal
- Top 10 things people forget in cabs
- Madurai's Chithirai festival: Photos of celestial wedding and temple car fest
- Indians attending King Charles III coronation
- Foreign Secretary assures no Indian will be left behind in conflict-hit Sudan
- Welcome to Pune's 'Bhoot Bungalow'
- How radical preacher Amritpal Singh was arrested

URL: https://timesofindia.indiatimes.com/home/sunday-times/all-that-matters/ai-camera-plan-based-on-pseudo-science-i-could-look-distressed-if-i-need-the-loo-says-rights-lawyer-vidushi-marda/articleshow/80425762.cms

URL: https://thewire.in/women/uttar-pradesh-lucknow-police-artificial-intelligence-camera-women
- The Uttar Pradesh police's plan to set up cameras that will detect distress on women's faces and alert officers has led to fear of targeted tracking and harassment of interfaith couples.
- Illustration: Pariplab Chakraborty
- Saharanpur: Latest among questionable efforts announced by law enforcement to tackle women’s safety is Lucknow police’s plan to set up cameras equipped with artificial intelligence that will automatically take a photo of a woman in distress on the basis of her facial expression, thus pushing police to act.
- “The cameras will be able to detect any change in the facial expressions of a woman being subjected to stalking, threats or harassment on the streets, and an alert will be sent to the police control room,” said additional director general of police
(law and order) Prashant Kumar.
- This is being done under the Smart City project of the Adityanath-led Bharatiya Janata Party government in the state.
- The idea was announced at a workshop, ‘Ashish: Abhay aur Abhyudaya’, conducted at the Lucknow University campus on Wednesday (January 20). The event was attended by women residents of the institution’s hostel, the hostel provost Bhuvneshwari Bhardwaj, the dean of students’ welfare, Poonam Tandon, and Lucknow Police Commissioner D.K. Thakur.
- Thakur said at the workshop that about 200 “hotspots” will be identified from across Lucknow on the basis of where women’s movement is the most and from where most complaints are received. As soon as there is noticeable sign of distress in a woman’s expression, the cameras will be activated, he said.
- “Before she dials 100 [the national police helpline] or 112 [the UP emergency service], the police will be alerted,” he said.
- In addition to questions on whether such a plan is feasible for rapid action at all, activists and lawyers have said that not only is this an infringement on an Indian citizen’s right to privacy, but it can also be used as a tool of surveillance against vulnerable communities.
- Also read: Caging Women Is Violence – Not ‘Safety’ or ‘Protection’
- Many have pointed out that such a measure will take away women’s right to decide if or when they want to report a crime.
- Between 2015 and 2019, crimes against women in the state had increased by over 66% according to a report by the National Crime Records Bureau.  Across India, cases of rape against women belonging to Scheduled Castes communities increased by 37%, and those of assault by 20%.
- Uttar Pradesh had recently also been in the news for the brutal gang-rape and murder of a young Dalit woman by four upper caste Thakur men in Hathras.
- Permanent surveillance
- A few days ago, Madhya Pradesh Chief Minister Shivraj Singh Chouhan had mulled a new system where women will be required to register themselves at the nearest police station when they are going out of home for work using which information the police will then track them for their “safety.”
- Mishi Choudhary, legal director at the Software Freedom Law Center, a movement to protect freedom online, wrote in a tweet, “…Stop this nonsense. The entire world is  putting moratoriums but in India, we seem to be not only travelling at top speed in the other direction, the actions are indicating that rule of law is no more than a small bump in the road on our way.”
- Activists hold placards during a protest against the death of a 19-year-old Dalit woman who was allegedly gang-raped two weeks ago in Hathras (UP), outside Chaitya Bhoomi in Mumbai, Tuesday, Oct. 6, 2020. Photo: PTI/Mitesh Bhuvad
- In another tweet, she said, “Where govt reads every face, political dissent is under permanent intimidation. We cannot live our lives outside the range of others’ cameras anymore. Wherever we go and whatever we do in the public sphere we can expect our image to be recorded without our knowledge or consent.”
- Anja Kovacs, founder and director of Internet Democracy, an organisation that works around women, technology and surveillance, said in a tweet, “Apart from the tremendous extension of control in public space this entails, it’s based on so many problematic gendered assumptions, including the laughable one that women in distress necessarily look at the police as desirable, or even possible, ‘saviours’ #genderingsurveillance.”
- She added, “What women, and most other people, need is more freedom, not more social control. We need a rights framework to deal with problems and challenges, not a protectionist one #genderingsurveillance.”
- What women, and most other people, need is more freedom, not more social control. We need a rights framework to deal with problems and challenges, not a protectionist one #genderingsurveillance
- — Anja Kovacs (@anjakovacs) January 21, 2021
- Interfaith couples an easy target
- Kovacs said that surveillance will only take people’s freedom away, it will put some at greater risk than others.
- “I can’t see any advantages here. Using surveillance tools to ensure safety is a really problematic notion. What it actually does is that it increases social control. If people do not fit the norms made by the institution or person doing the surveillance, it will make it easy to harm those people who are already vulnerable in our society,” she said.
- She added that this puts interfaith couples at risk in the wake of Uttar Pradesh’s notorious ‘love jihad’ ordinance and the actions taken under it.
- Also read: Uttar Pradesh’s ‘Love Jihad’ Law Is a Moment of Glory for Hindutva Foot Soldiers
- She also said that following women all the time to ensure their safety is not feasible and that the police should instead invest in infrastructure and creating a safe space instead. “Changing gender norms and holding culprits accountable for their actions is what will actually work,” she added.
- “There are some women who want to stay away from the police. For example, consider sex workers. What will happen if the police begins to track them?” asked Kovacs.
- Assessing the reason for a woman’s distress will be difficult for police, she added. “What if she is distressed because somebody informed her that somebody in her family died?” she said.
- ‘Fancy technology’
- Anushka Jain, associate counsel with the Internet Freedom Foundation that works on defending online freedom, privacy and innovation, feels that artificial intelligence systems are not the solution to the problem of sexual harassment.
- Also read: India Accounts for 45.8 Million of the World’s ‘Missing Females’: UN Report
- “You can’t just install some technology and say that everything is going to be fine from then. I know, it sounds good because it is fancy technology. But it is also almost impossible to implement this in reality, because the police will not be able to handle the barrage of alerts it will receive once this is in place,” she said.
- Jain also questioned some of the rationale behind picking spots in the city.
- “On what basis are they going to decide which areas they should put the cameras in? Why certain parts of the city? This will only lead to more surveillance on women. To continuously track women’s gestures is so, so invasive,” she said, adding that the right to privacy covers privacy even in public spaces and should be respected.
- “It is a patriarchal concept to want to save women. We don’t know how this data will be stored, who will have access to it, and what if it is some police officer who is harassing a woman?” she said.

URL: https://gadgets.ndtv.com/internet/news/lucknow-police-facial-recognition-technology-expressions-ai-cameras-women-in-distress-alerts-2355865
- Photo Credit: Pexels
- Lucknow police is initially deploying five AI-enabled cameras to start detecting women in distress
- Lucknow police is deploying facial recognition technology backed by security cameras that will read expressions of women in distress and alert their nearest police station. The technology is claimed to use artificial intelligence (AI) to read facial expressions of women. The new project will take place under the Uttar Pradesh government's Mission Shakti programme that was launched in October. It is projected to help reduce cases of harassment of women subjected to stalking and threat in the city.
- Lucknow Police Commissioner Dhruv Kumar Thakur told the media on Wednesday that the police department had identified 200 hotspots where the movement of women was maximum in the city and from where most of the complaints were received. The police will initially deploy five AI-based cameras to recognise the expression of women citizens.
- The facial recognition technology is touted to alert the nearest police station of women in distress even before they dialled the police helpline. It will activate the cameras upon detecting facial expressions of women and trigger the system to notify the police.
- However, technology policy experts and researchers aren't pleased by the new move of the Lucknow police.
- Anupam Guha, Assistant Professor at the Centre for Policy Studies, Indian Institute of Technology (IIT) Bombay, took to Twitter to express his dissent. He noted that the recording of facial expressions using security cameras could be wrong-headed and potentially harmful.
- Similar to Guha, Anushka Jain, Associate Counsel (Transparency and Right to Information) at non-profit Internet Freedom Foundation, told Gadgets 360 that the new project could lead to unnecessary harassment by the police and situation of over-policing in the areas where emotion-tracking is being deployed.
- “We don't know what expressions they are tracking and how accurate the system of tracking these expressions is,” she said. “Also, it's not necessary that a person who's making an expression of anger or distress is actually being harassed in a manner wherein police intervention is needed. I could be talking to a friend, and I could get upset over something. And that could also trigger the cameras.”
- Jain is working on an initiative called Project Panoptic under which she's tracking the deployment of facial recognition technology by various government departments in the country.
- “We've already been tracking the use of facial recognition by the Lucknow police. And now, the latest announcement is something with which they've taken it a step further,” she said.
- One of the other key concerns by experts is whether the cameras are accessed by certain police officers or all police personnel. The police department has also not provided any clarity on the involvement of third parties that could further raise concerns over data privacy.
- What will be the most exciting tech launch of 2021? We discussed this on Orbital, our weekly technology podcast, which you can subscribe to via Apple Podcasts, Google Podcasts, or RSS, download the episode, or just hit the play button below.
- For the latest tech news and reviews, follow Gadgets 360 on Twitter, Facebook, and Google News. For the latest videos on gadgets and tech, subscribe to our YouTube channel.
- Advertisement
- 03:03
- 01:39
- 02:57
- 11:36
- 02:49
- Advertisement

URL: https://hbr.org/2019/11/the-risks-of-using-ai-to-interpret-human-emotions
- Companies have historically used focus groups and surveys to understand how people felt. Now, emotional AI technology can help businesses capture the emotional reactions of both employees and consumers in real time — by decoding facial expressions, analyzing voice patterns, monitoring eye movements, and measuring neurological immersion levels, for example. The ultimate outcome is a much better understanding both of workers and customers. But, because of the subjective nature of emotions, emotional AI is especially prone to bias. AI is often also not sophisticated enough to understand cultural differences in expressing and reading emotions, making it harder to draw accurate conclusions. For instance, a smile might mean one thing in Germany and another in Japan. Confusing these meanings can lead businesses to make wrong decisions. Imagine a Japanese tourist needing assistance while visiting a shop in Berlin. Using emotion recognition to prioritize which customers to support, the shop assistant might mistake their smile — a sign of politeness back home — as an indication that they don’t require help. If left unaddressed, conscious or unconscious emotional biases like this can perpetuate stereotypes and assumptions at an unprecedented scale.
- What do people really feel?
- This has never been an easy thing for companies to determine. For one thing, emotions are inherently difficult to read. For another, there’s often a disconnect between what people say they feel and what they actually feel.
- Consider how people respond to Super Bowl commercials. In 2018, TV viewers voted Amazon’s “Alexa Loses Her Voice” — where celebrities attempt to (unsuccessfully) replace Alexa — as the best commercial, according to the USA Today Ad Meter. Diet Coke’s “Groove,” which featured a woman dancing awkwardly after drinking a can of Diet Coke Twisted Mango, was rated as the worst commercial. Based on this poll, one might conclude that the Alexa commercial had the bigger impact. Not so, according to Paul Zak, neuroscience researcher and chief executive officer of Immersion Neuroscience, whose team studied people’s neurologic immersion in the ads. Zak’s team assessed viewers’ level of emotional engagement by measuring changes in oxytocin levels, the brain’s “neural signature of emotional resonance.” The research found that “Groove” actually had greater impact — proof to Zak that for Super Bowl commercials, there is “zero correlation” between what people say and how they subconsciously feel.
- When we interviewed Zak about this phenomenon, he summed it up by saying: “People lie, their brains don’t.”
- A lot of companies use focus groups and surveys to understand how people feel. Now, emotional AI technology can help businesses capture the emotional reactions in real time — by decoding facial expressions, analyzing voice patterns, monitoring eye movements, and measuring neurological immersion levels, for example. The ultimate outcome is a much better understanding of their customers — and even their employees.
- Because of the subjective nature of emotions, emotional AI is especially prone to bias. For example, one study found that emotional analysis technology assigns more negative emotions to people of certain ethnicities than to others. Consider the ramifications in the workplace, where an algorithm consistently identifying an individual as exhibiting negative emotions might affect career progression.
- AI is often also not sophisticated enough to understand cultural differences in expressing and reading emotions, making it harder to draw accurate conclusions. For instance, a smile might mean one thing in Germany and another in Japan. Confusing these meanings can lead businesses to make wrong decisions. Imagine a Japanese tourist needing assistance while visiting a shop in Berlin. If the shop used emotion recognition to prioritize which customers to support, the shop assistant might mistake their smile — a sign of politeness back home — as an indication that they didn’t require help.
- In short, if left unaddressed, conscious or unconscious emotional bias can perpetuate stereotypes and assumptions at an unprecedented scale.
- Based on our research and experience working with global clients, we see businesses using emotional AI technology in four ways. Through each, the implications of algorithmic bias are a clear reminder that business and technology leaders must understand and prevent such biases from seeping in.
- Understanding how emotionally engaged employees actually are. When AI is used to gauge employee emotions, it can have serious impacts on how work is allocated. For example, employees often think they’re in the right role, but upon trying new projects might find their skills are better aligned elsewhere. Some companies are already allowing employees to try different roles once a month to see what jobs they like most. Here’s where bias in AI could reinforce existing stereotypes. For example, in the U.S., where 89% of civil engineers and 81% of first-line police and detective supervisors are male, an algorithm that has been conditioned to analyze male features might struggle to read emotional responses and engagement levels among female recruits. This could lead to flawed role allocation and training decisions.
- Improving the ability to create products that adapt to consumer emotions. With emotion tracking, product developers can learn which features elicit the most excitement and engagement in users. Take, for example, Affectiva’s Auto AI platform, which can recognize emotions like joy and anger and adapt a vehicle’s in-cabin environment accordingly. Cameras and microphones can pick up on passenger drowsiness — and may lower the temperature or jolt the seatbelt as a result. A smart assistant might change its tone in response to a frustrated passenger. With emotional AI, any product or service — whether in the car or elsewhere — can become an adaptive experience.  But a biased adaptive in-cabin environment could mean that some passengers are misunderstood. Elderly people, for example, might be more likely to be wrongly identified as having driver fatigue (the older the age of the face, the less likely it is that expressions are accurately decoded). And as these systems become more commonplace, insurance companies are going to want a piece of the data. This could mean higher premiums for older people, as the data would suggest that, despite many prompts to rest, the driver pressed on.
- Improving tools to measure customer satisfaction. Companies like Boston-based startup Cogito are giving businesses the tools to help their employees interact better with customers.  Its algorithms can not only identify “compassion fatigue” in customer service agents, but can also guide agents on how to respond to callers via an app. An upset customer might, for example, call to complain about a product. Recording and analyzing the conversation, Cogito’s platform would then suggest that the agent slow down or prompt them on when to display empathy. A biased algorithm, perhaps skewed by an accent or a deeper voice, might result in some customers being treated better than others — pushing those bearing the brunt of bad treatment away from the brand.  A male caller could be subject to less empathy than a woman, reinforcing societal perceptions of men as “emotionally strong.” On the flip side, a female caller may be viewed as a less tough negotiator, resulting in less compensation being offered. Ironically, the agents themselves may not even possess these biases, but clouded by the misconception that the algorithms are highly accurate, they may follow their advice blindly. In this way, biases spread, unquestioned and systematically.
- Transforming the learning experience. Emotional insights could be used to augment the learning experience across all ages. It could, for example, allow teachers to design lessons that spur maximum engagement, putting key information at engagement peaks and switching content at troughs. It also offers insights into the students themselves, helping to identify who needs more attention. China is already introducing emotion detection systems into classrooms to track how focused students are. But, if biases exist, wrongly suggesting someone is disengaged could result in learning experiences tailored toward certain groups rather than others. Think about different learning styles: Some people are visual learners. Some learn by doing. Others favor intense solitary concentration. But an algorithm, perhaps designed by a visual learner, might completely miss or misinterpret such cues.  Incorrect engagement readings could affect learning outcomes all the way to the workplace, meaning that even in work training programs, only a fraction of employees can enjoy full professional development. Such misassumptions could affect learning outcomes all the way to the workplace, meaning that even in work training programs, only a fraction of employees can enjoy full professional development.
- As more and more companies incorporate emotional AI in their operations and products, it’s going to be imperative that they’re aware of the potential for bias to creep in and that they actively work to prevent it.
- Whether it is the subjective nature of emotions, or discrepancies in emotions, it is clear that detecting emotions is no easy task. Some technologies are better than others at tracking certain emotions, so combining these technologies could help to mitigate bias. In fact, a Nielsen study testing the accuracy of neuroscience technologies such as facial coding, biometrics, and electroencephalography (EEG) found that when used alone, accuracy levels were at 9%, 27%, and 62% respectively. When combined, accuracy levels shot up to 77%. Testing the results with a survey brought this up to 84%. Such combinations therefore serve as a check on the accuracy of results — a referencing system of sorts.
- But accounting for cultural nuances in algorithms will take more than just combining and referencing multiple technologies. Having diverse teams creating emotional AI algorithms will be crucial to keeping bias at bay and fully capturing the complexity of emotions. This means not just gender and ethnic diversity, but also diversity in socioeconomic status and views – negating anything from xenophobia to homophobia to ageism. The more diverse the inputs and data points, the more likely it is that we’ll be able to develop AI that’s fair and unbiased.
- Companies will also need to be vigilant about not perpetuating historical biases when training emotional AI. While historical data might be used as a basis to train AI on different emotional states, real-time, live data will be needed for context. Take smiles, for example. One study showed that of the 19 different types of smile, only six happen when people are having a good time. We also smile when we are in pain, embarrassed, and uncomfortable — distinctions that can only be drawn with context.
- In sum, emotional AI will be a powerful tool indeed, forcing businesses to reconsider their relationships with consumers and employees alike. It will not only offer new metrics to understand people, but will also redefine products as we know them. But as businesses foray into the world of emotional intelligence, the need to prevent biases from seeping in will be essential. Failure to act will leave certain groups systematically more misunderstood than ever — a far cry from the promises offered by emotional AI.
- 
- The authors would like to thank Accenture Research colleagues Xiao Chang, Paul Barbagallo, Dave Light, and H. James Wilson for their significant contributions to this article.

URL: https://thenextweb.com/neural/2021/01/22/an-indian-city-plans-to-use-facial-recognition-to-spot-women-in-distress-what-could-go-wrong/
- You have been blacklisted, KTHXBAI
- XID: 13635577
- Varnish cache server

URL: https://www.newsweek.com/indian-city-deploys-facial-recognition-detect-harassed-womens-expressions-1563761

URL: https://www.dailypioneer.com/2021/state-editions/emotion-recognition-technology-----a-new-challenge-to-privacy.html
- Emotion Recognition Technology’, yes, it is a thing, in the ever-evolving technology landscape. Emotion Recognition Technology (“ERT”) is an emerging technology which seeks to recognize and categorize the myriad human emotions depicted by humans through their facial features into certainbroad categories of seemingly universal human emotions which then will be captured through cameras and then read and processed through machine learning and used by the Governments for various purposes.
It is already happening in India, when under ‘Mission Shakti’ the Police in Lucknow deployed many close in cameras to capture the facial expressions of women seemingly in distress which then will alertthe police to take pre-emptive actions and prevent crimes. This move raised the hackles of social media activists and civil society bodies as being infringing the personal life and privacy of person in question. This issue deserves a closer examination.
To set context ERT similar to facial recognition technology in which with the help of close in cameras, big data and machine learning the government or private entities intends to appropriate motives, basis which then be deployed to various use cases such as security threat in public places, attentiveness in a classroom etc.
ERT is based on the premise arrived at through a study conducted by American Psychologist, Paul Ekman that all various and myriad human emotions can be broadly classified into seven universal emotions irrespective of their social and cultural context. They are happiness, sadness, anger, disgust,contempt, fear and surprise. This theory seminal at the time, when it was postulated is the basis of most of the ERT projects and use cases.
However, it has now been debunked by many subsequent studies by psychologists and researchers such as James Russell a psychologist of Boston College who through rigorous research concluded thatthat the manifestation of emotions on the face of human is not as universal as it is thought out to be.Many other studies followed which have debunked the universal emotion theory. An article published in The Washington Post, stated that in a research conducted by Association for Psychological Sciencewhich spent two years exploring this idea after reviewing more than 1,000 studies, concluded that the relationship between facial expression and emotion is ‘nebulous, convoluted and far from universal’. Many other similar credible research studies claim similarly, however going into the details of those studies is beyond the pale of this Article.
That the very basis of ERT is built on dubious scientific theory is now beyond doubt, however the more worrisome part of the problem lies in the continued usage of this dubious scientific argument to push the different use cases of ERT and thus bringing into question the legality of the use of the ERT and of course accompanying very valid concerns of privacy and its invasion. As of today, the most popular use cases of ERT are mass surveillance for national and internal security, monitoring activities in enclosed spaces such as classrooms, shopping malls, casinos, workplaces, policing and hiring processes.
The writer is a Technology and Commercial Lawyer, with keen interest in law and emerging technologies.
- Emotion Recognition Technology’, yes, it is a thing, in the ever-evolving technology landscape. Emotion Recognition Technology (“ERT”) is an emerging technology which seeks to recognize and categorize the myriad human emotions depicted by humans through their facial features into certainbroad categories of seemingly universal human emotions which then will be captured through cameras and then read and processed through machine learning and used by the Governments for various purposes.
- It is already happening in India, when under ‘Mission Shakti’ the Police in Lucknow deployed many close in cameras to capture the facial expressions of women seemingly in distress which then will alertthe police to take pre-emptive actions and prevent crimes. This move raised the hackles of social media activists and civil society bodies as being infringing the personal life and privacy of person in question. This issue deserves a closer examination.
- To set context ERT similar to facial recognition technology in which with the help of close in cameras, big data and machine learning the government or private entities intends to appropriate motives, basis which then be deployed to various use cases such as security threat in public places, attentiveness in a classroom etc.
- ERT is based on the premise arrived at through a study conducted by American Psychologist, Paul Ekman that all various and myriad human emotions can be broadly classified into seven universal emotions irrespective of their social and cultural context. They are happiness, sadness, anger, disgust,contempt, fear and surprise. This theory seminal at the time, when it was postulated is the basis of most of the ERT projects and use cases.
- However, it has now been debunked by many subsequent studies by psychologists and researchers such as James Russell a psychologist of Boston College who through rigorous research concluded thatthat the manifestation of emotions on the face of human is not as universal as it is thought out to be.Many other studies followed which have debunked the universal emotion theory. An article published in The Washington Post, stated that in a research conducted by Association for Psychological Sciencewhich spent two years exploring this idea after reviewing more than 1,000 studies, concluded that the relationship between facial expression and emotion is ‘nebulous, convoluted and far from universal’. Many other similar credible research studies claim similarly, however going into the details of those studies is beyond the pale of this Article.
- That the very basis of ERT is built on dubious scientific theory is now beyond doubt, however the more worrisome part of the problem lies in the continued usage of this dubious scientific argument to push the different use cases of ERT and thus bringing into question the legality of the use of the ERT and of course accompanying very valid concerns of privacy and its invasion. As of today, the most popular use cases of ERT are mass surveillance for national and internal security, monitoring activities in enclosed spaces such as classrooms, shopping malls, casinos, workplaces, policing and hiring processes.
- The writer is a Technology and Commercial Lawyer, with keen interest in law and emerging technologies.
- 

URL: https://www.businessinsider.in/news/a-scary-proposal-to-use-facial-recognition-and-ai-by-an-indian-state-has-experts-fuming/articleshow/80421935.cms
- Copyright © 2023. Times Internet Limited. All rights reserved.For reprint rights. Times Syndication Service.

- India Human Efficiency Tracking System sanitation worker surveillance
- Uyghur emotion detection testing
- Page infoType: Incident Published: January 2023
