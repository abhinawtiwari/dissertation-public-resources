- Released: 2002
- Can you improve this page?Share your insights with us
- Virginia Non-violent Risk Assessment (NVRA) is a method for diverting 25 percent of non-violent offenders into sanction programmes other than jail and prison, such as rehabilitation outpatient drug or mental health programmes. The risk assessment aims to identify the lowest risk offenders.
- Developed by the Virginia Criminal Sentencing Commission (VCSC), the system was introduced in 2002, partly in order to avert a 'fiscal collapse' of the state prison system, partly to 'expand alternative punishment/treatment options for some non-violent felons.'
- In November 2019, the Washington Post reported that a George Mason University and Texas A&M analysis of sentencing in Virginia revealed that the state’s risk assessment system increased sentences for Black and young defendants.
- The study showed that defendants younger than 23 were 4 percentage points more likely to be incarcerated after the risk assessment was adopted, and that their sentences were 12 percent longer than their older peers.
- The researchers argue that part of the reason for the results is that judges did not follow the algorithm’s suggestions in most cases because they had not been trained to use the tool, said no alternative programmes had been available, or did not use risk scores when senctencing.
- 'Virginia’s nonviolent risk assessment reduced neither incarceration nor recidivism; its use disadvantaged a vulnerable group (the young); and failed to reduce racial disparities,' the study authors concluded.
- Operator: Virginia Criminal Sentencing Commission (VCSC) Developer: Virginia Criminal Sentencing Commission (VCSC)
- Country: USA
- Sector: Govt - justice
- Purpose: Identify low risk offenders
- Technology: Risk assessment algorithm Issue: Bias/discrimination - race, ethnicity, age
- Transparency:
- Virginia Criminal Sentencing Commission. Re-validation of the Nonviolent Offender Risk Assessment Instrument: Study Update (pdf)
- Brookings Institution (2020). Understanding Risk Assessment Instruments in Criminal Justice
- Stevenson M., T., Doleac J. (2019). Algorithmic Risk Assessment in the Hands of Humans
- Picard-Fritsche S., Rempel M., Tallon J. A., Adler J., Reyes A. (2017). Demistifying Risk Assessment - Key principles and Controversies (pdf)
- Kleiman M., Ostrom B. J., Chessman II F. (2016). Using Risk Assessment to Inform Sentencing Decisions for Nonviolent Offenders in Virginia
- Ostrom B. J., Kauder N., B. (2013). The Evolution of Offender Risk Assessment in Virginia
- Virginia Criminal Justice Reform Project (2018). Non Violent Risk Assessment in Virginia Sentencing (pdf)
- Ostrom B. J., Kleiman M., Chessman II F., Hansen R. M., Kauder N., B. (2002). Offender Risk Assessment in Virginia: A Three-Stage Evaluation
URL: https://www.washingtonpost.com/business/2019/11/19/algorithms-were-supposed-make-virginia-judges-more-fair-what-actually-happened-was-far-more-complicated/
- This article was published more than 3 years ago
- We tend to assume the near-term future of automation will be built on man-machine partnerships. Our robot sidekicks will compensate for the squishy inefficiencies of the human brain, while human judgment will sand down their cold, mechanical edges.
- But what if the partnership, especially in the early stages, instead accentuates the flaws of both? For example, a formula designed to reduce prison populations in Virginia led some judges to impose harsher sentences for young or black defendants, and more lenient ones for rapists.
- In an age when artificial intelligence is widely expected to eat what’s left of the world, simple sentencing algorithms are a preview of the economy’s tool-assisted future. A working paper released Tuesday by Megan Stevenson of George Mason University and Jennifer Doleac of Texas A&M provides one of the first examinations of the unintended consequences that arise when algorithms and humans team up in the wild.
- The algorithms are intended to remove some of the guesswork from judges’ sentencing decisions by assigning a simple risk score to defendants. In Virginia, the score included data such as offense type, age, prior convictions and employment status. Larceny scores higher than drug offenses, men score higher than women, and unmarried folks score higher than their married peers. (Marriage and employment were removed from the score in 2013.)
- Similar algorithms are now used in 28 states (and parts of seven more). In Virginia, they were adopted statewide in 2002 to help keep prison populations down after discretionary parole was abolished. Stevenson and Doleac’s analysis relies on tens of thousands of felony convictions, with a particular focus on the period between 2000 and 2004.
- Judges were supposed to use risk scores to identify felons who were least likely to reoffend and either give them shorter jail sentences or send them to a program such as probation or substance-abuse treatment. Rather than focus on patterns of discrimination by algorithms or judges acting alone, as others have done, Stevenson and Doleac measured how the two interacted.
- After controlling for the effects of time, geography and demography, Stevenson and Doleac found the ambitious new nonviolent risk assessment system, often held up as a model for other states to follow, did not change the rate at which people were incarcerated, the length of their sentences or the rate at which they reoffended after release. But that doesn’t mean it had no effect.
- Judges followed the algorithm’s suggestions a bit less than half of the time. People who the algorithm deemed high-risk received longer sentences than they would have, and candidates assessed as low-risk got shorter ones. The two adjustments offset each other, so the overall numbers didn’t change, but the interaction between algorithmic sentencing recommendations and judges’ discretion nonetheless produced perverse effects.
- In a statement, Meredith Farrar-Owens, director of the Virginia Criminal Sentencing Commission, said the system’s goal was to avoid increasing crime or recidivism rates while diverting the lowest-risk nonviolent offenders from prison and freeing up space for violent offenders, who were expected to serve longer terms under the sentencing reforms that took effect in 1995.
- “While the Commission’s risk assessment instruments were developed based on empirical study of recidivism rates and patterns among Virginia felons, use of risk assessment takes place within the context of a complex and dynamic criminal justice system,” Farrar-Owens said. “Risk assessment recommendations are advisory and only one of many factors judges will consider when sentencing defendants.”
- She added that judges’ ability to follow the algorithm’s suggestions may also have been limited by plea agreements, and by a lack of alternative options in their particular circuit.
- The sentencing of young offenders is one example of the algorithm’s surprising side effects. Stevenson said some judges may not realize it, but someone’s risk score is largely a reflection of how old they are. That’s because age is such a strong predictor of recidivism. You get substantially more added to your risk score for being younger than 30 (13 points) than, for example, having been incarcerated five or more previous times as an adult (9 points).
- In a 2018 analysis, Stevenson and Vanderbilt’s Christopher Slobogin calculated that 58 percent of the widely used COMPAS algorithm’s violent recidivism risk score can be attributed to age.
- “People are getting this very stigmatic label — high risk for violent recidivism — largely because they’re 19 years old,” she said.
- Judges tended to be more merciful toward young defendants than the algorithm recommended. Nonetheless, defendants younger than 23 were 4 percentage points more likely to be incarcerated after risk assessment was adopted, and their sentences were 12 percent longer than their older peers.
- “Based on the Commission’s recidivism study, age is one of the most heavily weighted factors on the risk assessment tool. It makes sense that, once judges had an objective, research-based risk assessment tool, we would see sentences of young adult offenders increase relative to older offenders,” Farrar-Owens said.
- Racial disparities also increased among those circuits that used risk assessment most. Although computers can’t explicitly use prohibited variables like race in their sentencing calculations, black defendants were 4 percentage points more likely to be incarcerated after risk assessment was adopted, compared with otherwise equivalent whites. Black defendants’ sentences were also 17 percent longer.
- “This is partially explained by the fact that black defendants have higher risk scores, and partially because black defendants are sentenced more harshly than white defendants with the same risk score,” Stevenson and Doleac write.
- The authors also studied a similar risk-assessment program for sex offenders. Out of an abundance of caution, the program was built so the algorithm could only authorize longer-than-baseline sentences — yet its net effect was to decrease how often sex offenders were imprisoned by 5 percentage points, and to shorten their sentences by about 24 percent.
- Stevenson and Doleac suggest that by assigning a sex offender a low risk score, algorithms may help protect a judge from backlash if the offender goes on to commit another crime. This empowers them to offer shorter sentences than they otherwise would have meted out.
- “If they sentence someone leniently and that person goes out and commits a heinous crime, all fingers are pointed at them,” Stevenson said. “If they make a mistake in the other direction — failing to release someone who would have done anything if released — nobody sees that. There are no consequences to the judge.”
- The University of Maryland’s Frank Pasquale, who made the case for oversight of algorithms in 2015’s “The Black Box Society,” said most judges aren’t adequately trained to evaluate the claims made by sentencing systems such as the one in Virginia. Because judges have the option to reject the algorithm’s conclusion, Pasquale said, they may follow it only when it provides a convenient excuse.
- “You can have a lot of scenarios where the AI algorithms end up being a rationalization for what the judge wants to do,” Pasquale said.
- Stevenson and Doleac write that, especially when the goals of the algorithm’s human partners differ from those of its designers, we should expect unexpected results.
- “Virginia’s nonviolent risk assessment reduced neither incarceration nor recidivism; its use disadvantaged a vulnerable group (the young); and failed to reduce racial disparities,” Stevenson and Doleac write. “Virginia’s sex offender risk assessment lowered sentences for those convicted of rape: a group that the Sentencing Commission had targeted for increased sentences.”
- Why did the Virginia algorithm struggle? It turns out future crime is hard to predict. “Even under ideal conditions,” Stevenson and Doleac write, “predictions of future offending are unable to explain more than a tiny fraction of the variation in recidivism.”
- Duke University professor Brandon Garrett, who has separately studied the Virginia system, said: “You can’t just adopt a tool and expect it to magically solve problems. You have to put real thought and information into implementation.”
- Garrett and his collaborators spoke with judges from across Virginia. Some said they weren’t trained or didn’t trust the formula. Some said they didn’t even have the programs necessary to divert people from prisons. One judge even dryly equated using the algorithm to visiting a psychic.
- In an analysis forthcoming in the California Law Review, Garrett and collaborator John Monahan of the University of Virginia write that judges didn’t follow risk assessments consistently — and that some didn’t use them at all.
- “Nor should that be a surprise,” they write, “given that judges and other decisionmakers typically receive almost no training in risk assessment, and their discretion to ignore risk assessment is virtually unchecked.”
- Farrar-Owens said that newly appointed judges are taught about the risk-assessment program during orientation but that “judicial philosophy in regards to risk assessment certainly varies across the Commonwealth.”
- Aurélie Ouss, who researches criminal sentencing, recidivism and related subjects at the University of Pennsylvania, praised Stevenson and Doleac’s work, and said it showed Virginia’s algorithms had neither been the panacea some hoped for nor the nightmare others feared. Like Garrett, she said it might come down to implementation.
- “There are so many ways in which you can present the information,” Ouss pointed out. “It may be a case that a different tool that’s designed differently — that judges use differently — would yield different results.”
- Yet in the case of both race and age, Stevenson and Doleac find that if judges had fully complied with the algorithm’s recommendations, the disparities would have been even greater.
- “If you’re sentencing purely on a risk rationale, then you want to lock up all the teenagers,” Stevenson said. “But if you’re sentencing on other rationale like mercy for the vulnerable or evaluating the level of culpability that individuals have, then people are young. There are a lot of reasons why we don’t necessarily want to punish them more harshly than older people.”

URL: https://www.nytimes.com/2005/01/02/magazine/sentencing-by-the-numbers.html
- Please enable JS and disable any ad blocker

URL: https://hdsr.mitpress.mit.edu/pub/dudgcmk3/release/5
- Rudin, Wang, and Coker (2020, henceforth RWC) present a convincing argument against black box algorithms like COMPAS that are sometimes used in the United States to help judges sentence convicted offenders in court. They point out that the lack of transparency means that defendants (and victims) cannot assess the accuracy of the score driving the decision, and researchers cannot accurately assess the fairness of any given decision rule. They then argue that these black box algorithms should be replaced with simple and transparent risk assessment algorithms that are based primarily on age and criminal history. Prior work has shown that these tools perform at least as well as the more costly proprietary tools.
- Although the article is focused narrowly on proprietary risk tools like COMPAS, RWC’s argument potentially has a much broader scope. As they discuss in their final section, the RWC arguments that black box algorithms are not fair applies not only to COMPAS, but also to all discretionary sentencing done by judges. RWC state:
- Interestingly, a system that relies only on judges—and does not use machine learning at all—has similar disadvantages to COMPAS; the thought processes of judges is (like COMPAS) a black box that provides inconsistent error-prone decisions. Removing COMPAS from the criminal justice system, without a transparent alternative, would still leave us with a black box.
- Prior work has shown not only that judges are ‘black boxes.’ but that they also not very good at identifying high-risk offenders (Gottfredson, 1999). An extension of the RWC argument then, if I might be allowed to take the argument to an extreme that RWC did not advocate, would replace all black box algorithms, including judges, with a simple risk tool that uses age and criminal history to assign sentences.
- The recognition by RWC that judges are also poorly performing black boxes connects the RWC discussion back to the sentencing reform movement that began to build momentum in the 1960s (Aharonson, 2013; Spohn, 2008). The U.S. sentencing system was then best described as an indeterminate sentencing structure focused on the goal of rehabilitation. Judges sentenced offenders to broad ranges, and parole boards made the final decisions about sentence length based on their assessment of an individual’s rehabilitation. This individualized and subjective approach was often criticized because it led to widely disparate outcomes that defendants were not able to contest. Criticism of this sentencing regime crystalized in a short but widely read book by Judge Marvin Frankel called Criminal Sentences: Law Without Order (Frankel, 1973). The current fairness argument against COMPAS by RWC is reminiscent of Frankel’s argument that the indeterminate system is nontransparent and nonreviewable and therefore inherently unfair. The main difference is that while RWC talks about one black box (COMPAS), Frankel talks about many different black boxes (judges).
- Frankel’s solution was the creation of “a detailed chart or calculus to be used (1) by the sentencing judge in weighing the many elements that go into the sentence; (2) by lawyers, probation officers and others undertaking to persuade or enlighten the judge; and (3) by appellate courts in reviewing what the judge has done” (Frankel 1973, p. 113).  The suggestion of a chart or “calculus” is similar to RWC’s call for a simple, transparent tool. The emphasis on risk, based primarily on prior history, also coincides nicely with the final result of the guidelines movement started by Frankel, who is sometimes called the Father of Guidelines (Adelman & Deitrich, 2008).
- According to Harcourt (2015), the U.S. Sentencing Commission initially set out to produce an actuarial risk tool. Although they eventually abandoned this idea as too complex, the commission was heavily influenced by ideas about selective incapacitation, which developed after Frankel’s original book was published (Harcourt, 2015). Prior history was used by the commission specifically because of existing research linking prior history to risk (Robinson, 2001). As a result of this initial effort, prior history is a prominent feature of all state sentencing guideline systems, as well as many mandatory minimum sentencing schemes for chronic offenders.
- RWC go beyond making prior history a prominent feature, and instead advocate a system that relies almost solely on prior history and age on the basis of a horse race between COMPAS and their preferred risk tool. While I agree wholeheartedly with RWC that transparent tools are better than black box ones, I believe that our experience with prior history over the last 30 years should raise some red flags about this recommendation. Harcourt (2015) identified the high correlation between race and prior history as one reason to be cautious. In the remainder of this discussion, I identify three other important concerns about sentencing policies that rely heavily on prior history.
- First, prior history is an endogenous determinant of future criminal involvement. For example, judges and juries appear to use criminal history as a measure of culpability, particularly in marginal cases (Eisenberg & Hans, 2009). There is also now causal evidence not only that a criminal record can lead to problems in the labor market through the explicit action of the government (Chin, 2012), but that those problems can then lead directly to additional crime (Denver, Siwach, & Bushway, 2017). In this case, the prior record is not capturing some inherent risk of the individual, but the risk created through labeling by the criminal justice system. Although the full extent of this endogeneity is not yet well understood, the endogeneity itself is well established. Sentencing systems that focus on prior history run the risk of becoming self-fulfilling prophecies.
- Second, the finding that prior history is the main factor predicting risk is the result of risk prediction exercises that do not account for the current penal treatment. Usually, risk prediction is conducted on a population prior to treatment (Bushway & Smith, 2007). However, in the case of the criminal justice system, risk prediction is conducted on a sample of people who have already been sentenced—that is, ‘treated’—differentially on the basis of risk. In other words, the data used by RWC to evaluate risk comes from a group of convicted offenders who have all been treated differently based both on COMPAS and on the judge’s assessment of risk. The problem with evaluating risk in an environment of differential treatment has long been understood to create endogeneity (Gottfredson, 1999) and flawed inference about the relevance of a given factor (Bushway & Smith, 2007). The RWC conclusion that their tool performs better than COMPAS should be modified to say that, conditional on the use of COMPAS and individual discretion, the proposed risk tool performed better than COMPAS. This does not mean that the two algorithms would have performed the same way if they had been implemented in two independent universes with the same initial conditions.
- Consider the experience of Virginia. Virginia became one of the first states to use a transparent risk assessment instrument as a formal part of sentencing. In the initial risk assessment, created in a world without formal risk assessment, age was a major factor predicting risk (Kleiman, Ostrom, & Cheesman, 2007). In a risk analysis conducted after the new risk tool was implemented and used by the judges, age was no longer predictive—not because age does not predict risk, but because age was now being used to assign treatment, and the treatment apparently reduced recidivism (Kleiman et al., 2007; Bushway & Smith, 2007). The meaning of the comparison depends critically on the initial conditions.
- It is extremely difficult to unwind the impact of these initial conditions, unless we know explicitly how sentencing was conducted. As RWC showed in their analysis of the ProPublica analysis, false assumptions about the nature of the sentencing process will lead directly to false conclusions about the consequences of that sentencing. Transparent risk tools could help make the sentencing process clearer. However, in most sentencing environments, risk tools are advisory, and judges are free to sentence as they choose (Kleiman et al., 2007; Stevenson, 2018).
- In Virginia, the tool had some impact (albeit not the desired 25% decline in incarceration), but the impact of the tool varied by judge (Garrett, Jakubow, & Monahan, 2019; Monahan, Metz, & Garrett, 2018). Although sentencing research tends to assume that the same factors are used in the same way by all actors in the system (Mitchell, 2005), the reality is that each judge and courtroom work group is using its own weighting system (Gottfredson, 1999). As a result, it is virtually impossible to unpack the prior system, even in a world where a transparent risk instrument is available to the judges.
- More generally, it appears to be impossible to achieve consensus about what the goals of sentencing should be. Indeed, there is clearly no consensus that risk should be the driving factor. For example, in the final proposed draft of the American Law Institute’s Model Penal Code (American Law Institute, 2017), proportionality/retribution is identified as the primary goal of sentencing. Recidivism prevention and other utilitarian goals are allowed on the margin, and individual judicial discretion is preserved as an important goal before transparency. The current article abstracts from a conversation about the role of transparency relative to other goals. In the conversation around the Model Penal Code, transparency was one of many goals. Fortunately, transparency can coexist with a variety of sentencing goals, including retribution, deterrence, and recidivism prediction. However, it cannot as easily coexist with an emphasis on individualized sentences or judicial discretion, key features of many discussions about ideal sentencing structures.
- Third, and finally, a sentencing system that emphasizes prior criminal history can become an active cause for the growth in incarceration. For example, King (2019) has shown that in Minnesota, a state with a strict guideline system that emphasized prior history as a factor in sentencing, virtually all of the increase in an individual’s probability of incarceration in Minnesota from 1981 to 2013 can be attributed to an increase in prior criminal histories. More broadly, a short-term crime wave that differentially affects young people combined with a sentencing system that prioritizes prior history as a rule for punishment can lead directly to higher levels of criminal justice involvement over the life course for the birth cohort that came of age during the crime wave (Shen, Bushway, Sorensen, & Smith, 2019). Short-term crime waves will have fewer long-term consequences for the criminal justice system in sentencing environments that do not prioritize prior history.
- The last 40 years of sentencing reform in the United States was launched primarily because of concerns about the lack of transparency and fairness in the dominate model of indeterminate sentencing (Frankel, 1973). The net result was the invention of new sentencing rules like sentencing guidelines, mandatory minimums, and truth-in-sentencing mandates that dramatically reduced judicial discretion. These rules reduced disparity in sentences, but at the same time, these new, less discretionary rules were often used to dramatically increase incarceration rates (National Research Council, 2014). Yet, there is no necessary connection between a fairer, more transparent system and harsher sentences. The Minnesota sentencing guidelines, which, unlike the federal guidelines, were designed specifically to limit prison growth, showed that guidelines could create a relatively simple, transparent sentencing system that reduced disparity without dramatically increasing incarceration rates (Frase, 2005).
- The RWC article makes an argument that transparent risk tools can tackle the same problems of lack of transparency and disparity originally diagnosed by Frankel (1973). However, the experience of guidelines should have taught us that the devil is in the details, and that substantive sentencing rules, particularly those that reduce individual discretion, can affect overall incarceration as well as transparency and fairness. In this respect, RWC’s emphasis on transparency distracts the reader from the nature of the rules themselves. All else equal, I think we can all agree with RWC that transparent rules are better than black box ones. But, the ultimate prescription for a risk tool that relies on criminal history and age has many consequences, many of which are rather opaque. Even if we agree that tools and rules should be simple and transparent, we need not agree that they should be based on prior history and age (Frase and Roberts, 2020). For example, the creators of the Model Penal Code have a very different view on what should guide sentencing.
- Previous work has noted that a reliance on criminal history might create or perpetuate racial disparities (Harcourt, 2015). In my opinion, there are also deeper, less well understood problems with a reliance on criminal history. These include the fact that the prominent use of prior history can create new criminal behavior, as well as exacerbate the impact of short-term crime waves on the size of the criminal justice system. Moreover, the very fact that prior history is so correlated with recidivism could be an artifact of the current system, rather than an underlying truth about human behavior that truly identifies risk. The inherently black box nature of our current sentencing structures makes it very difficult to unpack the true meaning of any risk assessment conducted on data generated from our current sentencing structure, and should make us deeply skeptical about any policy that places all of our eggs in one basket. This is particularly the case when that basket—risk reduction—is just one of the many potentially relevant goals of sentencing.
- Adelman, L., & Deitrich, J. (2008). Marvin Frankel's mistakes and the need to rethink federal sentencing, Berkeley Journal of Criminal Law, 13:239-260. https://doi.org/10.2139/ssrn.1393469
- Aharonson, E. (2013). Determinate sentencing and American exceptionalism: The underpinnings and effects of cross-national differences in the regulation of sentencing discretion. Law and Contemporary Problems, 76,161–187. Retrieved from  https://scholarship.law.duke.edu/lcp
- American Law Institute. (2017). Model Penal Code, proposed final draft. Retrieved from https://robinainstitute.umn.edu/publications/model-penal-code-sentencing-proposed-final-draft-approved-may-2017
- Atwood, M. (2015). Morning in the Burned House. New York, NY: Houghton Mifflin Harcourt.
- Bushway, S., & Smith, J. (2007). Sentencing using statistical treatment rules: What we don’t know can hurt us. Journal of Quantitative Criminology, 23, 377–387. https://doi.org/10.1007/s10940-007-9035-1
- Chin, G. J. (2012). The new civil death: Rethinking punishment in the era of mass conviction. University of Pennsylvania Law Review, 160, 1789–1833. Retrieved from https://scholarship.law.upenn.edu/penn_law_review/
- Denver, M., Siwach, G., & Bushway, S. (2017). A new look at the employment and recidivism relationship through the lens of a criminal record. Criminology, 55(1), 174–204. https://doi/org/10.1111/1745-9125.12130
- Eisenberg, T., & Hans, V. P. (2009). Taking a stand on taking the stand: The effect of a prior criminal record on the decision to testify and on trial outcomes. Cornell Law Review, 94, 1353–1390. https://doi.org/10.2139/ssrn.998529
- Frankel, M. (1973). Criminal sentences: Law without order. New York, NY: Hill and Wang.
- Frase, R. (2005). Sentencing guidelines in Minnesota, 1978–2003. Crime & Justice, 32:131- 219. https://doi.org/10.1086/655354
- Frase, R. & Roberts, J. (2020). Paying for the Past: The Case Against Prior Record Sentence Enhancements. New York, NY: Oxford University Press.
- Garrett, B., Jakubow, A., & Monahan, J. (2019). Judicial reliance on risk assessment in sentencing drug and property offenders: A test of the treatment resource hypothesis. Criminal Justice and Behavior 46:799–810. https://doi.org/10.1177/0093854819842589
- Gottfredson, D. (1999, November). Effects of judges sentencing decisions on criminal careers. National Institute of Justice Research in Brief.  Washington, D.C.; U.S. Department of Justice. https://doi.org/10.1037/e513192006-001
- Harcourt, B. (2015). Risk as a proxy for race: The dangers of risk assessment. Federal Sentencing Reporter, 27, 237–243. https://doi.org/10.1525/fsr.2015.27.4.237
- King, R. D. (2019). Cumulative impact: Why prison sentences have increased. Criminology, 57(1), 157–180. https://doi.org/10.1111/1745-9125.12197
- Kleiman, M., Ostrom, B., & Cheesman, F. (2007). Using risk assessment to inform sentencing decisions for nonviolent offenders in Virginia. Crime and Delinquency, 53(1), 106–132. https://doi.org/10.1177/0011128706294442
- Monahan, J., Metz, A., & Garrett, B. (2018). Judicial appraisals of risk assessment in sentencing. Behavioral Sciences and the Law, 36, 565–575. https://doi.org/10.1002/bsl.2380
- Mitchell, O. (2005). A meta-analysis of race and sentencing research: Explaining the inconsistencies. Journal of Quantitative Criminology, 21, 439–466. https://doi.org/10.1007/s10940-005-7362-7
- National Research Council. (2014). The growth of incarceration in the United States: Exploring causes and consequences. J. Travis, B. Western, & F. S. Redburn (Eds.). Washington, DC: National Academies Press. https://doi.org/10.17226/18613
- Robinson, P. (2001). Punishing dangerousness: Cloaking preventive detention as criminal justice. Harvard Law Review, 114, 1429-1458. https://doi.org/10.4324/9781315258089-8
- Rudin, C., Wang, C., & Coker, B. (2020). The age of secrecy and unfairness in recidivism prediction. Harvard Data Science Review, 2(1).
- Spohn, C. (2008). How do judges decide? The search for fairness and justice in punishment (2nd ed.). Thousand Oaks, CA: Sage Publications. https://doi.org/10.4135/9781452275048
- Shen, Y., Bushway, S. D., Sorensen, L., & Smith, H. L. (2019). Locking up my generation: Cohort differences in prison spells over the life course. Working Paper, University at Albany.
- Stevenson, M. (2018). Assessing risk assessment in action. Minnesota Law Review, 103, 303-384. https://doi.org/10.2139/ssrn.3016088
- Stith, K., & Cabranes, J. (1998). Fear Of judging: Sentencing guidelines in the federal courts.  Chicago, IL: University of Chicago Press.
- 
- This article is © 2020 by Shawn D. Bushway. The article is licensed under a Creative Commons Attribution (CC BY 4.0) International license (https://creativecommons.org/licenses/by/4.0/legalcode), except where otherwise indicated with respect to particular material included in the article. The article should be attributed to the authors identified above.

URL: https://thecrimereport.org/2019/09/17/risk-assessment-shows-no-evidence-of-race-bias-psychologist/
- The sometimes-controversial use of risk assessments to advise judges on sentences and pretrial release decisions was defended Monday by a psychologist who has studied the practice.
- Risk assessments “can increase consistency, transparency and accuracy” of judicial decisions, Jennifer Skeem, associate dean of research and associate professor of social welfare and public policy at the University of California, Berkeley, said in a major address to the National Forum on Criminal Justice.
- Skeem spoke at the opening session of the annual forum, held this year in Crystal City, Va., and sponsored by the National Criminal Justice Association and the International Community Corrections Association.
- In the risk assessment process, the backgrounds of convicted and accused persons are analyzed with the idea of predicting their likelihood of committing a crime.
- Skeem acknowledged that the practice has come under criticism, including by former Attorney General Eric Holder and the ProPublica website, in a story headlined “Machine Bias.”
- Holder and others have argued that including defendants’ criminal records in the risk assessment analysis can increase racial bias in a justice system that already is stacked against minorities.
- Skeem contended that research has shown that risk assessment offers better predictions than does the “unaided judgment” of courts.
- An extensive review of post-conviction risk assessments of federal convicts found “no evidence of predictive bias by race,” Skeem said.
- Virginia, one of the first states to use risk assessment extensively, has been able to divert 25 percent of prison-bound low-risk bound offenders away from serving time behind bars, without experiencing an increase in crime, Skeem told the conference.
- She emphasized that properly used, risk assessments do not determine a convict’s sentence or a defendant’s potential release, but merely provide guidance to courts.
- So far, the research shows that risk assessments are a “promising tool” for public safety, to reduce mass incarceration, recidivism, and sentencing disparities, Skeem contended.
- Skeem delivered the annual Edwin I. Megargee Honorary Lecture sponsored by the community corrections organization co-sponsoring the conference.
- Ted Gest is president of criminal justice journalists and Washington bureau chief of The Crime Report.
- Can anyone tell me how to file a motion for a new trial for my son, who was sentenced to LIFE and they never offered him a mental evaluation to see if he could withstand trial? Had they done that it would’ve shown he had mental issues, diagnosed with PTSD, tried suicide twice, was baker acted….his own public defender never brought it up, very unfair to my son! Now, 12 yrs later, he has been treated on occasion (very little, and ignored when he cried for help, didn’t get it) then had a breakdown, stabbed someone and now in a psych ward. Tried suicide in prison also. Such neglect on there end! He has been diagnosed with SEVERE PTSD, due to severity of Prison life, abuse, neglect, solitary confinement all the time along with other mental diagnoses, it never ends. He needed help before trial, was never given the evaluation and obviously it all has affected him! PLEASE, someone help us with advice….how to go back to court for new trial…..I really don’t think he’ll survive this! Anyone out there?….thank you, DEnglish
- I think I am going to believe the 27 academics who are experts in the area of predictive algorithms [and who] recently issued a report concluding that ..”Pretrial risk assessments do not guarantee or even increase the likelihood of better pretrial outcomes. Risk assessment tools can simply shift or obscure problems with current pretrial practices…..”.In addition to this recent statement, over 100 civil rights groups have come out against risk assessments as racially biased and over 80 technology companies (including Google, Facebook and Amazon) have stated that algorithms in the criminal justice system are ineffective and dangerous. [this post has been condensed for space]
- 
- 
- 
- 
- Save my name, email, and website in this browser for the next time I comment.
- 
- 
- Δ
- Republish
- This work is licensed under a Creative Commons Attribution-NoDerivatives 4.0 International License.
- by Ted Gest, The Crime Report September 17, 2019
- This <a target="_blank" href="https://thecrimereport.org/2019/09/17/risk-assessment-shows-no-evidence-of-race-bias-psychologist/">article</a> first appeared on <a target="_blank" href="https://thecrimereport.org">The Crime Report</a> and is republished here under a Creative Commons license.<img src="https://thecrimereport.org/wp-content/uploads/2023/01/2023-favicon.png" style="width:1em;height:1em;margin-left:10px;"><img id="republication-tracker-tool-source" src="https://thecrimereport.org/?republication-pixel=true&post=643128&ga=UA-67192143-1" style="width:1px;height:1px;">
- Type above and press Enter to search. Press Esc to cancel.

URL: https://www.wired.com/story/algorithms-shouldve-made-courts-more-fair-what-went-wrong/
- To revist this article, visit My Profile, then View saved stories.
- To revist this article, visit My Profile, then View saved stories.
- Tom Simonite
- Application
- Ethics
- Prediction
- Safety
- Regulation
- End User
- Government
- Sector
- Public safety
- Kentucky lawmakers thought requiring that judges consult an algorithm when deciding whether to hold a defendant in jail before trial would make the state’s justice system cheaper and fairer by setting more people free. That’s not how it turned out.
- Before the 2011 law took effect, there was little difference between the proportion of black and white defendants granted release to await trial at home without cash bail. After being mandated to consider a score predicting the risk a person would reoffend or skip court, the state’s judges began offering no-bail release to white defendants much more often than to blacks. The proportion of black defendants granted release without bail increased only slightly, to a little over 25 percent. The rate for whites jumped to more than 35 percent. Kentucky has changed its algorithm twice since 2011, but available data shows the gap remained roughly constant through early 2016.
- The Kentucky experience, detailed in a study published earlier this year, is timely. Many states and counties now calculate “risk scores” for criminal defendants that estimate the chance a person will reoffend before trial or skip court; some use similar tools in sentencing. They are supposed to help judges make fairer decisions and cut the number of people in jail or prison, sometimes as part of eliminating cash bail. Since 2017, Kentucky has released some defendants scored as low-risk based purely on an algorithm’s say-so, without a judge being involved.
- How these algorithms change the way justice is administered is largely unknown. Journalists and academics have shown that risk-scoring algorithms can be unfair or racially biased. The more crucial question of whether they help judges make better decisions and achieve the tools’ stated goals is largely unanswered.
- The Kentucky study is one of the first in-depth, independent assessments of what happens when algorithms are injected into a justice system. It found that the project missed its goals and even created new inequities. “The impacts are different than what policymakers may have hoped for,” says Megan Stevenson, a law professor at George Mason University who authored that study.
- Stevenson looked at Kentucky in part because it was a pioneer of bail reform and algorithm-assisted justice. The state began using pretrial risk scores in 1976, a simple system that assigned defendants points based on questions about their employment status, education, and criminal record. The system was refined over time, but the scores were used inconsistently. In 2011, a law called HB 463 mandated their use for judges’ pretrial decisions, creating a natural experiment.
- 
- Kentucky’s lawmakers intended HB 463 to reduce incarceration rates, a common motivation for using risk scores. They are supposed to make judges better at assessing who is safe to release. Sending a person home makes it easier for them to continue their work and family life and saves the government money. More than 60 percent of the 730,000 people held in local jails in the US have not been convicted, according to the nonprofit Prison Policy Initiative.
- The system used in Kentucky in 2011 employed a point system to produce a score estimating the risk that a defendant will skip their court date or reoffend before trial. A simple framework translated the score into a rating of low-, moderate-, or high-risk. People tagged as low- or moderate-risk generally should be released without cash bail, the law says.
- Angela Watercutter
- WIRED Staff
- Chris Stokel-Walker
- Rhett Allain
- But judges appear not to have trusted that system. After the law took effect, they overruled the system’s recommendation more than two-thirds of the time. More people got sent home, but the increase was small; around the same time, authorities reported more alleged crimes by people on release pending trial. Over time, judges reverted to their prior ways. Within a couple of years, a smaller proportion of defendants was being released than before the bill came into force.
- Although more defendants were granted release without bail, the change mostly helped white people. “On average white defendants benefited more than black defendants,” Stevenson says. The pattern held after Kentucky adopted a more complex risk-scoring algorithm in 2013.
- One explanation supported by Kentucky data, she says, is that judges responded to risk scores differently in different parts of the state. In rural counties, where most defendants were white, judges granted release without bond to significantly more people. Judges in urban counties, where the defendant pool was more mixed, changed their habits less.
- A separate study using Kentucky data, presented at a conference this summer, suggests a more troubling effect was also at work. It found that judges were more likely to overrule the default recommendation to waive a financial bond for moderate-risk defendants if the defendants were black.
- Harvard researcher Alex Albright, who authored that study, says it shows more attention is needed to how humans interpret algorithms’ predictions. “We should put as much effort into how we train people to use predictions as we do into the predictions,” she says.
- Michael Thacker, risk-assessment coordinator with Kentucky pretrial services, said his agency tries to mitigate potential bias in risk-assessment tools and talks with judges about the potential for “implicit bias” in how they interpret the risk scores.
- An experiment that tested how judges react to hypothetical risk scores for determining sentences also found evidence that algorithmic advice can cause unexpected problems. The study, which is pending publication, asked 340 judges to decide sentences for made-up drug cases. Half of the judges saw “cases” with risk scores estimating the defendant had a medium to high risk of rearrest and half did not.
- When they weren’t given a risk score, judges were tougher on more-affluent defendants than poor ones. Adding the algorithm reversed the trend: Richer defendants had a 44 percent chance of doing time but poorer ones a 61 percent chance. The pattern held after controlling for the sex, race, political orientation, and jurisdiction of the judge.
- “I thought that risk assessment probably wouldn’t have much effect on sentencing,” says Jennifer Skeem, a UC Berkeley professor who worked on the study with colleagues from UC Irvine and the University of Virginia. “Now we understand that risk assessment can interact with judges to make disparities worse.”
- There is reason to think that if risk scores were implemented carefully, they could help make the criminal justice system fairer. The common practice of requiring cash bail is widely acknowledged to exacerbate inequality by penalizing people of limited means. A National Bureau of Economic Research study from 2017 used past New York City records to project that an algorithm predicting whether someone will skip a court date could cut the jail population by 42 percent and shrink the proportion of black and Hispanic inmates, without increasing crime.
- Angela Watercutter
- WIRED Staff
- Chris Stokel-Walker
- Rhett Allain
- Unfortunately, the way risk-scoring algorithms have been rolled out across the US is much messier than in the hypothetical world of such studies.
- Criminal justice algorithms are generally relatively simple and produce scores from a small number of inputs such as age, offense, and prior convictions. But their developers have sometimes restricted government agencies using their tools from releasing information about their design and performance. Jurisdictions haven’t allowed outsiders access to the data needed to check their performance.
- “These tools were deployed out of reasonable desire for evidence-based decisionmaking, but it was not done with sufficient caution,” says Peter Eckersley, director of research at Partnership on AI, a nonprofit founded by major tech companies to examine how the technology affects society. PAI released a report in April that detailed problems with risk assessment algorithms and recommended agencies appoint outside bodies to audit their systems and their effects.
- Stevenson agrees that greater transparency is needed—but also admits to feeling it may be too late to turn risk-scoring algorithms into a success, given their poor reputation and the slim gains they seem to offer. “The criminal justice system has such little good will already that I don’t want people to lose any more hope or faith at this point,” she says.
- Will Knight
- Justin Ling
- Matt Burgess
- Tracy Wen Liu
- Morgan Meaker
- Vittoria Elliott
- Dell Cameron
- More From WIRED
- Contact
- © 2023 Condé Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights. WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast. Ad Choices

URL: https://eji.org/news/risk-assessment-tool-led-to-harsher-sentences-for-young-or-black-defendants/
- Criminal Justice Reform
- Racial Justice
- Anti-Poverty
- Public Education
- 124 results for "Prison"
- 11.25.19
- An analysis of sentencing in Virginia showed that the state’s risk assessment system increased sentences for Black and young defendants, The Washington Post reported.
- Researchers at George Mason University and Texas A&M examined tens of thousands of felony convictions in Virginia, focusing on the period between 2000 and 2004, to measure how judges used a risk assessment algorithm adopted statewide in 2002.
- The study revealed that defendants younger than 23 were 4 percentage points more likely to be incarcerated after risk assessment was adopted, and their sentences were 12% longer than their older peers.
- Risk scores are largely based on age—in Virginia, 13 points are added to the score for being younger than 30. (Having five or more previous incarcerations as an adult adds only 9 points.) A 2018 analysis showed 58% of the popular COMPAS algorithm’s score can be traced to age.
- “People are getting this very stigmatic label—high risk for violent recidivism—largely because they’re 19 years old,” researcher Megan Stevenson said.
- Racial disparities also increased in the Virginia circuits that used risk assessment most. The algorithm doesn’t use race in its calculations, but Black defendants were 4 percentage points more likely to be incarcerated after risk assessment was adopted, compared with otherwise equivalent white defendants. Black defendants’ sentences were also 17% longer.
- The risk assessment system was intended to divert from prison people convicted of nonviolent offenses to free up space for people convicted of violent offenses, who were expected to serve longer terms after Virginia abolished discretionary parole, Meredith Farrar-Owens, director of the Virginia Criminal Sentencing Commission, told The Post.
- But the algorithm did not change the rate at which people were incarcerated or the length of their sentences, in part because judges did not follow the algorithm’s suggestions in most cases.
- Judges were supposed to impose shorter sentences on people with low risk scores, or send them to probation or substance abuse treatment instead of prison. Some judges reported they weren’t trained to use the risk assessment tool; others said no diversion programs were available. Some didn’t use the risk scores at all in their sentencing decisions.
- If judges had fully complied with the algorithm’s recommendations, researchers found, disparities based on race and age would have been even greater.
- “Virginia’s nonviolent risk assessment reduced neither incarceration nor recidivism; its use disadvantaged a vulnerable group (the young); and failed to reduce racial disparities,” the study authors concluded.
- Similar algorithms are used in 28 states.
- 11-Year-Old Child Shot by Police After Calling 911 for Help
- EJI Brings Classical Musicians Together to Engage with History of Racial Inequality
- Minnesota Abolishes Juvenile Life-Without-Parole Sentences
- Three Men Murdered in Two Days at Alabama Prisons
- 122 Commerce Street Montgomery, AL 36104
(334) 269-1803
[email protected]
- Subscribe to our newsletter
- © Equal Justice Initiative 2023

URL: https://wisconsinwatch.org/2019/02/q-a-risk-assessments-explained/
- Sign up for our free story updates and Friday news roundups.
- 
- Wisconsin Watch
- Nonprofit, nonpartisan news about Wisconsin
- [vimeo 243723065 w=771 h=434]
- Wisconsin Watch is a nonpartisan, nonprofit investigative news outlet. We increase the quality and quantity of investigative reporting in Wisconsin, while training current and future investigative journalists. Our work fosters an informed citizenry and strengthens democracy.
- We currently have reporters based in Madison, Milwaukee, and Oshkosh, and have other members of our robust editorial and business teams located across the state.We are  a 501(c)(3) charitable organization operated by a professional staff  under the guidance of a nationally noted board of directors. Our legal name is Wisconsin Center for Investigative Journalism (WCIJ Inc.).
- Wisconsin Watch collaborates with, but is independent of, the University of Wisconsin-Madison School of Journalism & Mass Communication, where it is housed; Marquette University, where we share office space with the Milwaukee Neighborhood News Service; Wisconsin Public Radio; Wisconsin Public Television — and with mainstream and ethnic news media across the nation. All works created, published, posted or disseminated by Wisconsin Watch do not necessarily reflect the views or opinions of the University of Wisconsin-Madison or any of its affiliates.
- Wisconsin Watch is a founding member of the Institute for Nonprofit News, a group of nonprofit journalism organizations that conduct investigative reporting in the public interest.
- Wisconsin Watch is a member of the Trust Project, a global network of news organizations that has developed transparency standards to help news readers assess the quality and credibility of journalism. Learn more about how we incorporate the Trust Indicators on our site.
- Wisconsin Watch is also a member of The Global Investigative Journalism Network, an international network of nonprofit organizations founded to support, promote and produce investigative journalism.
- If you value what you get from Wisconsin Watch, make a tax-deductible donation today so we can continue doing the statewide investigations that matter to you.
- Your contribution is appreciated.
- How we workRepublishing guidelinesEthicsFact-checkingCorrections and clarificationsGuidelines on use of unnamed sourcesUser agreement and privacy policy
- StaffBoard of DirectorsJournalism Advisory BoardFormer staff, fellows and interns: Where they are now
- DEI at Wisconsin WatchDiversity, Equity and Inclusion Report, 2022DEI Board Task Force and Staff CommitteeStaff demographicsOur anti-racism stand and a pledge of action
- Contact usPublic Engagement and OpinionFollow us on Facebook, Twitter and InstagramSubscribe to our free email newsletters
- Make a giftWatchdog Club and Leadership CircleJoin the Legacy SocietySponsorshipHow we are fundedTax returns and financial statementsFundraising policy
- Project archivesAudio storiesInstagramWisconsinWeeklyWisconsinWatch.org newsletter archivesNews about Wisconsin Watch
- Awards and honorsAnnual ReportsPartners and collaboratorsImpact: How our stories make a differenceThe Trust ProjectHow we track our storiesTestimonials from journalists, educators and the public
- InternshipsCurrent job openingsFor students: How to get involved with Wisconsin WatchPitch guidelines for freelancersSend us a tipBe your own watchdogOpen government resourcesWisconsin Watch logos
- Reading Time:  4 minutespost
- Answer: No. Judges and court commissioners emphasize that pretrial risk assessment tools are just one piece of information they take into account when making bail decisions.
- During bail hearings in Dane and Milwaukee counties — which are both using a risk assessment tool called the Public Safety Assessment — court commissioners also hear bail arguments from defense attorneys and prosecutors.
- Answer: There are many different risk assessment tools, used at various points in the criminal justice system including during bail hearings, sentencing, probation determinations and parole decisions. As of 2017, there were as many as 60 risk assessment tools used across the United States, according to a report by the Center for Court Innovation, a nonprofit that conducts research and assists with criminal justice reform efforts around the world.
- The tools can be used to connect offenders to the right rehabilitation programs, determine appropriate levels of supervision like electronic monitoring, reduce the sentences of offenders who pose low risks of recidivism and divert low-risk offenders from jail or prison.
- Some risk assessment tools are better-designed than others, so if one is not working well, that is not a reflection on all of the tools. Each tool should be validated, meaning it needs to be tested for accuracy on the population of the jurisdiction where it will be used.
- Answer: Sometimes. One risk assessment tool called the Correctional Offender Management
- Profiling for Alternative Sanctions, or COMPAS, faced controversy in 2016 because of its proprietary nature.
- In a case that went to the Wisconsin Supreme Court, the tool’s creator, then called Northpointe, Inc., would not reveal how the COMPAS algorithm determined its risk scores or how it weighted certain factors, citing trade secrets. A Wisconsin man who was sentenced to six years — in part because of a COMPAS score — argued that the secretive nature of the tool violated his due process rights. The court upheld the use of the tool but outlined precautions for judges to consider.
- Other tools, including the PSA, are more transparent. The risk factors the PSA uses and how they are weighted when calculating risk are publicly available. Another tool, The Virginia Pretrial Risk Assessment Instrument (VPRAI), is also transparent about its methods.
- Still, Logan Koepke, senior policy analyst at Upturn, a nonprofit that researches and advocates for technologies that promote equity in the criminal justice system, said even Arnold Ventures, which developed the PSA, does not reveal the data it uses to create the algorithm, which includes about 750,000 cases from across the United States. He would like a “full line of sight” into the datasets used to create any pretrial risk assessment tool.
- Answer: It’s complicated.
- 
- If you value news from Wisconsin Watch, make a tax-deductible donation today so we can continue doing statewide investigations that matter to you.
- Your contribution is appreciated.
- In 2016, ProPublica found that COMPAS was “particularly likely to falsely flag black defendants as (potential) future criminals, wrongly labeling them this way at almost twice the rate as white defendants.” Because black defendants were disproportionately affected by these “false positives,” ProPublica concluded the algorithm was “biased against blacks.”
- Researchers from California State University, the Administrative Office of the U.S. Courts and the Crime and Justice Institute challenged ProPublica’s analysis. When they re-analyzed ProPublica’s data, they found no evidence of racial bias.
- COMPAS predicted risk correctly at essentially the same rates for both black and white defendants, they found. For example, for defendants who were rated high risk, 73 percent of white defendants reoffended and 75 percent of black defendants reoffended.
- But when one group — in this case, black defendants — has a higher base rate of recidivism, that group is mathematically guaranteed to have more false positives, said Sharad Goel, an assistant professor at Stanford University in management science and engineering.
- But that does not mean the algorithm itself is biased; it reflects “real statistical patterns” driven by “social inequalities” in the justice system, Goel said.
- “If black defendants have a higher overall recidivism rate, then a greater share of black defendants will be classified as high risk … (and) a greater share of black defendants who do not reoffend will also be classified as high risk,” Goel said in a 2016 column he co-wrote in the Washington Post.
- One way to reduce the differences would be to systematically score white defendants as riskier than they actually are, but then the algorithm would be treating defendants differently based on race, he said.
- If racial disparities in the criminal justice system were to be corrected — a big task that is outside the scope of a single tool — better data could be put into the algorithms, said Chris Griffin, consultant to the Harvard Access to Justice Lab, which is doing research in Dane County on the PSA.
- Early data from the PSA show more black and Latino defendants are released pretrial when a risk assessment tool is being used, according to Arnold Ventures. According to the Pretrial Justice Institute, both the PSA and the VPRAI are capable of producing race-neutral results.
- Answer: Data show that the vast majority of defendants are low risk when they are released pretrial, meaning most show up for court and do not commit new crimes. Data have also shown that 98 percent or more of pretrial defendants do not commit new violent crimes when released.
- Risk assessment tools recommend the release of the low-risk defendants and the detention of the high risk, potentially violent offenders. Counties get to choose where to draw the line on how much risk they are willing to accept.
- In any system, mistakes will be made because it is impossible to predict future human behavior with 100 percent accuracy. In a system without risk assessment tools, dangerous defendants can post bail and commit a new violent crime while released.
- The only way to prevent all new violent criminal activity among defendants awaiting trial would be to detain everyone. That would be unconstitutional.
- The nonprofit Wisconsin Center for Investigative Journalism (www.WisconsinWatch.org) collaborates with Wisconsin Public Radio, Wisconsin Public Television, other news media and the UW-Madison School of Journalism and Mass Communication. All works created, published, posted or disseminated by the Center do not necessarily reflect the views or opinions of UW-Madison or any of its affiliates.
- Republish This Story
- 
- Republish our articles for free, online or in print, under a Creative Commons license.
- This work is licensed under a Creative Commons Attribution-NoDerivatives 4.0 International License.
- For questions regarding republishing rules please contact Andy Hall, executive director, at ahall@wisconsinwatch.org
- by Emily Hamer, Wisconsin Watch February 17, 2019
- This <a target="_blank" href="https://wisconsinwatch.org/2019/02/q-a-risk-assessments-explained/">article</a> first appeared on <a target="_blank" href="https://wisconsinwatch.org">Wisconsin Watch</a> and is republished here under a Creative Commons license.<img src="https://i0.wp.com/wisconsinwatch.org/wp-content/uploads/2021/02/cropped-WCIJ_IconOnly_FullColor_RGB-1.png?fit=150%2C150&amp;quality=100&amp;ssl=1" style="width:1em;height:1em;margin-left:10px;"><img id="republication-tracker-tool-source" src="https://wisconsinwatch.org/?republication-pixel=true&post=665355&amp;ga3=UA-17896820-1" style="width:1px;height:1px;">
- Emily Hamer is a recent graduate of UW–Madison with degrees in journalism and philosophy. She has formerly worked as an intern for University Communications and WisPolitics, and as an editor at The Badger Herald newspaper.
- 
- Your contribution is appreciated.
- 
- Mailing address:
Wisconsin Watch
P.O. Box 5079
Milwaukee, WI 53205
- 608-262-3642
- info@wisconsinwatch.org
- Send Us A Tip
- Wisconsin Watch is a project of the Wisconsin Center for Investigative Journalism (WCIJ Inc.) — a 501(c)(3) nonprofit organization.

- Utah online dispute resolution system
- Verus prison inmate call monitoring
- Page infoType: SystemPublished: March 2023
