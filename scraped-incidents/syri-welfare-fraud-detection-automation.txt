- Netherlands SyRI welfare fraud detection automation
- Released: 2014
- Can you improve this page?Share your insights with us
- SyRI (or 'System Risk Indication') is a risk classification system developed and operated by the Netherlands government to detect and predict social security, tax and employment fraud.
- Deployed by the Department of Social Affairs and Employment, the system used data about employment, fines, penalties, taxes, property, housing, education, retirement, debts, benefits, allowances, subsidies, permits and exemptions, amongst others, to determine whether a welfare claimant should be investigated.
- In February 2020, SyRI was found to be in breach of human rights law by a Dutch court, which ordered an immediate stop to its use. The government had been taken to court (pdf) by a number of civil rights organisations and two citizens.
- The court ruled that SyRI violated article 8 of the European Convention on Human Rights (ECHR), which protects the right to respect for private and family life, and that by primarily targeting poor neighbourhoods, the system may have discriminated against people on on the basis of socioeconomic or migrant status.
- The court also ruled that the legislation which permitted SyRI contained insufficient safeguards against privacy intrusions, and that the 'fair balance' between its objectives and the violation of privacy that its use entailed, meant the legislation was unlawful.
- SyRI was also roundly criticised for its opaque nature. The court took it to talk for a 'serious lack of transparency' about how its risk scoring algorithm worked, stating that the use of the system is 'insufficiently clear and controllable'.
- Human Rights Watch complained that the Dutch government refused during the hearing to disclose 'meaningful information' about how SyRI uses personal data to draw inferences about possible fraud.
- Despite the Dutch government's decision not to appeal the ruling and stop using SyRI, a Lighthouse Reports investigation discovered that it had quietly continued to deploy an adapted SyRI in some of the country’s most vulnerable neighbourhoods.
- The finding led to a joint investigation by Lighthouse Reports and WIRED that found that a machine learning algorithm used by the Municipality of Rotterdam to detect welfare fraud discriminates against welfare claimants based on ethnicity, age, gender, and parenthood.
- Operator: Ministry of Social Affairs and Employment (CZW); Benefits Intelligence Agency Foundation; Municipality of Rotterdam Developer: Ministry of Social Affairs and Employment (CZW); Benefits Intelligence Agency FoundationCountry: NetherlandsSector: Govt - welfarePurpose: Detect and predict welfare fraud Technology: Risk assessment algorithm; Machine learning Issue: Bias/discrimination - race, ethnicity, economic; Privacy; Scope creep/normalisation Transparency: Governance; Black box; Complaints/appeals; Marketing; Legal
- Ministry of Social Affairs and Employment
- Court of the Hague (2020). NJCM v State of the Netherlands - Judgement
- The Office of the High Commissioner for Human Rights (2019). Brief by the United Nations Special Rapporteur
- Deikwijs Advocaten/NJCM PILP (2019). Subpoena vs State of the Netherlands (pdf)
- Platform Bescherming Burgerrechten. Bij Voorbat Verdacht
- de Bruijn H., Warnier M., Janssen M. (2022). The perils and pitfalls of explainable AI: Strategies for explaining algorithmic decision-making
- Rachovitsa A., Johann N. (2022). The Human Rights Implications of the Use of AI in the Digital Welfare State: Lessons Learned from the Dutch SyRI Case
- van Bekkum M., Borgesius F.Z. (2021). Digital welfare fraud detection and the Dutch SyRI judgment
- Privacy International (2020). The SyRI case: A landmark ruling for benefits claimants around the world
- Human Rights Watch (2019). Welfare surveillance on trial in the Netherlands
- Center for Human Rights and Global Justice (2019). Profiling the poor in the Dutch welfare state
- Digital Freedom Fund. NJCM, Platform Bescherming Burgerrechten and others v. The Netherlands (the SyRI case) (pdf)
- The Public Interest Litigation Project (PILP) (2015). Profiling and SyRI
- WIRED (2023). This Algorithm Could Ruin Your Life
- Lighthouse Reports (2023). Suspicion Machines
- Lighthouse Reports (2022). The Algorithm Addiction. Mass profiling system SyRI resurfaces in the Netherlands despite ban
- Rekenkamer Rotterdam (2021). Gekleurde Technologie (pdf)
URL: https://techcrunch.com/2020/02/06/blackbox-welfare-fraud-detection-system-breaches-human-rights-dutch-court-rules/
- An algorithmic risk scoring system deployed by the Dutch state to try to predict the likelihood that social security claimants will commit benefits or tax fraud is a breach of human rights law, a court in the Netherlands has ruled.
- The Dutch government’s System Risk Indication (SyRI) legislation uses a non-disclosed algorithmic risk model to profile citizens and has been exclusively targeted at neighborhoods with mostly low-income and minority residents. Human rights campaigners have dubbed it a “welfare surveillance state.”
- A number of civil society organizations in the Netherlands and two citizens instigated the legal action against SyRI — seeking to block its use. The court has today ordered an immediate halt to the use of the system.
- The ruling (now here in English) is being hailed as a landmark judgement by human rights campaigners, with the court basing its reasoning on European human rights law — specifically the right to a private life that’s set out by Article 8 of the European Convention on Human Rights (ECHR) — rather than a dedicated provision in the EU’s data protection framework (GDPR) which relates to automated processing.
- GDPR’s Article 22 includes the right for individuals not to be subject to solely automated individual decision-making where they can produce significant legal effects. But there can be some fuzziness around whether this applies if there’s a human somewhere in the loop, such as to review a decision on objection.
- In this instance the court has sidestepped such questions by finding SyRI directly interferes with rights set out in the ECHR.
- Specifically, the court found that the SyRI legislation fails a balancing test in Article 8 of the ECHR which requires that any social interest to be weighed against the violation of individuals’ private life, with a fair and reasonable balance being required. The automated risk assessment system failed this test in the court’s view.
- Legal experts suggest the decision sets some clear limits on how the public sector in the UK can make use of AI tools — with the court objecting in particular to the lack of transparency about how the algorithmic risk scoring system functioned.
- In a press release about the judgement (translated to English using Google Translate), the court writes that the use of SyRI is “insufficiently clear and controllable”. While, per Human Rights Watch, the Dutch government refused during the hearing to disclose “meaningful information” about how SyRI uses personal data to draw inferences about possible fraud.
- The court clearly took a dim view of the state trying to circumvent scrutiny of human rights risk by pointing to an algorithmic “blackbox” and shrugging.
- The Court's reasoning doesn't imply there should be full disclosure, but it clearly expects much more robust information on the way (objective criteria) that the model and scores were developed and the way in which particular risks for individuals were addressed.
- — Joris van Hoboken (@jorisvanhoboken) February 6, 2020
- 
- The UN special rapporteur on extreme poverty and human rights, Philip Alston — who intervened in the case by providing the court with a human rights analysis — welcomed the judgement, describing it as “a clear victory for all those who are justifiably concerned about the serious threats digital welfare systems pose for human rights.”
- “This decision sets a strong legal precedent for other courts to follow. This is one of the first times a court anywhere has stopped the use of digital technologies and abundant digital information by welfare authorities on human rights grounds,” he added in a press statement.
- Back in 2018, Alston warned that the UK government’s rush to apply digital technologies and data tools to socially re-engineer the delivery of public services at scale risked having an immense impact on the human rights of the most vulnerable.
- So the decision by the Dutch court could have some near-term implications for UK policy in this area.
- The judgement does not shut the door on the use by states of automated profiling systems entirely, but it does make it clear that human rights law in Europe must be central to the design and implementation of rights risking tools.
- It also comes at a key time when EU policymakers are working on a framework to regulate artificial intelligence — with the Commission pledging to devise rules that ensure AI technologies are applied ethically and in a human-centric way.
- It remains to be seen whether the Commission will push for pan-EU limits on specific public sector uses of AI (such as for social security assessments). A recent leaked draft of a white paper on AI regulation suggests it’s leaning towards risk assessments and a patchwork of risk-based rules.

URL: https://www.theguardian.com/technology/2020/feb/05/welfare-surveillance-system-violates-human-rights-dutch-court-rules
- Government told to halt use of AI to detect fraud in decision hailed by privacy campaigners
- A Dutch court has ordered the immediate halt of an automated surveillance system for detecting welfare fraud because it violates human rights, in a judgment likely to resonate well beyond the Netherlands.
- The case was seen as an important legal challenge to the controversial but growing use by governments around the world of artificial intelligence (AI) and risk modelling in administering welfare benefits and other core services.
- Campaigners say such “digital welfare states” – developed often without consultation, and operated secretively and without adequate oversight – amount to spying on the poor, breaching privacy and human rights norms and unfairly penalising the most vulnerable.
- In the UK, where the government is accelerating the development of robots in the benefits system, the chairman of the House of Commons work and pensions select committee, Stephen Timms, said: “This ruling by the Dutch courts demonstrates that parliaments ought to look very closely at the ways in which governments use technology in the social security system, to protect the rights of their citizens.”
- The UN special rapporteur on extreme poverty and human rights, Philip Alston, applauded the verdict and said it was “a clear victory for all those who are justifiably concerned about the serious threats digital welfare systems pose for human rights”.
- The decision “sets a strong legal precedent for other courts to follow”, he added. “This is one of the first times a court anywhere has stopped the use of digital technologies and abundant digital information by welfare authorities on human rights grounds.”
- The verdict will be watched closely by welfare rights campaigners in the UK, where the Department for Work and Pensions is engaged in a digitisation drive that vulnerable claimants fear could plunge them further into hunger and debt.
- Artificial Intelligence has various definitions, but in general it means a program that uses data to build a model of some aspect of the world. This model is then used to make informed decisions and predictions about future events. The technology is used widely, to provide speech and face recognition, language translation, and personal recommendations on music, film and shopping sites. In the future, it could deliver driverless cars, smart personal assistants, and intelligent energy grids. AI has the potential to make organisations more effective and efficient, but the technology raises serious issues of ethics, governance, privacy and law.
- A Guardian investigation in October found the Department for Work and Pensions (DWP) had increased spending to about £8m a year on a specialist “intelligent automation garage” where computer scientists were developing more than 100 welfare robots, deep learning and intelligent automation for use in the welfare system.
- The Dutch government’s risk indication system (SyRI) is a risk calculation model developed over the past decade by the social affairs and employment ministry to predict the likelihood of an individual committing benefit or tax fraud or violating labour laws.
- Deployed primarily in low-income neighbourhoods, it gathers government data previously held in separate silos, such as employment, personal debt and benefit records, and education and housing histories, then analyses it using a secret algorithm to identify which individuals might be at higher risk of committing benefit fraud.
- A broad coalition of privacy and welfare rights groups, backed by the largest Dutch trade union, argued that poor neighbourhoods and their inhabitants were being spied on digitally without any concrete suspicion of individual wrongdoing. SyRI was disproportionately targeting poorer citizens, they said, violating human rights norms.
- The court ruled that the SyRI legislation contained insufficient safeguards against privacy intrusions and criticised a “serious lack of transparency” about how it worked. It concluded in its ruling that, in the absence of more information, the system may, in targeting poor neighbourhoods, amount to discrimination on the basis of socioeconomic or migrant status.
- The system did not pass the test required by the European convention on human rights of a “fair balance” between its objectives, namely to prevent and combat fraud in the interest of economic wellbeing, and the violation of privacy that its use entailed, the court added, declaring the legislation was therefore unlawful. The Dutch government can appeal against the decision.
- Christiaan van Veen, director of the digital welfare state and human rights project at New York University School of Law, said it was “important to underline that SyRI is not a unique system; many other governments are experimenting with automated decision-making in the welfare state”.
- Van Veen cited Australia and the UK as countries where such concerns were particularly acute. “This strong ruling will set a strong precedent globally that will encourage activists in other countries to challenge their governments,” he said.
- Alston predicted the judgment would be “a wake-up call for politicians and others, not just in the Netherlands”. The special rapporteur presented a report to the UN general assembly in October on the emergence of the “digital welfare state” in countries around the globe, warning of the need “to alter course significantly and rapidly to avoid stumbling, zombie-like, into a digital welfare dystopia”.
- In the UK, as well as contracts with the outsourcing multinationals IBM, Tata Consultancy and Capgemini, the DWP is also working with UiPath, a New York-based company co-founded by Daniel Dines, the world’s first “bot billionaire”, who last month said: “I want a robot for every person.”
- His software is being deployed in an effort to introduce machine learning to check benefit claims, which suggests welfare computers will autonomously learn and alter the way they make decisions with minimum human intervention.

URL: https://www.irishtimes.com/news/world/europe/dutch-court-halts-programme-where-algorithms-hunt-for-welfare-fraudsters-1.4191784
- Dutch prime minister Mark Rutte:  those in his  government who decided how the algorithms were going to work must have known what they were doing.  Photograph: Getty Images
- In a world in which algorithms are increasingly trusted to run many of the automated systems that manage our lives – often just because they can – it was heartening to see a Dutch court cry halt the other day to the government’s most ambitious programme yet used to “detect” fraud.
- Mark Rutte’s coalition government was told by judges in The Hague that not alone was its much-heralded SyRI system – which stands for Systeem Risico Indicatie or system of indicated risks – in breach of domestic privacy laws, but also contrary to the European Convention on Human Rights.
- The court also made public a letter from Philip Alston, international lawyer and UN Rapporteur on Extreme Poverty and Human Rights, in which he warned that of its nature the system was weighted against the less well-off and those from minority backgrounds.
- On paper SyRI must have looked like a great idea. The idea was to take a lead among EU countries, connecting all government departments to the SyRI system, overseen by the Dutch department of social affairs since 2014.
- Algorithms were then created based on profiles of people who had been caught committing social security fraud or housing fraud, for example, and who were blissfully unaware their data was being used. Those algorithms were then “released” into the system to hunt for others with similar profiles – who were allegedly just waiting to do the same.
- On that basis blameless individuals were being treated as suspects “without reason”, the court ruled.
- It also criticised the “secrecy” surrounding the construction of the algorithms, which it said made them “difficult to monitor and control”.
- The thing about algorithms – essentially sets of digital instructions that tell a computerised system to work in a particular way in response to particular information – is that they’re only, at this stage of their development at least, as good as the programmers who create them.
- Often what they do most strikingly is highlight the biases of their creators as to what they are likely to “detect” when they run the dubious assumptions at the heart of their architecture over the administrative minutiae of our daily lives – what they take to be our identities.
- In the case of SyRI they were starting to reveal an even more ominous picture: the self-validating – not to mention often downright incorrect – assumptions of those who run government as to who in society is trying, or likely to try, to rip off the state and why.
- In that sense they are essentially a cheap alternative to proper social services where suitably qualified human beings interact with the population, establish which systems work and which don’t, and fix them for the benefit of everyone involved. Sounds like a plan? The problem is it costs a fortune.
- The same applies when it comes to crime or terrorism, for instance. Profiling only tells you what you already know. It puts you at ease with your expectations. Meanwhile – as has happened – the jihadists turn out to be the blond-haired guys in American-style SUVs speaking English.
- There’s good news and bad news in the SyRI case. The good news is that the courts are playing an invaluable role in protecting society.
- The bad news is that those in the Dutch government who decided how the algorithms were going to work must have known what they were doing. They must have known better. But they went ahead anyway.
- © 2023 The Irish Times DAC
- © 2023 The Irish Times DAC

URL: https://www.economist.com/technology-quarterly/2020/06/11/humans-will-add-to-ais-limitations
- IN 1958 A psychologist and computer-science researcher named Frank Rosenblatt gave a public demonstration of his Perceptron, the distant ancestor of modern machine-learning algorithms. The Perceptron had been developed on a 9-tonne IBM 704, a mainframe computer with less power than a modern television remote control. Its party trick was its ability to learn, without any direct programming, to recognise cards printed on the left from those printed on the right.
- Your browser does not support the <audio> element.
- America’s navy, which funded the work, hoped the Perceptron would be “the embryo of an electronic computer that…will be able to walk, talk, see, write, reproduce itself and become conscious of its own existence”. The machine would be able to “recognise people and call out their names” and “instantly translate speech in one language to speech or writing in another”.
- A sure way to fail is to set expectations too high. In 1969 a widely read book by Marvin Minsky and Seymour Papert, two AI researchers, described how, in contrast to the grand promises, the Perceptron was constitutionally incapable of performing certain elementary tasks. Research into neural networks languished for a decade.
- The history of AI is of periodic bouts of overexcitement interspersed with “AI winters”, in which limits become apparent, enthusiasm drains away and funding is slashed. In 1973 the British government abandoned nearly all AI research, citing the field’s failure to meet its “grandiose objectives”. After Minsky and Papert’s book, the focus of AI research turned to “symbolic” approaches that focused on formal logic and rigid, top-down reasoning. Once again, early progress caused much excitement before the unmet promises piled high enough to cause another backlash in the 1980s. Funding cuts closed research projects. American officials dismissed efforts at AI as little more than “clever programming”.
- The current bout of enthusiasm has been the biggest yet. Some researchers therefore worry that, as the limits of modern AI become apparent, a correspondingly big bust is coming. In 2018 Filip Piekniewski, an AI researcher at Accel Robotics, a shop-automation startup, likened the excitement around deep learning to a stockmarket just before the bubble bursts. It is impossible to tell precisely when a crash will happen, wrote Dr Piekniewski, but it is “almost certain that it will at some point.”
- Perhaps. But disillusionment, if it comes, could arise not just from the growing realisation of modern AI’s technical limitations, but from how they interact with its strengths. Although this report has focused on areas in which AI falls short, there are good reasons for the excitement surrounding it. The ability to spot patterns in tranches of data, sometimes with superhuman accuracy, has a thousand uses. Governments and businesses are rushing to adopt it, in ways that will directly affect ordinary people.
- In authoritarian countries those people will have little choice about what is done to them by AI. China already offers a blueprint for AI-enhanced control, most clearly in the police state it has built, with the help of facial recognition, in Xinjiang. In democracies, though, ordinary people—or their representatives—will have some say. They are unlikely to be uncritically welcoming.
- On February 5th, for instance, a Dutch court ruled that syRI, an AI-powered system designed to detect tax and social-benefit fraud, was unlawful. The system was designed to inhale reams of data from various government departments and spot odd-looking patterns within them that might reveal mischief. The court held that the Dutch government had failed to balance its obligation to crack down on fraud with its obligation to protect the privacy of its citizens. The campaigners who had brought the case pointed out that the Netherlands is hardly the only place using AI in such ways. Britain and Australia are running similar systems. Researchers hoping to apply AI to covid-19 might reflect on a poll which suggested that around half of Americans would refuse to install a location-tracking contact-tracing app on their phones.
- As people become familiar with AI’s peculiar mix of power and fragility they may be reluctant to trust it with important decisions. A study in 2019 from the New York University School of Law and the AI Now Institute examined predictive-policing algorithms. These tell police forces where best to deploy their officers, based on trends from historical crime data. The researchers found 13 jurisdictions, including Chicago and New Orleans, whose police have a history of racist behaviour, were investigating the technology. Training algorithms on racially biased data, say the researchers, risks reinforcing those same biases, this time backed by the potent techno-mystique of “artificial intelligence”. Even in China, there is disquiet: one recent poll found 74% of respondents unhappy about the growing use of facial-recognition for identification.
- AI researchers are starting to grapple with these sorts of problems. Two years ago, Google published a set of “AI principles”, saying systems should be “socially beneficial”, “avoid creating or reinforcing unfair bias”, and “be built and tested for safety”. Microsoft and Facebook, have made similar promises. The field of “AI safety”, which wrestles with these questions, is nascent but growing. AI’s incubation among the internet giants means investigations into its broader effects have lagged behind questions about the bottom line. “So much of the development in machine learning has been around ads and social media,” says one executive. “So the focus has been…on accuracy and less on ethics or consequences.”
- Facebook employs over 15,000 human content moderators
- Indeed, social media is an example of an anti-AI backlash that is already under way. Dismayed by the conspiracy theories and misinformation promoted by algorithms that prioritise “engagement” over all else, countries are passing laws to force firms to blunt their worst impacts. AI is creating jobs, not destroying them: Facebook employs over 15,000 human content moderators to police its algorithms, twice the number it had in 2017.
- Brad Smith, the president of Microsoft, has said that instead of asking what AI can do, humans need to think about what it should do. The technological limits of naive, fallible AI, in other words, will lead humans to impose additional political and social limits upon it. Clever algorithms will have to fit into a world that is full of humans, and, in theory at least, run by them. Artificial intelligence is both powerful and limited. As that realisation spreads, some of the dreams of high summer will fade in the autumnal chill. ■
- 
- This article appeared in the Technology Quarterly section of the print edition under the headline "Autumn is coming"
- Discover stories from this section and more in the list of contents
- Published since September 1843 to take part in “a severe contest between intelligence, which presses forward, and an unworthy, timid ignorance obstructing our progress.”
- Copyright © The Economist Newspaper Limited 2023. All rights reserved.

URL: https://diginomica.com/un-report-our-algorithmic-world-creating-social-welfare-dystopia
- 
- Millions of the world’s poorest citizens depend on social welfare programs to survive. Governments are increasingly turning to IT for solutions. Not everybody thinks this is a good idea.
- Philip Alston, the United Nations Special Rapporteur on human rights and extreme poverty, presented a new report before the UN General Assembly on Friday (Oct. 19) that claims the combination of new digital technologies and an amoral Big Tech industry are dramatically changing the interactions between governments and the world’s poorest and most vulnerable. And not for the better.
- In what the scathing report calls the rise of the “digital welfare state,” billions of dollars of public money are now being invested in automated systems powered by artificial intelligence (AI), predictive algorithms, risk modeling and biometrics that are radically changing the nature of social protection. Alston writes:
- The digitization of welfare systems has very often been used to promote deep reductions in the overall welfare budget, a narrowing of the beneficiary pool, the elimination of some services, the introduction of demanding and intrusive forms of conditionality, the pursuit of behavioural modification goals, the imposition of stronger sanctions regimes, and a complete reversal of the traditional notion that the state should be accountable to the individual.
- And:
- In these states, systems of social protection and assistance are increasingly driven by digital data and technologies that are used to automate, predict, identify, surveil, detect, target and punish… As humankind moves, perhaps inexorably, towards the digital welfare future it needs to alter course significantly and rapidly to avoid stumbling zombie-like into a digital welfare dystopia.
- Alston argues that the digital welfare state is usually marketed as an altruistic and noble enterprise designed to ensure that all citizens benefit from new technologies, more efficient government, and enjoy higher levels of well-being. For example, the United Kingdom’s digital strategy proclaims that it will “transform the relationship between citizens and the state”, thus “putting more power in the hands of citizens and being more responsive to their needs.” But, the UN report notes the UK is:
- …an example of a wealthy country in which, even in 2019, 11.9 million people (22% of the population) do not have the ‘essential digital skills’ needed for day-to-day life. An additional 19% cannot perform fundamental tasks such as turning on a device or opening an app. In addition, 4.1 million adults (8%) are offline because of fears that the internet is an insecure environment, and proportionately almost half of those are from a low income household and almost half are under sixty years of age.
- Alston is particularly alarmed by governments that justify the introduction of expensive and complex biometric digital identity card systems on the grounds that they improve welfare services and reduce fraud. Too often, he writes, the real motives are to slash welfare spending, set up intrusive government surveillance systems and generate profits for private companies. He writes :
- The process is commonly referred to as ‘digital transformation’ by governments and the tech consultancies that advise them, but this somewhat neutral term should not be permitted to conceal the revolutionary, politically-driven, character of many such innovations.
- A case in point is a new digital tool called System Risk Indication (SyRI) that is being used by the government of the Netherlands for the stated purpose to detecting welfare fraud by collecting massive amounts of data about its citizens and algorithmically creating “risk models” to determine who is most likely to commit benefit fraud.
- The result of such risk profiling, Alson writes, is that it assumes the individual will commit fraud at some point despite never actually doing so because their risk profile suggests they will. That guilt associated with someone who hasn’t committed a crime results in closer monitoring of their behavior, completely bypassing any due process that would be granted to anyone else receiving assistance. Writes Alston:
- That level of scrutiny may turn into an obstacle for those most in need to even get the help that is available to them. When they know that they will be questioned every step of the way, a presumption of criminality will be applied to their every action, they may just choose not to participate in the system at all. On paper, that means that SyRI worked—it prevented fraud from occurring. In practice, it means that a vulnerable member of society in need of help has been pushed away and likely won’t get access to the services they need.
- Alston reserves his most scorching criticism for Big Tech companies who, he says, are putting profits above human rights. He writes:
- A handful of powerful executives are replacing governments and legislators in determining the directions in which societies will move and the values and assumptions which will drive those developments…
- Most governments have stopped short of requiring Big Tech companies to abide by human rights standards, and because the companies themselves have steadfastly resisted any such efforts, the companies often operate in a virtually human rights free-zone.
- The UN report was drawn from Alston’s country visits to the UK, US and elsewhere, as well as 60 submissions from 34 countries.
- Alston’s findings are not without merit. Governments across the world are spending enormous amounts of money on automating their social welfare delivery systems and, in the process, turning real people into algorithms and replacing human judgment with machines that have no compassion, empathy or understanding of circumstances that fall outside the mean.
- A similar revolution is now taking place in criminal justice where AI is often the determining factor in who gets bail and who stays in jail. But, as we are learning every day, algorithms based on biased and incomplete data produce incomplete and biased results. In too many cases, we don’t yet know what we don’t know well enough to justify eliminating human oversight.
- There is also the risk of automation bias, the conclusion that we spent millions of dollars on this stuff so it must be right even in cases where it is clearly wrong.
- And, of course, Big Tech marketers make big promises about how new technologies will speed up processes, increase efficiency and transparency, reduce waste, save money for taxpayers, take human fallibility and prejudice out of the equation, and ensure that limited resources reach those who need it most. Sometimes, perhaps, that even happens. Most real-life implementations are more complicated.
- Is that likely to change? Not really or, at least, not voluntarily. High tech executives aren’t going to wake up tomorrow and say “You know automation really is killing jobs for millions and making life harder for people who are poor because they don’t have jobs.”
- But, a reckoning is coming. One positive outcome of the recent high tech fall from grace is that the regulators and the public at large are more skeptical about products and the promises of executives who promote them. That is a good thing.
- Image credit - Problem Solving - Hand Stopping Domino Effect © Romolo Tavani - Fotolia.com
- diginomica and the diginomica logo are trademarks of diginomica Limited.
- © Diginomica Limited and its licensors 2013-2023
- Developed by BRAINSUM.

URL: https://algorithmwatch.org/en/story/syri-netherlands-algorithm/
- 
- This story is part of AlgorithmWatch's upcoming report Automating Society 2020, to be published later this year. Subscribe to our newsletter to be alerted when the report is out.
- In its fight against fraud, the Dutch government has been cross-referencing personal data from citizens in various databases since 2014. This system, called SyRI (for “system risk indication”), wants to find “unlikely citizen profiles” that warrant further investigation. Despite major objections from the Dutch Data Protection Authority and the Council of State, SyRI has been implemented without any transparency for citizens about what happens with their data.
- The idea is this: if some government agency suspects fraud with benefits, allowances or taxes in a specific neighborhood, it can make use of SyRI. Municipalities, the UWV, the social security bank, inspectors of the Ministry of Social Affairs and Employment and the tax authority have access to the system. SyRI decides which citizens in the neighborhood need to be investigated further.
- SyRI has not been a success for the government. In its first five years, five municipalities requested to analyze a neighborhood. Only two of these projects were actually executed, the other three were canceled. According to research from the Dutch newspaper De Volkskrant in 2019, none of these algorithmic investigations have been able to detect new cases of fraud.
- There is a detailed procedure for government agencies that want to use SyRI. Two agencies should cooperate and ask the Ministry of Social Affairs and Employment (SZW in its Dutch acronym) to conduct an analysis. Before a SyRI project starts, SZW publishes an advisory in the online version of the official gazette. “The municipality has no obligation to inform citizens of a neighborhood that they are being analyzed”, says Ronald Huissen from the Platform Bescherming Burgerrechten (Platform for Civil Rights Protection). “And if they are informed, it is by a city bulletin that is not necessarily read by them, and in very vague terms, without the details of what data SyRI uses and how.”
- The agency that asked for the analysis cannot just penalize the citizens that are flagged for an unlikely combination of data: it has to investigate, for every flagged citizen, whether an actual case of fraud took place. Moreover, the flagged citizens are first examined at the Ministry of Social Affairs and Employment for false positives. Data on citizens that are deemed false positives is not handed over to the agency that asked for the analysis.
- But even with these checks in place, the lack of transparency is still a big issue. Residents of whole neighborhoods were put under a magnifying glass without them even knowing which privacy-sensitive data SyRI had about them. Each ‘risk indication’ is logged into a register that citizens can look into if they ask. But citizens are not automatically warned if they are flagged for fraud risk by SyRI, and they cannot access the reasons why they have been flagged.
- In the beginning of 2018, the Platform Bescherming Burgerrechten, together with a couple of other Dutch civil rights organizations, filed a case against the Dutch state to stop the use of SyRI. At the same time, they wanted to spark a public debate about SyRI with their media campaign Bij Voorbaat Verdacht (Suspected from the outset).
- According to the official resolution that is the legal basis for SyRI, the system can cross-reference data about work, fines, penalties, taxes, properties, housing, education, retirement, debts, benefits, allowances, subsidies, permits and exemptions, and more. These are described so broadly that in 2014 the Council of State concluded in its negative opinion on SyRI that there is “hardly any personal data that cannot be processed”.
- SyRI pseudonymizes the data sources it uses with a ‘black box’ method. That is, for each data source that is linked, all citizen names are replaced by a unique identifier for each individual. The identifier makes it possible to link data about the citizen from these various data sources. After the analysis, the result is a list of identifiers that represent possibly fraudulent beneficiaries. These identifiers are then translated back to their real names.
- In the case of Platform Bescherming Burgerrechten versus the Dutch State, the latter gave some examples of “discrepancies” that could lead to a risk indication. One of these discrepancies is a low usage of running water. This could be a sign that someone who receives benefits is living together with someone else at another address and thus does not have the right to the higher benefit for singles. However, there are many other possible causes for low water usage, such as using rainwater, a frugal life, or even a broken water meter.
- It is still unclear what is happening in this ‘black box’, and the Dutch government blocked all attempts from concerned parties to shed light on this. In 2017 the Ministry of Social Affairs decided that the risk models it used should be kept secret. In 2018, the political party D66 wrote a motion (but did not file it) to publish SyRI’s algorithms or to conduct a technical audit if publishing the algorithms is not possible.
- Tamara van Ark, State Secretary for Social Affairs and Employment, strongly advised against filing the motion (so that it was never put to a vote), and she warned that potential offenders could adapt their behavior if the state disclosed SyRI’s risk models. But many of the factors in the risk models are already known or expected, or have already been used before SyRI to detect fraud, such as low water usage. It is hard to imagine that someone who commits fraud to get a higher benefit will leave the faucet open to increase their water usage.
- There’s another problem with SyRI: according to freedom of information requests by the Platform Bescherming Burgerrechten, it turns out SyRI has been primarily used in low-income neighborhoods. This exacerbates biases and discrimination: if the government only uses SyRI’s risk analysis in neighborhoods that are already deemed high-risk, it is no wonder that it will find more high-risk citizens there.
- Philip Alston, United Nations Special Rapporteur on extreme poverty and human rights, expressed his concerns about SyRI in a letter to the Dutch court on 26 September 2019 : “Whole neighborhoods are deemed suspect and are made subject to special scrutiny, which is the digital equivalent of fraud inspectors knocking on every door in a certain area and looking at every person’s records in an attempt to identify cases of fraud, while no such scrutiny is applied to those living in better off areas.”
- Mr Alston does not question that welfare fraud exists and that it should be punished, but he warns that SyRI’s focus seems to be wrong: “If the focus on fraud seen to be committed by the poor is highly disproportionate to equivalent efforts targeted at other income groups, there is an element of victimization and discrimination that should not be sanctioned by the law.”
- Maranke Wieringa, a PhD candidate at Utrecht University doing research on algorithmic accountability in Dutch municipalities, adds another problem: “One goal of municipalities using SyRI for specific neighborhoods is to improve their living standards. However, SyRI is not designed for that purpose. If you take a national instrument that is designed for fraud detection and then apply it with a social purpose of improving living standards and social cohesion in a neighborhood, you can question whether you should depart from the same repressive position for both goals.”
- On 29 November 2019, SyRI won the Big Brother Award of the Dutch digital rights organization Bits of Freedom. This prize is awarded to the biggest privacy intrusion of the year. When director general Carsten Herstel accepted the award in the name of the Ministry of Social Affairs and Employment, he told the audience “I find it logical that the government gets alerted when someone gets a rental allowance and owns the house at the same time.”
- According to Mr Huissen from the Platform Bescherming Burgerrechten, the government does not need this kind of mass surveillance to prevent fraud: “The government already has information about who owns which house, so it could check this before granting the person a rental allowance. For all big fraud scandals in social security we have seen in the past decades it became clear afterwards that they could have been prevented with simple checks beforehand. That happens far too little. It is tempting to look for solutions in secret algorithms analyzing big data sets, but often the solution is far simpler.”
- Ms Wieringa agrees that this is a better way. “SyRI has been introduced from the point of view of a repressive welfare state: it does not trust the citizens. But that is just one stance of many possible ones. For instance, the government could check, perhaps even while using fewer data sources, who has the right to an allowance.”
- On 5 February 2020, the Dutch court of The Hague ordered the immediate halt of SyRI because it violates article 8 of the European Convention on Human Rights (ECHR), which protects the right to respect for private and family life. Article 8 requires that any legislation has a “fair balance” between social interests and any violation of the private life of citizens.
- SyRI’s goal, or “social interest”, is to prevent and fight fraud. The Dutch state claimed that the SyRI legislation offered sufficient guarantees to do this while protecting the privacy of citizens, but the court disagreed. The legislation is insufficiently transparent and verifiable, and there are not enough safeguards against privacy intrusions, judges wrote.
- The Dutch government can appeal against the decision until 5 May 2020. In a reaction on its website, the ministry said it will investigate the decision thoroughly.
- According to Ms Wieringa, the court’s decision makes it clear that the biggest problem of SyRI is not that it wants to battle fraud (this is a legitimate aim, the court says), but the way it does it: “The system is deemed too opaque by the judges. If the government wants to ‘fix’ this problem, it will have to add more transparency to SyRI. A ‘SyRI 2.0’ will likely be SyRI but less opaque. Previous experiences point to that course of action. The “Waterproof” system, a forerunner of SyRI, was deemed illegal in 2007 on privacy grounds. Later, the government simply passed a law to make circumvent the problem, thus creating SyRI. Another ‘lazy fix’, this time geared towards increasing transparency, would be a logical step for SyRI.”
- On the other hand, two public organizations in the Netherlands, the UWV and the tax authority, have reacted to the court’s decision by reassessing their own algorithmic systems for fraud detection. “This is a sign that the court’s decision is urging everyone to find a new way of dealing with algorithms in the public sector”, Ms Wieringa maintains.
- Tijmen Wisman, chairman of the Platform Bescherming Burgerrechten, hopes that the government will do more. “Just adapting SyRI to be more transparent will still result in information asymmetry. Citizens do not want to give information anymore to their government if the latter can use this information in all possible ways against them.”
- According to Mr Wisman, public organizations need a change in their data management: “Data must no longer be allowed to roam freely, but must reside in an authentic data source. Each organization should keep logs for every time that their data is consulted. Citizens should be able to easily access these logs. This way it becomes clear to citizens what their data is used for, and they can challenge this use. This requirement of transparency also follows from the court’s ruling in the SyRI case, as well as the GDPR.”
- In the Netherlands, welfare fraud is estimated at 150 million euros a year. Municipalities, the employee insurance agency (UWV) and the social security bank together have detected fraudulent claims for more than 744 million euros in the period from 2013 to 2018. This compares to an estimated 22 billion euros lost to tax fraud, each year.
- Did you like this story?
- Every two weeks, our newsletter Automated Society delves into the unreported ways automated systems affect society and the world around you. Subscribe now to receive the next issue in your inbox!

URL: https://algorithmwatch.org/en/high-risk-citizens/
- 
- By Ilja Braun
- 
- Capelle aan den Ijssel is a Dutch town on the edge of Rotterdam. If one took the town administration's word for it, you’d better stay away from the municipality of 66,000 residents. Allegedly, the place suffers from an "erosion of values and norms" in "multi-problem families" who are "mainly occupied with surviving on a day-to-day basis". A disproportionally large share of council housing and fading social cohesion, paired with work migrants who only stay temporarily and often do not report their residence, lead to moonlighting in illegal garage workshops, illicit trade of baby food, debt problems, truants and child poverty. Sodom and Gomorrah in the Tulip State? Surely, many people outside the Netherlands have quite a different picture of the country.
- For a long time, the Netherlands were famous for their welfare state. When things were going well for the small country, it was possible to live carefree off social and unemployment benefits. If you didn't want to work, you weren't really forced to. Nowadays, the Dutch government's all-round care system has been scaled back and replaced by appeals to self-responsibility. What's more, the Dutch authorities are using far-reaching technical facilities to identify citizens who may be cheating the state by cashing in on wrongfully collected housing, unemployment or other benefits, committing tax fraud, moonlighting or illicit trafficking.
- The magic word is SyRI: System Risk Indication, a big data analysis system. Citizen data stored by various authorities and state institutions are combined and analysed. I.e., tax data can be compared with information on who receives state aid and support. Or information on the place of residence with data from the naturalisation authority. Based on certain risk indicators, the software allegedly detects an "increased risk of irregularities", i.e. whether someone is acting against the law. Thus, SyRI would set off an alarm if someone received housing benefits but was not registered at the address in question. An employee from the Ministry of Social Affairs would then take a closer look at the data. If they believed that something could be amiss, a "risk report" would be made, which is passed on to the relevant authorities. The agency responsible for housing benefits would send an inspector to the "risk address" in question. If the suspicion is confirmed, the state aid can be reclaimed.
- For Tijmen Wisman, chairman of the civil rights NGO Platform Bescherming Burgerrechten (Platform for Civil Rights Protection), this clearly goes too far. The data protection principle of purpose limitation is turned on its head because it is not defined for which purposes the data was collected. And to make things worse, individual citizens are not even informed if the software classifies them as a "high-risk citizen". "You end up in a register and get a score for a certain risk," says Wisman. "On the basis of the data in this register, decisions are made that affect you personally as an individual. But you don't even know that you're in the register or what consequences that can have for you personally."
- Does this mean that the Netherlands are a surveillance state? It’s obvious that many things are done differently there compared to other countries. So-called "intervention teams" have existed in the Netherlands for around 15 years: police, tax authorities, the public prosecutor's office, immigration authorities, the service providers for employee insurance, social security and local authorities have been working together in interdisciplinary teams to intervene in socially deprived areas. This is not necessarily meant as a repressive approach.  It is no secret that crime and social problems are often connected. Different authorities working together in this area can also be of advantage – social issues are not just left to the police.
- But what if those who work together in teams now also merge the citizen’s data to form profiles – is that a breach of privacy, or rather the extension of a cooperative approach?
- It’s impossible to say because those who apply for the use of SyRI for their community probably do not know exactly how SyRI works, where its application makes sense or how significant the data and the models really are. At least, this information is not mentioned in the requests they filed in order to be allowed to use the system. An implementing provision to the Dutch Labour and Income Implementation Structure Act stipulates that 17 categories of personal data may be processed within the framework of a SyRI project. (Chapter 5a SUWI). The list ranges from employment data, information on fines and tax data to data of state support services, information regarding the place of residence, data from the naturalisation authority, data concerning reintegration at work after a long illness and personal debts up to information on the health insurance status. According to civil rights activist Tijmen Wisman, there had been no intensive discussions on the new regulation; on the contrary, the regulation was "simply waved through" by Parliament in 2014.
- 
- Video of NGO "Bij Voorbaat Verdacht"
- But what criteria does the software use to assess whether there is an increased risk of welfare abuse? An organization called Bij Voorbaat Verdacht ("Suspected from the outset") tried to prise this out with a freedom of information request. The Ministry's answer:
- "The risk model is a collection of one or more sets of related risk indicators that may be combined to assess the risk that certain natural or legal persons are not acting in accordance with applicable law. If one were to disclose what data and connections the Inspectie SZW is looking for, (potential) lawbreakers would know exactly on which stored data they would have to concentrate".
- And the results? Have the SyRI projects up to now really made a significant contribution to combating welfare abuse, tax fraud or violations of labour law?  In a reply to a request by AlgorithmWatch, the Dutch Ministry of Social Affairs and Employment says that results are only available for a single project so far, namely for the operation in Capelle aan den Ijssel described above. There were 113 risk reports for this project, according to the ministry, "which triggered various measures. The identified violations of the law have resulted in the discontinuation or recovery of state benefits and allowances in a total volume of 496,000 euros (including subsequent savings)".
- [su_box title="When and How is SyRI Used?"]SyRI is not used throughout the whole of the Netherlands, but only at the specific request of a so-called co-operation association. The request for such a cooperation usually comes from a municipality whose administration would like to work together with other administrations, such as the social insurance carrier for employees or the tax authorities. The goal is defined by law: To combat or prevent the unlawful use or recourse to public money or social security institutions or other income-related state benefits. In addition, action is supposed to be taken against tax and duty fraud and labour law violations, such as moonlighting, but also against illegal exploitation of labour.
- According to official data, SyRI has been applied four times so far: in addition to Capelle aan den Ijssel also in Eindhoven, and projects have been running in Haarlem and Rotterdam since 2018. In an industrial area in Eindhoven, according to the applicants, "neglect is to be noted. So far, the municipality of Eindhoven has not sufficiently succeeded in gaining an overview of the developments and steering them in a positive direction”. With a "risk-oriented approach", the plan was to " break through the intimidating climate and contribute to improving the quality of life in this district and the industrial area". This should be done by using the data "required for the 'Neighbourhood-centred approach' risk model built into SyRI". "It was determined that the use of the respective data is necessary to achieve the objectives of the project." In Haarlem, data is compared coming from the municipality, the tax office, the employee insurance provider, the social security bank responsible for child benefit and pensions, the immigration authority and the Inspectie SZW, which is responsible for the prosecution of labour law violations. "This may reveal unlikely data combinations that give rise to further investigations," the application states.
- However, trying to find out exactly how the SyRI software is supposed to contribute to solving these problems, the only answer is that thanks to a "risk analysis" supported by SyRI, "a selection of risk addresses" is going to be provided. The inspectors of the agencies and authorities do not seem to know where to start. That’s why it comes in handy if the software suggests names and addresses of people for whom there is any irregularity in the data.[/su_box]
- To avoid unjustifiably suspecting people, the data is examined for so-called "false positive" signals, as Ministry spokesman Ivar Noordenbos explains to AlgorithmWatch: "Since the data is linked automatically in accordance to a pre-defined risk model, it can happen that a risk is wrongly indicated because of imperfections in the model. To give a fictitious example: Assuming that the model would not be familiar with the concept of retirement homes, then recipients of the basic state pension could be suspected of living in a common household without having reported this. After all, they all have the same address. A data analyst at Inspectie SZW therefore first examines the software's signals for such obviously false risk signals so that no risk reports are generated. In this example, the data analyst would notice that the situation of the recipients of a basic state pension is different than presumed. The corresponding data would be sorted out as 'false positives' so that they would not be used in any subsequent checks. Such false signals are also used to adjust the risk model as far as possible so that the corresponding error does not occur again”. According to the Ministry, however, 62 of the 113 cases in Capelle aan den Ijssel have not been found to be in violation of any law.
- A group of several civil rights initiatives recently filed a landmark case against the use of the software. It is in the first instance; a trial has not yet taken place. The civil rights activists claim that SyRI places citizens under general suspicion and violates the European Convention on Human Rights. The legal powers for invading privacy had been structured so unrestrictedly by the legislature, say the activists, that they could no longer be brought into line with Article 8 of the Convention on Human Rights, which was intended precisely to guarantee the protection of privacy.
- The Ministry of Social Affairs and Employment declined to comment on the current proceedings. If the procedure were successful, the part of the law that concerns the use of SyRI would have to be declared unlawful. It would then have to be amended or even be completely abolished. The Dutch government would have to put its risk profiling on hold. It is still unclear, however, whether the civil rights initiatives can sue SyRI at all or whether a person directly affected would have to prove that their fundamental rights had been violated. As long as it is not known just how SyRI works, proving exactly that should be extremely difficult.
- Translation by Louisa Well
- ---
This story was made possible with the support of Bertelsmann Stiftung
- Did you like this story?
- Every two weeks, our newsletter Automated Society delves into the unreported ways automated systems affect society and the world around you. Subscribe now to receive the next issue in your inbox!

- Netherlands childcare benefits fraud automation
- Rotterdam welfare fraud risk algorithm
- Page infoType: SystemPublished: April 2023
